{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AI_3D_GAN_for_Mass_Generating.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generating 3D mass using GAN\n",
        "This notebook is used to generate voxel 3d models trained on tower dataset. These voxel models are post processed with blender for further enhancement of the final shape mass."
      ],
      "metadata": {
        "id": "4A7yj5bDX13J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z13SVcj8p1P"
      },
      "source": [
        "##Importing usefull libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ue1BlbIxDsH"
      },
      "source": [
        "!pip install voxelfuse\n",
        "!pip install tensorflow-gan\n",
        "\n",
        "import os\n",
        "import scipy\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Conv3D, Conv3DTranspose, Activation, BatchNormalization, LeakyReLU, Flatten, Reshape, Dense, Dropout, UpSampling3D\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras import backend \n",
        "from tensorflow.keras.constraints import Constraint\n",
        "import tensorflow_gan as tfgan\n",
        "from google.colab import output\n",
        "from voxelfuse.voxel_model import VoxelModel\n",
        "from voxelfuse.mesh import Mesh\n",
        "from voxelfuse.primitives import generateMaterials\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "#Implementation based on original paper https://arxiv.org/abs/1707.09557\n",
        "\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA9QHZIbGo-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b36140-746e-45e0-aecb-efa43c625697"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdWpJZhRxNVC"
      },
      "source": [
        "## Building Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p3oPTztxLqj"
      },
      "source": [
        "def make_Generator(kernel_size=5, strides=2, latent_dim=32):\n",
        "     model = Sequential()\n",
        "     model.add(Input(shape=(latent_dim,)))\n",
        "\n",
        "     model.add(Dense(units=2048, input_shape=(latent_dim,), use_bias=False))\n",
        "     model.add(BatchNormalization(momentum=0.9))\n",
        "     model.add(Reshape((2, 2, 2, 256)))\n",
        "     model.add(Activation('relu'))\n",
        "     #model.add(Reshape((2, 2, 2, 256)))\n",
        "     model.add(Dropout(0.4))\n",
        "\n",
        "     model.add(UpSampling3D())\n",
        "     model.add(Conv3DTranspose(filters=512, kernel_size=kernel_size, padding='same', use_bias=False))\n",
        "     model.add(BatchNormalization(momentum=0.9))\n",
        "     model.add(Activation('relu'))\n",
        "\n",
        "     model.add(UpSampling3D())\n",
        "     model.add(Conv3DTranspose(filters=256, kernel_size=kernel_size, padding='same', use_bias=False))\n",
        "     model.add(BatchNormalization(momentum=0.9))\n",
        "     model.add(Activation('relu'))\n",
        "\n",
        "     model.add(UpSampling3D())\n",
        "     model.add(Conv3DTranspose(filters=128, kernel_size=kernel_size, padding='same', use_bias=False))\n",
        "     model.add(BatchNormalization(momentum=0.9))\n",
        "     model.add(Activation('relu'))\n",
        "\n",
        "     model.add(UpSampling3D())\n",
        "     model.add(Conv3DTranspose(filters=64, kernel_size=kernel_size, padding='same', use_bias=False))\n",
        "     model.add(BatchNormalization(momentum=0.9))\n",
        "     model.add(Activation('relu'))\n",
        "\n",
        "     model.add(UpSampling3D())\n",
        "     model.add(Conv3DTranspose(filters=1, kernel_size=kernel_size, padding='same', use_bias=False))\n",
        "     model.add(Activation('sigmoid'))\n",
        "\n",
        "     return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDCCLjoaxW5B"
      },
      "source": [
        "## Building Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwYhui5IxY6u"
      },
      "source": [
        "def make_Discriminator(kernel_size=5, strides=2, im_dim=64):\n",
        "    #const = ClipConstraint(0.01)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(im_dim, im_dim, im_dim, 1)))\n",
        "    \n",
        "    model.add(Conv3D(filters=64, kernel_size=kernel_size, strides=strides, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.35))\n",
        "\n",
        "    model.add(Conv3D(filters=128,kernel_size=kernel_size, strides=strides, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.35))\n",
        "\n",
        "    model.add(Conv3D(filters=256,kernel_size=kernel_size, strides=strides, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.35))\n",
        "\n",
        "    model.add(Conv3D(filters=512,kernel_size=kernel_size, strides=strides, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.35))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1))\n",
        "    #model.add(Activation('sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxgY0h4eZgpy"
      },
      "source": [
        "## Preparing data for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xif_Jna-Zmo-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af0b3e74-4568-4ae1-bd1d-aa27141a62bc"
      },
      "source": [
        "training_data = []\n",
        "\n",
        "for i, filename in enumerate(os.listdir('/content/gdrive/MyDrive/AI_Artathon/tower_dataset_npz')):\n",
        "  print(i, filename)\n",
        "  voxel_model = np.load(f'/content/gdrive/MyDrive/AI_Artathon/tower_dataset_npz/{filename}')['arr_0']\n",
        "  new_array = zoom(voxel_model, (0.5, 0.5, 0.5)) # change size of dataset to fit the model\n",
        "  training_data.append(new_array)\n",
        "\n",
        "training_data = np.array(training_data).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 166bd86229867093031b562a9def0c1_surface.npz\n",
            "1 1683c5bc839969f438cd8eb23853c009_surface.npz\n",
            "2 15cc3d9020384e8d6e09a8e31c7575c5_solid.npz\n",
            "3 166bd86229867093031b562a9def0c1_solid.npz\n",
            "4 10312d9d07db5d9a4159aeb47682f2cb_surface.npz\n",
            "5 1683c5bc839969f438cd8eb23853c009_solid.npz\n",
            "6 10be0ff0e5eb12584edd82d34d189d0d_surface.npz\n",
            "7 10be0ff0e5eb12584edd82d34d189d0d_solid.npz\n",
            "8 10312d9d07db5d9a4159aeb47682f2cb_solid.npz\n",
            "9 149f13db5c358ecfde1c31612629d3dc_solid.npz\n",
            "10 149f13db5c358ecfde1c31612629d3dc_surface.npz\n",
            "11 15cc3d9020384e8d6e09a8e31c7575c5_surface.npz\n",
            "12 224934a28705403238cd8eb23853c009_surface.npz\n",
            "13 1b781ad3315b36510f233dcbf8432d5_surface.npz\n",
            "14 17f5482dd8b9dbddc850800ac472a578_solid.npz\n",
            "15 19e66890128da2d629647c2672d3167a_surface.npz\n",
            "16 17e229dbb42786e6b164f257e05e5435_solid.npz\n",
            "17 1fc436fd53db26d667ff3be08608de4d_solid.npz\n",
            "18 283c8fd32d9b9a95c2c6f10e262c6c44_solid.npz\n",
            "19 1ebb17d0790e384844e2f9503fe415b5_surface.npz\n",
            "20 1ec4566d10cc58d7a2f3410d5eb66b3d_surface.npz\n",
            "21 26e099d5d7040b03b96aa5dc23c036c_solid.npz\n",
            "22 224934a28705403238cd8eb23853c009_solid.npz\n",
            "23 1fc436fd53db26d667ff3be08608de4d_surface.npz\n",
            "24 26e099d5d7040b03b96aa5dc23c036c_surface.npz\n",
            "25 17f5482dd8b9dbddc850800ac472a578_surface.npz\n",
            "26 19e66890128da2d629647c2672d3167a_solid.npz\n",
            "27 17e229dbb42786e6b164f257e05e5435_surface.npz\n",
            "28 1ec4566d10cc58d7a2f3410d5eb66b3d_solid.npz\n",
            "29 1f1f62c52fe4d27946fb3db517d0dfc0_solid.npz\n",
            "30 1ebb17d0790e384844e2f9503fe415b5_solid.npz\n",
            "31 1b781ad3315b36510f233dcbf8432d5_solid.npz\n",
            "32 1f1f62c52fe4d27946fb3db517d0dfc0_surface.npz\n",
            "33 2b6257595edd2e62159e1f7290c9a432_surface.npz\n",
            "34 35824a59a19511c538cd8eb23853c009_surface.npz\n",
            "35 2ecb6add2316f940ffad7221e3cdb3ef_surface.npz\n",
            "36 2af6c073f8b0ece92030f08aa0abc03e_surface.npz\n",
            "37 3668753b904e53c4b96aa5dc23c036c_surface.npz\n",
            "38 34e391f9e9a72f97c5e86946f2168706_solid.npz\n",
            "39 3735b272459f1b15db439ec9067ff33f_surface.npz\n",
            "40 2b6257595edd2e62159e1f7290c9a432_solid.npz\n",
            "41 376cc07140ada8e0b21a5c4b7083cf07_surface.npz\n",
            "42 37d62783bdefc54a1ffedce3943a1ba2_surface.npz\n",
            "43 283c8fd32d9b9a95c2c6f10e262c6c44_surface.npz\n",
            "44 2af6c073f8b0ece92030f08aa0abc03e_solid.npz\n",
            "45 376cc07140ada8e0b21a5c4b7083cf07_solid.npz\n",
            "46 28685f1adb96fb1eb6eefc259009d337_solid.npz\n",
            "47 3668753b904e53c4b96aa5dc23c036c_solid.npz\n",
            "48 28685f1adb96fb1eb6eefc259009d337_surface.npz\n",
            "49 3735b272459f1b15db439ec9067ff33f_solid.npz\n",
            "50 35824a59a19511c538cd8eb23853c009_solid.npz\n",
            "51 2ecb6add2316f940ffad7221e3cdb3ef_solid.npz\n",
            "52 34e391f9e9a72f97c5e86946f2168706_surface.npz\n",
            "53 37d62783bdefc54a1ffedce3943a1ba2_solid.npz\n",
            "54 44e8212fe3d44b1ec8b200f28f6c5853_solid.npz\n",
            "55 3a1e36329722686c2c406efbeb1811b0_surface.npz\n",
            "56 40ec49e3e3147508ca9081e720bd7dff_solid.npz\n",
            "57 47ce1a5380447249c26ade84d8048a3_solid.npz\n",
            "58 48a1e31e57862a63ff679899febcb0d4_solid.npz\n",
            "59 3defb00d43c0ebc853b60a2c38c44521_surface.npz\n",
            "60 4059fb45b27d8b1f148a76a78b10eb5d_solid.npz\n",
            "61 4884377f5433a9324f2ea904df2fe040_solid.npz\n",
            "62 40ec49e3e3147508ca9081e720bd7dff_surface.npz\n",
            "63 46a090f0bd14180bf1a48a667d61ced6_surface.npz\n",
            "64 3f95f49a3c400e9eebde864f704d194b_solid.npz\n",
            "65 4264b506533b1e2e473f10e6caaeca56_solid.npz\n",
            "66 46a090f0bd14180bf1a48a667d61ced6_solid.npz\n",
            "67 3a1e36329722686c2c406efbeb1811b0_solid.npz\n",
            "68 4884377f5433a9324f2ea904df2fe040_surface.npz\n",
            "69 4059fb45b27d8b1f148a76a78b10eb5d_surface.npz\n",
            "70 47ce1a5380447249c26ade84d8048a3_surface.npz\n",
            "71 3f95f49a3c400e9eebde864f704d194b_surface.npz\n",
            "72 3defb00d43c0ebc853b60a2c38c44521_solid.npz\n",
            "73 4264b506533b1e2e473f10e6caaeca56_surface.npz\n",
            "74 44e8212fe3d44b1ec8b200f28f6c5853_surface.npz\n",
            "75 607a0e44587e043a3e974a0808687b8c_surface.npz\n",
            "76 5b0dc77df3bd924c25ed5c2b422915db_solid.npz\n",
            "77 4fc1268f6aa26fd76ec5e0e1e130ecd8_surface.npz\n",
            "78 547e1ef6922840b23899914213d0efde_solid.npz\n",
            "79 5da1ddcc8d5f62577f5d4139a48ddb6f_solid.npz\n",
            "80 607a0e44587e043a3e974a0808687b8c_solid.npz\n",
            "81 48d4e52fb735e6b6599d77531b1d13a9_surface.npz\n",
            "82 48a1e31e57862a63ff679899febcb0d4_surface.npz\n",
            "83 5da1ddcc8d5f62577f5d4139a48ddb6f_surface.npz\n",
            "84 4d1af8461018b714579e0a60e99d2683_solid.npz\n",
            "85 48d4e52fb735e6b6599d77531b1d13a9_solid.npz\n",
            "86 4fc1268f6aa26fd76ec5e0e1e130ecd8_solid.npz\n",
            "87 58695979fe916c43327144edfb578d92_solid.npz\n",
            "88 5bae6e286a9ce96de6bffcc6da770837_surface.npz\n",
            "89 58695979fe916c43327144edfb578d92_surface.npz\n",
            "90 547e1ef6922840b23899914213d0efde_surface.npz\n",
            "91 5bae6e286a9ce96de6bffcc6da770837_solid.npz\n",
            "92 5495ec547c13d8f2de1c31612629d3dc_solid.npz\n",
            "93 4d1af8461018b714579e0a60e99d2683_surface.npz\n",
            "94 614ba760fb825f93ad5067eac75a07f7_solid.npz\n",
            "95 5495ec547c13d8f2de1c31612629d3dc_surface.npz\n",
            "96 5b0dc77df3bd924c25ed5c2b422915db_surface.npz\n",
            "97 6d839fc8ad123620f20d6153247513c5_surface.npz\n",
            "98 6d43d4642c7a474638cd8eb23853c009_surface.npz\n",
            "99 6b78705cd18a7b7767b959cddc3d22e_solid.npz\n",
            "100 63150ddf5088cb6a1b4b48d3a6cc767_solid.npz\n",
            "101 6d839fc8ad123620f20d6153247513c5_solid.npz\n",
            "102 63150ddf5088cb6a1b4b48d3a6cc767_surface.npz\n",
            "103 6372299ee506123b52f2aeb043ecdce8_solid.npz\n",
            "104 6cd9f0f548979d5160ddc468fe733ed1_solid.npz\n",
            "105 6cd9f0f548979d5160ddc468fe733ed1_surface.npz\n",
            "106 64942df95c33b2911dacca859c0fc7c_solid.npz\n",
            "107 6d43d4642c7a474638cd8eb23853c009_solid.npz\n",
            "108 64942df95c33b2911dacca859c0fc7c_surface.npz\n",
            "109 64f50754e6b67305ea3f1ffc49ae6b01_solid.npz\n",
            "110 64ae93807abcaaa38cd8eb23853c009_solid.npz\n",
            "111 614ba760fb825f93ad5067eac75a07f7_surface.npz\n",
            "112 64f50754e6b67305ea3f1ffc49ae6b01_surface.npz\n",
            "113 614ea50301bf9179514f720d40bfbeb2_surface.npz\n",
            "114 6d69bdc0bd266b7af7f1024a921d6d8_solid.npz\n",
            "115 6b78705cd18a7b7767b959cddc3d22e_surface.npz\n",
            "116 64ae93807abcaaa38cd8eb23853c009_surface.npz\n",
            "117 614ea50301bf9179514f720d40bfbeb2_solid.npz\n",
            "118 6372299ee506123b52f2aeb043ecdce8_surface.npz\n",
            "119 61dfd94f040757892606d242dcfd275a_surface.npz\n",
            "120 6d69bdc0bd266b7af7f1024a921d6d8_surface.npz\n",
            "121 61dfd94f040757892606d242dcfd275a_solid.npz\n",
            "122 7696d47199f9055a79ea17c9c8a4feb0_solid.npz\n",
            "123 7c78b826f0f46ef3b6b5161efde62bf9_solid.npz\n",
            "124 75ffcdedd253c62aa86cbf46be15aeca_solid.npz\n",
            "125 7344d6188932b2de72f6aef300d24d64_surface.npz\n",
            "126 75ffcdedd253c62aa86cbf46be15aeca_surface.npz\n",
            "127 7f38c8033cc2f0854d423544e7c5cb27_surface.npz\n",
            "128 7696d47199f9055a79ea17c9c8a4feb0_surface.npz\n",
            "129 76377cf0dc70d027e7abbab3021b6409_solid.npz\n",
            "130 759121663da89b5e7cb17ae4c4c3b9d7_surface.npz\n",
            "131 759121663da89b5e7cb17ae4c4c3b9d7_solid.npz\n",
            "132 6e87ad11e8c3c273af621148ecacc588_surface.npz\n",
            "133 75f72747ea9ea6cc459aed24ffc76d42_surface.npz\n",
            "134 7635df079a9d126ff9c0f9cbb10e38a2_surface.npz\n",
            "135 7635df079a9d126ff9c0f9cbb10e38a2_solid.npz\n",
            "136 70ea9d97de7de827221d54273fff89cf_surface.npz\n",
            "137 76377cf0dc70d027e7abbab3021b6409_surface.npz\n",
            "138 7c78b826f0f46ef3b6b5161efde62bf9_surface.npz\n",
            "139 6e87ad11e8c3c273af621148ecacc588_solid.npz\n",
            "140 70ea9d97de7de827221d54273fff89cf_solid.npz\n",
            "141 75f72747ea9ea6cc459aed24ffc76d42_solid.npz\n",
            "142 7344d6188932b2de72f6aef300d24d64_solid.npz\n",
            "143 7f38c8033cc2f0854d423544e7c5cb27_solid.npz\n",
            "144 9024bb81d726d584dfda46c9a34dab22_surface.npz\n",
            "145 7feb6c263e565cfb16a06efd4ad41db2_solid.npz\n",
            "146 8ceb7f6e170f793f38cd8eb23853c009_solid.npz\n",
            "147 8af5e20ddd4ac314d3e8c09edf9de80a_surface.npz\n",
            "148 8074bede148f1cbf8a982597ea241696_solid.npz\n",
            "149 8ceb7f6e170f793f38cd8eb23853c009_surface.npz\n",
            "150 8b5f398ede93e1fd8da631348bdc760a_solid.npz\n",
            "151 8cf718ed6a5fefd8fcaad7f5ff5ee65c_surface.npz\n",
            "152 8956ecd996dc60af97802b1c3f15658f_surface.npz\n",
            "153 7feb6c263e565cfb16a06efd4ad41db2_surface.npz\n",
            "154 90143acb141fb1cb4292de5cdfae65e2_surface.npz\n",
            "155 8af5e20ddd4ac314d3e8c09edf9de80a_solid.npz\n",
            "156 8b5f398ede93e1fd8da631348bdc760a_surface.npz\n",
            "157 8074bede148f1cbf8a982597ea241696_surface.npz\n",
            "158 88b8559c748d1d326eaa9b51f546908e_surface.npz\n",
            "159 8b52d84dc97eb3a74740473002adcaee_surface.npz\n",
            "160 8956ecd996dc60af97802b1c3f15658f_solid.npz\n",
            "161 800f8586461b18c238cd8eb23853c009_solid.npz\n",
            "162 8cf718ed6a5fefd8fcaad7f5ff5ee65c_solid.npz\n",
            "163 90143acb141fb1cb4292de5cdfae65e2_solid.npz\n",
            "164 8b52d84dc97eb3a74740473002adcaee_solid.npz\n",
            "165 9024bb81d726d584dfda46c9a34dab22_solid.npz\n",
            "166 88b8559c748d1d326eaa9b51f546908e_solid.npz\n",
            "167 800f8586461b18c238cd8eb23853c009_surface.npz\n",
            "168 9c3a256496120013d444bcd674952301_surface.npz\n",
            "169 a0cc96c45bc569d9ebc1e59456a1eaad_surface.npz\n",
            "170 90e931890719c1b43d36e0a1be1c720_surface.npz\n",
            "171 92e4cc97df1ebe141203ec4ca6ccf208_solid.npz\n",
            "172 927ee8b508cde67f5ac73f8a88a91040_solid.npz\n",
            "173 92e4cc97df1ebe141203ec4ca6ccf208_surface.npz\n",
            "174 a422b6788bb8b2d5663d09b37dd6bd68_solid.npz\n",
            "175 99316252e1dfbde1d810b14a81e12eca_solid.npz\n",
            "176 927ee8b508cde67f5ac73f8a88a91040_surface.npz\n",
            "177 a2beb5f14190c1702637f1559971e0a6_surface.npz\n",
            "178 9e72b97a5af45ded7c272b953086dacf_surface.npz\n",
            "179 a2beb5f14190c1702637f1559971e0a6_solid.npz\n",
            "180 9151fdcecbac4d2fbfde35fcbc037c53_surface.npz\n",
            "181 9ac90e8814cfe72dca7b89674ac5c6e2_solid.npz\n",
            "182 9151fdcecbac4d2fbfde35fcbc037c53_solid.npz\n",
            "183 9b1903c94dc60e7e38cd8eb23853c009_solid.npz\n",
            "184 90e931890719c1b43d36e0a1be1c720_solid.npz\n",
            "185 a07c8e2d0127b26195405124ff912ff6_surface.npz\n",
            "186 99316252e1dfbde1d810b14a81e12eca_surface.npz\n",
            "187 a07c8e2d0127b26195405124ff912ff6_solid.npz\n",
            "188 9b1903c94dc60e7e38cd8eb23853c009_surface.npz\n",
            "189 9c3a256496120013d444bcd674952301_solid.npz\n",
            "190 9ac90e8814cfe72dca7b89674ac5c6e2_surface.npz\n",
            "191 9e72b97a5af45ded7c272b953086dacf_solid.npz\n",
            "192 a0cc96c45bc569d9ebc1e59456a1eaad_solid.npz\n",
            "193 c47a34f2a3acdc0bce8973e274e9f27f_solid.npz\n",
            "194 b8a35075f420cc7066ea44c6d828197f_solid.npz\n",
            "195 c47a34f2a3acdc0bce8973e274e9f27f_surface.npz\n",
            "196 b13143d5f71e38d24738aee9841818fe_solid.npz\n",
            "197 acc8313c1f285a51d3a20b36eab6731d_surface.npz\n",
            "198 b13143d5f71e38d24738aee9841818fe_surface.npz\n",
            "199 ab10a5607c25e6b0d730ccbc8780d609_surface.npz\n",
            "200 b88387c316d85bf6544a7584e1bfdef4_surface.npz\n",
            "201 aa20fa2c82a588d381b0d56f7467d8ca_surface.npz\n",
            "202 b8a35075f420cc7066ea44c6d828197f_surface.npz\n",
            "203 c2a2b8c3b9884d5095c46bada0d9437f_solid.npz\n",
            "204 a739f388def3e027a72b66695a920fe2_surface.npz\n",
            "205 b7600836218f2556e86e8111763264e_solid.npz\n",
            "206 c6e94a8464305378cedded8270815eaf_solid.npz\n",
            "207 a739f388def3e027a72b66695a920fe2_solid.npz\n",
            "208 aec546edcda7c5abe579ef1e3d185e3c_solid.npz\n",
            "209 b88387c316d85bf6544a7584e1bfdef4_solid.npz\n",
            "210 aa20fa2c82a588d381b0d56f7467d8ca_solid.npz\n",
            "211 ab10a5607c25e6b0d730ccbc8780d609_solid.npz\n",
            "212 acc8313c1f285a51d3a20b36eab6731d_solid.npz\n",
            "213 aec546edcda7c5abe579ef1e3d185e3c_surface.npz\n",
            "214 a422b6788bb8b2d5663d09b37dd6bd68_surface.npz\n",
            "215 b7600836218f2556e86e8111763264e_surface.npz\n",
            "216 c2a2b8c3b9884d5095c46bada0d9437f_surface.npz\n",
            "217 d9b4d966b63ba5e12a83093ac1bb2d64_solid.npz\n",
            "218 d9b4d966b63ba5e12a83093ac1bb2d64_surface.npz\n",
            "219 d24729b45b9192b8f1390d726a8b3e99_surface.npz\n",
            "220 c85bd50e2ac803aedfca80608ef618ad_surface.npz\n",
            "221 dcc345e983796eeecb9157ba706d9589_surface.npz\n",
            "222 dcc345e983796eeecb9157ba706d9589_solid.npz\n",
            "223 d24729b45b9192b8f1390d726a8b3e99_solid.npz\n",
            "224 d143fcdf4e9810f13488053ccea5d42b_surface.npz\n",
            "225 dd82af6c4dc9e26cd9bc2d75eb1cab87_surface.npz\n",
            "226 dd36bd331fce23aa5a4e821f5ddcc98f_solid.npz\n",
            "227 ce4acc0cd5f5a33cc4406905076c720_solid.npz\n",
            "228 dd82af6c4dc9e26cd9bc2d75eb1cab87_solid.npz\n",
            "229 d4dae64aeda5a50f4f87ba59623453fc_solid.npz\n",
            "230 d143fcdf4e9810f13488053ccea5d42b_solid.npz\n",
            "231 d4dae64aeda5a50f4f87ba59623453fc_surface.npz\n",
            "232 c6e94a8464305378cedded8270815eaf_surface.npz\n",
            "233 c85bd50e2ac803aedfca80608ef618ad_solid.npz\n",
            "234 ce4acc0cd5f5a33cc4406905076c720_surface.npz\n",
            "235 dd36bd331fce23aa5a4e821f5ddcc98f_surface.npz\n",
            "236 e5819354e7ddc4a2545370dbbc80d144_solid.npz\n",
            "237 f982e86909ed9e7f2c8891dbc3e6988a_surface.npz\n",
            "238 f982e86909ed9e7f2c8891dbc3e6988a_solid.npz\n",
            "239 edf288c16aa9797a4d423544e7c5cb27_solid.npz\n",
            "240 fe5ca50ef83ab52438cd8eb23853c009_solid.npz\n",
            "241 de08da18d316f927a72fcffccc240663_surface.npz\n",
            "242 edf288c16aa9797a4d423544e7c5cb27_surface.npz\n",
            "243 de24d2498ff052c3fe10598e9bcc69e4_solid.npz\n",
            "244 df8591577e0ef1095ae4226ec4ca9d4d_solid.npz\n",
            "245 e7a492bbc4838de285c7c66844cba238_solid.npz\n",
            "246 f14b153ab99da1bb73a921481ee71edc_solid.npz\n",
            "247 f01c3ee2b1210cafdb3180683be7ca4f_surface.npz\n",
            "248 fd6ca820662bbbf3ba10616cfe5316d6_surface.npz\n",
            "249 f584f1a14904b958ba9419f3b43eb3bd_surface.npz\n",
            "250 e3f5234bfe086a3438cd8eb23853c009_solid.npz\n",
            "251 f14b153ab99da1bb73a921481ee71edc_surface.npz\n",
            "252 f679821836f140f39ebe905ef4009f84_surface.npz\n",
            "253 f01c3ee2b1210cafdb3180683be7ca4f_solid.npz\n",
            "254 f3b26a49739b20a08b7e02a440eafc36_surface.npz\n",
            "255 de08da18d316f927a72fcffccc240663_solid.npz\n",
            "256 de24d2498ff052c3fe10598e9bcc69e4_surface.npz\n",
            "257 df8591577e0ef1095ae4226ec4ca9d4d_surface.npz\n",
            "258 f3b26a49739b20a08b7e02a440eafc36_solid.npz\n",
            "259 e5819354e7ddc4a2545370dbbc80d144_surface.npz\n",
            "260 fd6ca820662bbbf3ba10616cfe5316d6_solid.npz\n",
            "261 f679821836f140f39ebe905ef4009f84_solid.npz\n",
            "262 e7a492bbc4838de285c7c66844cba238_surface.npz\n",
            "263 f584f1a14904b958ba9419f3b43eb3bd_solid.npz\n",
            "264 e3f5234bfe086a3438cd8eb23853c009_surface.npz\n",
            "265 fe5ca50ef83ab52438cd8eb23853c009_surface.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fG7JDX9Kqw8",
        "outputId": "7b930c03-7dca-4325-9153-bddb1834fd80"
      },
      "source": [
        "training_data[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 64, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu1ImRKw--xX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930ea64a-6751-49f1-cfeb-35916df52d26"
      },
      "source": [
        "file_list = os.listdir('/content/gdrive/MyDrive/AI_Artathon/tower_dataset_npz')\n",
        "len(file_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "266"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O6yzNlA5-_L",
        "outputId": "34dc9690-68c4-43ab-ad7f-ceae4e7a5f7f"
      },
      "source": [
        "print(file_list[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "853.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5z0pPtF75X2"
      },
      "source": [
        "##Defining usefull functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz6ZbvH7sIJ8"
      },
      "source": [
        "def save_voxel(voxel_model, path):\n",
        "    model = VoxelModel(voxel_model)  \n",
        "    mesh = Mesh.fromVoxelModel(model)\n",
        "    mesh.export(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V15SC_Bmv-n5"
      },
      "source": [
        "def gradient_penalty(real, fake, epsilon): \n",
        "    global discriminator\n",
        "    #mixed_images = real * epsilon + fake * (1 - epsilon)\n",
        "    mixed_images = fake + epsilon * (real - fake)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(mixed_images) \n",
        "        mixed_scores = discriminator(mixed_images)\n",
        "        \n",
        "\n",
        "    gradient = tape.gradient(mixed_scores, mixed_images)[0]\n",
        "    \n",
        "    gradient_norm = tf.norm(gradient)\n",
        "    penalty = tf.math.reduce_mean((gradient_norm - 1)**2)\n",
        "    return penalty\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    gen_loss = -1. * tf.math.reduce_mean(fake_output)\n",
        "    return gen_loss\n",
        "\n",
        "def discriminator_loss(real_output, fake_output, gradient_penalty):\n",
        "    c_lambda = 10\n",
        "    loss = tf.math.reduce_mean(fake_output) - tf.math.reduce_mean(real_output) + c_lambda * gradient_penalty\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u1UoAuD8aBV"
      },
      "source": [
        "##Main training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAXudYnVpRas"
      },
      "source": [
        "discriminator = make_Discriminator()\n",
        "generator = make_Generator()\n",
        "\n",
        "def train(sample_path, checkpoints_path, num_epochs=1000, batch_size=8, latent_dim=32, restore_D=None, restore_G=None, restore_epoch=0):\n",
        "    global discriminator\n",
        "    global generator\n",
        "\n",
        "    sample_epoch = 1000\n",
        "    save_epoch = 1000\n",
        "\n",
        "    dis_optim = RMSprop(lr=0.0002, decay=6e-8)\n",
        "    gen_optim = RMSprop(lr=0.0001, decay=3e-8)\n",
        "\n",
        "    #discriminator = make_Discriminator()\n",
        "\n",
        "    if restore_D!=None:\n",
        "      discriminator=tf.keras.models.load_model(restore_D)\n",
        "\n",
        "    #generator = make_Generator()\n",
        "\n",
        "    if restore_G!=None:\n",
        "      generator=tf.keras.models.load_model(restore_G)\n",
        "\n",
        "    print('Generator')\n",
        "    generator.summary()\n",
        "    generator.compile(optimizer=gen_optim)\n",
        "    #z = Input(shape=(latent_dim,))\n",
        "    #img = generator(z)\n",
        "\n",
        "    #discriminator.trainable = False\n",
        "    #validity = discriminator(img)\n",
        "\n",
        "    #combined = Model(inputs=z, outputs=validity)\n",
        "    #combined.compile(loss='binary_crossentropy', optimizer=gen_optim, metrics=['accuracy'])\n",
        "    #print('Combined...')\n",
        "    #combined.summary()\n",
        "\n",
        "    #discriminator.trainable = True\n",
        "    discriminator.compile(optimizer=dis_optim)\n",
        "    print('Discriminator...')\n",
        "    discriminator.summary()\n",
        "\n",
        "    # load data\n",
        "    # data_loader = DataLoader(args)\n",
        "    # X_train = np.array(data_loader.load_data()).astype(np.float32)\n",
        "    dl, gl = [],[]\n",
        "    for epoch in range(restore_epoch, num_epochs):\n",
        "        #sample a random batch\n",
        "        idx = np.random.randint(len(training_data), size=batch_size)\n",
        "        # print('Sampling indices...' + str(idx))\n",
        "        real = training_data[idx]\n",
        "        real = real.reshape(real.shape+(1,))\n",
        "\n",
        "        noise = tf.random.normal([batch_size, latent_dim])\n",
        "\n",
        "        #training discriminator 3 times for each batch\n",
        "        for i in range(3):\n",
        "\n",
        "          with tf.GradientTape() as disc_tape:\n",
        "        \n",
        "            generated_images = generator(noise, training=True)\n",
        "\n",
        "            real_output = discriminator(real, training=True)\n",
        "            fake_output = discriminator(generated_images, training=True)\n",
        "        \n",
        "            epsilon = tf.random.normal([batch_size, 1, 1, 1, 1], 0.0, 1.0)\n",
        "        \n",
        "            gp = gradient_penalty(real, generated_images, epsilon)\n",
        "        \n",
        "            disc_loss = discriminator_loss(real_output, fake_output, gp)\n",
        "\n",
        "    \n",
        "          gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "          dis_optim.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "          generated_images = generator(noise, training=True)\n",
        "          fake_output = discriminator(generated_images, training=True)\n",
        "          gen_loss = generator_loss(fake_output)\n",
        "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "        gen_optim.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "        print('Training epoch {}/{}, d_loss: {},  g_loss: {}'.format(epoch+1, num_epochs, disc_loss, gen_loss))\n",
        "\n",
        "        # sampling\n",
        "        if epoch % sample_epoch == 0:\n",
        "            if not os.path.exists(sample_path):\n",
        "                os.makedirs(sample_path)\n",
        "            print('Sampling...')\n",
        "            sample_noise = np.random.uniform(-1.0, 1.0, size=[1, latent_dim]).astype(np.float64)\n",
        "            voxel_model = generator.predict(sample_noise, verbose=1)\n",
        "            voxel_model = voxel_model.reshape(voxel_model[0].shape[:-1])\n",
        "            voxel_model = np.rint(voxel_model)\n",
        "            try:\n",
        "              save_voxel(voxel_model, sample_path + f'/epoch_{epoch+1}.obj')\n",
        "            except:\n",
        "              print('Could not create voxel model... Continuing training')\n",
        "\n",
        "        # save weights\n",
        "        if epoch % save_epoch == 0:\n",
        "            if not os.path.exists(checkpoints_path):\n",
        "                os.makedirs(checkpoints_path)\n",
        "            generator.save(checkpoints_path + '/generator_epoch_' + str(epoch+1))\n",
        "            discriminator.save(checkpoints_path + '/discriminator_epoch_' + str(epoch+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nHsL4f_uoH6",
        "outputId": "0d2c5570-128d-4885-e54a-ddb1d2d2d0cd"
      },
      "source": [
        "sample_path = '/content/gdrive/MyDrive/AI_Artathon/voxel_model/samplings'\n",
        "checkpoints_path = '/content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints'\n",
        "\n",
        "train(sample_path, checkpoints_path, num_epochs=1000000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_7 (Dense)             (None, 2048)              65536     \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 2048)             8192      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " reshape_3 (Reshape)         (None, 2, 2, 2, 256)      0         \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 2, 2, 2, 256)      0         \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 2, 2, 2, 256)      0         \n",
            "                                                                 \n",
            " up_sampling3d_15 (UpSamplin  (None, 4, 4, 4, 256)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_transpose_15 (Conv3D  (None, 4, 4, 4, 512)     16384000  \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 4, 4, 4, 512)     2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 4, 4, 4, 512)      0         \n",
            "                                                                 \n",
            " up_sampling3d_16 (UpSamplin  (None, 8, 8, 8, 512)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_transpose_16 (Conv3D  (None, 8, 8, 8, 256)     16384000  \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 8, 8, 8, 256)     1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 8, 8, 8, 256)      0         \n",
            "                                                                 \n",
            " up_sampling3d_17 (UpSamplin  (None, 16, 16, 16, 256)  0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_transpose_17 (Conv3D  (None, 16, 16, 16, 128)  4096000   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 16, 16, 16, 128)  512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_21 (Activation)  (None, 16, 16, 16, 128)   0         \n",
            "                                                                 \n",
            " up_sampling3d_18 (UpSamplin  (None, 32, 32, 32, 128)  0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_transpose_18 (Conv3D  (None, 32, 32, 32, 64)   1024000   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 32, 32, 32, 64)   256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_22 (Activation)  (None, 32, 32, 32, 64)    0         \n",
            "                                                                 \n",
            " up_sampling3d_19 (UpSamplin  (None, 64, 64, 64, 64)   0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_transpose_19 (Conv3D  (None, 64, 64, 64, 1)    8000      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " activation_23 (Activation)  (None, 64, 64, 64, 1)     0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37,973,568\n",
            "Trainable params: 37,967,552\n",
            "Non-trainable params: 6,016\n",
            "_________________________________________________________________\n",
            "Discriminator...\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_12 (Conv3D)          (None, 32, 32, 32, 64)    8064      \n",
            "                                                                 \n",
            " leaky_re_lu_12 (LeakyReLU)  (None, 32, 32, 32, 64)    0         \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 32, 32, 32, 64)    0         \n",
            "                                                                 \n",
            " conv3d_13 (Conv3D)          (None, 16, 16, 16, 128)   1024128   \n",
            "                                                                 \n",
            " leaky_re_lu_13 (LeakyReLU)  (None, 16, 16, 16, 128)   0         \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 16, 16, 16, 128)   0         \n",
            "                                                                 \n",
            " conv3d_14 (Conv3D)          (None, 8, 8, 8, 256)      4096256   \n",
            "                                                                 \n",
            " leaky_re_lu_14 (LeakyReLU)  (None, 8, 8, 8, 256)      0         \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 8, 8, 8, 256)      0         \n",
            "                                                                 \n",
            " conv3d_15 (Conv3D)          (None, 4, 4, 4, 512)      16384512  \n",
            "                                                                 \n",
            " leaky_re_lu_15 (LeakyReLU)  (None, 4, 4, 4, 512)      0         \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 4, 4, 4, 512)      0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 32769     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,545,729\n",
            "Trainable params: 21,545,729\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training epoch 1/1000000, d_loss: -55.09537887573242,  g_loss: -90.82402038574219\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 455ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 7/7 [00:00<00:00, 838.60it/s]\n",
            "Meshing: 100%|██████████| 1082/1082 [00:00<00:00, 5941.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_1/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_1/assets\n",
            "Training epoch 2/1000000, d_loss: -276.4544372558594,  g_loss: -268.59686279296875\n",
            "Training epoch 3/1000000, d_loss: -48.90522384643555,  g_loss: -5.487723350524902\n",
            "Training epoch 4/1000000, d_loss: -66.7907485961914,  g_loss: 86.01617431640625\n",
            "Training epoch 5/1000000, d_loss: -215.08181762695312,  g_loss: -113.71449279785156\n",
            "Training epoch 6/1000000, d_loss: -235.7108154296875,  g_loss: -181.21682739257812\n",
            "Training epoch 7/1000000, d_loss: -102.75450134277344,  g_loss: 53.26789855957031\n",
            "Training epoch 8/1000000, d_loss: -125.82190704345703,  g_loss: 47.15963363647461\n",
            "Training epoch 9/1000000, d_loss: -45.089271545410156,  g_loss: 79.95674133300781\n",
            "Training epoch 10/1000000, d_loss: -43.33488464355469,  g_loss: 98.82160949707031\n",
            "Training epoch 11/1000000, d_loss: -113.34325408935547,  g_loss: -27.270793914794922\n",
            "Training epoch 12/1000000, d_loss: -46.33847427368164,  g_loss: -3.3003311157226562\n",
            "Training epoch 13/1000000, d_loss: -101.56539154052734,  g_loss: 49.03115463256836\n",
            "Training epoch 14/1000000, d_loss: 4.99652099609375,  g_loss: -19.03443145751953\n",
            "Training epoch 15/1000000, d_loss: -42.47234344482422,  g_loss: -19.54097557067871\n",
            "Training epoch 16/1000000, d_loss: -86.18858337402344,  g_loss: -12.997735023498535\n",
            "Training epoch 17/1000000, d_loss: -50.35517883300781,  g_loss: 1.407780647277832\n",
            "Training epoch 18/1000000, d_loss: -34.68579864501953,  g_loss: 52.3428840637207\n",
            "Training epoch 19/1000000, d_loss: 0.8448982238769531,  g_loss: 39.53840255737305\n",
            "Training epoch 20/1000000, d_loss: -100.0711898803711,  g_loss: -2.351799488067627\n",
            "Training epoch 21/1000000, d_loss: -65.80653381347656,  g_loss: -15.824413299560547\n",
            "Training epoch 22/1000000, d_loss: -174.67051696777344,  g_loss: -58.40026092529297\n",
            "Training epoch 23/1000000, d_loss: -144.47463989257812,  g_loss: -52.607940673828125\n",
            "Training epoch 24/1000000, d_loss: -51.74828338623047,  g_loss: 5.9797868728637695\n",
            "Training epoch 25/1000000, d_loss: -87.29864501953125,  g_loss: -19.095638275146484\n",
            "Training epoch 26/1000000, d_loss: -63.127227783203125,  g_loss: 17.814693450927734\n",
            "Training epoch 27/1000000, d_loss: -222.1190185546875,  g_loss: 9.514775276184082\n",
            "Training epoch 28/1000000, d_loss: -212.46563720703125,  g_loss: -51.01129913330078\n",
            "Training epoch 29/1000000, d_loss: -245.10316467285156,  g_loss: -156.27853393554688\n",
            "Training epoch 30/1000000, d_loss: -46.751853942871094,  g_loss: 75.94363403320312\n",
            "Training epoch 31/1000000, d_loss: -112.34095001220703,  g_loss: 126.7179183959961\n",
            "Training epoch 32/1000000, d_loss: -61.15467071533203,  g_loss: 179.11044311523438\n",
            "Training epoch 33/1000000, d_loss: -194.96177673339844,  g_loss: -169.39134216308594\n",
            "Training epoch 34/1000000, d_loss: -154.71800231933594,  g_loss: -197.47030639648438\n",
            "Training epoch 35/1000000, d_loss: -251.50247192382812,  g_loss: 28.886751174926758\n",
            "Training epoch 36/1000000, d_loss: -225.51104736328125,  g_loss: -38.91588592529297\n",
            "Training epoch 37/1000000, d_loss: -68.96218872070312,  g_loss: 177.61798095703125\n",
            "Training epoch 38/1000000, d_loss: -156.75421142578125,  g_loss: 14.063611030578613\n",
            "Training epoch 39/1000000, d_loss: -258.91461181640625,  g_loss: -260.2986755371094\n",
            "Training epoch 40/1000000, d_loss: -66.62310028076172,  g_loss: 159.3953094482422\n",
            "Training epoch 41/1000000, d_loss: -86.12540435791016,  g_loss: 271.4656066894531\n",
            "Training epoch 42/1000000, d_loss: 15.029094696044922,  g_loss: 260.13238525390625\n",
            "Training epoch 43/1000000, d_loss: -97.5628662109375,  g_loss: 162.57492065429688\n",
            "Training epoch 44/1000000, d_loss: -72.67236328125,  g_loss: 130.89248657226562\n",
            "Training epoch 45/1000000, d_loss: -105.24634552001953,  g_loss: 128.221435546875\n",
            "Training epoch 46/1000000, d_loss: -197.4728240966797,  g_loss: -91.44022369384766\n",
            "Training epoch 47/1000000, d_loss: -38.0316162109375,  g_loss: 33.002845764160156\n",
            "Training epoch 48/1000000, d_loss: -264.79791259765625,  g_loss: -72.24722290039062\n",
            "Training epoch 49/1000000, d_loss: -48.10783767700195,  g_loss: 10.775426864624023\n",
            "Training epoch 50/1000000, d_loss: -32.371742248535156,  g_loss: -4.562512397766113\n",
            "Training epoch 51/1000000, d_loss: -103.16091918945312,  g_loss: 65.56732177734375\n",
            "Training epoch 52/1000000, d_loss: -38.928592681884766,  g_loss: 69.21055603027344\n",
            "Training epoch 53/1000000, d_loss: -46.749000549316406,  g_loss: 61.85721969604492\n",
            "Training epoch 54/1000000, d_loss: 75.75445556640625,  g_loss: 98.07503509521484\n",
            "Training epoch 55/1000000, d_loss: 15.720170974731445,  g_loss: 167.3118896484375\n",
            "Training epoch 56/1000000, d_loss: -247.24559020996094,  g_loss: -9.0381441116333\n",
            "Training epoch 57/1000000, d_loss: -140.7796630859375,  g_loss: -26.299789428710938\n",
            "Training epoch 58/1000000, d_loss: -114.44255065917969,  g_loss: -154.54298400878906\n",
            "Training epoch 59/1000000, d_loss: -74.09025573730469,  g_loss: 88.48824310302734\n",
            "Training epoch 60/1000000, d_loss: -23.719358444213867,  g_loss: 69.6495590209961\n",
            "Training epoch 61/1000000, d_loss: -264.3967590332031,  g_loss: -222.96841430664062\n",
            "Training epoch 62/1000000, d_loss: -114.36593627929688,  g_loss: -111.39508056640625\n",
            "Training epoch 63/1000000, d_loss: -128.70240783691406,  g_loss: -22.222747802734375\n",
            "Training epoch 64/1000000, d_loss: -156.7647705078125,  g_loss: -42.16736602783203\n",
            "Training epoch 65/1000000, d_loss: -187.34933471679688,  g_loss: -160.12368774414062\n",
            "Training epoch 66/1000000, d_loss: 3.9749755859375,  g_loss: -75.98799896240234\n",
            "Training epoch 67/1000000, d_loss: -226.00775146484375,  g_loss: -187.44332885742188\n",
            "Training epoch 68/1000000, d_loss: -83.52127838134766,  g_loss: 13.260692596435547\n",
            "Training epoch 69/1000000, d_loss: -157.25430297851562,  g_loss: 264.2008361816406\n",
            "Training epoch 70/1000000, d_loss: -147.28517150878906,  g_loss: 36.83015060424805\n",
            "Training epoch 71/1000000, d_loss: -120.54891204833984,  g_loss: 87.78630065917969\n",
            "Training epoch 72/1000000, d_loss: -171.82972717285156,  g_loss: 67.19428253173828\n",
            "Training epoch 73/1000000, d_loss: -133.76828002929688,  g_loss: 54.65544128417969\n",
            "Training epoch 74/1000000, d_loss: -154.1199188232422,  g_loss: -153.00717163085938\n",
            "Training epoch 75/1000000, d_loss: -94.14034271240234,  g_loss: 134.18446350097656\n",
            "Training epoch 76/1000000, d_loss: -30.92172622680664,  g_loss: 296.848388671875\n",
            "Training epoch 77/1000000, d_loss: 60.042362213134766,  g_loss: 144.9097900390625\n",
            "Training epoch 78/1000000, d_loss: -96.83505249023438,  g_loss: -65.86518859863281\n",
            "Training epoch 79/1000000, d_loss: -216.06349182128906,  g_loss: -236.40869140625\n",
            "Training epoch 80/1000000, d_loss: -37.95777893066406,  g_loss: 7.483908653259277\n",
            "Training epoch 81/1000000, d_loss: -74.41262817382812,  g_loss: 166.0758514404297\n",
            "Training epoch 82/1000000, d_loss: -215.98544311523438,  g_loss: -152.37228393554688\n",
            "Training epoch 83/1000000, d_loss: -41.27029037475586,  g_loss: 71.81654357910156\n",
            "Training epoch 84/1000000, d_loss: 16.19839096069336,  g_loss: 165.98385620117188\n",
            "Training epoch 85/1000000, d_loss: 1.5384654998779297,  g_loss: 224.20285034179688\n",
            "Training epoch 86/1000000, d_loss: -62.8511962890625,  g_loss: 136.48922729492188\n",
            "Training epoch 87/1000000, d_loss: -139.74459838867188,  g_loss: 111.568359375\n",
            "Training epoch 88/1000000, d_loss: -160.8003387451172,  g_loss: 71.67289733886719\n",
            "Training epoch 89/1000000, d_loss: -81.25016784667969,  g_loss: -106.65943145751953\n",
            "Training epoch 90/1000000, d_loss: -269.8872375488281,  g_loss: -139.07357788085938\n",
            "Training epoch 91/1000000, d_loss: -68.77381134033203,  g_loss: -26.57354164123535\n",
            "Training epoch 92/1000000, d_loss: -164.60507202148438,  g_loss: 307.6596374511719\n",
            "Training epoch 93/1000000, d_loss: -74.2611083984375,  g_loss: 254.72059631347656\n",
            "Training epoch 94/1000000, d_loss: -42.091041564941406,  g_loss: 325.62969970703125\n",
            "Training epoch 95/1000000, d_loss: -166.48306274414062,  g_loss: 66.6134033203125\n",
            "Training epoch 96/1000000, d_loss: -56.312503814697266,  g_loss: 49.26588821411133\n",
            "Training epoch 97/1000000, d_loss: -133.0087890625,  g_loss: 88.12759399414062\n",
            "Training epoch 98/1000000, d_loss: -19.80873680114746,  g_loss: 112.26639556884766\n",
            "Training epoch 99/1000000, d_loss: -46.80453872680664,  g_loss: 77.5882797241211\n",
            "Training epoch 100/1000000, d_loss: -289.345947265625,  g_loss: -120.90771484375\n",
            "Training epoch 101/1000000, d_loss: 49.33317565917969,  g_loss: 41.652488708496094\n",
            "Training epoch 102/1000000, d_loss: -147.40419006347656,  g_loss: -70.32705688476562\n",
            "Training epoch 103/1000000, d_loss: -111.46488189697266,  g_loss: -49.616207122802734\n",
            "Training epoch 104/1000000, d_loss: -132.97193908691406,  g_loss: -141.780029296875\n",
            "Training epoch 105/1000000, d_loss: 14.280704498291016,  g_loss: -25.53256607055664\n",
            "Training epoch 106/1000000, d_loss: -293.89300537109375,  g_loss: -222.6200714111328\n",
            "Training epoch 107/1000000, d_loss: 5.367973327636719,  g_loss: -67.26824188232422\n",
            "Training epoch 108/1000000, d_loss: -624.6997680664062,  g_loss: -495.7392578125\n",
            "Training epoch 109/1000000, d_loss: 28.611852645874023,  g_loss: 48.74491882324219\n",
            "Training epoch 110/1000000, d_loss: -158.4291534423828,  g_loss: 124.66060638427734\n",
            "Training epoch 111/1000000, d_loss: -304.92706298828125,  g_loss: 543.79736328125\n",
            "Training epoch 112/1000000, d_loss: -265.8636169433594,  g_loss: 518.4539794921875\n",
            "Training epoch 113/1000000, d_loss: -131.5814666748047,  g_loss: 272.7838134765625\n",
            "Training epoch 114/1000000, d_loss: -57.034149169921875,  g_loss: 344.98565673828125\n",
            "Training epoch 115/1000000, d_loss: -96.50631713867188,  g_loss: 376.1641540527344\n",
            "Training epoch 116/1000000, d_loss: -71.1834716796875,  g_loss: 233.8843231201172\n",
            "Training epoch 117/1000000, d_loss: -181.95370483398438,  g_loss: -2.0762176513671875\n",
            "Training epoch 118/1000000, d_loss: -77.47259521484375,  g_loss: -53.31403350830078\n",
            "Training epoch 119/1000000, d_loss: -46.85026550292969,  g_loss: 27.965219497680664\n",
            "Training epoch 120/1000000, d_loss: -44.82391357421875,  g_loss: 41.85913848876953\n",
            "Training epoch 121/1000000, d_loss: -184.82115173339844,  g_loss: -44.764129638671875\n",
            "Training epoch 122/1000000, d_loss: -328.5299987792969,  g_loss: -180.02316284179688\n",
            "Training epoch 123/1000000, d_loss: -46.32664489746094,  g_loss: 122.87017059326172\n",
            "Training epoch 124/1000000, d_loss: -114.64330291748047,  g_loss: 109.41685485839844\n",
            "Training epoch 125/1000000, d_loss: -25.20880699157715,  g_loss: 149.32504272460938\n",
            "Training epoch 126/1000000, d_loss: -164.34397888183594,  g_loss: -10.861772537231445\n",
            "Training epoch 127/1000000, d_loss: -17.828941345214844,  g_loss: 11.283535957336426\n",
            "Training epoch 128/1000000, d_loss: -21.341167449951172,  g_loss: -1.4478178024291992\n",
            "Training epoch 129/1000000, d_loss: -102.97744750976562,  g_loss: 16.83036994934082\n",
            "Training epoch 130/1000000, d_loss: 10.39680290222168,  g_loss: 31.2366943359375\n",
            "Training epoch 131/1000000, d_loss: -120.02565002441406,  g_loss: -8.123485565185547\n",
            "Training epoch 132/1000000, d_loss: -242.39817810058594,  g_loss: -93.62049865722656\n",
            "Training epoch 133/1000000, d_loss: -83.47329711914062,  g_loss: 39.966064453125\n",
            "Training epoch 134/1000000, d_loss: -71.13499450683594,  g_loss: 133.6458740234375\n",
            "Training epoch 135/1000000, d_loss: 10.619743347167969,  g_loss: 156.4014892578125\n",
            "Training epoch 136/1000000, d_loss: 3.0667953491210938,  g_loss: 40.51101303100586\n",
            "Training epoch 137/1000000, d_loss: 251.91552734375,  g_loss: 117.6971435546875\n",
            "Training epoch 138/1000000, d_loss: 25.887649536132812,  g_loss: 107.26409149169922\n",
            "Training epoch 139/1000000, d_loss: -51.08740997314453,  g_loss: 135.6863250732422\n",
            "Training epoch 140/1000000, d_loss: -69.30450439453125,  g_loss: 95.06027221679688\n",
            "Training epoch 141/1000000, d_loss: -152.93350219726562,  g_loss: 13.228556632995605\n",
            "Training epoch 142/1000000, d_loss: -10.672140121459961,  g_loss: 55.36305618286133\n",
            "Training epoch 143/1000000, d_loss: -134.3809814453125,  g_loss: 35.30731201171875\n",
            "Training epoch 144/1000000, d_loss: -211.94578552246094,  g_loss: 1.061617374420166\n",
            "Training epoch 145/1000000, d_loss: -31.58008575439453,  g_loss: 55.33116912841797\n",
            "Training epoch 146/1000000, d_loss: -78.272705078125,  g_loss: 267.527099609375\n",
            "Training epoch 147/1000000, d_loss: -48.28651428222656,  g_loss: -62.845909118652344\n",
            "Training epoch 148/1000000, d_loss: -120.45407104492188,  g_loss: 20.49058723449707\n",
            "Training epoch 149/1000000, d_loss: -230.44024658203125,  g_loss: -186.37179565429688\n",
            "Training epoch 150/1000000, d_loss: -145.27589416503906,  g_loss: -128.27865600585938\n",
            "Training epoch 151/1000000, d_loss: -43.11933898925781,  g_loss: 68.86819458007812\n",
            "Training epoch 152/1000000, d_loss: -121.78154754638672,  g_loss: 79.82307434082031\n",
            "Training epoch 153/1000000, d_loss: -80.2673110961914,  g_loss: 46.1302490234375\n",
            "Training epoch 154/1000000, d_loss: -170.53219604492188,  g_loss: -98.03868865966797\n",
            "Training epoch 155/1000000, d_loss: -105.25716400146484,  g_loss: 137.32199096679688\n",
            "Training epoch 156/1000000, d_loss: -125.53433227539062,  g_loss: 5.821538925170898\n",
            "Training epoch 157/1000000, d_loss: -181.65733337402344,  g_loss: 122.89881134033203\n",
            "Training epoch 158/1000000, d_loss: -66.7174301147461,  g_loss: -45.51429748535156\n",
            "Training epoch 159/1000000, d_loss: -120.18962097167969,  g_loss: -34.69242858886719\n",
            "Training epoch 160/1000000, d_loss: -64.52116394042969,  g_loss: 75.90303802490234\n",
            "Training epoch 161/1000000, d_loss: -138.91029357910156,  g_loss: 257.988037109375\n",
            "Training epoch 162/1000000, d_loss: -109.25877380371094,  g_loss: 37.040550231933594\n",
            "Training epoch 163/1000000, d_loss: -2.352397918701172,  g_loss: -62.84882354736328\n",
            "Training epoch 164/1000000, d_loss: -174.55824279785156,  g_loss: -205.3585662841797\n",
            "Training epoch 165/1000000, d_loss: -79.71421813964844,  g_loss: -163.0288543701172\n",
            "Training epoch 166/1000000, d_loss: -253.48980712890625,  g_loss: -185.9859619140625\n",
            "Training epoch 167/1000000, d_loss: -267.13189697265625,  g_loss: -283.13946533203125\n",
            "Training epoch 168/1000000, d_loss: -108.24076843261719,  g_loss: -23.457115173339844\n",
            "Training epoch 169/1000000, d_loss: -39.156036376953125,  g_loss: 127.92776489257812\n",
            "Training epoch 170/1000000, d_loss: -42.9724235534668,  g_loss: 51.004173278808594\n",
            "Training epoch 171/1000000, d_loss: -43.041385650634766,  g_loss: -77.3097915649414\n",
            "Training epoch 172/1000000, d_loss: -132.75704956054688,  g_loss: -67.59602355957031\n",
            "Training epoch 173/1000000, d_loss: -95.69158935546875,  g_loss: -60.340850830078125\n",
            "Training epoch 174/1000000, d_loss: -221.6907958984375,  g_loss: 39.91313934326172\n",
            "Training epoch 175/1000000, d_loss: -76.2721939086914,  g_loss: -83.73575592041016\n",
            "Training epoch 176/1000000, d_loss: -134.17962646484375,  g_loss: -124.3914794921875\n",
            "Training epoch 177/1000000, d_loss: -319.90972900390625,  g_loss: -376.83935546875\n",
            "Training epoch 178/1000000, d_loss: -326.43658447265625,  g_loss: -174.79684448242188\n",
            "Training epoch 179/1000000, d_loss: -229.1526641845703,  g_loss: 8.901397705078125\n",
            "Training epoch 180/1000000, d_loss: -779.0054321289062,  g_loss: -531.0933837890625\n",
            "Training epoch 181/1000000, d_loss: 9.492073059082031,  g_loss: 230.00299072265625\n",
            "Training epoch 182/1000000, d_loss: -98.99765014648438,  g_loss: 290.06243896484375\n",
            "Training epoch 183/1000000, d_loss: -6.9550018310546875,  g_loss: 310.27264404296875\n",
            "Training epoch 184/1000000, d_loss: -250.80557250976562,  g_loss: 111.24996948242188\n",
            "Training epoch 185/1000000, d_loss: -77.15744018554688,  g_loss: 224.6298828125\n",
            "Training epoch 186/1000000, d_loss: -134.161376953125,  g_loss: 287.4846496582031\n",
            "Training epoch 187/1000000, d_loss: -52.76019287109375,  g_loss: 141.9254608154297\n",
            "Training epoch 188/1000000, d_loss: -108.40538024902344,  g_loss: 180.55435180664062\n",
            "Training epoch 189/1000000, d_loss: 52.14594650268555,  g_loss: 148.14613342285156\n",
            "Training epoch 190/1000000, d_loss: -424.08563232421875,  g_loss: 43.70262908935547\n",
            "Training epoch 191/1000000, d_loss: -1301.2796630859375,  g_loss: -217.00167846679688\n",
            "Training epoch 192/1000000, d_loss: 43.66877746582031,  g_loss: -210.9711151123047\n",
            "Training epoch 193/1000000, d_loss: -95.8738784790039,  g_loss: -195.93667602539062\n",
            "Training epoch 194/1000000, d_loss: 86.23284149169922,  g_loss: -12.703786849975586\n",
            "Training epoch 195/1000000, d_loss: -349.0986328125,  g_loss: -137.84584045410156\n",
            "Training epoch 196/1000000, d_loss: -81.86670684814453,  g_loss: 144.05795288085938\n",
            "Training epoch 197/1000000, d_loss: -181.51861572265625,  g_loss: 346.55120849609375\n",
            "Training epoch 198/1000000, d_loss: -183.57872009277344,  g_loss: 379.6409912109375\n",
            "Training epoch 199/1000000, d_loss: -146.06478881835938,  g_loss: -120.24384307861328\n",
            "Training epoch 200/1000000, d_loss: 251.6015625,  g_loss: 61.64395523071289\n",
            "Training epoch 201/1000000, d_loss: -66.99739074707031,  g_loss: 63.58067321777344\n",
            "Training epoch 202/1000000, d_loss: -45.22905349731445,  g_loss: 42.9363899230957\n",
            "Training epoch 203/1000000, d_loss: -316.9427490234375,  g_loss: -102.01763916015625\n",
            "Training epoch 204/1000000, d_loss: -82.6996078491211,  g_loss: -71.07212829589844\n",
            "Training epoch 205/1000000, d_loss: -116.55207061767578,  g_loss: -43.749717712402344\n",
            "Training epoch 206/1000000, d_loss: -51.088775634765625,  g_loss: -26.90023422241211\n",
            "Training epoch 207/1000000, d_loss: -719.0177001953125,  g_loss: -317.27545166015625\n",
            "Training epoch 208/1000000, d_loss: -18.53274917602539,  g_loss: -98.97529602050781\n",
            "Training epoch 209/1000000, d_loss: -35.371761322021484,  g_loss: -97.43794250488281\n",
            "Training epoch 210/1000000, d_loss: -96.33483123779297,  g_loss: -2.501249313354492\n",
            "Training epoch 211/1000000, d_loss: -57.453365325927734,  g_loss: 1.0835323333740234\n",
            "Training epoch 212/1000000, d_loss: -81.16345977783203,  g_loss: 78.22109985351562\n",
            "Training epoch 213/1000000, d_loss: -121.77799987792969,  g_loss: 73.3218994140625\n",
            "Training epoch 214/1000000, d_loss: -737.9386596679688,  g_loss: -242.81787109375\n",
            "Training epoch 215/1000000, d_loss: -48.137733459472656,  g_loss: -108.82867431640625\n",
            "Training epoch 216/1000000, d_loss: -64.14038848876953,  g_loss: 26.78434944152832\n",
            "Training epoch 217/1000000, d_loss: -109.86591339111328,  g_loss: -38.56057357788086\n",
            "Training epoch 218/1000000, d_loss: 22.46483612060547,  g_loss: 58.52064514160156\n",
            "Training epoch 219/1000000, d_loss: -104.18457794189453,  g_loss: 174.48326110839844\n",
            "Training epoch 220/1000000, d_loss: -121.23245239257812,  g_loss: -86.47900390625\n",
            "Training epoch 221/1000000, d_loss: -50.881690979003906,  g_loss: 90.37259674072266\n",
            "Training epoch 222/1000000, d_loss: -531.2440185546875,  g_loss: -249.56260681152344\n",
            "Training epoch 223/1000000, d_loss: 64.13255310058594,  g_loss: -66.14668273925781\n",
            "Training epoch 224/1000000, d_loss: -184.52374267578125,  g_loss: -13.406481742858887\n",
            "Training epoch 225/1000000, d_loss: -96.80554962158203,  g_loss: -50.21220779418945\n",
            "Training epoch 226/1000000, d_loss: -82.09798431396484,  g_loss: 7.99151611328125\n",
            "Training epoch 227/1000000, d_loss: -222.7159881591797,  g_loss: 319.36474609375\n",
            "Training epoch 228/1000000, d_loss: -485.2305603027344,  g_loss: 708.0686645507812\n",
            "Training epoch 229/1000000, d_loss: -35.884761810302734,  g_loss: 190.95179748535156\n",
            "Training epoch 230/1000000, d_loss: -362.8267822265625,  g_loss: -196.33889770507812\n",
            "Training epoch 231/1000000, d_loss: -245.78570556640625,  g_loss: -153.33023071289062\n",
            "Training epoch 232/1000000, d_loss: -65.33981323242188,  g_loss: -82.765380859375\n",
            "Training epoch 233/1000000, d_loss: -16.731765747070312,  g_loss: 7.496330261230469\n",
            "Training epoch 234/1000000, d_loss: -975.7799682617188,  g_loss: -425.3191833496094\n",
            "Training epoch 235/1000000, d_loss: -348.90325927734375,  g_loss: -178.4261474609375\n",
            "Training epoch 236/1000000, d_loss: -43.805274963378906,  g_loss: -142.84783935546875\n",
            "Training epoch 237/1000000, d_loss: -218.1949462890625,  g_loss: 19.661182403564453\n",
            "Training epoch 238/1000000, d_loss: -55.89617919921875,  g_loss: 14.7012939453125\n",
            "Training epoch 239/1000000, d_loss: -128.0663299560547,  g_loss: 33.08047103881836\n",
            "Training epoch 240/1000000, d_loss: -57.061614990234375,  g_loss: -9.0524320602417\n",
            "Training epoch 241/1000000, d_loss: -62.5406379699707,  g_loss: -24.535335540771484\n",
            "Training epoch 242/1000000, d_loss: -267.29498291015625,  g_loss: -25.990869522094727\n",
            "Training epoch 243/1000000, d_loss: -211.8151092529297,  g_loss: -27.28489112854004\n",
            "Training epoch 244/1000000, d_loss: -759.7275390625,  g_loss: -145.72744750976562\n",
            "Training epoch 245/1000000, d_loss: -18.621421813964844,  g_loss: 49.984375\n",
            "Training epoch 246/1000000, d_loss: -135.42788696289062,  g_loss: 22.684587478637695\n",
            "Training epoch 247/1000000, d_loss: -60.587921142578125,  g_loss: 8.37179183959961\n",
            "Training epoch 248/1000000, d_loss: -84.77236938476562,  g_loss: -66.86058807373047\n",
            "Training epoch 249/1000000, d_loss: 18.79892349243164,  g_loss: -98.08549499511719\n",
            "Training epoch 250/1000000, d_loss: -38.453521728515625,  g_loss: -40.45305633544922\n",
            "Training epoch 251/1000000, d_loss: -14.821937561035156,  g_loss: -43.613338470458984\n",
            "Training epoch 252/1000000, d_loss: -24.801490783691406,  g_loss: -2.439786911010742\n",
            "Training epoch 253/1000000, d_loss: -46.70878982543945,  g_loss: -56.566444396972656\n",
            "Training epoch 254/1000000, d_loss: -212.4516143798828,  g_loss: -155.1879425048828\n",
            "Training epoch 255/1000000, d_loss: -212.890625,  g_loss: -171.6443634033203\n",
            "Training epoch 256/1000000, d_loss: -30.257936477661133,  g_loss: -66.64982604980469\n",
            "Training epoch 257/1000000, d_loss: -17.074127197265625,  g_loss: -4.96272611618042\n",
            "Training epoch 258/1000000, d_loss: -440.632568359375,  g_loss: -163.9720458984375\n",
            "Training epoch 259/1000000, d_loss: -53.08832550048828,  g_loss: -50.504737854003906\n",
            "Training epoch 260/1000000, d_loss: -232.5274658203125,  g_loss: -178.61801147460938\n",
            "Training epoch 261/1000000, d_loss: -158.5290985107422,  g_loss: -97.10187530517578\n",
            "Training epoch 262/1000000, d_loss: -48.339088439941406,  g_loss: 15.14433479309082\n",
            "Training epoch 263/1000000, d_loss: -569.5126342773438,  g_loss: -192.97052001953125\n",
            "Training epoch 264/1000000, d_loss: -30.291141510009766,  g_loss: -51.71381378173828\n",
            "Training epoch 265/1000000, d_loss: -151.07681274414062,  g_loss: -118.41152954101562\n",
            "Training epoch 266/1000000, d_loss: -147.2165985107422,  g_loss: -168.7796630859375\n",
            "Training epoch 267/1000000, d_loss: 5.333412170410156,  g_loss: -18.526283264160156\n",
            "Training epoch 268/1000000, d_loss: -107.72872161865234,  g_loss: -81.1972885131836\n",
            "Training epoch 269/1000000, d_loss: -98.53490447998047,  g_loss: -7.805821418762207\n",
            "Training epoch 270/1000000, d_loss: -128.2671661376953,  g_loss: 112.75425720214844\n",
            "Training epoch 271/1000000, d_loss: -5.130100250244141,  g_loss: 16.631072998046875\n",
            "Training epoch 272/1000000, d_loss: -52.28785705566406,  g_loss: -4.693083763122559\n",
            "Training epoch 273/1000000, d_loss: -69.60772705078125,  g_loss: -23.27178382873535\n",
            "Training epoch 274/1000000, d_loss: -69.39285278320312,  g_loss: 156.77325439453125\n",
            "Training epoch 275/1000000, d_loss: -113.97710418701172,  g_loss: -19.387826919555664\n",
            "Training epoch 276/1000000, d_loss: -995.3590087890625,  g_loss: -471.7666015625\n",
            "Training epoch 277/1000000, d_loss: 40.02119445800781,  g_loss: -107.2840347290039\n",
            "Training epoch 278/1000000, d_loss: -33.011199951171875,  g_loss: 217.170166015625\n",
            "Training epoch 279/1000000, d_loss: -94.6700668334961,  g_loss: 140.2620849609375\n",
            "Training epoch 280/1000000, d_loss: -91.54723358154297,  g_loss: 320.87750244140625\n",
            "Training epoch 281/1000000, d_loss: -144.19972229003906,  g_loss: 365.19195556640625\n",
            "Training epoch 282/1000000, d_loss: 2.933307647705078,  g_loss: 72.33378601074219\n",
            "Training epoch 283/1000000, d_loss: -89.06591033935547,  g_loss: 103.2748031616211\n",
            "Training epoch 284/1000000, d_loss: -41.39990234375,  g_loss: 95.34963989257812\n",
            "Training epoch 285/1000000, d_loss: -51.111061096191406,  g_loss: 78.81837463378906\n",
            "Training epoch 286/1000000, d_loss: -173.05519104003906,  g_loss: 16.861352920532227\n",
            "Training epoch 287/1000000, d_loss: -129.42807006835938,  g_loss: 6.438722610473633\n",
            "Training epoch 288/1000000, d_loss: -72.89826965332031,  g_loss: 41.27434158325195\n",
            "Training epoch 289/1000000, d_loss: -285.07489013671875,  g_loss: -120.41504669189453\n",
            "Training epoch 290/1000000, d_loss: -14.527427673339844,  g_loss: 15.90078353881836\n",
            "Training epoch 291/1000000, d_loss: -62.80724334716797,  g_loss: -4.596827983856201\n",
            "Training epoch 292/1000000, d_loss: -66.88594055175781,  g_loss: 64.32644653320312\n",
            "Training epoch 293/1000000, d_loss: -148.21263122558594,  g_loss: -16.210308074951172\n",
            "Training epoch 294/1000000, d_loss: -56.6139030456543,  g_loss: -2.805316925048828\n",
            "Training epoch 295/1000000, d_loss: -33.95367431640625,  g_loss: 55.178611755371094\n",
            "Training epoch 296/1000000, d_loss: -89.57039642333984,  g_loss: 56.506473541259766\n",
            "Training epoch 297/1000000, d_loss: -160.0297393798828,  g_loss: -7.925227642059326\n",
            "Training epoch 298/1000000, d_loss: -214.20864868164062,  g_loss: -78.95854187011719\n",
            "Training epoch 299/1000000, d_loss: -719.9332275390625,  g_loss: -534.2071533203125\n",
            "Training epoch 300/1000000, d_loss: -423.86822509765625,  g_loss: -252.87013244628906\n",
            "Training epoch 301/1000000, d_loss: -6.64434814453125,  g_loss: -70.31410217285156\n",
            "Training epoch 302/1000000, d_loss: -85.39144897460938,  g_loss: -48.03046417236328\n",
            "Training epoch 303/1000000, d_loss: -84.79096221923828,  g_loss: -129.6572723388672\n",
            "Training epoch 304/1000000, d_loss: -206.5373077392578,  g_loss: -121.80415344238281\n",
            "Training epoch 305/1000000, d_loss: 16.254302978515625,  g_loss: -142.74559020996094\n",
            "Training epoch 306/1000000, d_loss: -146.05657958984375,  g_loss: -70.60387420654297\n",
            "Training epoch 307/1000000, d_loss: -35.00993347167969,  g_loss: -60.54906463623047\n",
            "Training epoch 308/1000000, d_loss: -2.1501150131225586,  g_loss: -35.31553649902344\n",
            "Training epoch 309/1000000, d_loss: -21.789169311523438,  g_loss: 14.293923377990723\n",
            "Training epoch 310/1000000, d_loss: -125.57070922851562,  g_loss: 9.574739456176758\n",
            "Training epoch 311/1000000, d_loss: 1.2157058715820312,  g_loss: 95.41432189941406\n",
            "Training epoch 312/1000000, d_loss: -174.16310119628906,  g_loss: -16.891860961914062\n",
            "Training epoch 313/1000000, d_loss: -80.03643798828125,  g_loss: 51.13612365722656\n",
            "Training epoch 314/1000000, d_loss: -165.12979125976562,  g_loss: -81.80125427246094\n",
            "Training epoch 315/1000000, d_loss: -151.77621459960938,  g_loss: -89.30459594726562\n",
            "Training epoch 316/1000000, d_loss: -38.95718765258789,  g_loss: 129.909912109375\n",
            "Training epoch 317/1000000, d_loss: -80.30585479736328,  g_loss: 125.31961059570312\n",
            "Training epoch 318/1000000, d_loss: 544.2503662109375,  g_loss: 31.177989959716797\n",
            "Training epoch 319/1000000, d_loss: -69.22857666015625,  g_loss: 8.667675018310547\n",
            "Training epoch 320/1000000, d_loss: -98.77189636230469,  g_loss: -16.023338317871094\n",
            "Training epoch 321/1000000, d_loss: -101.12136840820312,  g_loss: -13.231624603271484\n",
            "Training epoch 322/1000000, d_loss: -60.83015441894531,  g_loss: -80.55564880371094\n",
            "Training epoch 323/1000000, d_loss: -67.59800720214844,  g_loss: -20.089256286621094\n",
            "Training epoch 324/1000000, d_loss: -53.82735061645508,  g_loss: 83.52032470703125\n",
            "Training epoch 325/1000000, d_loss: -50.97209930419922,  g_loss: 85.89476013183594\n",
            "Training epoch 326/1000000, d_loss: -64.38221740722656,  g_loss: 8.493595123291016\n",
            "Training epoch 327/1000000, d_loss: -178.45840454101562,  g_loss: -13.819608688354492\n",
            "Training epoch 328/1000000, d_loss: -63.04705047607422,  g_loss: 73.73238372802734\n",
            "Training epoch 329/1000000, d_loss: -751.0316162109375,  g_loss: -140.27938842773438\n",
            "Training epoch 330/1000000, d_loss: -86.93566131591797,  g_loss: 0.25562572479248047\n",
            "Training epoch 331/1000000, d_loss: -199.78543090820312,  g_loss: -56.90032958984375\n",
            "Training epoch 332/1000000, d_loss: 15.077691078186035,  g_loss: -82.19709777832031\n",
            "Training epoch 333/1000000, d_loss: -3.8369369506835938,  g_loss: 85.29202270507812\n",
            "Training epoch 334/1000000, d_loss: -45.35974884033203,  g_loss: 94.44661712646484\n",
            "Training epoch 335/1000000, d_loss: -64.33914947509766,  g_loss: 70.76636505126953\n",
            "Training epoch 336/1000000, d_loss: -163.87701416015625,  g_loss: 50.45359802246094\n",
            "Training epoch 337/1000000, d_loss: -139.8636932373047,  g_loss: 100.57281494140625\n",
            "Training epoch 338/1000000, d_loss: -74.6775131225586,  g_loss: 356.025146484375\n",
            "Training epoch 339/1000000, d_loss: -141.36717224121094,  g_loss: 166.05718994140625\n",
            "Training epoch 340/1000000, d_loss: -87.86911010742188,  g_loss: 13.41301155090332\n",
            "Training epoch 341/1000000, d_loss: -61.038124084472656,  g_loss: 171.09622192382812\n",
            "Training epoch 342/1000000, d_loss: -108.2859878540039,  g_loss: 138.04019165039062\n",
            "Training epoch 343/1000000, d_loss: -177.97354125976562,  g_loss: -51.37823486328125\n",
            "Training epoch 344/1000000, d_loss: -129.7698211669922,  g_loss: 52.365272521972656\n",
            "Training epoch 345/1000000, d_loss: -109.48870849609375,  g_loss: 8.881274223327637\n",
            "Training epoch 346/1000000, d_loss: -60.449893951416016,  g_loss: 119.02944946289062\n",
            "Training epoch 347/1000000, d_loss: 5.536590576171875,  g_loss: 122.92735290527344\n",
            "Training epoch 348/1000000, d_loss: -39.41896438598633,  g_loss: 174.122314453125\n",
            "Training epoch 349/1000000, d_loss: -110.24995422363281,  g_loss: 57.286685943603516\n",
            "Training epoch 350/1000000, d_loss: -123.71340942382812,  g_loss: 35.417762756347656\n",
            "Training epoch 351/1000000, d_loss: -83.03227233886719,  g_loss: -2.1808114051818848\n",
            "Training epoch 352/1000000, d_loss: -483.88739013671875,  g_loss: -183.58761596679688\n",
            "Training epoch 353/1000000, d_loss: -196.20648193359375,  g_loss: -103.54457092285156\n",
            "Training epoch 354/1000000, d_loss: -132.6436767578125,  g_loss: 97.54463195800781\n",
            "Training epoch 355/1000000, d_loss: -107.79116821289062,  g_loss: 176.37722778320312\n",
            "Training epoch 356/1000000, d_loss: -111.95578002929688,  g_loss: 146.0096893310547\n",
            "Training epoch 357/1000000, d_loss: 8.484771728515625,  g_loss: 57.1043586730957\n",
            "Training epoch 358/1000000, d_loss: -52.47283935546875,  g_loss: 22.047466278076172\n",
            "Training epoch 359/1000000, d_loss: -31.025053024291992,  g_loss: 11.635883331298828\n",
            "Training epoch 360/1000000, d_loss: -69.2529067993164,  g_loss: -89.3824691772461\n",
            "Training epoch 361/1000000, d_loss: -705.988525390625,  g_loss: -403.38348388671875\n",
            "Training epoch 362/1000000, d_loss: -81.06840515136719,  g_loss: 97.23023223876953\n",
            "Training epoch 363/1000000, d_loss: -19.027999877929688,  g_loss: 232.84014892578125\n",
            "Training epoch 364/1000000, d_loss: -66.86592864990234,  g_loss: 349.73504638671875\n",
            "Training epoch 365/1000000, d_loss: 0.26480865478515625,  g_loss: 242.32872009277344\n",
            "Training epoch 366/1000000, d_loss: -4.53147029876709,  g_loss: 180.924560546875\n",
            "Training epoch 367/1000000, d_loss: -59.96200942993164,  g_loss: 152.70291137695312\n",
            "Training epoch 368/1000000, d_loss: -102.78147888183594,  g_loss: 162.55191040039062\n",
            "Training epoch 369/1000000, d_loss: -177.51544189453125,  g_loss: 94.75830078125\n",
            "Training epoch 370/1000000, d_loss: -243.02346801757812,  g_loss: -72.8978500366211\n",
            "Training epoch 371/1000000, d_loss: -51.617103576660156,  g_loss: 358.4552917480469\n",
            "Training epoch 372/1000000, d_loss: -305.0611572265625,  g_loss: 505.779541015625\n",
            "Training epoch 373/1000000, d_loss: -163.7615509033203,  g_loss: 493.51141357421875\n",
            "Training epoch 374/1000000, d_loss: -33.93572998046875,  g_loss: 400.796875\n",
            "Training epoch 375/1000000, d_loss: -63.50969696044922,  g_loss: 76.73728942871094\n",
            "Training epoch 376/1000000, d_loss: -159.60159301757812,  g_loss: 36.80392074584961\n",
            "Training epoch 377/1000000, d_loss: -489.17840576171875,  g_loss: -160.86578369140625\n",
            "Training epoch 378/1000000, d_loss: 43.11778259277344,  g_loss: -22.395824432373047\n",
            "Training epoch 379/1000000, d_loss: 25.267807006835938,  g_loss: 59.21240997314453\n",
            "Training epoch 380/1000000, d_loss: -33.088897705078125,  g_loss: 6.250368595123291\n",
            "Training epoch 381/1000000, d_loss: -258.4527587890625,  g_loss: -61.82548141479492\n",
            "Training epoch 382/1000000, d_loss: -1481.27001953125,  g_loss: -837.0931396484375\n",
            "Training epoch 383/1000000, d_loss: -18.829055786132812,  g_loss: 143.88812255859375\n",
            "Training epoch 384/1000000, d_loss: -82.22016906738281,  g_loss: 142.25384521484375\n",
            "Training epoch 385/1000000, d_loss: 3.852245330810547,  g_loss: 316.8251953125\n",
            "Training epoch 386/1000000, d_loss: -147.2213134765625,  g_loss: 538.873291015625\n",
            "Training epoch 387/1000000, d_loss: 174.95370483398438,  g_loss: 116.22883605957031\n",
            "Training epoch 388/1000000, d_loss: -56.83909606933594,  g_loss: 217.7483367919922\n",
            "Training epoch 389/1000000, d_loss: -71.40221405029297,  g_loss: 182.23316955566406\n",
            "Training epoch 390/1000000, d_loss: -32.440399169921875,  g_loss: 85.48433685302734\n",
            "Training epoch 391/1000000, d_loss: -63.90138626098633,  g_loss: 50.19189453125\n",
            "Training epoch 392/1000000, d_loss: -168.1997528076172,  g_loss: -152.09408569335938\n",
            "Training epoch 393/1000000, d_loss: -18.757251739501953,  g_loss: 127.77890014648438\n",
            "Training epoch 394/1000000, d_loss: -97.95828247070312,  g_loss: 119.43155670166016\n",
            "Training epoch 395/1000000, d_loss: -35.976409912109375,  g_loss: 151.1669464111328\n",
            "Training epoch 396/1000000, d_loss: -71.96363830566406,  g_loss: 242.05838012695312\n",
            "Training epoch 397/1000000, d_loss: -79.5689697265625,  g_loss: 92.05711364746094\n",
            "Training epoch 398/1000000, d_loss: -49.13519287109375,  g_loss: 126.70846557617188\n",
            "Training epoch 399/1000000, d_loss: -876.2031860351562,  g_loss: -278.6478576660156\n",
            "Training epoch 400/1000000, d_loss: -39.84952926635742,  g_loss: -7.698178768157959\n",
            "Training epoch 401/1000000, d_loss: -70.36788177490234,  g_loss: -0.3463163375854492\n",
            "Training epoch 402/1000000, d_loss: -27.07018280029297,  g_loss: 21.26913833618164\n",
            "Training epoch 403/1000000, d_loss: -110.4951400756836,  g_loss: 98.58396911621094\n",
            "Training epoch 404/1000000, d_loss: -90.47171020507812,  g_loss: 34.68706512451172\n",
            "Training epoch 405/1000000, d_loss: -497.8248291015625,  g_loss: -245.79098510742188\n",
            "Training epoch 406/1000000, d_loss: -207.0323486328125,  g_loss: -219.67274475097656\n",
            "Training epoch 407/1000000, d_loss: -159.2901611328125,  g_loss: 449.1602478027344\n",
            "Training epoch 408/1000000, d_loss: -133.09950256347656,  g_loss: 452.6251220703125\n",
            "Training epoch 409/1000000, d_loss: -92.85774230957031,  g_loss: 289.81890869140625\n",
            "Training epoch 410/1000000, d_loss: -117.62211608886719,  g_loss: 300.3946533203125\n",
            "Training epoch 411/1000000, d_loss: -44.082923889160156,  g_loss: 93.9670181274414\n",
            "Training epoch 412/1000000, d_loss: -105.12919616699219,  g_loss: 145.849609375\n",
            "Training epoch 413/1000000, d_loss: -41.34053039550781,  g_loss: 151.89862060546875\n",
            "Training epoch 414/1000000, d_loss: -161.00405883789062,  g_loss: -68.42637634277344\n",
            "Training epoch 415/1000000, d_loss: -205.5593719482422,  g_loss: -34.5981559753418\n",
            "Training epoch 416/1000000, d_loss: -62.21173858642578,  g_loss: 97.23307037353516\n",
            "Training epoch 417/1000000, d_loss: -95.04693603515625,  g_loss: 191.50892639160156\n",
            "Training epoch 418/1000000, d_loss: 83.21713256835938,  g_loss: 186.16123962402344\n",
            "Training epoch 419/1000000, d_loss: -192.89002990722656,  g_loss: -47.70606231689453\n",
            "Training epoch 420/1000000, d_loss: -77.60489654541016,  g_loss: 20.310606002807617\n",
            "Training epoch 421/1000000, d_loss: -83.48982238769531,  g_loss: 259.7652587890625\n",
            "Training epoch 422/1000000, d_loss: -94.24518585205078,  g_loss: 218.39195251464844\n",
            "Training epoch 423/1000000, d_loss: -25.507417678833008,  g_loss: 68.85066986083984\n",
            "Training epoch 424/1000000, d_loss: -94.94149780273438,  g_loss: 54.31992721557617\n",
            "Training epoch 425/1000000, d_loss: -120.54360961914062,  g_loss: -52.54629135131836\n",
            "Training epoch 426/1000000, d_loss: -92.29700469970703,  g_loss: 5.306624889373779\n",
            "Training epoch 427/1000000, d_loss: -91.48358154296875,  g_loss: -12.999083518981934\n",
            "Training epoch 428/1000000, d_loss: -68.10904693603516,  g_loss: 28.33469581604004\n",
            "Training epoch 429/1000000, d_loss: -117.55085754394531,  g_loss: -35.320045471191406\n",
            "Training epoch 430/1000000, d_loss: -159.11434936523438,  g_loss: -374.526123046875\n",
            "Training epoch 431/1000000, d_loss: -34.92226028442383,  g_loss: 41.41986083984375\n",
            "Training epoch 432/1000000, d_loss: 2.050273895263672,  g_loss: 40.98575973510742\n",
            "Training epoch 433/1000000, d_loss: -14.369504928588867,  g_loss: 56.41340637207031\n",
            "Training epoch 434/1000000, d_loss: -53.00756072998047,  g_loss: -13.642038345336914\n",
            "Training epoch 435/1000000, d_loss: -33.42450714111328,  g_loss: -30.838356018066406\n",
            "Training epoch 436/1000000, d_loss: -46.61097717285156,  g_loss: 50.86849594116211\n",
            "Training epoch 437/1000000, d_loss: -20.497291564941406,  g_loss: 59.108795166015625\n",
            "Training epoch 438/1000000, d_loss: -120.58447265625,  g_loss: -19.152099609375\n",
            "Training epoch 439/1000000, d_loss: -0.7351846694946289,  g_loss: 27.98003387451172\n",
            "Training epoch 440/1000000, d_loss: -98.66178131103516,  g_loss: 146.35655212402344\n",
            "Training epoch 441/1000000, d_loss: -71.17133331298828,  g_loss: 34.6641731262207\n",
            "Training epoch 442/1000000, d_loss: -72.54891967773438,  g_loss: 21.070186614990234\n",
            "Training epoch 443/1000000, d_loss: -3.1066370010375977,  g_loss: 18.990713119506836\n",
            "Training epoch 444/1000000, d_loss: -106.49839782714844,  g_loss: -80.6681137084961\n",
            "Training epoch 445/1000000, d_loss: -79.42943572998047,  g_loss: -53.15961456298828\n",
            "Training epoch 446/1000000, d_loss: -102.35848999023438,  g_loss: 0.10834884643554688\n",
            "Training epoch 447/1000000, d_loss: -54.53400421142578,  g_loss: 111.55680847167969\n",
            "Training epoch 448/1000000, d_loss: -85.29061126708984,  g_loss: -26.063232421875\n",
            "Training epoch 449/1000000, d_loss: -187.10890197753906,  g_loss: -64.10650634765625\n",
            "Training epoch 450/1000000, d_loss: -134.1457061767578,  g_loss: -75.02946472167969\n",
            "Training epoch 451/1000000, d_loss: -87.5843734741211,  g_loss: -25.78130531311035\n",
            "Training epoch 452/1000000, d_loss: -35.01675796508789,  g_loss: -25.56112289428711\n",
            "Training epoch 453/1000000, d_loss: -49.90623474121094,  g_loss: 152.724365234375\n",
            "Training epoch 454/1000000, d_loss: -65.43079376220703,  g_loss: 159.66514587402344\n",
            "Training epoch 455/1000000, d_loss: -38.912994384765625,  g_loss: 140.14120483398438\n",
            "Training epoch 456/1000000, d_loss: -135.73202514648438,  g_loss: 49.56949996948242\n",
            "Training epoch 457/1000000, d_loss: 274.4764099121094,  g_loss: 156.61495971679688\n",
            "Training epoch 458/1000000, d_loss: -116.3933334350586,  g_loss: 220.99960327148438\n",
            "Training epoch 459/1000000, d_loss: -140.04312133789062,  g_loss: 89.551025390625\n",
            "Training epoch 460/1000000, d_loss: -104.47032928466797,  g_loss: 129.37991333007812\n",
            "Training epoch 461/1000000, d_loss: -94.7241439819336,  g_loss: 215.39224243164062\n",
            "Training epoch 462/1000000, d_loss: -147.0505828857422,  g_loss: -70.40193939208984\n",
            "Training epoch 463/1000000, d_loss: -78.21161651611328,  g_loss: 11.269461631774902\n",
            "Training epoch 464/1000000, d_loss: -93.23689270019531,  g_loss: 97.37063598632812\n",
            "Training epoch 465/1000000, d_loss: -85.079345703125,  g_loss: 141.12606811523438\n",
            "Training epoch 466/1000000, d_loss: -41.52586364746094,  g_loss: 9.335500717163086\n",
            "Training epoch 467/1000000, d_loss: -174.1526336669922,  g_loss: -27.217281341552734\n",
            "Training epoch 468/1000000, d_loss: 3.1008129119873047,  g_loss: 44.27355194091797\n",
            "Training epoch 469/1000000, d_loss: -134.41586303710938,  g_loss: -75.80526733398438\n",
            "Training epoch 470/1000000, d_loss: -261.4596252441406,  g_loss: -256.65240478515625\n",
            "Training epoch 471/1000000, d_loss: -19.289953231811523,  g_loss: 20.670103073120117\n",
            "Training epoch 472/1000000, d_loss: -194.1314697265625,  g_loss: -77.5059585571289\n",
            "Training epoch 473/1000000, d_loss: -155.94155883789062,  g_loss: -80.61024475097656\n",
            "Training epoch 474/1000000, d_loss: -1068.87744140625,  g_loss: -710.768798828125\n",
            "Training epoch 475/1000000, d_loss: 124.81065368652344,  g_loss: -294.94464111328125\n",
            "Training epoch 476/1000000, d_loss: 75.84081268310547,  g_loss: -64.12693786621094\n",
            "Training epoch 477/1000000, d_loss: -89.69647979736328,  g_loss: 241.48495483398438\n",
            "Training epoch 478/1000000, d_loss: -183.1992950439453,  g_loss: 327.86798095703125\n",
            "Training epoch 479/1000000, d_loss: -150.20816040039062,  g_loss: 283.2006530761719\n",
            "Training epoch 480/1000000, d_loss: -305.73431396484375,  g_loss: 702.3310546875\n",
            "Training epoch 481/1000000, d_loss: -20.177001953125,  g_loss: 172.7119598388672\n",
            "Training epoch 482/1000000, d_loss: -124.55728149414062,  g_loss: 381.455810546875\n",
            "Training epoch 483/1000000, d_loss: 5.551582336425781,  g_loss: -0.07385730743408203\n",
            "Training epoch 484/1000000, d_loss: -180.27955627441406,  g_loss: -170.24722290039062\n",
            "Training epoch 485/1000000, d_loss: -776.1090087890625,  g_loss: -351.12322998046875\n",
            "Training epoch 486/1000000, d_loss: -10.39520263671875,  g_loss: -190.76425170898438\n",
            "Training epoch 487/1000000, d_loss: -38.48789978027344,  g_loss: -14.952810287475586\n",
            "Training epoch 488/1000000, d_loss: -41.12408447265625,  g_loss: -42.90856170654297\n",
            "Training epoch 489/1000000, d_loss: -21.550350189208984,  g_loss: 29.96318817138672\n",
            "Training epoch 490/1000000, d_loss: -54.028480529785156,  g_loss: 26.832429885864258\n",
            "Training epoch 491/1000000, d_loss: -108.18190002441406,  g_loss: -52.3793830871582\n",
            "Training epoch 492/1000000, d_loss: -80.84207153320312,  g_loss: -62.753662109375\n",
            "Training epoch 493/1000000, d_loss: -142.08859252929688,  g_loss: 231.15065002441406\n",
            "Training epoch 494/1000000, d_loss: -42.75282287597656,  g_loss: 294.1519470214844\n",
            "Training epoch 495/1000000, d_loss: -178.03065490722656,  g_loss: 501.72637939453125\n",
            "Training epoch 496/1000000, d_loss: -24.489845275878906,  g_loss: 449.3262023925781\n",
            "Training epoch 497/1000000, d_loss: -99.67915344238281,  g_loss: 285.68731689453125\n",
            "Training epoch 498/1000000, d_loss: -272.13055419921875,  g_loss: -40.11932373046875\n",
            "Training epoch 499/1000000, d_loss: -70.40071105957031,  g_loss: 105.43305969238281\n",
            "Training epoch 500/1000000, d_loss: -317.3791809082031,  g_loss: -141.07960510253906\n",
            "Training epoch 501/1000000, d_loss: -80.87966918945312,  g_loss: 118.75950622558594\n",
            "Training epoch 502/1000000, d_loss: -30.77107048034668,  g_loss: 121.576171875\n",
            "Training epoch 503/1000000, d_loss: -134.70913696289062,  g_loss: 36.418704986572266\n",
            "Training epoch 504/1000000, d_loss: -161.94381713867188,  g_loss: -27.267902374267578\n",
            "Training epoch 505/1000000, d_loss: -78.65333557128906,  g_loss: -96.99472045898438\n",
            "Training epoch 506/1000000, d_loss: -98.78861999511719,  g_loss: -2.0517139434814453\n",
            "Training epoch 507/1000000, d_loss: -103.23504638671875,  g_loss: 64.91869354248047\n",
            "Training epoch 508/1000000, d_loss: -73.15022277832031,  g_loss: -44.923030853271484\n",
            "Training epoch 509/1000000, d_loss: -54.016578674316406,  g_loss: 3.000758647918701\n",
            "Training epoch 510/1000000, d_loss: -29.103046417236328,  g_loss: 0.6346421241760254\n",
            "Training epoch 511/1000000, d_loss: -207.81552124023438,  g_loss: -128.9453887939453\n",
            "Training epoch 512/1000000, d_loss: -108.69255065917969,  g_loss: -77.50137329101562\n",
            "Training epoch 513/1000000, d_loss: -30.38811492919922,  g_loss: -49.360809326171875\n",
            "Training epoch 514/1000000, d_loss: -152.76687622070312,  g_loss: -104.91168975830078\n",
            "Training epoch 515/1000000, d_loss: -94.07353210449219,  g_loss: 32.82743453979492\n",
            "Training epoch 516/1000000, d_loss: -16.67264175415039,  g_loss: 29.510562896728516\n",
            "Training epoch 517/1000000, d_loss: -68.68150329589844,  g_loss: 17.529735565185547\n",
            "Training epoch 518/1000000, d_loss: -70.00828552246094,  g_loss: -16.19875717163086\n",
            "Training epoch 519/1000000, d_loss: -39.67262268066406,  g_loss: -49.9844970703125\n",
            "Training epoch 520/1000000, d_loss: -59.89892578125,  g_loss: -57.04841613769531\n",
            "Training epoch 521/1000000, d_loss: -140.2372283935547,  g_loss: -58.136138916015625\n",
            "Training epoch 522/1000000, d_loss: -176.69580078125,  g_loss: -170.94467163085938\n",
            "Training epoch 523/1000000, d_loss: -129.9759521484375,  g_loss: -125.0934066772461\n",
            "Training epoch 524/1000000, d_loss: -49.948150634765625,  g_loss: -92.63362121582031\n",
            "Training epoch 525/1000000, d_loss: -74.40180969238281,  g_loss: 20.1237735748291\n",
            "Training epoch 526/1000000, d_loss: -55.11194610595703,  g_loss: 53.695770263671875\n",
            "Training epoch 527/1000000, d_loss: -45.35466766357422,  g_loss: -27.69676971435547\n",
            "Training epoch 528/1000000, d_loss: -153.80584716796875,  g_loss: -105.60704040527344\n",
            "Training epoch 529/1000000, d_loss: -107.33171844482422,  g_loss: 36.062286376953125\n",
            "Training epoch 530/1000000, d_loss: -99.80766296386719,  g_loss: -78.19023132324219\n",
            "Training epoch 531/1000000, d_loss: -71.20844268798828,  g_loss: 6.757195472717285\n",
            "Training epoch 532/1000000, d_loss: -137.82644653320312,  g_loss: -80.13906860351562\n",
            "Training epoch 533/1000000, d_loss: -313.7017822265625,  g_loss: -368.33416748046875\n",
            "Training epoch 534/1000000, d_loss: -106.37991333007812,  g_loss: -92.47215270996094\n",
            "Training epoch 535/1000000, d_loss: 19.409942626953125,  g_loss: -34.77574157714844\n",
            "Training epoch 536/1000000, d_loss: -151.55410766601562,  g_loss: -190.38180541992188\n",
            "Training epoch 537/1000000, d_loss: -76.529296875,  g_loss: -46.30497360229492\n",
            "Training epoch 538/1000000, d_loss: -38.360355377197266,  g_loss: 45.67332458496094\n",
            "Training epoch 539/1000000, d_loss: -108.77458190917969,  g_loss: 117.67658233642578\n",
            "Training epoch 540/1000000, d_loss: -123.80461120605469,  g_loss: 229.4279022216797\n",
            "Training epoch 541/1000000, d_loss: -90.09098815917969,  g_loss: 10.921464920043945\n",
            "Training epoch 542/1000000, d_loss: -45.70414733886719,  g_loss: 193.68653869628906\n",
            "Training epoch 543/1000000, d_loss: -14.812103271484375,  g_loss: 107.69986724853516\n",
            "Training epoch 544/1000000, d_loss: -17.155471801757812,  g_loss: 119.11203002929688\n",
            "Training epoch 545/1000000, d_loss: -60.003173828125,  g_loss: 68.31008911132812\n",
            "Training epoch 546/1000000, d_loss: -56.056251525878906,  g_loss: 56.128074645996094\n",
            "Training epoch 547/1000000, d_loss: -134.6658935546875,  g_loss: -11.212197303771973\n",
            "Training epoch 548/1000000, d_loss: -92.53919982910156,  g_loss: 56.12969970703125\n",
            "Training epoch 549/1000000, d_loss: -38.43687057495117,  g_loss: -9.002866744995117\n",
            "Training epoch 550/1000000, d_loss: -66.2984848022461,  g_loss: -64.56857299804688\n",
            "Training epoch 551/1000000, d_loss: -80.25599670410156,  g_loss: 7.228569984436035\n",
            "Training epoch 552/1000000, d_loss: -74.90359497070312,  g_loss: 18.33116912841797\n",
            "Training epoch 553/1000000, d_loss: -50.34696578979492,  g_loss: 133.57037353515625\n",
            "Training epoch 554/1000000, d_loss: -35.85242462158203,  g_loss: 44.03717041015625\n",
            "Training epoch 555/1000000, d_loss: -38.838470458984375,  g_loss: 66.84793090820312\n",
            "Training epoch 556/1000000, d_loss: -58.06763458251953,  g_loss: 100.8066177368164\n",
            "Training epoch 557/1000000, d_loss: -24.33919906616211,  g_loss: 29.14723777770996\n",
            "Training epoch 558/1000000, d_loss: -224.5020294189453,  g_loss: -106.44546508789062\n",
            "Training epoch 559/1000000, d_loss: -73.36343383789062,  g_loss: -68.25372314453125\n",
            "Training epoch 560/1000000, d_loss: -65.47370147705078,  g_loss: -3.5485777854919434\n",
            "Training epoch 561/1000000, d_loss: -56.87876892089844,  g_loss: -63.868106842041016\n",
            "Training epoch 562/1000000, d_loss: -181.1141357421875,  g_loss: -173.77816772460938\n",
            "Training epoch 563/1000000, d_loss: -811.9530029296875,  g_loss: -451.12860107421875\n",
            "Training epoch 564/1000000, d_loss: -40.16520690917969,  g_loss: -67.01573181152344\n",
            "Training epoch 565/1000000, d_loss: -189.06484985351562,  g_loss: -140.96978759765625\n",
            "Training epoch 566/1000000, d_loss: 33.183197021484375,  g_loss: 171.0497589111328\n",
            "Training epoch 567/1000000, d_loss: -271.67877197265625,  g_loss: 376.27069091796875\n",
            "Training epoch 568/1000000, d_loss: -102.9862060546875,  g_loss: -0.9855670928955078\n",
            "Training epoch 569/1000000, d_loss: -83.66493225097656,  g_loss: -2.5075950622558594\n",
            "Training epoch 570/1000000, d_loss: -86.141357421875,  g_loss: 83.59066772460938\n",
            "Training epoch 571/1000000, d_loss: -211.9383087158203,  g_loss: -45.639190673828125\n",
            "Training epoch 572/1000000, d_loss: -100.3175048828125,  g_loss: 50.30266571044922\n",
            "Training epoch 573/1000000, d_loss: -5.084583282470703,  g_loss: 37.91518783569336\n",
            "Training epoch 574/1000000, d_loss: -208.5320281982422,  g_loss: -102.5118408203125\n",
            "Training epoch 575/1000000, d_loss: -63.00825500488281,  g_loss: 378.84600830078125\n",
            "Training epoch 576/1000000, d_loss: -37.55015563964844,  g_loss: 21.117298126220703\n",
            "Training epoch 577/1000000, d_loss: -129.02069091796875,  g_loss: 162.6954803466797\n",
            "Training epoch 578/1000000, d_loss: -169.0225830078125,  g_loss: 372.3412170410156\n",
            "Training epoch 579/1000000, d_loss: -56.055328369140625,  g_loss: 111.3897705078125\n",
            "Training epoch 580/1000000, d_loss: -158.04885864257812,  g_loss: 398.2318115234375\n",
            "Training epoch 581/1000000, d_loss: -121.42138671875,  g_loss: 308.96075439453125\n",
            "Training epoch 582/1000000, d_loss: -61.230628967285156,  g_loss: 122.36746215820312\n",
            "Training epoch 583/1000000, d_loss: -119.52406311035156,  g_loss: 92.69891357421875\n",
            "Training epoch 584/1000000, d_loss: -233.2164306640625,  g_loss: 69.30525207519531\n",
            "Training epoch 585/1000000, d_loss: -55.81822204589844,  g_loss: 69.77861022949219\n",
            "Training epoch 586/1000000, d_loss: -95.65669250488281,  g_loss: 57.76212692260742\n",
            "Training epoch 587/1000000, d_loss: -0.13074493408203125,  g_loss: 205.1240997314453\n",
            "Training epoch 588/1000000, d_loss: -50.026737213134766,  g_loss: 129.21900939941406\n",
            "Training epoch 589/1000000, d_loss: 1.373311996459961,  g_loss: 99.94453430175781\n",
            "Training epoch 590/1000000, d_loss: -296.8243408203125,  g_loss: 36.76204299926758\n",
            "Training epoch 591/1000000, d_loss: -130.7144317626953,  g_loss: 31.21658706665039\n",
            "Training epoch 592/1000000, d_loss: -126.51826477050781,  g_loss: 67.46757507324219\n",
            "Training epoch 593/1000000, d_loss: -78.29658508300781,  g_loss: 160.9285888671875\n",
            "Training epoch 594/1000000, d_loss: -76.44975280761719,  g_loss: 202.6291046142578\n",
            "Training epoch 595/1000000, d_loss: -50.06332778930664,  g_loss: 132.40194702148438\n",
            "Training epoch 596/1000000, d_loss: -282.78326416015625,  g_loss: -225.76019287109375\n",
            "Training epoch 597/1000000, d_loss: -9.942203521728516,  g_loss: -17.714649200439453\n",
            "Training epoch 598/1000000, d_loss: -40.647743225097656,  g_loss: -8.193967819213867\n",
            "Training epoch 599/1000000, d_loss: -44.98103332519531,  g_loss: 36.47581481933594\n",
            "Training epoch 600/1000000, d_loss: -125.44593811035156,  g_loss: 52.48649978637695\n",
            "Training epoch 601/1000000, d_loss: -226.5726318359375,  g_loss: 23.1341552734375\n",
            "Training epoch 602/1000000, d_loss: -61.92564392089844,  g_loss: -7.721077919006348\n",
            "Training epoch 603/1000000, d_loss: -114.69489288330078,  g_loss: 125.333984375\n",
            "Training epoch 604/1000000, d_loss: -175.67745971679688,  g_loss: 21.679447174072266\n",
            "Training epoch 605/1000000, d_loss: -584.9646606445312,  g_loss: -439.99505615234375\n",
            "Training epoch 606/1000000, d_loss: -194.4976806640625,  g_loss: -211.45896911621094\n",
            "Training epoch 607/1000000, d_loss: -168.2738800048828,  g_loss: 380.9237365722656\n",
            "Training epoch 608/1000000, d_loss: -43.936588287353516,  g_loss: 202.08827209472656\n",
            "Training epoch 609/1000000, d_loss: -63.308250427246094,  g_loss: 239.41567993164062\n",
            "Training epoch 610/1000000, d_loss: -113.50523376464844,  g_loss: 140.1116180419922\n",
            "Training epoch 611/1000000, d_loss: -14.757407188415527,  g_loss: 192.43978881835938\n",
            "Training epoch 612/1000000, d_loss: -112.36903381347656,  g_loss: 424.88616943359375\n",
            "Training epoch 613/1000000, d_loss: -84.4210205078125,  g_loss: 201.86489868164062\n",
            "Training epoch 614/1000000, d_loss: -408.931396484375,  g_loss: -118.8319320678711\n",
            "Training epoch 615/1000000, d_loss: 4.0187530517578125,  g_loss: 155.20590209960938\n",
            "Training epoch 616/1000000, d_loss: -137.35177612304688,  g_loss: 55.44536590576172\n",
            "Training epoch 617/1000000, d_loss: -156.17471313476562,  g_loss: 27.144939422607422\n",
            "Training epoch 618/1000000, d_loss: -166.2947540283203,  g_loss: -54.19873809814453\n",
            "Training epoch 619/1000000, d_loss: -25.713333129882812,  g_loss: -97.7616958618164\n",
            "Training epoch 620/1000000, d_loss: -156.54685974121094,  g_loss: -118.78738403320312\n",
            "Training epoch 621/1000000, d_loss: -5.013648986816406,  g_loss: -107.49072265625\n",
            "Training epoch 622/1000000, d_loss: -185.60203552246094,  g_loss: -73.88938903808594\n",
            "Training epoch 623/1000000, d_loss: 16.678665161132812,  g_loss: 44.454925537109375\n",
            "Training epoch 624/1000000, d_loss: -235.0411376953125,  g_loss: -267.0301818847656\n",
            "Training epoch 625/1000000, d_loss: -248.2005615234375,  g_loss: 524.58984375\n",
            "Training epoch 626/1000000, d_loss: -41.47917175292969,  g_loss: 377.64556884765625\n",
            "Training epoch 627/1000000, d_loss: -131.43994140625,  g_loss: 49.83271026611328\n",
            "Training epoch 628/1000000, d_loss: -132.57803344726562,  g_loss: 51.23646926879883\n",
            "Training epoch 629/1000000, d_loss: -61.86754608154297,  g_loss: 184.59194946289062\n",
            "Training epoch 630/1000000, d_loss: -226.19161987304688,  g_loss: 41.61308288574219\n",
            "Training epoch 631/1000000, d_loss: -820.1495361328125,  g_loss: -487.4578857421875\n",
            "Training epoch 632/1000000, d_loss: -15.872329711914062,  g_loss: -139.4364776611328\n",
            "Training epoch 633/1000000, d_loss: -8.76531982421875,  g_loss: -79.47569274902344\n",
            "Training epoch 634/1000000, d_loss: -124.1563720703125,  g_loss: 63.198577880859375\n",
            "Training epoch 635/1000000, d_loss: -115.66410827636719,  g_loss: -7.847949028015137\n",
            "Training epoch 636/1000000, d_loss: -95.10513305664062,  g_loss: 389.9662780761719\n",
            "Training epoch 637/1000000, d_loss: -95.67752075195312,  g_loss: 375.0362548828125\n",
            "Training epoch 638/1000000, d_loss: -37.15364074707031,  g_loss: 383.5929260253906\n",
            "Training epoch 639/1000000, d_loss: -388.1026916503906,  g_loss: 33.31470489501953\n",
            "Training epoch 640/1000000, d_loss: -184.9168701171875,  g_loss: 21.082304000854492\n",
            "Training epoch 641/1000000, d_loss: -28.47711944580078,  g_loss: 26.97738265991211\n",
            "Training epoch 642/1000000, d_loss: -247.33628845214844,  g_loss: 11.1153564453125\n",
            "Training epoch 643/1000000, d_loss: -69.58929443359375,  g_loss: -0.15622246265411377\n",
            "Training epoch 644/1000000, d_loss: -100.79059600830078,  g_loss: 8.655339241027832\n",
            "Training epoch 645/1000000, d_loss: -84.83497619628906,  g_loss: -32.84493637084961\n",
            "Training epoch 646/1000000, d_loss: -331.4986572265625,  g_loss: -197.897216796875\n",
            "Training epoch 647/1000000, d_loss: -60.105224609375,  g_loss: 191.01034545898438\n",
            "Training epoch 648/1000000, d_loss: 25.04828643798828,  g_loss: 49.78028869628906\n",
            "Training epoch 649/1000000, d_loss: -230.2997283935547,  g_loss: -27.06829071044922\n",
            "Training epoch 650/1000000, d_loss: -29.01481819152832,  g_loss: 122.12026977539062\n",
            "Training epoch 651/1000000, d_loss: -84.98579406738281,  g_loss: 182.79544067382812\n",
            "Training epoch 652/1000000, d_loss: -75.31898498535156,  g_loss: 265.5851745605469\n",
            "Training epoch 653/1000000, d_loss: -132.20750427246094,  g_loss: 425.6163330078125\n",
            "Training epoch 654/1000000, d_loss: -170.17686462402344,  g_loss: 301.01019287109375\n",
            "Training epoch 655/1000000, d_loss: -144.14427185058594,  g_loss: 67.28816986083984\n",
            "Training epoch 656/1000000, d_loss: -95.55674743652344,  g_loss: 75.50050354003906\n",
            "Training epoch 657/1000000, d_loss: -179.59446716308594,  g_loss: 37.85373306274414\n",
            "Training epoch 658/1000000, d_loss: -329.41851806640625,  g_loss: -220.8171844482422\n",
            "Training epoch 659/1000000, d_loss: -55.82378387451172,  g_loss: -29.41079330444336\n",
            "Training epoch 660/1000000, d_loss: -399.51739501953125,  g_loss: -216.28729248046875\n",
            "Training epoch 661/1000000, d_loss: -55.0959587097168,  g_loss: -1.5969419479370117\n",
            "Training epoch 662/1000000, d_loss: -137.18499755859375,  g_loss: 514.516845703125\n",
            "Training epoch 663/1000000, d_loss: -429.6511535644531,  g_loss: -95.52203369140625\n",
            "Training epoch 664/1000000, d_loss: -185.93328857421875,  g_loss: 484.1597900390625\n",
            "Training epoch 665/1000000, d_loss: 32.99522399902344,  g_loss: 254.96449279785156\n",
            "Training epoch 666/1000000, d_loss: -179.43197631835938,  g_loss: 229.3095703125\n",
            "Training epoch 667/1000000, d_loss: -169.7380828857422,  g_loss: 375.84417724609375\n",
            "Training epoch 668/1000000, d_loss: -152.69937133789062,  g_loss: 489.02264404296875\n",
            "Training epoch 669/1000000, d_loss: -98.81053161621094,  g_loss: 242.76397705078125\n",
            "Training epoch 670/1000000, d_loss: -28.155960083007812,  g_loss: 173.9603271484375\n",
            "Training epoch 671/1000000, d_loss: -32.79638671875,  g_loss: 174.29006958007812\n",
            "Training epoch 672/1000000, d_loss: -70.95250701904297,  g_loss: 215.9785614013672\n",
            "Training epoch 673/1000000, d_loss: 30.908065795898438,  g_loss: 247.9421844482422\n",
            "Training epoch 674/1000000, d_loss: -3.626729965209961,  g_loss: 115.04057312011719\n",
            "Training epoch 675/1000000, d_loss: -156.92283630371094,  g_loss: 178.69497680664062\n",
            "Training epoch 676/1000000, d_loss: -61.33277130126953,  g_loss: 199.14761352539062\n",
            "Training epoch 677/1000000, d_loss: -50.638038635253906,  g_loss: 150.9547119140625\n",
            "Training epoch 678/1000000, d_loss: -227.9352264404297,  g_loss: -395.9701232910156\n",
            "Training epoch 679/1000000, d_loss: -44.35499954223633,  g_loss: -175.44107055664062\n",
            "Training epoch 680/1000000, d_loss: 38.2008056640625,  g_loss: -82.16145324707031\n",
            "Training epoch 681/1000000, d_loss: -285.79241943359375,  g_loss: -291.1198425292969\n",
            "Training epoch 682/1000000, d_loss: -12.212326049804688,  g_loss: -116.8699951171875\n",
            "Training epoch 683/1000000, d_loss: -74.044921875,  g_loss: 12.460735321044922\n",
            "Training epoch 684/1000000, d_loss: -156.10205078125,  g_loss: -37.957855224609375\n",
            "Training epoch 685/1000000, d_loss: -46.79996109008789,  g_loss: 39.6214714050293\n",
            "Training epoch 686/1000000, d_loss: -183.68934631347656,  g_loss: 60.12742614746094\n",
            "Training epoch 687/1000000, d_loss: -196.44534301757812,  g_loss: -72.99058532714844\n",
            "Training epoch 688/1000000, d_loss: -572.2801513671875,  g_loss: -608.4969482421875\n",
            "Training epoch 689/1000000, d_loss: -17.855480194091797,  g_loss: -476.8291015625\n",
            "Training epoch 690/1000000, d_loss: -30.59326171875,  g_loss: 207.710693359375\n",
            "Training epoch 691/1000000, d_loss: -392.1485595703125,  g_loss: 912.517333984375\n",
            "Training epoch 692/1000000, d_loss: -59.03446578979492,  g_loss: 132.59347534179688\n",
            "Training epoch 693/1000000, d_loss: -236.6030731201172,  g_loss: -123.79017639160156\n",
            "Training epoch 694/1000000, d_loss: -76.05810546875,  g_loss: -21.167510986328125\n",
            "Training epoch 695/1000000, d_loss: -191.28717041015625,  g_loss: -161.7484893798828\n",
            "Training epoch 696/1000000, d_loss: 19.390972137451172,  g_loss: 67.87564849853516\n",
            "Training epoch 697/1000000, d_loss: -60.1624870300293,  g_loss: 123.21195220947266\n",
            "Training epoch 698/1000000, d_loss: -158.84190368652344,  g_loss: 128.76251220703125\n",
            "Training epoch 699/1000000, d_loss: -81.8931884765625,  g_loss: 194.7259063720703\n",
            "Training epoch 700/1000000, d_loss: -123.58263397216797,  g_loss: 188.15733337402344\n",
            "Training epoch 701/1000000, d_loss: -30.997526168823242,  g_loss: 96.84765625\n",
            "Training epoch 702/1000000, d_loss: -60.32533264160156,  g_loss: 33.55805206298828\n",
            "Training epoch 703/1000000, d_loss: -64.46045684814453,  g_loss: -22.28363037109375\n",
            "Training epoch 704/1000000, d_loss: -154.70321655273438,  g_loss: -90.96861267089844\n",
            "Training epoch 705/1000000, d_loss: -30.683780670166016,  g_loss: -33.81022262573242\n",
            "Training epoch 706/1000000, d_loss: -262.7933349609375,  g_loss: -195.66879272460938\n",
            "Training epoch 707/1000000, d_loss: -384.1481628417969,  g_loss: -360.86968994140625\n",
            "Training epoch 708/1000000, d_loss: -7.2219390869140625,  g_loss: -34.00038146972656\n",
            "Training epoch 709/1000000, d_loss: -90.36254119873047,  g_loss: -85.72037506103516\n",
            "Training epoch 710/1000000, d_loss: -97.29376983642578,  g_loss: -75.68964385986328\n",
            "Training epoch 711/1000000, d_loss: -91.60440063476562,  g_loss: -146.69174194335938\n",
            "Training epoch 712/1000000, d_loss: -148.79067993164062,  g_loss: -184.65902709960938\n",
            "Training epoch 713/1000000, d_loss: -329.78790283203125,  g_loss: -273.0150146484375\n",
            "Training epoch 714/1000000, d_loss: 17.260143280029297,  g_loss: 58.19248962402344\n",
            "Training epoch 715/1000000, d_loss: -139.55322265625,  g_loss: -24.732532501220703\n",
            "Training epoch 716/1000000, d_loss: -26.085956573486328,  g_loss: 114.43553924560547\n",
            "Training epoch 717/1000000, d_loss: -235.02664184570312,  g_loss: 15.255001068115234\n",
            "Training epoch 718/1000000, d_loss: -49.108116149902344,  g_loss: 48.670684814453125\n",
            "Training epoch 719/1000000, d_loss: -99.87240600585938,  g_loss: 166.1651153564453\n",
            "Training epoch 720/1000000, d_loss: 3.270009994506836,  g_loss: 62.388145446777344\n",
            "Training epoch 721/1000000, d_loss: -39.45774841308594,  g_loss: 13.399446487426758\n",
            "Training epoch 722/1000000, d_loss: -22.22171401977539,  g_loss: 20.143325805664062\n",
            "Training epoch 723/1000000, d_loss: -287.1433410644531,  g_loss: -157.9998321533203\n",
            "Training epoch 724/1000000, d_loss: -294.1119689941406,  g_loss: -225.44390869140625\n",
            "Training epoch 725/1000000, d_loss: -85.97689819335938,  g_loss: 109.68761444091797\n",
            "Training epoch 726/1000000, d_loss: -64.98172760009766,  g_loss: 120.07988739013672\n",
            "Training epoch 727/1000000, d_loss: -64.85836791992188,  g_loss: -43.18040084838867\n",
            "Training epoch 728/1000000, d_loss: -78.14256286621094,  g_loss: 0.2724456787109375\n",
            "Training epoch 729/1000000, d_loss: -20.743440628051758,  g_loss: 40.488426208496094\n",
            "Training epoch 730/1000000, d_loss: -149.99574279785156,  g_loss: 20.176729202270508\n",
            "Training epoch 731/1000000, d_loss: -202.44715881347656,  g_loss: -89.3023681640625\n",
            "Training epoch 732/1000000, d_loss: -98.29039001464844,  g_loss: -76.8077392578125\n",
            "Training epoch 733/1000000, d_loss: -68.9233627319336,  g_loss: 29.201093673706055\n",
            "Training epoch 734/1000000, d_loss: -159.6338348388672,  g_loss: -121.7625503540039\n",
            "Training epoch 735/1000000, d_loss: -480.5179138183594,  g_loss: -517.137939453125\n",
            "Training epoch 736/1000000, d_loss: -134.11904907226562,  g_loss: -352.38360595703125\n",
            "Training epoch 737/1000000, d_loss: -108.62589263916016,  g_loss: 0.01904296875\n",
            "Training epoch 738/1000000, d_loss: -267.0961608886719,  g_loss: -204.6510772705078\n",
            "Training epoch 739/1000000, d_loss: -193.04859924316406,  g_loss: 157.3677520751953\n",
            "Training epoch 740/1000000, d_loss: -130.94390869140625,  g_loss: 370.69476318359375\n",
            "Training epoch 741/1000000, d_loss: -129.78335571289062,  g_loss: 132.45242309570312\n",
            "Training epoch 742/1000000, d_loss: -224.94996643066406,  g_loss: 93.9302978515625\n",
            "Training epoch 743/1000000, d_loss: -524.1810913085938,  g_loss: -231.62139892578125\n",
            "Training epoch 744/1000000, d_loss: -53.61576461791992,  g_loss: 56.89765930175781\n",
            "Training epoch 745/1000000, d_loss: -688.7034912109375,  g_loss: -330.318115234375\n",
            "Training epoch 746/1000000, d_loss: 22.137714385986328,  g_loss: -139.6927490234375\n",
            "Training epoch 747/1000000, d_loss: -72.77582550048828,  g_loss: 4.823158264160156\n",
            "Training epoch 748/1000000, d_loss: 89.92703247070312,  g_loss: 126.09999084472656\n",
            "Training epoch 749/1000000, d_loss: -117.52493286132812,  g_loss: 199.48660278320312\n",
            "Training epoch 750/1000000, d_loss: -133.9691162109375,  g_loss: 294.3062744140625\n",
            "Training epoch 751/1000000, d_loss: -470.904296875,  g_loss: -173.7794952392578\n",
            "Training epoch 752/1000000, d_loss: -45.94825744628906,  g_loss: -22.950984954833984\n",
            "Training epoch 753/1000000, d_loss: -128.09048461914062,  g_loss: -50.238826751708984\n",
            "Training epoch 754/1000000, d_loss: -242.111572265625,  g_loss: -161.33763122558594\n",
            "Training epoch 755/1000000, d_loss: 373.13726806640625,  g_loss: 183.8258819580078\n",
            "Training epoch 756/1000000, d_loss: -66.02828979492188,  g_loss: 108.14608001708984\n",
            "Training epoch 757/1000000, d_loss: -120.58152770996094,  g_loss: 10.217342376708984\n",
            "Training epoch 758/1000000, d_loss: -52.58246612548828,  g_loss: 69.65789031982422\n",
            "Training epoch 759/1000000, d_loss: -247.63580322265625,  g_loss: -211.74754333496094\n",
            "Training epoch 760/1000000, d_loss: 88.83686065673828,  g_loss: -5.515119552612305\n",
            "Training epoch 761/1000000, d_loss: -63.47960662841797,  g_loss: -44.08454895019531\n",
            "Training epoch 762/1000000, d_loss: -238.4223175048828,  g_loss: -142.59991455078125\n",
            "Training epoch 763/1000000, d_loss: -54.083038330078125,  g_loss: 46.47312545776367\n",
            "Training epoch 764/1000000, d_loss: -320.5970153808594,  g_loss: -131.8028564453125\n",
            "Training epoch 765/1000000, d_loss: -125.1758804321289,  g_loss: -100.68289184570312\n",
            "Training epoch 766/1000000, d_loss: -164.42904663085938,  g_loss: 43.64897155761719\n",
            "Training epoch 767/1000000, d_loss: -92.56425476074219,  g_loss: 158.81173706054688\n",
            "Training epoch 768/1000000, d_loss: -50.03446578979492,  g_loss: 231.6048583984375\n",
            "Training epoch 769/1000000, d_loss: -6.474674224853516,  g_loss: 100.686767578125\n",
            "Training epoch 770/1000000, d_loss: -29.30364990234375,  g_loss: 60.662906646728516\n",
            "Training epoch 771/1000000, d_loss: -233.70999145507812,  g_loss: -89.61813354492188\n",
            "Training epoch 772/1000000, d_loss: 118.21482849121094,  g_loss: 31.440725326538086\n",
            "Training epoch 773/1000000, d_loss: -52.96760177612305,  g_loss: -18.425392150878906\n",
            "Training epoch 774/1000000, d_loss: -123.56283569335938,  g_loss: -17.70186996459961\n",
            "Training epoch 775/1000000, d_loss: -74.65465545654297,  g_loss: 44.33501434326172\n",
            "Training epoch 776/1000000, d_loss: -91.72465515136719,  g_loss: 90.47537231445312\n",
            "Training epoch 777/1000000, d_loss: -14.26839828491211,  g_loss: 27.533796310424805\n",
            "Training epoch 778/1000000, d_loss: -54.66339874267578,  g_loss: 65.57490539550781\n",
            "Training epoch 779/1000000, d_loss: -64.79975891113281,  g_loss: 113.09326934814453\n",
            "Training epoch 780/1000000, d_loss: -8.176599502563477,  g_loss: 84.84217071533203\n",
            "Training epoch 781/1000000, d_loss: -37.02113342285156,  g_loss: 6.666242599487305\n",
            "Training epoch 782/1000000, d_loss: -155.17735290527344,  g_loss: 65.56405639648438\n",
            "Training epoch 783/1000000, d_loss: -324.9094543457031,  g_loss: -140.41336059570312\n",
            "Training epoch 784/1000000, d_loss: -90.31591796875,  g_loss: -112.0383071899414\n",
            "Training epoch 785/1000000, d_loss: -98.34234619140625,  g_loss: -110.84022521972656\n",
            "Training epoch 786/1000000, d_loss: 30.028480529785156,  g_loss: -56.238731384277344\n",
            "Training epoch 787/1000000, d_loss: -40.6302490234375,  g_loss: -90.3686752319336\n",
            "Training epoch 788/1000000, d_loss: -27.335189819335938,  g_loss: -42.68328094482422\n",
            "Training epoch 789/1000000, d_loss: -648.8695678710938,  g_loss: -401.1193542480469\n",
            "Training epoch 790/1000000, d_loss: -19.507400512695312,  g_loss: -122.08216094970703\n",
            "Training epoch 791/1000000, d_loss: -23.446918487548828,  g_loss: -55.36127471923828\n",
            "Training epoch 792/1000000, d_loss: -156.49830627441406,  g_loss: 23.539867401123047\n",
            "Training epoch 793/1000000, d_loss: -139.86265563964844,  g_loss: 65.00494384765625\n",
            "Training epoch 794/1000000, d_loss: -82.41732788085938,  g_loss: 93.3646240234375\n",
            "Training epoch 795/1000000, d_loss: -164.39674377441406,  g_loss: 2.422880172729492\n",
            "Training epoch 796/1000000, d_loss: -231.48324584960938,  g_loss: -162.13929748535156\n",
            "Training epoch 797/1000000, d_loss: -219.777099609375,  g_loss: -55.67160415649414\n",
            "Training epoch 798/1000000, d_loss: -65.00875091552734,  g_loss: -34.29429626464844\n",
            "Training epoch 799/1000000, d_loss: -129.98306274414062,  g_loss: -34.694580078125\n",
            "Training epoch 800/1000000, d_loss: -61.00740051269531,  g_loss: 96.81141662597656\n",
            "Training epoch 801/1000000, d_loss: -109.57600402832031,  g_loss: 38.65727615356445\n",
            "Training epoch 802/1000000, d_loss: -101.50828552246094,  g_loss: 174.95887756347656\n",
            "Training epoch 803/1000000, d_loss: -92.10664367675781,  g_loss: 150.37168884277344\n",
            "Training epoch 804/1000000, d_loss: -19.884441375732422,  g_loss: 10.376763343811035\n",
            "Training epoch 805/1000000, d_loss: -207.56060791015625,  g_loss: -106.25178527832031\n",
            "Training epoch 806/1000000, d_loss: -27.96392822265625,  g_loss: -77.82632446289062\n",
            "Training epoch 807/1000000, d_loss: -368.34478759765625,  g_loss: -214.57444763183594\n",
            "Training epoch 808/1000000, d_loss: -134.91114807128906,  g_loss: 84.0592041015625\n",
            "Training epoch 809/1000000, d_loss: -246.45816040039062,  g_loss: -258.470703125\n",
            "Training epoch 810/1000000, d_loss: -65.43995666503906,  g_loss: 139.41867065429688\n",
            "Training epoch 811/1000000, d_loss: -76.1388168334961,  g_loss: 83.43207550048828\n",
            "Training epoch 812/1000000, d_loss: -77.3404541015625,  g_loss: 57.89105987548828\n",
            "Training epoch 813/1000000, d_loss: -138.97665405273438,  g_loss: 68.43060302734375\n",
            "Training epoch 814/1000000, d_loss: -92.11830139160156,  g_loss: 78.9694595336914\n",
            "Training epoch 815/1000000, d_loss: -61.37982177734375,  g_loss: 120.02936553955078\n",
            "Training epoch 816/1000000, d_loss: -34.001468658447266,  g_loss: 120.16178894042969\n",
            "Training epoch 817/1000000, d_loss: -538.9552612304688,  g_loss: -246.17156982421875\n",
            "Training epoch 818/1000000, d_loss: -34.805728912353516,  g_loss: 8.174750328063965\n",
            "Training epoch 819/1000000, d_loss: -261.71441650390625,  g_loss: -31.118698120117188\n",
            "Training epoch 820/1000000, d_loss: -68.42613983154297,  g_loss: 111.323974609375\n",
            "Training epoch 821/1000000, d_loss: -111.59549713134766,  g_loss: 277.57806396484375\n",
            "Training epoch 822/1000000, d_loss: -71.68665313720703,  g_loss: 67.0634765625\n",
            "Training epoch 823/1000000, d_loss: -51.50288391113281,  g_loss: 201.8951416015625\n",
            "Training epoch 824/1000000, d_loss: -116.34945678710938,  g_loss: 150.41258239746094\n",
            "Training epoch 825/1000000, d_loss: -44.52986526489258,  g_loss: 48.32422637939453\n",
            "Training epoch 826/1000000, d_loss: -85.16415405273438,  g_loss: 80.72755432128906\n",
            "Training epoch 827/1000000, d_loss: -147.47647094726562,  g_loss: -36.978668212890625\n",
            "Training epoch 828/1000000, d_loss: -102.91085815429688,  g_loss: -23.797819137573242\n",
            "Training epoch 829/1000000, d_loss: -97.18348693847656,  g_loss: -63.699256896972656\n",
            "Training epoch 830/1000000, d_loss: -119.71247100830078,  g_loss: 137.9928436279297\n",
            "Training epoch 831/1000000, d_loss: -107.73539733886719,  g_loss: 142.7930145263672\n",
            "Training epoch 832/1000000, d_loss: -266.4390563964844,  g_loss: -179.84194946289062\n",
            "Training epoch 833/1000000, d_loss: -63.46064376831055,  g_loss: -64.67546081542969\n",
            "Training epoch 834/1000000, d_loss: -36.64032745361328,  g_loss: -28.567903518676758\n",
            "Training epoch 835/1000000, d_loss: -190.42156982421875,  g_loss: -123.86176300048828\n",
            "Training epoch 836/1000000, d_loss: 810.86083984375,  g_loss: 140.0355987548828\n",
            "Training epoch 837/1000000, d_loss: -126.38862609863281,  g_loss: 140.84616088867188\n",
            "Training epoch 838/1000000, d_loss: -147.41421508789062,  g_loss: 122.28208923339844\n",
            "Training epoch 839/1000000, d_loss: -50.326847076416016,  g_loss: 102.73609924316406\n",
            "Training epoch 840/1000000, d_loss: -338.8856506347656,  g_loss: -41.789039611816406\n",
            "Training epoch 841/1000000, d_loss: 32.39786148071289,  g_loss: 100.64122009277344\n",
            "Training epoch 842/1000000, d_loss: -191.968994140625,  g_loss: 81.93521881103516\n",
            "Training epoch 843/1000000, d_loss: -128.56375122070312,  g_loss: 94.74978637695312\n",
            "Training epoch 844/1000000, d_loss: -117.7756118774414,  g_loss: 220.69204711914062\n",
            "Training epoch 845/1000000, d_loss: -151.83432006835938,  g_loss: 364.0281066894531\n",
            "Training epoch 846/1000000, d_loss: -82.83270263671875,  g_loss: 99.83956146240234\n",
            "Training epoch 847/1000000, d_loss: -118.25426483154297,  g_loss: 29.864044189453125\n",
            "Training epoch 848/1000000, d_loss: -18.61264419555664,  g_loss: 99.00701904296875\n",
            "Training epoch 849/1000000, d_loss: -79.32303619384766,  g_loss: 134.9091796875\n",
            "Training epoch 850/1000000, d_loss: 26.478118896484375,  g_loss: 174.92855834960938\n",
            "Training epoch 851/1000000, d_loss: -215.80673217773438,  g_loss: 16.80596160888672\n",
            "Training epoch 852/1000000, d_loss: -356.03472900390625,  g_loss: -236.1134796142578\n",
            "Training epoch 853/1000000, d_loss: -189.6746826171875,  g_loss: -74.1129150390625\n",
            "Training epoch 854/1000000, d_loss: -84.60698699951172,  g_loss: 51.36151885986328\n",
            "Training epoch 855/1000000, d_loss: -122.04305267333984,  g_loss: -33.63677215576172\n",
            "Training epoch 856/1000000, d_loss: -33.35894012451172,  g_loss: 30.57700538635254\n",
            "Training epoch 857/1000000, d_loss: -69.34290313720703,  g_loss: 4.577178955078125\n",
            "Training epoch 858/1000000, d_loss: -210.14744567871094,  g_loss: -94.62860107421875\n",
            "Training epoch 859/1000000, d_loss: -237.98658752441406,  g_loss: -200.71109008789062\n",
            "Training epoch 860/1000000, d_loss: -168.44131469726562,  g_loss: -320.29296875\n",
            "Training epoch 861/1000000, d_loss: -179.4398956298828,  g_loss: 63.83584976196289\n",
            "Training epoch 862/1000000, d_loss: -190.33575439453125,  g_loss: 223.8284912109375\n",
            "Training epoch 863/1000000, d_loss: -103.68055725097656,  g_loss: -54.58639144897461\n",
            "Training epoch 864/1000000, d_loss: -88.63207244873047,  g_loss: 114.7636947631836\n",
            "Training epoch 865/1000000, d_loss: -82.77842712402344,  g_loss: 244.87911987304688\n",
            "Training epoch 866/1000000, d_loss: -34.341941833496094,  g_loss: 20.295045852661133\n",
            "Training epoch 867/1000000, d_loss: -138.619140625,  g_loss: -39.27861404418945\n",
            "Training epoch 868/1000000, d_loss: -174.98101806640625,  g_loss: -152.38392639160156\n",
            "Training epoch 869/1000000, d_loss: -28.09072494506836,  g_loss: -65.9227066040039\n",
            "Training epoch 870/1000000, d_loss: 21.23114776611328,  g_loss: 45.63912582397461\n",
            "Training epoch 871/1000000, d_loss: -47.859256744384766,  g_loss: -11.808978080749512\n",
            "Training epoch 872/1000000, d_loss: -84.36886596679688,  g_loss: -70.15383911132812\n",
            "Training epoch 873/1000000, d_loss: 16.32282066345215,  g_loss: 36.799766540527344\n",
            "Training epoch 874/1000000, d_loss: -49.86198425292969,  g_loss: 3.702706813812256\n",
            "Training epoch 875/1000000, d_loss: -26.468881607055664,  g_loss: -17.03375244140625\n",
            "Training epoch 876/1000000, d_loss: -60.187034606933594,  g_loss: -34.6573371887207\n",
            "Training epoch 877/1000000, d_loss: -95.22147369384766,  g_loss: -7.8189473152160645\n",
            "Training epoch 878/1000000, d_loss: -29.679515838623047,  g_loss: 31.7662296295166\n",
            "Training epoch 879/1000000, d_loss: -47.59034729003906,  g_loss: -11.591706275939941\n",
            "Training epoch 880/1000000, d_loss: -99.74325561523438,  g_loss: -26.216238021850586\n",
            "Training epoch 881/1000000, d_loss: -58.56230926513672,  g_loss: -51.06454849243164\n",
            "Training epoch 882/1000000, d_loss: -118.88024139404297,  g_loss: -109.22129821777344\n",
            "Training epoch 883/1000000, d_loss: -88.97091674804688,  g_loss: -74.49880981445312\n",
            "Training epoch 884/1000000, d_loss: -63.321258544921875,  g_loss: -78.77594757080078\n",
            "Training epoch 885/1000000, d_loss: -88.99992370605469,  g_loss: -147.56375122070312\n",
            "Training epoch 886/1000000, d_loss: -66.34909057617188,  g_loss: 96.9693603515625\n",
            "Training epoch 887/1000000, d_loss: -99.8949966430664,  g_loss: -16.20110511779785\n",
            "Training epoch 888/1000000, d_loss: -96.79999542236328,  g_loss: -84.28152465820312\n",
            "Training epoch 889/1000000, d_loss: -32.19464111328125,  g_loss: -92.63896179199219\n",
            "Training epoch 890/1000000, d_loss: -259.7724609375,  g_loss: -261.681640625\n",
            "Training epoch 891/1000000, d_loss: -64.36073303222656,  g_loss: -13.502086639404297\n",
            "Training epoch 892/1000000, d_loss: -78.9524154663086,  g_loss: 33.44383239746094\n",
            "Training epoch 893/1000000, d_loss: -200.85377502441406,  g_loss: -293.570068359375\n",
            "Training epoch 894/1000000, d_loss: 23.19439697265625,  g_loss: -1.7953977584838867\n",
            "Training epoch 895/1000000, d_loss: -175.18304443359375,  g_loss: -53.6815071105957\n",
            "Training epoch 896/1000000, d_loss: -247.56936645507812,  g_loss: -345.23614501953125\n",
            "Training epoch 897/1000000, d_loss: -82.65715026855469,  g_loss: 35.42030334472656\n",
            "Training epoch 898/1000000, d_loss: -110.1675796508789,  g_loss: 51.22511672973633\n",
            "Training epoch 899/1000000, d_loss: -60.782012939453125,  g_loss: 8.76081657409668\n",
            "Training epoch 900/1000000, d_loss: -11.366077423095703,  g_loss: -1.6270780563354492\n",
            "Training epoch 901/1000000, d_loss: -31.443449020385742,  g_loss: 7.645441055297852\n",
            "Training epoch 902/1000000, d_loss: -27.976024627685547,  g_loss: -17.610105514526367\n",
            "Training epoch 903/1000000, d_loss: -46.146671295166016,  g_loss: 55.23164749145508\n",
            "Training epoch 904/1000000, d_loss: -164.40232849121094,  g_loss: -21.922870635986328\n",
            "Training epoch 905/1000000, d_loss: -192.37753295898438,  g_loss: -127.76828002929688\n",
            "Training epoch 906/1000000, d_loss: -173.45358276367188,  g_loss: -129.50857543945312\n",
            "Training epoch 907/1000000, d_loss: -120.76358032226562,  g_loss: 68.271240234375\n",
            "Training epoch 908/1000000, d_loss: -112.88560485839844,  g_loss: 135.87684631347656\n",
            "Training epoch 909/1000000, d_loss: -46.046875,  g_loss: 32.64765930175781\n",
            "Training epoch 910/1000000, d_loss: -184.85079956054688,  g_loss: 98.05259704589844\n",
            "Training epoch 911/1000000, d_loss: 3.576061248779297,  g_loss: 116.36090087890625\n",
            "Training epoch 912/1000000, d_loss: -80.36634063720703,  g_loss: 40.104026794433594\n",
            "Training epoch 913/1000000, d_loss: -116.09184265136719,  g_loss: 13.284549713134766\n",
            "Training epoch 914/1000000, d_loss: -84.38450622558594,  g_loss: 49.417911529541016\n",
            "Training epoch 915/1000000, d_loss: 120.06488037109375,  g_loss: 70.1657943725586\n",
            "Training epoch 916/1000000, d_loss: -129.0061798095703,  g_loss: 48.98979949951172\n",
            "Training epoch 917/1000000, d_loss: -33.15167999267578,  g_loss: 87.67131805419922\n",
            "Training epoch 918/1000000, d_loss: -65.78587341308594,  g_loss: 137.26197814941406\n",
            "Training epoch 919/1000000, d_loss: -52.51878356933594,  g_loss: 95.68183898925781\n",
            "Training epoch 920/1000000, d_loss: -50.65011215209961,  g_loss: 31.997831344604492\n",
            "Training epoch 921/1000000, d_loss: -88.79978942871094,  g_loss: 2.312471389770508\n",
            "Training epoch 922/1000000, d_loss: -158.9951934814453,  g_loss: -54.8822021484375\n",
            "Training epoch 923/1000000, d_loss: -90.14472198486328,  g_loss: 73.13937377929688\n",
            "Training epoch 924/1000000, d_loss: -249.53799438476562,  g_loss: -141.0629425048828\n",
            "Training epoch 925/1000000, d_loss: -102.85279846191406,  g_loss: -53.208953857421875\n",
            "Training epoch 926/1000000, d_loss: -472.8764953613281,  g_loss: -306.01324462890625\n",
            "Training epoch 927/1000000, d_loss: -56.361846923828125,  g_loss: 248.699951171875\n",
            "Training epoch 928/1000000, d_loss: -23.623031616210938,  g_loss: 379.69482421875\n",
            "Training epoch 929/1000000, d_loss: -146.66822814941406,  g_loss: 74.97245788574219\n",
            "Training epoch 930/1000000, d_loss: -37.05939483642578,  g_loss: 112.69375610351562\n",
            "Training epoch 931/1000000, d_loss: -51.42540740966797,  g_loss: 163.42910766601562\n",
            "Training epoch 932/1000000, d_loss: -71.84027862548828,  g_loss: 142.37501525878906\n",
            "Training epoch 933/1000000, d_loss: -70.32777404785156,  g_loss: 84.50096893310547\n",
            "Training epoch 934/1000000, d_loss: -88.82589721679688,  g_loss: 100.54720306396484\n",
            "Training epoch 935/1000000, d_loss: -187.915771484375,  g_loss: -15.436222076416016\n",
            "Training epoch 936/1000000, d_loss: -85.39236450195312,  g_loss: 99.27197265625\n",
            "Training epoch 937/1000000, d_loss: -151.4757080078125,  g_loss: -10.818853378295898\n",
            "Training epoch 938/1000000, d_loss: -415.6429138183594,  g_loss: -198.21878051757812\n",
            "Training epoch 939/1000000, d_loss: -97.98896789550781,  g_loss: 45.96965789794922\n",
            "Training epoch 940/1000000, d_loss: -236.41348266601562,  g_loss: -70.20243835449219\n",
            "Training epoch 941/1000000, d_loss: 19.80402374267578,  g_loss: 28.487865447998047\n",
            "Training epoch 942/1000000, d_loss: -544.9639892578125,  g_loss: -264.6727294921875\n",
            "Training epoch 943/1000000, d_loss: -29.85247230529785,  g_loss: -10.363874435424805\n",
            "Training epoch 944/1000000, d_loss: -153.1786651611328,  g_loss: 346.3417053222656\n",
            "Training epoch 945/1000000, d_loss: -197.8871612548828,  g_loss: 327.6666259765625\n",
            "Training epoch 946/1000000, d_loss: -133.69552612304688,  g_loss: 154.5330352783203\n",
            "Training epoch 947/1000000, d_loss: -47.8662109375,  g_loss: 142.82791137695312\n",
            "Training epoch 948/1000000, d_loss: -136.93954467773438,  g_loss: 174.63470458984375\n",
            "Training epoch 949/1000000, d_loss: -175.00576782226562,  g_loss: -3.179330825805664\n",
            "Training epoch 950/1000000, d_loss: -138.93653869628906,  g_loss: -140.531494140625\n",
            "Training epoch 951/1000000, d_loss: -35.52467727661133,  g_loss: -1.1748991012573242\n",
            "Training epoch 952/1000000, d_loss: -29.78271484375,  g_loss: 5.790072917938232\n",
            "Training epoch 953/1000000, d_loss: -126.5578384399414,  g_loss: -17.295869827270508\n",
            "Training epoch 954/1000000, d_loss: -12.585922241210938,  g_loss: 86.55804443359375\n",
            "Training epoch 955/1000000, d_loss: -134.69374084472656,  g_loss: 17.70580291748047\n",
            "Training epoch 956/1000000, d_loss: -102.7244644165039,  g_loss: -56.629703521728516\n",
            "Training epoch 957/1000000, d_loss: -90.62326049804688,  g_loss: -17.257339477539062\n",
            "Training epoch 958/1000000, d_loss: -105.60620880126953,  g_loss: 39.20723342895508\n",
            "Training epoch 959/1000000, d_loss: -289.0842590332031,  g_loss: -226.62979125976562\n",
            "Training epoch 960/1000000, d_loss: -114.73043823242188,  g_loss: 111.82604217529297\n",
            "Training epoch 961/1000000, d_loss: -115.68494415283203,  g_loss: -28.58352279663086\n",
            "Training epoch 962/1000000, d_loss: -100.3394546508789,  g_loss: -7.550384044647217\n",
            "Training epoch 963/1000000, d_loss: -181.80587768554688,  g_loss: -53.43648910522461\n",
            "Training epoch 964/1000000, d_loss: -74.70319366455078,  g_loss: 107.85822296142578\n",
            "Training epoch 965/1000000, d_loss: -74.18206787109375,  g_loss: 166.45362854003906\n",
            "Training epoch 966/1000000, d_loss: -53.35822677612305,  g_loss: 97.00672912597656\n",
            "Training epoch 967/1000000, d_loss: -29.533079147338867,  g_loss: 94.86607360839844\n",
            "Training epoch 968/1000000, d_loss: -113.11430358886719,  g_loss: 11.951210021972656\n",
            "Training epoch 969/1000000, d_loss: -88.6832275390625,  g_loss: 6.892603874206543\n",
            "Training epoch 970/1000000, d_loss: -62.31425857543945,  g_loss: -7.846202850341797\n",
            "Training epoch 971/1000000, d_loss: -93.24822998046875,  g_loss: 45.08275604248047\n",
            "Training epoch 972/1000000, d_loss: -69.58062744140625,  g_loss: 40.4447021484375\n",
            "Training epoch 973/1000000, d_loss: -32.15991973876953,  g_loss: 15.854390144348145\n",
            "Training epoch 974/1000000, d_loss: -121.82742309570312,  g_loss: -51.62428283691406\n",
            "Training epoch 975/1000000, d_loss: -55.05403518676758,  g_loss: 46.73131561279297\n",
            "Training epoch 976/1000000, d_loss: -80.57398986816406,  g_loss: -3.889993667602539\n",
            "Training epoch 977/1000000, d_loss: -109.16465759277344,  g_loss: -50.78968048095703\n",
            "Training epoch 978/1000000, d_loss: -48.22437286376953,  g_loss: 67.11418914794922\n",
            "Training epoch 979/1000000, d_loss: -51.02033233642578,  g_loss: 54.4332160949707\n",
            "Training epoch 980/1000000, d_loss: -82.85905456542969,  g_loss: -19.082530975341797\n",
            "Training epoch 981/1000000, d_loss: -71.83734893798828,  g_loss: 31.79216766357422\n",
            "Training epoch 982/1000000, d_loss: -73.056884765625,  g_loss: 83.76626586914062\n",
            "Training epoch 983/1000000, d_loss: -182.48851013183594,  g_loss: -85.40087127685547\n",
            "Training epoch 984/1000000, d_loss: -42.71561050415039,  g_loss: -0.9209518432617188\n",
            "Training epoch 985/1000000, d_loss: -117.63131713867188,  g_loss: -115.84942626953125\n",
            "Training epoch 986/1000000, d_loss: -54.67106246948242,  g_loss: 16.635826110839844\n",
            "Training epoch 987/1000000, d_loss: -32.51382064819336,  g_loss: -26.114505767822266\n",
            "Training epoch 988/1000000, d_loss: -40.80149841308594,  g_loss: 26.562660217285156\n",
            "Training epoch 989/1000000, d_loss: -79.92750549316406,  g_loss: 5.850844383239746\n",
            "Training epoch 990/1000000, d_loss: -86.74226379394531,  g_loss: -28.13959312438965\n",
            "Training epoch 991/1000000, d_loss: -89.8124771118164,  g_loss: -13.896150588989258\n",
            "Training epoch 992/1000000, d_loss: -36.883602142333984,  g_loss: 103.19243621826172\n",
            "Training epoch 993/1000000, d_loss: -87.14503479003906,  g_loss: -11.936990737915039\n",
            "Training epoch 994/1000000, d_loss: -81.29947662353516,  g_loss: -31.932729721069336\n",
            "Training epoch 995/1000000, d_loss: -59.07600021362305,  g_loss: 19.82065773010254\n",
            "Training epoch 996/1000000, d_loss: -92.71151733398438,  g_loss: 26.37499237060547\n",
            "Training epoch 997/1000000, d_loss: -53.031681060791016,  g_loss: 44.49161911010742\n",
            "Training epoch 998/1000000, d_loss: -30.93781852722168,  g_loss: 44.90959930419922\n",
            "Training epoch 999/1000000, d_loss: -86.73306274414062,  g_loss: -21.893203735351562\n",
            "Training epoch 1000/1000000, d_loss: -128.82354736328125,  g_loss: -66.07965087890625\n",
            "Training epoch 1001/1000000, d_loss: -64.5770263671875,  g_loss: 129.09315490722656\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 64/64 [00:00<00:00, 139.50it/s]\n",
            "Meshing: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not create voxel model... Continuing training\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_1001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_1001/assets\n",
            "Training epoch 1002/1000000, d_loss: -19.311567306518555,  g_loss: 79.51116180419922\n",
            "Training epoch 1003/1000000, d_loss: -75.09881591796875,  g_loss: -8.911635398864746\n",
            "Training epoch 1004/1000000, d_loss: -293.72418212890625,  g_loss: -138.05364990234375\n",
            "Training epoch 1005/1000000, d_loss: -154.1171112060547,  g_loss: -236.5572509765625\n",
            "Training epoch 1006/1000000, d_loss: -151.2901611328125,  g_loss: 36.07177734375\n",
            "Training epoch 1007/1000000, d_loss: -61.66889190673828,  g_loss: 11.820697784423828\n",
            "Training epoch 1008/1000000, d_loss: -120.58753967285156,  g_loss: 33.96487045288086\n",
            "Training epoch 1009/1000000, d_loss: -15.018882751464844,  g_loss: -1.81535005569458\n",
            "Training epoch 1010/1000000, d_loss: -326.2631530761719,  g_loss: -146.99813842773438\n",
            "Training epoch 1011/1000000, d_loss: -63.539852142333984,  g_loss: 108.16161346435547\n",
            "Training epoch 1012/1000000, d_loss: -104.67028045654297,  g_loss: 34.126953125\n",
            "Training epoch 1013/1000000, d_loss: -187.6114959716797,  g_loss: -26.200857162475586\n",
            "Training epoch 1014/1000000, d_loss: -3.834186553955078,  g_loss: 8.542886734008789\n",
            "Training epoch 1015/1000000, d_loss: -80.40469360351562,  g_loss: 41.74213790893555\n",
            "Training epoch 1016/1000000, d_loss: -102.43425750732422,  g_loss: 127.04446411132812\n",
            "Training epoch 1017/1000000, d_loss: -172.56680297851562,  g_loss: 206.38284301757812\n",
            "Training epoch 1018/1000000, d_loss: -63.70658874511719,  g_loss: 84.71067810058594\n",
            "Training epoch 1019/1000000, d_loss: -35.535400390625,  g_loss: 44.08891296386719\n",
            "Training epoch 1020/1000000, d_loss: -60.038719177246094,  g_loss: 84.30421447753906\n",
            "Training epoch 1021/1000000, d_loss: -104.00399017333984,  g_loss: 19.308029174804688\n",
            "Training epoch 1022/1000000, d_loss: -72.52430725097656,  g_loss: 56.867515563964844\n",
            "Training epoch 1023/1000000, d_loss: -71.26395416259766,  g_loss: 39.839500427246094\n",
            "Training epoch 1024/1000000, d_loss: -272.9195861816406,  g_loss: -110.78056335449219\n",
            "Training epoch 1025/1000000, d_loss: -15.348255157470703,  g_loss: 127.64884948730469\n",
            "Training epoch 1026/1000000, d_loss: -99.77375793457031,  g_loss: 119.75411987304688\n",
            "Training epoch 1027/1000000, d_loss: -37.687721252441406,  g_loss: 43.33975601196289\n",
            "Training epoch 1028/1000000, d_loss: -101.38264465332031,  g_loss: 5.4727349281311035\n",
            "Training epoch 1029/1000000, d_loss: -204.6201629638672,  g_loss: 16.2615966796875\n",
            "Training epoch 1030/1000000, d_loss: -33.437217712402344,  g_loss: 52.31622314453125\n",
            "Training epoch 1031/1000000, d_loss: -238.619873046875,  g_loss: -43.520751953125\n",
            "Training epoch 1032/1000000, d_loss: -77.79205322265625,  g_loss: -1.8380335569381714\n",
            "Training epoch 1033/1000000, d_loss: -751.7989501953125,  g_loss: -357.333984375\n",
            "Training epoch 1034/1000000, d_loss: -114.79668426513672,  g_loss: -134.53680419921875\n",
            "Training epoch 1035/1000000, d_loss: -278.2889099121094,  g_loss: -246.30978393554688\n",
            "Training epoch 1036/1000000, d_loss: -9.084243774414062,  g_loss: -117.8153076171875\n",
            "Training epoch 1037/1000000, d_loss: -109.78267669677734,  g_loss: -33.896728515625\n",
            "Training epoch 1038/1000000, d_loss: -108.99205780029297,  g_loss: 157.7762451171875\n",
            "Training epoch 1039/1000000, d_loss: -177.68429565429688,  g_loss: 216.09632873535156\n",
            "Training epoch 1040/1000000, d_loss: -94.77042388916016,  g_loss: 231.20977783203125\n",
            "Training epoch 1041/1000000, d_loss: -42.50878143310547,  g_loss: 76.66323852539062\n",
            "Training epoch 1042/1000000, d_loss: -182.7708282470703,  g_loss: -10.467707633972168\n",
            "Training epoch 1043/1000000, d_loss: -51.733360290527344,  g_loss: 75.62443542480469\n",
            "Training epoch 1044/1000000, d_loss: -53.763484954833984,  g_loss: 169.89788818359375\n",
            "Training epoch 1045/1000000, d_loss: -51.01495361328125,  g_loss: 28.50547981262207\n",
            "Training epoch 1046/1000000, d_loss: -179.58099365234375,  g_loss: -93.88687896728516\n",
            "Training epoch 1047/1000000, d_loss: -78.09017944335938,  g_loss: 35.28748321533203\n",
            "Training epoch 1048/1000000, d_loss: -12.778091430664062,  g_loss: 34.934688568115234\n",
            "Training epoch 1049/1000000, d_loss: -73.68336486816406,  g_loss: 185.8391876220703\n",
            "Training epoch 1050/1000000, d_loss: -102.04345703125,  g_loss: 171.05601501464844\n",
            "Training epoch 1051/1000000, d_loss: -56.68601989746094,  g_loss: 41.9202880859375\n",
            "Training epoch 1052/1000000, d_loss: -83.69670104980469,  g_loss: 11.9520845413208\n",
            "Training epoch 1053/1000000, d_loss: -101.03778076171875,  g_loss: 52.618751525878906\n",
            "Training epoch 1054/1000000, d_loss: -65.02157592773438,  g_loss: 42.655609130859375\n",
            "Training epoch 1055/1000000, d_loss: -209.56021118164062,  g_loss: -66.46843719482422\n",
            "Training epoch 1056/1000000, d_loss: -69.3583755493164,  g_loss: -47.44845199584961\n",
            "Training epoch 1057/1000000, d_loss: -108.94989013671875,  g_loss: -101.73189544677734\n",
            "Training epoch 1058/1000000, d_loss: -71.17070770263672,  g_loss: -46.661964416503906\n",
            "Training epoch 1059/1000000, d_loss: -81.21759033203125,  g_loss: -5.9699578285217285\n",
            "Training epoch 1060/1000000, d_loss: -247.0047149658203,  g_loss: -110.50518798828125\n",
            "Training epoch 1061/1000000, d_loss: -295.09588623046875,  g_loss: -301.2833251953125\n",
            "Training epoch 1062/1000000, d_loss: -44.70835494995117,  g_loss: -50.14910888671875\n",
            "Training epoch 1063/1000000, d_loss: -71.3771743774414,  g_loss: 45.061092376708984\n",
            "Training epoch 1064/1000000, d_loss: -92.28375244140625,  g_loss: -12.414836883544922\n",
            "Training epoch 1065/1000000, d_loss: -96.61266326904297,  g_loss: 115.69956970214844\n",
            "Training epoch 1066/1000000, d_loss: -123.79730224609375,  g_loss: -63.485328674316406\n",
            "Training epoch 1067/1000000, d_loss: -85.7635726928711,  g_loss: -62.252845764160156\n",
            "Training epoch 1068/1000000, d_loss: -263.2674560546875,  g_loss: -244.80712890625\n",
            "Training epoch 1069/1000000, d_loss: -230.865478515625,  g_loss: -173.39007568359375\n",
            "Training epoch 1070/1000000, d_loss: -77.65228271484375,  g_loss: 32.99046325683594\n",
            "Training epoch 1071/1000000, d_loss: -96.67707061767578,  g_loss: 113.92662048339844\n",
            "Training epoch 1072/1000000, d_loss: 78.81873321533203,  g_loss: 49.460872650146484\n",
            "Training epoch 1073/1000000, d_loss: -41.4068603515625,  g_loss: 89.68505096435547\n",
            "Training epoch 1074/1000000, d_loss: -41.38212585449219,  g_loss: 48.388694763183594\n",
            "Training epoch 1075/1000000, d_loss: -26.596195220947266,  g_loss: 53.4454345703125\n",
            "Training epoch 1076/1000000, d_loss: -100.2729721069336,  g_loss: 17.70343017578125\n",
            "Training epoch 1077/1000000, d_loss: -95.46951293945312,  g_loss: 33.47651672363281\n",
            "Training epoch 1078/1000000, d_loss: -91.01475524902344,  g_loss: -8.50071907043457\n",
            "Training epoch 1079/1000000, d_loss: -88.80142211914062,  g_loss: -48.978031158447266\n",
            "Training epoch 1080/1000000, d_loss: -372.666015625,  g_loss: -207.8115234375\n",
            "Training epoch 1081/1000000, d_loss: -57.155479431152344,  g_loss: -48.564083099365234\n",
            "Training epoch 1082/1000000, d_loss: -57.97186279296875,  g_loss: 50.153099060058594\n",
            "Training epoch 1083/1000000, d_loss: -69.49967956542969,  g_loss: 98.759033203125\n",
            "Training epoch 1084/1000000, d_loss: -53.69391632080078,  g_loss: -43.67573547363281\n",
            "Training epoch 1085/1000000, d_loss: -61.11228942871094,  g_loss: 21.043790817260742\n",
            "Training epoch 1086/1000000, d_loss: -231.19244384765625,  g_loss: -100.48436737060547\n",
            "Training epoch 1087/1000000, d_loss: -339.8471984863281,  g_loss: -98.02379608154297\n",
            "Training epoch 1088/1000000, d_loss: -247.82766723632812,  g_loss: -132.78526306152344\n",
            "Training epoch 1089/1000000, d_loss: -442.0294189453125,  g_loss: -305.52105712890625\n",
            "Training epoch 1090/1000000, d_loss: -65.17499542236328,  g_loss: -109.17848205566406\n",
            "Training epoch 1091/1000000, d_loss: -98.0040512084961,  g_loss: -60.19252395629883\n",
            "Training epoch 1092/1000000, d_loss: -67.88859558105469,  g_loss: -68.12772369384766\n",
            "Training epoch 1093/1000000, d_loss: -25.17098617553711,  g_loss: -101.64317321777344\n",
            "Training epoch 1094/1000000, d_loss: -89.77407836914062,  g_loss: -33.75891876220703\n",
            "Training epoch 1095/1000000, d_loss: -12.326160430908203,  g_loss: 60.3620491027832\n",
            "Training epoch 1096/1000000, d_loss: -75.59371948242188,  g_loss: 13.03904914855957\n",
            "Training epoch 1097/1000000, d_loss: -111.83668518066406,  g_loss: 17.857303619384766\n",
            "Training epoch 1098/1000000, d_loss: -68.34513092041016,  g_loss: 10.62459945678711\n",
            "Training epoch 1099/1000000, d_loss: -180.7099609375,  g_loss: -21.692317962646484\n",
            "Training epoch 1100/1000000, d_loss: -382.1011962890625,  g_loss: -270.1451416015625\n",
            "Training epoch 1101/1000000, d_loss: -37.51188659667969,  g_loss: 55.296627044677734\n",
            "Training epoch 1102/1000000, d_loss: -253.07733154296875,  g_loss: 113.62026977539062\n",
            "Training epoch 1103/1000000, d_loss: -10.662117004394531,  g_loss: 339.3164367675781\n",
            "Training epoch 1104/1000000, d_loss: -28.299726486206055,  g_loss: 221.6364288330078\n",
            "Training epoch 1105/1000000, d_loss: -72.50389099121094,  g_loss: 188.81292724609375\n",
            "Training epoch 1106/1000000, d_loss: -73.49673461914062,  g_loss: 128.04469299316406\n",
            "Training epoch 1107/1000000, d_loss: -59.218040466308594,  g_loss: 138.75389099121094\n",
            "Training epoch 1108/1000000, d_loss: -110.15630340576172,  g_loss: 121.94208526611328\n",
            "Training epoch 1109/1000000, d_loss: -56.17352294921875,  g_loss: 70.86858367919922\n",
            "Training epoch 1110/1000000, d_loss: -111.03844451904297,  g_loss: 99.73966217041016\n",
            "Training epoch 1111/1000000, d_loss: -169.75698852539062,  g_loss: 89.70195007324219\n",
            "Training epoch 1112/1000000, d_loss: -142.20030212402344,  g_loss: 49.198822021484375\n",
            "Training epoch 1113/1000000, d_loss: -76.01556396484375,  g_loss: 116.58504486083984\n",
            "Training epoch 1114/1000000, d_loss: -55.84284591674805,  g_loss: 262.6539001464844\n",
            "Training epoch 1115/1000000, d_loss: -71.52218627929688,  g_loss: 112.96807861328125\n",
            "Training epoch 1116/1000000, d_loss: -257.3091735839844,  g_loss: 1.4293522834777832\n",
            "Training epoch 1117/1000000, d_loss: -167.45132446289062,  g_loss: 18.062931060791016\n",
            "Training epoch 1118/1000000, d_loss: -52.82537841796875,  g_loss: 131.2353057861328\n",
            "Training epoch 1119/1000000, d_loss: -287.11419677734375,  g_loss: -9.849071502685547\n",
            "Training epoch 1120/1000000, d_loss: -75.83657836914062,  g_loss: 106.74361419677734\n",
            "Training epoch 1121/1000000, d_loss: -78.74871826171875,  g_loss: -19.8798828125\n",
            "Training epoch 1122/1000000, d_loss: -36.45934295654297,  g_loss: -10.889751434326172\n",
            "Training epoch 1123/1000000, d_loss: -114.6702880859375,  g_loss: 22.357717514038086\n",
            "Training epoch 1124/1000000, d_loss: -70.6146240234375,  g_loss: -34.82661056518555\n",
            "Training epoch 1125/1000000, d_loss: 17.279356002807617,  g_loss: -1.7680301666259766\n",
            "Training epoch 1126/1000000, d_loss: -157.4918212890625,  g_loss: -83.64643096923828\n",
            "Training epoch 1127/1000000, d_loss: -159.85446166992188,  g_loss: 29.299571990966797\n",
            "Training epoch 1128/1000000, d_loss: 75.20751953125,  g_loss: -6.9190263748168945\n",
            "Training epoch 1129/1000000, d_loss: -142.26467895507812,  g_loss: -49.79838943481445\n",
            "Training epoch 1130/1000000, d_loss: -16.491809844970703,  g_loss: -20.201648712158203\n",
            "Training epoch 1131/1000000, d_loss: -80.05268859863281,  g_loss: 20.788570404052734\n",
            "Training epoch 1132/1000000, d_loss: -275.2569274902344,  g_loss: -136.93174743652344\n",
            "Training epoch 1133/1000000, d_loss: -173.92929077148438,  g_loss: -169.94784545898438\n",
            "Training epoch 1134/1000000, d_loss: -190.20069885253906,  g_loss: 49.65778350830078\n",
            "Training epoch 1135/1000000, d_loss: -245.4087677001953,  g_loss: -241.71218872070312\n",
            "Training epoch 1136/1000000, d_loss: -56.385902404785156,  g_loss: 42.80084228515625\n",
            "Training epoch 1137/1000000, d_loss: -96.11321258544922,  g_loss: 65.7713851928711\n",
            "Training epoch 1138/1000000, d_loss: -76.57420349121094,  g_loss: 3.072030544281006\n",
            "Training epoch 1139/1000000, d_loss: -180.15492248535156,  g_loss: -10.527397155761719\n",
            "Training epoch 1140/1000000, d_loss: -57.75530242919922,  g_loss: -53.76591491699219\n",
            "Training epoch 1141/1000000, d_loss: -132.4017791748047,  g_loss: -33.21883773803711\n",
            "Training epoch 1142/1000000, d_loss: -138.7296905517578,  g_loss: 177.52615356445312\n",
            "Training epoch 1143/1000000, d_loss: -90.51066589355469,  g_loss: 190.9177703857422\n",
            "Training epoch 1144/1000000, d_loss: -70.74437713623047,  g_loss: 176.9421844482422\n",
            "Training epoch 1145/1000000, d_loss: -31.43277931213379,  g_loss: 136.82421875\n",
            "Training epoch 1146/1000000, d_loss: -34.97809600830078,  g_loss: 7.750069618225098\n",
            "Training epoch 1147/1000000, d_loss: -44.41038513183594,  g_loss: 105.18670654296875\n",
            "Training epoch 1148/1000000, d_loss: -137.92974853515625,  g_loss: -50.20024108886719\n",
            "Training epoch 1149/1000000, d_loss: -91.49592590332031,  g_loss: -38.404075622558594\n",
            "Training epoch 1150/1000000, d_loss: -103.59806823730469,  g_loss: -17.39576530456543\n",
            "Training epoch 1151/1000000, d_loss: -39.769935607910156,  g_loss: 56.0699462890625\n",
            "Training epoch 1152/1000000, d_loss: -207.70974731445312,  g_loss: -45.284637451171875\n",
            "Training epoch 1153/1000000, d_loss: -52.93257522583008,  g_loss: -33.97919464111328\n",
            "Training epoch 1154/1000000, d_loss: -469.7862243652344,  g_loss: -347.3768310546875\n",
            "Training epoch 1155/1000000, d_loss: -76.53746032714844,  g_loss: -39.62259292602539\n",
            "Training epoch 1156/1000000, d_loss: -541.84423828125,  g_loss: -294.0863037109375\n",
            "Training epoch 1157/1000000, d_loss: -183.11026000976562,  g_loss: -106.56217956542969\n",
            "Training epoch 1158/1000000, d_loss: -101.1784896850586,  g_loss: -87.40852355957031\n",
            "Training epoch 1159/1000000, d_loss: -57.75128173828125,  g_loss: -100.2168960571289\n",
            "Training epoch 1160/1000000, d_loss: -55.18192672729492,  g_loss: -126.55292510986328\n",
            "Training epoch 1161/1000000, d_loss: -439.3450622558594,  g_loss: -258.5780944824219\n",
            "Training epoch 1162/1000000, d_loss: -45.164878845214844,  g_loss: -132.94683837890625\n",
            "Training epoch 1163/1000000, d_loss: -51.72679901123047,  g_loss: -113.75074768066406\n",
            "Training epoch 1164/1000000, d_loss: -88.0693359375,  g_loss: -79.742919921875\n",
            "Training epoch 1165/1000000, d_loss: -40.67888641357422,  g_loss: -65.44651794433594\n",
            "Training epoch 1166/1000000, d_loss: -652.3494262695312,  g_loss: -229.16281127929688\n",
            "Training epoch 1167/1000000, d_loss: 28.03813934326172,  g_loss: -189.12094116210938\n",
            "Training epoch 1168/1000000, d_loss: -40.37470245361328,  g_loss: -212.5244903564453\n",
            "Training epoch 1169/1000000, d_loss: -16.25803565979004,  g_loss: -105.61161804199219\n",
            "Training epoch 1170/1000000, d_loss: -47.69013214111328,  g_loss: -152.81532287597656\n",
            "Training epoch 1171/1000000, d_loss: -5.399936676025391,  g_loss: -92.28871154785156\n",
            "Training epoch 1172/1000000, d_loss: -52.76845932006836,  g_loss: -0.8527603149414062\n",
            "Training epoch 1173/1000000, d_loss: 35.813011169433594,  g_loss: 36.96501541137695\n",
            "Training epoch 1174/1000000, d_loss: -62.35203170776367,  g_loss: 5.9866228103637695\n",
            "Training epoch 1175/1000000, d_loss: -187.84246826171875,  g_loss: -66.64965057373047\n",
            "Training epoch 1176/1000000, d_loss: -54.909423828125,  g_loss: 119.73066711425781\n",
            "Training epoch 1177/1000000, d_loss: -82.79088592529297,  g_loss: 36.69438934326172\n",
            "Training epoch 1178/1000000, d_loss: -141.21510314941406,  g_loss: 11.584161758422852\n",
            "Training epoch 1179/1000000, d_loss: -94.83098602294922,  g_loss: 37.97419357299805\n",
            "Training epoch 1180/1000000, d_loss: -264.06048583984375,  g_loss: 26.53247833251953\n",
            "Training epoch 1181/1000000, d_loss: -428.08807373046875,  g_loss: -390.5294189453125\n",
            "Training epoch 1182/1000000, d_loss: -500.52056884765625,  g_loss: -386.41741943359375\n",
            "Training epoch 1183/1000000, d_loss: 3.7315597534179688,  g_loss: 89.77556610107422\n",
            "Training epoch 1184/1000000, d_loss: -143.48587036132812,  g_loss: 9.226036071777344\n",
            "Training epoch 1185/1000000, d_loss: -146.71629333496094,  g_loss: 242.14895629882812\n",
            "Training epoch 1186/1000000, d_loss: -197.11886596679688,  g_loss: 213.8908233642578\n",
            "Training epoch 1187/1000000, d_loss: 8.292755126953125,  g_loss: -32.676658630371094\n",
            "Training epoch 1188/1000000, d_loss: -37.60307312011719,  g_loss: 90.10765075683594\n",
            "Training epoch 1189/1000000, d_loss: -59.272972106933594,  g_loss: 44.60362243652344\n",
            "Training epoch 1190/1000000, d_loss: -168.86099243164062,  g_loss: 29.779170989990234\n",
            "Training epoch 1191/1000000, d_loss: -101.61885070800781,  g_loss: 35.58745574951172\n",
            "Training epoch 1192/1000000, d_loss: -91.01469421386719,  g_loss: 158.83050537109375\n",
            "Training epoch 1193/1000000, d_loss: -212.93170166015625,  g_loss: -33.271358489990234\n",
            "Training epoch 1194/1000000, d_loss: -52.10945510864258,  g_loss: 69.58087158203125\n",
            "Training epoch 1195/1000000, d_loss: -133.84292602539062,  g_loss: -80.74545288085938\n",
            "Training epoch 1196/1000000, d_loss: -6.8075408935546875,  g_loss: 246.46636962890625\n",
            "Training epoch 1197/1000000, d_loss: -89.8899154663086,  g_loss: 77.28504943847656\n",
            "Training epoch 1198/1000000, d_loss: -128.73878479003906,  g_loss: 113.34217834472656\n",
            "Training epoch 1199/1000000, d_loss: 14.728595733642578,  g_loss: 201.6531982421875\n",
            "Training epoch 1200/1000000, d_loss: -40.29356002807617,  g_loss: 250.54055786132812\n",
            "Training epoch 1201/1000000, d_loss: -226.69873046875,  g_loss: -20.15973663330078\n",
            "Training epoch 1202/1000000, d_loss: -1.511758804321289,  g_loss: 65.10343170166016\n",
            "Training epoch 1203/1000000, d_loss: -51.635276794433594,  g_loss: 77.83788299560547\n",
            "Training epoch 1204/1000000, d_loss: 8.529327392578125,  g_loss: 85.28182983398438\n",
            "Training epoch 1205/1000000, d_loss: 3.6718673706054688,  g_loss: -30.342567443847656\n",
            "Training epoch 1206/1000000, d_loss: -345.8897705078125,  g_loss: -302.2991638183594\n",
            "Training epoch 1207/1000000, d_loss: -384.4588317871094,  g_loss: -511.9654235839844\n",
            "Training epoch 1208/1000000, d_loss: -96.16293334960938,  g_loss: 29.53035545349121\n",
            "Training epoch 1209/1000000, d_loss: -9.803680419921875,  g_loss: 167.60313415527344\n",
            "Training epoch 1210/1000000, d_loss: -123.2301254272461,  g_loss: 376.9527893066406\n",
            "Training epoch 1211/1000000, d_loss: -237.17124938964844,  g_loss: 739.7008056640625\n",
            "Training epoch 1212/1000000, d_loss: 1.984344482421875,  g_loss: 163.55029296875\n",
            "Training epoch 1213/1000000, d_loss: -40.43274688720703,  g_loss: 374.8898620605469\n",
            "Training epoch 1214/1000000, d_loss: -64.44032287597656,  g_loss: 194.51031494140625\n",
            "Training epoch 1215/1000000, d_loss: -69.64178466796875,  g_loss: 207.47286987304688\n",
            "Training epoch 1216/1000000, d_loss: -145.434814453125,  g_loss: 49.681270599365234\n",
            "Training epoch 1217/1000000, d_loss: 36.18040084838867,  g_loss: 59.367706298828125\n",
            "Training epoch 1218/1000000, d_loss: -154.7247314453125,  g_loss: 38.0975227355957\n",
            "Training epoch 1219/1000000, d_loss: -29.366165161132812,  g_loss: 166.28233337402344\n",
            "Training epoch 1220/1000000, d_loss: -62.81478500366211,  g_loss: 206.19903564453125\n",
            "Training epoch 1221/1000000, d_loss: -124.5867691040039,  g_loss: 185.4356231689453\n",
            "Training epoch 1222/1000000, d_loss: -21.679092407226562,  g_loss: 119.30966186523438\n",
            "Training epoch 1223/1000000, d_loss: -87.60987854003906,  g_loss: 14.02859878540039\n",
            "Training epoch 1224/1000000, d_loss: -161.80523681640625,  g_loss: 37.862220764160156\n",
            "Training epoch 1225/1000000, d_loss: -112.52418518066406,  g_loss: 18.3501033782959\n",
            "Training epoch 1226/1000000, d_loss: -104.13994598388672,  g_loss: 4.311801910400391\n",
            "Training epoch 1227/1000000, d_loss: -165.75743103027344,  g_loss: -16.462202072143555\n",
            "Training epoch 1228/1000000, d_loss: -58.59149932861328,  g_loss: 81.37352752685547\n",
            "Training epoch 1229/1000000, d_loss: 34.26900863647461,  g_loss: 72.26586151123047\n",
            "Training epoch 1230/1000000, d_loss: -137.34901428222656,  g_loss: 96.61517333984375\n",
            "Training epoch 1231/1000000, d_loss: -86.95507049560547,  g_loss: 89.07853698730469\n",
            "Training epoch 1232/1000000, d_loss: -79.07115173339844,  g_loss: 74.9534912109375\n",
            "Training epoch 1233/1000000, d_loss: -68.9119873046875,  g_loss: 64.30581665039062\n",
            "Training epoch 1234/1000000, d_loss: -58.567317962646484,  g_loss: 135.9881591796875\n",
            "Training epoch 1235/1000000, d_loss: -106.46722412109375,  g_loss: 19.8203182220459\n",
            "Training epoch 1236/1000000, d_loss: -50.150718688964844,  g_loss: 53.2006950378418\n",
            "Training epoch 1237/1000000, d_loss: -12.696935653686523,  g_loss: 57.491329193115234\n",
            "Training epoch 1238/1000000, d_loss: -52.75518798828125,  g_loss: 26.02206039428711\n",
            "Training epoch 1239/1000000, d_loss: -20.425518035888672,  g_loss: 12.995956420898438\n",
            "Training epoch 1240/1000000, d_loss: -122.71188354492188,  g_loss: 55.75075912475586\n",
            "Training epoch 1241/1000000, d_loss: -117.20343780517578,  g_loss: -26.731685638427734\n",
            "Training epoch 1242/1000000, d_loss: -97.44552612304688,  g_loss: 6.495050430297852\n",
            "Training epoch 1243/1000000, d_loss: -48.49955749511719,  g_loss: -56.063316345214844\n",
            "Training epoch 1244/1000000, d_loss: -111.53181457519531,  g_loss: -25.96241569519043\n",
            "Training epoch 1245/1000000, d_loss: -70.52953338623047,  g_loss: 31.71009063720703\n",
            "Training epoch 1246/1000000, d_loss: -49.11778259277344,  g_loss: 86.30990600585938\n",
            "Training epoch 1247/1000000, d_loss: -88.74606323242188,  g_loss: 7.83872127532959\n",
            "Training epoch 1248/1000000, d_loss: -83.95916748046875,  g_loss: -17.16612434387207\n",
            "Training epoch 1249/1000000, d_loss: -165.34518432617188,  g_loss: 13.160985946655273\n",
            "Training epoch 1250/1000000, d_loss: -249.36170959472656,  g_loss: -44.20922088623047\n",
            "Training epoch 1251/1000000, d_loss: -104.19027709960938,  g_loss: 191.5262451171875\n",
            "Training epoch 1252/1000000, d_loss: -47.89125061035156,  g_loss: 180.16213989257812\n",
            "Training epoch 1253/1000000, d_loss: -37.867034912109375,  g_loss: 248.94393920898438\n",
            "Training epoch 1254/1000000, d_loss: -135.71826171875,  g_loss: 110.0351333618164\n",
            "Training epoch 1255/1000000, d_loss: -66.53058624267578,  g_loss: 175.7991485595703\n",
            "Training epoch 1256/1000000, d_loss: -232.39927673339844,  g_loss: -53.392127990722656\n",
            "Training epoch 1257/1000000, d_loss: -9.5016450881958,  g_loss: 62.427459716796875\n",
            "Training epoch 1258/1000000, d_loss: -113.32244873046875,  g_loss: -123.37522888183594\n",
            "Training epoch 1259/1000000, d_loss: -88.36273193359375,  g_loss: -106.36723327636719\n",
            "Training epoch 1260/1000000, d_loss: -27.58651351928711,  g_loss: -119.24129486083984\n",
            "Training epoch 1261/1000000, d_loss: -132.9538116455078,  g_loss: -141.9109649658203\n",
            "Training epoch 1262/1000000, d_loss: -36.245079040527344,  g_loss: 36.611183166503906\n",
            "Training epoch 1263/1000000, d_loss: -20.39807891845703,  g_loss: 130.10885620117188\n",
            "Training epoch 1264/1000000, d_loss: -63.846458435058594,  g_loss: -14.260683059692383\n",
            "Training epoch 1265/1000000, d_loss: -480.8763427734375,  g_loss: -292.1893310546875\n",
            "Training epoch 1266/1000000, d_loss: -90.11273956298828,  g_loss: -3.1116409301757812\n",
            "Training epoch 1267/1000000, d_loss: -74.05193328857422,  g_loss: 125.49711608886719\n",
            "Training epoch 1268/1000000, d_loss: -120.11042022705078,  g_loss: 53.54438400268555\n",
            "Training epoch 1269/1000000, d_loss: -72.22885131835938,  g_loss: 141.3873291015625\n",
            "Training epoch 1270/1000000, d_loss: -156.2440948486328,  g_loss: -22.96308135986328\n",
            "Training epoch 1271/1000000, d_loss: -179.7730712890625,  g_loss: 82.0619125366211\n",
            "Training epoch 1272/1000000, d_loss: 7.71868896484375,  g_loss: 66.88876342773438\n",
            "Training epoch 1273/1000000, d_loss: -33.08085632324219,  g_loss: 106.88721466064453\n",
            "Training epoch 1274/1000000, d_loss: -102.79653930664062,  g_loss: 475.989501953125\n",
            "Training epoch 1275/1000000, d_loss: -91.12034606933594,  g_loss: 196.85739135742188\n",
            "Training epoch 1276/1000000, d_loss: -217.09115600585938,  g_loss: -59.50356674194336\n",
            "Training epoch 1277/1000000, d_loss: -90.80686950683594,  g_loss: 74.49287414550781\n",
            "Training epoch 1278/1000000, d_loss: -442.5209045410156,  g_loss: -276.26007080078125\n",
            "Training epoch 1279/1000000, d_loss: -355.80712890625,  g_loss: -196.82022094726562\n",
            "Training epoch 1280/1000000, d_loss: -321.01776123046875,  g_loss: -468.566650390625\n",
            "Training epoch 1281/1000000, d_loss: -62.865379333496094,  g_loss: 106.4942626953125\n",
            "Training epoch 1282/1000000, d_loss: -82.03430938720703,  g_loss: 56.433448791503906\n",
            "Training epoch 1283/1000000, d_loss: -118.21327209472656,  g_loss: 163.92562866210938\n",
            "Training epoch 1284/1000000, d_loss: -89.45960998535156,  g_loss: 127.41796112060547\n",
            "Training epoch 1285/1000000, d_loss: 6.1528472900390625,  g_loss: 88.19306945800781\n",
            "Training epoch 1286/1000000, d_loss: -148.89828491210938,  g_loss: 256.75341796875\n",
            "Training epoch 1287/1000000, d_loss: -120.62489318847656,  g_loss: 82.42986297607422\n",
            "Training epoch 1288/1000000, d_loss: -72.5793685913086,  g_loss: 10.701675415039062\n",
            "Training epoch 1289/1000000, d_loss: -32.435333251953125,  g_loss: 41.809085845947266\n",
            "Training epoch 1290/1000000, d_loss: -917.3804321289062,  g_loss: -379.59417724609375\n",
            "Training epoch 1291/1000000, d_loss: -44.901344299316406,  g_loss: -136.06256103515625\n",
            "Training epoch 1292/1000000, d_loss: 13.729537963867188,  g_loss: 77.78742218017578\n",
            "Training epoch 1293/1000000, d_loss: -103.93367767333984,  g_loss: 118.69417572021484\n",
            "Training epoch 1294/1000000, d_loss: -129.3306884765625,  g_loss: 62.90168762207031\n",
            "Training epoch 1295/1000000, d_loss: -46.87968444824219,  g_loss: 143.59979248046875\n",
            "Training epoch 1296/1000000, d_loss: -136.02072143554688,  g_loss: 133.0550537109375\n",
            "Training epoch 1297/1000000, d_loss: -98.06863403320312,  g_loss: 371.49578857421875\n",
            "Training epoch 1298/1000000, d_loss: -202.48516845703125,  g_loss: 45.98914337158203\n",
            "Training epoch 1299/1000000, d_loss: -90.64585876464844,  g_loss: 86.39065551757812\n",
            "Training epoch 1300/1000000, d_loss: -116.48988342285156,  g_loss: 104.83642578125\n",
            "Training epoch 1301/1000000, d_loss: -40.951663970947266,  g_loss: 111.50483703613281\n",
            "Training epoch 1302/1000000, d_loss: -24.222368240356445,  g_loss: 119.20184326171875\n",
            "Training epoch 1303/1000000, d_loss: -160.06961059570312,  g_loss: 138.99554443359375\n",
            "Training epoch 1304/1000000, d_loss: -33.278079986572266,  g_loss: 234.8216552734375\n",
            "Training epoch 1305/1000000, d_loss: -37.7920036315918,  g_loss: 58.54405212402344\n",
            "Training epoch 1306/1000000, d_loss: -92.57801818847656,  g_loss: 84.02090454101562\n",
            "Training epoch 1307/1000000, d_loss: -0.09920120239257812,  g_loss: 66.7748031616211\n",
            "Training epoch 1308/1000000, d_loss: -66.65228271484375,  g_loss: 61.12126922607422\n",
            "Training epoch 1309/1000000, d_loss: -66.47615814208984,  g_loss: 46.741111755371094\n",
            "Training epoch 1310/1000000, d_loss: -81.197998046875,  g_loss: 75.86168670654297\n",
            "Training epoch 1311/1000000, d_loss: -253.1056365966797,  g_loss: -149.78350830078125\n",
            "Training epoch 1312/1000000, d_loss: -410.62396240234375,  g_loss: -206.065673828125\n",
            "Training epoch 1313/1000000, d_loss: 8.371809005737305,  g_loss: -27.007984161376953\n",
            "Training epoch 1314/1000000, d_loss: -9.852777481079102,  g_loss: 32.864688873291016\n",
            "Training epoch 1315/1000000, d_loss: -172.72854614257812,  g_loss: -97.77874755859375\n",
            "Training epoch 1316/1000000, d_loss: -122.235595703125,  g_loss: -133.30320739746094\n",
            "Training epoch 1317/1000000, d_loss: -152.10202026367188,  g_loss: -139.43728637695312\n",
            "Training epoch 1318/1000000, d_loss: -0.13303756713867188,  g_loss: -10.954553604125977\n",
            "Training epoch 1319/1000000, d_loss: -138.2090606689453,  g_loss: 239.2019805908203\n",
            "Training epoch 1320/1000000, d_loss: -109.08570861816406,  g_loss: 30.873859405517578\n",
            "Training epoch 1321/1000000, d_loss: -55.41337585449219,  g_loss: 103.91127014160156\n",
            "Training epoch 1322/1000000, d_loss: -71.59814453125,  g_loss: 164.26522827148438\n",
            "Training epoch 1323/1000000, d_loss: -42.32395935058594,  g_loss: 227.62728881835938\n",
            "Training epoch 1324/1000000, d_loss: -20.095964431762695,  g_loss: 58.04835510253906\n",
            "Training epoch 1325/1000000, d_loss: -110.60259246826172,  g_loss: 24.772859573364258\n",
            "Training epoch 1326/1000000, d_loss: -172.2194366455078,  g_loss: -47.748863220214844\n",
            "Training epoch 1327/1000000, d_loss: -118.995361328125,  g_loss: -193.9508056640625\n",
            "Training epoch 1328/1000000, d_loss: -24.801044464111328,  g_loss: -68.39117431640625\n",
            "Training epoch 1329/1000000, d_loss: 98.00686645507812,  g_loss: 45.66423797607422\n",
            "Training epoch 1330/1000000, d_loss: -130.88836669921875,  g_loss: -0.7877426147460938\n",
            "Training epoch 1331/1000000, d_loss: -4.639577865600586,  g_loss: 62.707244873046875\n",
            "Training epoch 1332/1000000, d_loss: -71.65983581542969,  g_loss: 57.16706848144531\n",
            "Training epoch 1333/1000000, d_loss: -49.108787536621094,  g_loss: 25.248828887939453\n",
            "Training epoch 1334/1000000, d_loss: -425.24676513671875,  g_loss: -141.19021606445312\n",
            "Training epoch 1335/1000000, d_loss: 6.716460227966309,  g_loss: -3.962862491607666\n",
            "Training epoch 1336/1000000, d_loss: -145.4468536376953,  g_loss: -32.762264251708984\n",
            "Training epoch 1337/1000000, d_loss: -77.87531280517578,  g_loss: 23.60479736328125\n",
            "Training epoch 1338/1000000, d_loss: -29.5897216796875,  g_loss: 80.06230926513672\n",
            "Training epoch 1339/1000000, d_loss: -64.96755981445312,  g_loss: 52.293006896972656\n",
            "Training epoch 1340/1000000, d_loss: -53.57855224609375,  g_loss: 109.78054809570312\n",
            "Training epoch 1341/1000000, d_loss: -171.4392852783203,  g_loss: 254.11447143554688\n",
            "Training epoch 1342/1000000, d_loss: -5.286521911621094,  g_loss: 23.310224533081055\n",
            "Training epoch 1343/1000000, d_loss: -105.61752319335938,  g_loss: -49.28200149536133\n",
            "Training epoch 1344/1000000, d_loss: -42.081790924072266,  g_loss: -36.588802337646484\n",
            "Training epoch 1345/1000000, d_loss: -22.20587921142578,  g_loss: -52.95836639404297\n",
            "Training epoch 1346/1000000, d_loss: -81.21895599365234,  g_loss: -70.07481384277344\n",
            "Training epoch 1347/1000000, d_loss: -102.78082275390625,  g_loss: -17.412742614746094\n",
            "Training epoch 1348/1000000, d_loss: -50.68230438232422,  g_loss: 19.373743057250977\n",
            "Training epoch 1349/1000000, d_loss: -70.04138946533203,  g_loss: -9.901680946350098\n",
            "Training epoch 1350/1000000, d_loss: -152.63668823242188,  g_loss: -24.307641983032227\n",
            "Training epoch 1351/1000000, d_loss: -42.286075592041016,  g_loss: 0.8489251136779785\n",
            "Training epoch 1352/1000000, d_loss: -36.063758850097656,  g_loss: -60.038291931152344\n",
            "Training epoch 1353/1000000, d_loss: -445.7735290527344,  g_loss: -230.7970428466797\n",
            "Training epoch 1354/1000000, d_loss: -171.5326690673828,  g_loss: -159.6034393310547\n",
            "Training epoch 1355/1000000, d_loss: -55.31553649902344,  g_loss: 184.139892578125\n",
            "Training epoch 1356/1000000, d_loss: -75.68766784667969,  g_loss: 78.96090698242188\n",
            "Training epoch 1357/1000000, d_loss: -56.53965759277344,  g_loss: 108.3651351928711\n",
            "Training epoch 1358/1000000, d_loss: -134.00955200195312,  g_loss: 158.05224609375\n",
            "Training epoch 1359/1000000, d_loss: -37.981544494628906,  g_loss: 102.31236267089844\n",
            "Training epoch 1360/1000000, d_loss: -43.8368034362793,  g_loss: 75.83771514892578\n",
            "Training epoch 1361/1000000, d_loss: -95.29104614257812,  g_loss: 53.38910675048828\n",
            "Training epoch 1362/1000000, d_loss: -24.813182830810547,  g_loss: 114.5860595703125\n",
            "Training epoch 1363/1000000, d_loss: -165.1142120361328,  g_loss: 17.00684356689453\n",
            "Training epoch 1364/1000000, d_loss: -444.04095458984375,  g_loss: -217.46922302246094\n",
            "Training epoch 1365/1000000, d_loss: -22.237377166748047,  g_loss: -66.97013854980469\n",
            "Training epoch 1366/1000000, d_loss: -36.66986846923828,  g_loss: -44.303192138671875\n",
            "Training epoch 1367/1000000, d_loss: -179.57125854492188,  g_loss: -158.35348510742188\n",
            "Training epoch 1368/1000000, d_loss: -46.52042007446289,  g_loss: -40.347259521484375\n",
            "Training epoch 1369/1000000, d_loss: -103.49248504638672,  g_loss: -66.50669860839844\n",
            "Training epoch 1370/1000000, d_loss: -60.48493957519531,  g_loss: 87.65744018554688\n",
            "Training epoch 1371/1000000, d_loss: 25.198692321777344,  g_loss: -64.7774658203125\n",
            "Training epoch 1372/1000000, d_loss: -69.75205993652344,  g_loss: 24.26769256591797\n",
            "Training epoch 1373/1000000, d_loss: -97.45236206054688,  g_loss: 145.22332763671875\n",
            "Training epoch 1374/1000000, d_loss: -132.43777465820312,  g_loss: 5.54779052734375\n",
            "Training epoch 1375/1000000, d_loss: 1167.0623779296875,  g_loss: -99.60870361328125\n",
            "Training epoch 1376/1000000, d_loss: 14.239326477050781,  g_loss: -37.66453552246094\n",
            "Training epoch 1377/1000000, d_loss: -68.90801239013672,  g_loss: -28.41651725769043\n",
            "Training epoch 1378/1000000, d_loss: -112.776611328125,  g_loss: -29.07701873779297\n",
            "Training epoch 1379/1000000, d_loss: -87.43687438964844,  g_loss: -51.00083923339844\n",
            "Training epoch 1380/1000000, d_loss: -74.31731414794922,  g_loss: -57.02172088623047\n",
            "Training epoch 1381/1000000, d_loss: -81.97926330566406,  g_loss: -0.4952080249786377\n",
            "Training epoch 1382/1000000, d_loss: -24.76906967163086,  g_loss: 32.63043975830078\n",
            "Training epoch 1383/1000000, d_loss: -39.09410858154297,  g_loss: 90.94761657714844\n",
            "Training epoch 1384/1000000, d_loss: -92.78855895996094,  g_loss: -33.22267532348633\n",
            "Training epoch 1385/1000000, d_loss: -86.10616302490234,  g_loss: -28.79308319091797\n",
            "Training epoch 1386/1000000, d_loss: -41.4830207824707,  g_loss: -13.91755199432373\n",
            "Training epoch 1387/1000000, d_loss: -44.24853515625,  g_loss: 63.643585205078125\n",
            "Training epoch 1388/1000000, d_loss: -98.78555297851562,  g_loss: 4.311581134796143\n",
            "Training epoch 1389/1000000, d_loss: -34.388526916503906,  g_loss: -1.9809658527374268\n",
            "Training epoch 1390/1000000, d_loss: -75.31520080566406,  g_loss: 7.093290328979492\n",
            "Training epoch 1391/1000000, d_loss: -2.0322580337524414,  g_loss: 43.301090240478516\n",
            "Training epoch 1392/1000000, d_loss: -82.89761352539062,  g_loss: 63.18279266357422\n",
            "Training epoch 1393/1000000, d_loss: -101.1856918334961,  g_loss: -7.115320205688477\n",
            "Training epoch 1394/1000000, d_loss: -48.19647216796875,  g_loss: 35.73249053955078\n",
            "Training epoch 1395/1000000, d_loss: -35.16533279418945,  g_loss: 25.422800064086914\n",
            "Training epoch 1396/1000000, d_loss: -53.785369873046875,  g_loss: -105.4451904296875\n",
            "Training epoch 1397/1000000, d_loss: -60.98888397216797,  g_loss: -97.77595520019531\n",
            "Training epoch 1398/1000000, d_loss: -318.61505126953125,  g_loss: -262.1716613769531\n",
            "Training epoch 1399/1000000, d_loss: -301.9694519042969,  g_loss: -215.16778564453125\n",
            "Training epoch 1400/1000000, d_loss: -29.560546875,  g_loss: -45.46278381347656\n",
            "Training epoch 1401/1000000, d_loss: -115.64703369140625,  g_loss: -85.13101196289062\n",
            "Training epoch 1402/1000000, d_loss: -225.94528198242188,  g_loss: -169.86746215820312\n",
            "Training epoch 1403/1000000, d_loss: -156.91912841796875,  g_loss: 25.14251708984375\n",
            "Training epoch 1404/1000000, d_loss: -186.01727294921875,  g_loss: 225.308349609375\n",
            "Training epoch 1405/1000000, d_loss: -204.96705627441406,  g_loss: 284.7347412109375\n",
            "Training epoch 1406/1000000, d_loss: -74.3088150024414,  g_loss: 76.51231384277344\n",
            "Training epoch 1407/1000000, d_loss: -97.1616439819336,  g_loss: 40.565921783447266\n",
            "Training epoch 1408/1000000, d_loss: -97.29193115234375,  g_loss: 43.16029357910156\n",
            "Training epoch 1409/1000000, d_loss: -108.464111328125,  g_loss: 33.465396881103516\n",
            "Training epoch 1410/1000000, d_loss: -16.007213592529297,  g_loss: 140.4256134033203\n",
            "Training epoch 1411/1000000, d_loss: -193.00254821777344,  g_loss: -43.99626922607422\n",
            "Training epoch 1412/1000000, d_loss: -242.2555389404297,  g_loss: 3.0666160583496094\n",
            "Training epoch 1413/1000000, d_loss: -113.45669555664062,  g_loss: 96.02565002441406\n",
            "Training epoch 1414/1000000, d_loss: -71.88129425048828,  g_loss: -5.444465637207031\n",
            "Training epoch 1415/1000000, d_loss: -138.2769317626953,  g_loss: -23.030670166015625\n",
            "Training epoch 1416/1000000, d_loss: -86.61312103271484,  g_loss: 51.979190826416016\n",
            "Training epoch 1417/1000000, d_loss: -168.59292602539062,  g_loss: 80.56470489501953\n",
            "Training epoch 1418/1000000, d_loss: -295.1912536621094,  g_loss: -199.38751220703125\n",
            "Training epoch 1419/1000000, d_loss: -155.32809448242188,  g_loss: -10.111133575439453\n",
            "Training epoch 1420/1000000, d_loss: -62.79169464111328,  g_loss: 23.268024444580078\n",
            "Training epoch 1421/1000000, d_loss: -96.09761810302734,  g_loss: 58.93528747558594\n",
            "Training epoch 1422/1000000, d_loss: -32.52039337158203,  g_loss: 52.80722427368164\n",
            "Training epoch 1423/1000000, d_loss: -64.88002014160156,  g_loss: 97.96100616455078\n",
            "Training epoch 1424/1000000, d_loss: -347.7522888183594,  g_loss: -97.60443115234375\n",
            "Training epoch 1425/1000000, d_loss: -26.656890869140625,  g_loss: -6.6933135986328125\n",
            "Training epoch 1426/1000000, d_loss: -81.68016052246094,  g_loss: 96.39630126953125\n",
            "Training epoch 1427/1000000, d_loss: -458.1580810546875,  g_loss: -219.23690795898438\n",
            "Training epoch 1428/1000000, d_loss: 37.70259475708008,  g_loss: 37.17713165283203\n",
            "Training epoch 1429/1000000, d_loss: -238.50518798828125,  g_loss: 24.49820899963379\n",
            "Training epoch 1430/1000000, d_loss: -2.35064697265625,  g_loss: 137.19113159179688\n",
            "Training epoch 1431/1000000, d_loss: -221.45521545410156,  g_loss: 562.7007446289062\n",
            "Training epoch 1432/1000000, d_loss: -133.96595764160156,  g_loss: 150.71568298339844\n",
            "Training epoch 1433/1000000, d_loss: -143.80072021484375,  g_loss: -7.974632263183594\n",
            "Training epoch 1434/1000000, d_loss: -59.13970184326172,  g_loss: 135.65745544433594\n",
            "Training epoch 1435/1000000, d_loss: -273.53961181640625,  g_loss: 603.7903442382812\n",
            "Training epoch 1436/1000000, d_loss: -144.97149658203125,  g_loss: 318.98944091796875\n",
            "Training epoch 1437/1000000, d_loss: -103.6930923461914,  g_loss: 41.33713150024414\n",
            "Training epoch 1438/1000000, d_loss: -102.34043884277344,  g_loss: -28.96399688720703\n",
            "Training epoch 1439/1000000, d_loss: -125.99764251708984,  g_loss: 201.81475830078125\n",
            "Training epoch 1440/1000000, d_loss: -686.0596923828125,  g_loss: -217.1432647705078\n",
            "Training epoch 1441/1000000, d_loss: -118.98346710205078,  g_loss: -259.8838195800781\n",
            "Training epoch 1442/1000000, d_loss: -47.27315139770508,  g_loss: -98.20263671875\n",
            "Training epoch 1443/1000000, d_loss: -124.6250991821289,  g_loss: 89.87770080566406\n",
            "Training epoch 1444/1000000, d_loss: -11.376728057861328,  g_loss: 6.882745265960693\n",
            "Training epoch 1445/1000000, d_loss: -163.392578125,  g_loss: 32.81281280517578\n",
            "Training epoch 1446/1000000, d_loss: -81.40068817138672,  g_loss: -4.844296455383301\n",
            "Training epoch 1447/1000000, d_loss: -135.94821166992188,  g_loss: 21.33004379272461\n",
            "Training epoch 1448/1000000, d_loss: -128.81219482421875,  g_loss: 192.1354522705078\n",
            "Training epoch 1449/1000000, d_loss: -93.32202911376953,  g_loss: -27.051116943359375\n",
            "Training epoch 1450/1000000, d_loss: -64.79940795898438,  g_loss: 8.660360336303711\n",
            "Training epoch 1451/1000000, d_loss: -450.4468994140625,  g_loss: -200.94729614257812\n",
            "Training epoch 1452/1000000, d_loss: -227.7327117919922,  g_loss: -194.1725616455078\n",
            "Training epoch 1453/1000000, d_loss: 12.007920265197754,  g_loss: -81.54794311523438\n",
            "Training epoch 1454/1000000, d_loss: -110.25202178955078,  g_loss: 137.529052734375\n",
            "Training epoch 1455/1000000, d_loss: -87.30174255371094,  g_loss: 150.2635955810547\n",
            "Training epoch 1456/1000000, d_loss: -114.85779571533203,  g_loss: 10.311113357543945\n",
            "Training epoch 1457/1000000, d_loss: -38.69043731689453,  g_loss: 81.89805603027344\n",
            "Training epoch 1458/1000000, d_loss: -102.11820220947266,  g_loss: 74.2233657836914\n",
            "Training epoch 1459/1000000, d_loss: -86.91397857666016,  g_loss: 24.645427703857422\n",
            "Training epoch 1460/1000000, d_loss: -98.96550750732422,  g_loss: -1.9958124160766602\n",
            "Training epoch 1461/1000000, d_loss: -277.6083068847656,  g_loss: -114.18301391601562\n",
            "Training epoch 1462/1000000, d_loss: -37.28717803955078,  g_loss: -14.928241729736328\n",
            "Training epoch 1463/1000000, d_loss: -77.79052734375,  g_loss: -76.69505310058594\n",
            "Training epoch 1464/1000000, d_loss: -554.7171020507812,  g_loss: -329.1572265625\n",
            "Training epoch 1465/1000000, d_loss: -115.06703186035156,  g_loss: -119.57146453857422\n",
            "Training epoch 1466/1000000, d_loss: 12.511825561523438,  g_loss: -99.32257080078125\n",
            "Training epoch 1467/1000000, d_loss: 6.1745758056640625,  g_loss: 145.477783203125\n",
            "Training epoch 1468/1000000, d_loss: -157.22024536132812,  g_loss: -34.50023651123047\n",
            "Training epoch 1469/1000000, d_loss: -169.29379272460938,  g_loss: -191.83961486816406\n",
            "Training epoch 1470/1000000, d_loss: -66.24968719482422,  g_loss: -90.56704711914062\n",
            "Training epoch 1471/1000000, d_loss: -45.2315673828125,  g_loss: 104.14216613769531\n",
            "Training epoch 1472/1000000, d_loss: -34.151123046875,  g_loss: 25.35807991027832\n",
            "Training epoch 1473/1000000, d_loss: -32.95780563354492,  g_loss: 66.03060913085938\n",
            "Training epoch 1474/1000000, d_loss: -57.54673767089844,  g_loss: 42.773681640625\n",
            "Training epoch 1475/1000000, d_loss: -41.06510925292969,  g_loss: 80.98959350585938\n",
            "Training epoch 1476/1000000, d_loss: -38.338409423828125,  g_loss: 91.2861328125\n",
            "Training epoch 1477/1000000, d_loss: -72.32096099853516,  g_loss: 133.91580200195312\n",
            "Training epoch 1478/1000000, d_loss: -22.080486297607422,  g_loss: 27.473711013793945\n",
            "Training epoch 1479/1000000, d_loss: -59.72275161743164,  g_loss: 79.80088806152344\n",
            "Training epoch 1480/1000000, d_loss: -66.79219055175781,  g_loss: 1.0280075073242188\n",
            "Training epoch 1481/1000000, d_loss: -36.70671844482422,  g_loss: 84.1256103515625\n",
            "Training epoch 1482/1000000, d_loss: -117.23500061035156,  g_loss: 81.54237365722656\n",
            "Training epoch 1483/1000000, d_loss: -37.62261199951172,  g_loss: 9.646501541137695\n",
            "Training epoch 1484/1000000, d_loss: -587.517333984375,  g_loss: -240.23342895507812\n",
            "Training epoch 1485/1000000, d_loss: -122.53178405761719,  g_loss: -77.68505859375\n",
            "Training epoch 1486/1000000, d_loss: -101.49898529052734,  g_loss: -95.8545150756836\n",
            "Training epoch 1487/1000000, d_loss: -42.0860481262207,  g_loss: 44.741294860839844\n",
            "Training epoch 1488/1000000, d_loss: -58.156105041503906,  g_loss: 116.37974548339844\n",
            "Training epoch 1489/1000000, d_loss: -68.94751739501953,  g_loss: 73.78421020507812\n",
            "Training epoch 1490/1000000, d_loss: -121.21732330322266,  g_loss: 124.1024398803711\n",
            "Training epoch 1491/1000000, d_loss: -80.99299621582031,  g_loss: 152.22879028320312\n",
            "Training epoch 1492/1000000, d_loss: -51.40681457519531,  g_loss: 44.27946853637695\n",
            "Training epoch 1493/1000000, d_loss: -54.237937927246094,  g_loss: 98.99500274658203\n",
            "Training epoch 1494/1000000, d_loss: -76.6724624633789,  g_loss: 113.20758819580078\n",
            "Training epoch 1495/1000000, d_loss: 65.08758544921875,  g_loss: 24.923349380493164\n",
            "Training epoch 1496/1000000, d_loss: -76.02444458007812,  g_loss: 47.06436538696289\n",
            "Training epoch 1497/1000000, d_loss: -18.650188446044922,  g_loss: 43.38400650024414\n",
            "Training epoch 1498/1000000, d_loss: -17.776288986206055,  g_loss: 112.29444885253906\n",
            "Training epoch 1499/1000000, d_loss: 59.524383544921875,  g_loss: 111.87474060058594\n",
            "Training epoch 1500/1000000, d_loss: -113.45345306396484,  g_loss: -24.775609970092773\n",
            "Training epoch 1501/1000000, d_loss: -57.024330139160156,  g_loss: 27.94417953491211\n",
            "Training epoch 1502/1000000, d_loss: -157.65966796875,  g_loss: 167.26902770996094\n",
            "Training epoch 1503/1000000, d_loss: -195.97149658203125,  g_loss: -32.020259857177734\n",
            "Training epoch 1504/1000000, d_loss: -86.32704162597656,  g_loss: 48.7901611328125\n",
            "Training epoch 1505/1000000, d_loss: -157.68223571777344,  g_loss: 148.6653594970703\n",
            "Training epoch 1506/1000000, d_loss: 52.014564514160156,  g_loss: 63.13302230834961\n",
            "Training epoch 1507/1000000, d_loss: -79.56013488769531,  g_loss: 37.080909729003906\n",
            "Training epoch 1508/1000000, d_loss: -52.908687591552734,  g_loss: 69.97479248046875\n",
            "Training epoch 1509/1000000, d_loss: -71.16960144042969,  g_loss: 0.512995719909668\n",
            "Training epoch 1510/1000000, d_loss: -74.48700714111328,  g_loss: -31.843772888183594\n",
            "Training epoch 1511/1000000, d_loss: -179.83966064453125,  g_loss: -57.90776443481445\n",
            "Training epoch 1512/1000000, d_loss: -22.6686954498291,  g_loss: -28.968828201293945\n",
            "Training epoch 1513/1000000, d_loss: -655.3172607421875,  g_loss: -318.48553466796875\n",
            "Training epoch 1514/1000000, d_loss: -10.588409423828125,  g_loss: -35.71033477783203\n",
            "Training epoch 1515/1000000, d_loss: -21.120840072631836,  g_loss: -125.80992889404297\n",
            "Training epoch 1516/1000000, d_loss: -393.1473693847656,  g_loss: -185.79544067382812\n",
            "Training epoch 1517/1000000, d_loss: -144.67112731933594,  g_loss: -186.48519897460938\n",
            "Training epoch 1518/1000000, d_loss: 7.886989593505859,  g_loss: -44.07244873046875\n",
            "Training epoch 1519/1000000, d_loss: -229.84674072265625,  g_loss: -332.15557861328125\n",
            "Training epoch 1520/1000000, d_loss: -107.98934936523438,  g_loss: 132.40789794921875\n",
            "Training epoch 1521/1000000, d_loss: -80.83199310302734,  g_loss: 136.04745483398438\n",
            "Training epoch 1522/1000000, d_loss: -133.98472595214844,  g_loss: 36.2236328125\n",
            "Training epoch 1523/1000000, d_loss: -50.17864990234375,  g_loss: 29.392379760742188\n",
            "Training epoch 1524/1000000, d_loss: -91.47988891601562,  g_loss: 36.69298553466797\n",
            "Training epoch 1525/1000000, d_loss: -153.27081298828125,  g_loss: 233.28948974609375\n",
            "Training epoch 1526/1000000, d_loss: -177.63311767578125,  g_loss: 116.28053283691406\n",
            "Training epoch 1527/1000000, d_loss: -193.3514862060547,  g_loss: -16.217876434326172\n",
            "Training epoch 1528/1000000, d_loss: -55.658531188964844,  g_loss: 58.94987869262695\n",
            "Training epoch 1529/1000000, d_loss: -46.91912078857422,  g_loss: 165.2454833984375\n",
            "Training epoch 1530/1000000, d_loss: -109.73076629638672,  g_loss: 246.8916015625\n",
            "Training epoch 1531/1000000, d_loss: -21.376232147216797,  g_loss: 109.85198211669922\n",
            "Training epoch 1532/1000000, d_loss: -134.49267578125,  g_loss: 40.03058624267578\n",
            "Training epoch 1533/1000000, d_loss: -61.69935607910156,  g_loss: 73.34706115722656\n",
            "Training epoch 1534/1000000, d_loss: -30.108882904052734,  g_loss: 4.75082540512085\n",
            "Training epoch 1535/1000000, d_loss: -392.136474609375,  g_loss: -313.8858642578125\n",
            "Training epoch 1536/1000000, d_loss: -35.64796447753906,  g_loss: -163.705810546875\n",
            "Training epoch 1537/1000000, d_loss: -51.861793518066406,  g_loss: -76.70761108398438\n",
            "Training epoch 1538/1000000, d_loss: -62.461917877197266,  g_loss: 4.501655578613281\n",
            "Training epoch 1539/1000000, d_loss: -266.3679504394531,  g_loss: -159.90579223632812\n",
            "Training epoch 1540/1000000, d_loss: -126.76472473144531,  g_loss: -149.67210388183594\n",
            "Training epoch 1541/1000000, d_loss: -13.13819408416748,  g_loss: -3.0202598571777344\n",
            "Training epoch 1542/1000000, d_loss: -140.8001708984375,  g_loss: -119.78504180908203\n",
            "Training epoch 1543/1000000, d_loss: -91.51569366455078,  g_loss: -35.745033264160156\n",
            "Training epoch 1544/1000000, d_loss: -95.45379638671875,  g_loss: 35.32718276977539\n",
            "Training epoch 1545/1000000, d_loss: -85.86764526367188,  g_loss: 159.64608764648438\n",
            "Training epoch 1546/1000000, d_loss: -156.96990966796875,  g_loss: 78.1546401977539\n",
            "Training epoch 1547/1000000, d_loss: -34.61548614501953,  g_loss: 40.01103210449219\n",
            "Training epoch 1548/1000000, d_loss: -97.08515930175781,  g_loss: -14.22330093383789\n",
            "Training epoch 1549/1000000, d_loss: -54.02030944824219,  g_loss: 93.01423645019531\n",
            "Training epoch 1550/1000000, d_loss: -173.64112854003906,  g_loss: -43.84947967529297\n",
            "Training epoch 1551/1000000, d_loss: -33.665382385253906,  g_loss: 57.590675354003906\n",
            "Training epoch 1552/1000000, d_loss: -93.24697875976562,  g_loss: 1.641770839691162\n",
            "Training epoch 1553/1000000, d_loss: -68.278076171875,  g_loss: -8.025737762451172\n",
            "Training epoch 1554/1000000, d_loss: -71.54400634765625,  g_loss: -4.822023391723633\n",
            "Training epoch 1555/1000000, d_loss: -63.01817321777344,  g_loss: -24.850160598754883\n",
            "Training epoch 1556/1000000, d_loss: -114.68800354003906,  g_loss: 16.939178466796875\n",
            "Training epoch 1557/1000000, d_loss: -99.49439239501953,  g_loss: 32.85405731201172\n",
            "Training epoch 1558/1000000, d_loss: -133.3060302734375,  g_loss: 6.959827423095703\n",
            "Training epoch 1559/1000000, d_loss: -457.5447082519531,  g_loss: -296.1194763183594\n",
            "Training epoch 1560/1000000, d_loss: -86.45753479003906,  g_loss: -88.06697082519531\n",
            "Training epoch 1561/1000000, d_loss: 3.1869659423828125,  g_loss: 100.98609924316406\n",
            "Training epoch 1562/1000000, d_loss: -157.62649536132812,  g_loss: -72.9865951538086\n",
            "Training epoch 1563/1000000, d_loss: -117.6776351928711,  g_loss: 188.03173828125\n",
            "Training epoch 1564/1000000, d_loss: -48.63990020751953,  g_loss: 53.745704650878906\n",
            "Training epoch 1565/1000000, d_loss: -33.0128288269043,  g_loss: 7.090888023376465\n",
            "Training epoch 1566/1000000, d_loss: 7.397538185119629,  g_loss: 7.913183689117432\n",
            "Training epoch 1567/1000000, d_loss: -358.9827575683594,  g_loss: -233.7781982421875\n",
            "Training epoch 1568/1000000, d_loss: -191.5021514892578,  g_loss: -110.54916381835938\n",
            "Training epoch 1569/1000000, d_loss: -266.0885925292969,  g_loss: -188.6194610595703\n",
            "Training epoch 1570/1000000, d_loss: -302.6055603027344,  g_loss: -211.5930938720703\n",
            "Training epoch 1571/1000000, d_loss: -145.83975219726562,  g_loss: 286.6256408691406\n",
            "Training epoch 1572/1000000, d_loss: -589.9052734375,  g_loss: -717.3333740234375\n",
            "Training epoch 1573/1000000, d_loss: -34.60479736328125,  g_loss: 43.774322509765625\n",
            "Training epoch 1574/1000000, d_loss: -171.43577575683594,  g_loss: 204.66781616210938\n",
            "Training epoch 1575/1000000, d_loss: -551.5901489257812,  g_loss: 253.53587341308594\n",
            "Training epoch 1576/1000000, d_loss: 262.3413391113281,  g_loss: -77.02993774414062\n",
            "Training epoch 1577/1000000, d_loss: -55.90644836425781,  g_loss: 278.8568420410156\n",
            "Training epoch 1578/1000000, d_loss: -239.64169311523438,  g_loss: 537.7791748046875\n",
            "Training epoch 1579/1000000, d_loss: -6.579692840576172,  g_loss: 281.0366516113281\n",
            "Training epoch 1580/1000000, d_loss: -130.88186645507812,  g_loss: 125.3025131225586\n",
            "Training epoch 1581/1000000, d_loss: 31.06109619140625,  g_loss: 105.6686019897461\n",
            "Training epoch 1582/1000000, d_loss: -68.47439575195312,  g_loss: 146.8880157470703\n",
            "Training epoch 1583/1000000, d_loss: -95.81529235839844,  g_loss: 153.1968994140625\n",
            "Training epoch 1584/1000000, d_loss: -131.36135864257812,  g_loss: 119.99472045898438\n",
            "Training epoch 1585/1000000, d_loss: -196.2989044189453,  g_loss: 157.46665954589844\n",
            "Training epoch 1586/1000000, d_loss: -42.77988052368164,  g_loss: 113.4808578491211\n",
            "Training epoch 1587/1000000, d_loss: -99.90748596191406,  g_loss: 141.63381958007812\n",
            "Training epoch 1588/1000000, d_loss: -13.687047958374023,  g_loss: 148.53448486328125\n",
            "Training epoch 1589/1000000, d_loss: -113.11621856689453,  g_loss: 75.59977722167969\n",
            "Training epoch 1590/1000000, d_loss: -35.16565704345703,  g_loss: 76.0919189453125\n",
            "Training epoch 1591/1000000, d_loss: -71.59075927734375,  g_loss: -3.615720748901367\n",
            "Training epoch 1592/1000000, d_loss: -953.3796997070312,  g_loss: -352.66302490234375\n",
            "Training epoch 1593/1000000, d_loss: 165.23306274414062,  g_loss: -87.88970947265625\n",
            "Training epoch 1594/1000000, d_loss: -62.215553283691406,  g_loss: 5.71514892578125\n",
            "Training epoch 1595/1000000, d_loss: -41.77803421020508,  g_loss: -22.35062026977539\n",
            "Training epoch 1596/1000000, d_loss: -186.55520629882812,  g_loss: 245.48709106445312\n",
            "Training epoch 1597/1000000, d_loss: -27.50688934326172,  g_loss: -37.37258529663086\n",
            "Training epoch 1598/1000000, d_loss: 2.447500228881836,  g_loss: 15.085708618164062\n",
            "Training epoch 1599/1000000, d_loss: -81.97248077392578,  g_loss: -69.11357879638672\n",
            "Training epoch 1600/1000000, d_loss: -64.7342300415039,  g_loss: -92.93904113769531\n",
            "Training epoch 1601/1000000, d_loss: -77.76072692871094,  g_loss: -12.745824813842773\n",
            "Training epoch 1602/1000000, d_loss: -131.732666015625,  g_loss: -10.829122543334961\n",
            "Training epoch 1603/1000000, d_loss: 16.649250030517578,  g_loss: 20.639686584472656\n",
            "Training epoch 1604/1000000, d_loss: -51.8868522644043,  g_loss: 14.846365928649902\n",
            "Training epoch 1605/1000000, d_loss: -99.7221450805664,  g_loss: 19.374244689941406\n",
            "Training epoch 1606/1000000, d_loss: -85.68682861328125,  g_loss: -40.255577087402344\n",
            "Training epoch 1607/1000000, d_loss: 0.26851558685302734,  g_loss: -29.401199340820312\n",
            "Training epoch 1608/1000000, d_loss: -83.95633697509766,  g_loss: -10.401351928710938\n",
            "Training epoch 1609/1000000, d_loss: -78.78272247314453,  g_loss: -59.42557144165039\n",
            "Training epoch 1610/1000000, d_loss: -434.3473205566406,  g_loss: -548.3580322265625\n",
            "Training epoch 1611/1000000, d_loss: 17.84210205078125,  g_loss: -156.9866943359375\n",
            "Training epoch 1612/1000000, d_loss: 73.19737243652344,  g_loss: 17.782135009765625\n",
            "Training epoch 1613/1000000, d_loss: -86.38866424560547,  g_loss: 20.992891311645508\n",
            "Training epoch 1614/1000000, d_loss: -118.47122192382812,  g_loss: 210.52430725097656\n",
            "Training epoch 1615/1000000, d_loss: -232.01309204101562,  g_loss: -157.52206420898438\n",
            "Training epoch 1616/1000000, d_loss: -429.1506042480469,  g_loss: -238.45602416992188\n",
            "Training epoch 1617/1000000, d_loss: -345.2157897949219,  g_loss: -256.0537109375\n",
            "Training epoch 1618/1000000, d_loss: -69.78557586669922,  g_loss: -47.270450592041016\n",
            "Training epoch 1619/1000000, d_loss: -61.90082550048828,  g_loss: -41.12110137939453\n",
            "Training epoch 1620/1000000, d_loss: -207.83811950683594,  g_loss: -146.5688934326172\n",
            "Training epoch 1621/1000000, d_loss: -42.33299255371094,  g_loss: 84.28071594238281\n",
            "Training epoch 1622/1000000, d_loss: -77.93505859375,  g_loss: 0.21382904052734375\n",
            "Training epoch 1623/1000000, d_loss: -36.69369888305664,  g_loss: 3.764646530151367\n",
            "Training epoch 1624/1000000, d_loss: -124.06055450439453,  g_loss: 151.32571411132812\n",
            "Training epoch 1625/1000000, d_loss: -71.39002227783203,  g_loss: 16.83365821838379\n",
            "Training epoch 1626/1000000, d_loss: -69.49880981445312,  g_loss: 64.90702819824219\n",
            "Training epoch 1627/1000000, d_loss: -178.42259216308594,  g_loss: -18.749130249023438\n",
            "Training epoch 1628/1000000, d_loss: -100.54949951171875,  g_loss: 36.960208892822266\n",
            "Training epoch 1629/1000000, d_loss: -194.69947814941406,  g_loss: -115.2012939453125\n",
            "Training epoch 1630/1000000, d_loss: 1954.55419921875,  g_loss: 122.01625061035156\n",
            "Training epoch 1631/1000000, d_loss: -37.137901306152344,  g_loss: 127.90924835205078\n",
            "Training epoch 1632/1000000, d_loss: -112.72405242919922,  g_loss: 216.07003784179688\n",
            "Training epoch 1633/1000000, d_loss: -55.503814697265625,  g_loss: 113.69499206542969\n",
            "Training epoch 1634/1000000, d_loss: -11.278491973876953,  g_loss: 150.24900817871094\n",
            "Training epoch 1635/1000000, d_loss: -33.61717987060547,  g_loss: 39.66978073120117\n",
            "Training epoch 1636/1000000, d_loss: -92.14413452148438,  g_loss: 39.69011688232422\n",
            "Training epoch 1637/1000000, d_loss: -76.59613037109375,  g_loss: 17.278587341308594\n",
            "Training epoch 1638/1000000, d_loss: -222.49444580078125,  g_loss: 322.30047607421875\n",
            "Training epoch 1639/1000000, d_loss: -122.98477935791016,  g_loss: 257.2439270019531\n",
            "Training epoch 1640/1000000, d_loss: -19.41171646118164,  g_loss: 134.3958282470703\n",
            "Training epoch 1641/1000000, d_loss: -88.7618637084961,  g_loss: 139.63253784179688\n",
            "Training epoch 1642/1000000, d_loss: -33.28453826904297,  g_loss: 135.1053009033203\n",
            "Training epoch 1643/1000000, d_loss: -35.68169403076172,  g_loss: 176.41363525390625\n",
            "Training epoch 1644/1000000, d_loss: 14.812775611877441,  g_loss: 136.9208984375\n",
            "Training epoch 1645/1000000, d_loss: -7.065123558044434,  g_loss: 103.94120788574219\n",
            "Training epoch 1646/1000000, d_loss: -120.4249267578125,  g_loss: 111.99557495117188\n",
            "Training epoch 1647/1000000, d_loss: -76.30089569091797,  g_loss: 101.90757751464844\n",
            "Training epoch 1648/1000000, d_loss: -49.26091003417969,  g_loss: 54.608760833740234\n",
            "Training epoch 1649/1000000, d_loss: -53.317806243896484,  g_loss: 171.76502990722656\n",
            "Training epoch 1650/1000000, d_loss: -344.84033203125,  g_loss: -80.18582153320312\n",
            "Training epoch 1651/1000000, d_loss: -261.5830383300781,  g_loss: -21.193702697753906\n",
            "Training epoch 1652/1000000, d_loss: -241.07803344726562,  g_loss: -216.88790893554688\n",
            "Training epoch 1653/1000000, d_loss: -72.5153579711914,  g_loss: -150.66891479492188\n",
            "Training epoch 1654/1000000, d_loss: -245.85032653808594,  g_loss: -80.27078247070312\n",
            "Training epoch 1655/1000000, d_loss: -22.2103328704834,  g_loss: -117.064697265625\n",
            "Training epoch 1656/1000000, d_loss: -29.013141632080078,  g_loss: -142.16175842285156\n",
            "Training epoch 1657/1000000, d_loss: -169.0363006591797,  g_loss: -240.4373779296875\n",
            "Training epoch 1658/1000000, d_loss: -58.398948669433594,  g_loss: -86.44342803955078\n",
            "Training epoch 1659/1000000, d_loss: -193.02783203125,  g_loss: 153.32083129882812\n",
            "Training epoch 1660/1000000, d_loss: -126.18124389648438,  g_loss: -8.996917724609375\n",
            "Training epoch 1661/1000000, d_loss: -49.38075256347656,  g_loss: -78.84956359863281\n",
            "Training epoch 1662/1000000, d_loss: -233.47653198242188,  g_loss: -237.04290771484375\n",
            "Training epoch 1663/1000000, d_loss: -154.7165069580078,  g_loss: -170.45840454101562\n",
            "Training epoch 1664/1000000, d_loss: -374.96612548828125,  g_loss: -339.06170654296875\n",
            "Training epoch 1665/1000000, d_loss: 29.155517578125,  g_loss: -108.81915283203125\n",
            "Training epoch 1666/1000000, d_loss: -51.204341888427734,  g_loss: -65.73918151855469\n",
            "Training epoch 1667/1000000, d_loss: -70.2415542602539,  g_loss: 9.056228637695312\n",
            "Training epoch 1668/1000000, d_loss: -32.18750762939453,  g_loss: -107.68695831298828\n",
            "Training epoch 1669/1000000, d_loss: -13.912033081054688,  g_loss: 152.8883056640625\n",
            "Training epoch 1670/1000000, d_loss: -26.350021362304688,  g_loss: 71.15966796875\n",
            "Training epoch 1671/1000000, d_loss: -77.72986602783203,  g_loss: 103.80679321289062\n",
            "Training epoch 1672/1000000, d_loss: -45.81199645996094,  g_loss: 139.81600952148438\n",
            "Training epoch 1673/1000000, d_loss: -150.98509216308594,  g_loss: -17.0684757232666\n",
            "Training epoch 1674/1000000, d_loss: -39.99183654785156,  g_loss: 0.532989501953125\n",
            "Training epoch 1675/1000000, d_loss: -50.57697677612305,  g_loss: 181.24928283691406\n",
            "Training epoch 1676/1000000, d_loss: -63.37103271484375,  g_loss: 58.64305877685547\n",
            "Training epoch 1677/1000000, d_loss: -380.3829650878906,  g_loss: -143.44366455078125\n",
            "Training epoch 1678/1000000, d_loss: -69.32559204101562,  g_loss: -26.26236343383789\n",
            "Training epoch 1679/1000000, d_loss: -106.79751586914062,  g_loss: 49.86603927612305\n",
            "Training epoch 1680/1000000, d_loss: -64.23645782470703,  g_loss: -78.42134094238281\n",
            "Training epoch 1681/1000000, d_loss: 21.964458465576172,  g_loss: -24.79857063293457\n",
            "Training epoch 1682/1000000, d_loss: -93.58879852294922,  g_loss: 81.40855407714844\n",
            "Training epoch 1683/1000000, d_loss: -6.6675262451171875,  g_loss: 46.18252182006836\n",
            "Training epoch 1684/1000000, d_loss: -754.4246215820312,  g_loss: -603.03173828125\n",
            "Training epoch 1685/1000000, d_loss: 81.45845031738281,  g_loss: 119.38182830810547\n",
            "Training epoch 1686/1000000, d_loss: -284.1198425292969,  g_loss: 372.4619140625\n",
            "Training epoch 1687/1000000, d_loss: -21.310279846191406,  g_loss: 379.414794921875\n",
            "Training epoch 1688/1000000, d_loss: -136.71023559570312,  g_loss: 224.96542358398438\n",
            "Training epoch 1689/1000000, d_loss: -138.40794372558594,  g_loss: 115.31732177734375\n",
            "Training epoch 1690/1000000, d_loss: -128.19830322265625,  g_loss: 139.5608673095703\n",
            "Training epoch 1691/1000000, d_loss: -96.74351501464844,  g_loss: 73.89375305175781\n",
            "Training epoch 1692/1000000, d_loss: -79.96417236328125,  g_loss: 54.85639953613281\n",
            "Training epoch 1693/1000000, d_loss: -139.209716796875,  g_loss: 287.98077392578125\n",
            "Training epoch 1694/1000000, d_loss: -11.24056625366211,  g_loss: 77.37645721435547\n",
            "Training epoch 1695/1000000, d_loss: -77.17189025878906,  g_loss: 40.90971374511719\n",
            "Training epoch 1696/1000000, d_loss: -50.68302917480469,  g_loss: 94.23495483398438\n",
            "Training epoch 1697/1000000, d_loss: -67.54838562011719,  g_loss: 7.2531208992004395\n",
            "Training epoch 1698/1000000, d_loss: -359.6651611328125,  g_loss: -134.89401245117188\n",
            "Training epoch 1699/1000000, d_loss: -46.84468078613281,  g_loss: -100.05270385742188\n",
            "Training epoch 1700/1000000, d_loss: -98.5778579711914,  g_loss: 18.884780883789062\n",
            "Training epoch 1701/1000000, d_loss: -118.76243591308594,  g_loss: 91.435546875\n",
            "Training epoch 1702/1000000, d_loss: -185.8357696533203,  g_loss: 316.1210021972656\n",
            "Training epoch 1703/1000000, d_loss: 90.84191131591797,  g_loss: 56.737159729003906\n",
            "Training epoch 1704/1000000, d_loss: -83.95043182373047,  g_loss: 159.96690368652344\n",
            "Training epoch 1705/1000000, d_loss: -86.93611145019531,  g_loss: 140.16128540039062\n",
            "Training epoch 1706/1000000, d_loss: -130.96810913085938,  g_loss: 342.3729553222656\n",
            "Training epoch 1707/1000000, d_loss: -55.51651382446289,  g_loss: 73.86477661132812\n",
            "Training epoch 1708/1000000, d_loss: -119.1027603149414,  g_loss: 32.04985809326172\n",
            "Training epoch 1709/1000000, d_loss: -70.02995300292969,  g_loss: -44.30308532714844\n",
            "Training epoch 1710/1000000, d_loss: -38.190589904785156,  g_loss: 65.40568542480469\n",
            "Training epoch 1711/1000000, d_loss: -46.49327087402344,  g_loss: 140.3377685546875\n",
            "Training epoch 1712/1000000, d_loss: -1209.3306884765625,  g_loss: -444.5472106933594\n",
            "Training epoch 1713/1000000, d_loss: -55.58207321166992,  g_loss: -176.5792236328125\n",
            "Training epoch 1714/1000000, d_loss: 4.389892578125,  g_loss: -316.055419921875\n",
            "Training epoch 1715/1000000, d_loss: 50.62644958496094,  g_loss: -200.40281677246094\n",
            "Training epoch 1716/1000000, d_loss: -394.1673889160156,  g_loss: -288.87457275390625\n",
            "Training epoch 1717/1000000, d_loss: -12.315925598144531,  g_loss: -166.0496826171875\n",
            "Training epoch 1718/1000000, d_loss: -99.96884155273438,  g_loss: 23.891096115112305\n",
            "Training epoch 1719/1000000, d_loss: -164.3039093017578,  g_loss: 58.22672653198242\n",
            "Training epoch 1720/1000000, d_loss: -140.91815185546875,  g_loss: -111.40339660644531\n",
            "Training epoch 1721/1000000, d_loss: -71.33910369873047,  g_loss: 114.1528091430664\n",
            "Training epoch 1722/1000000, d_loss: -134.18202209472656,  g_loss: 202.33639526367188\n",
            "Training epoch 1723/1000000, d_loss: -88.74005126953125,  g_loss: 116.442138671875\n",
            "Training epoch 1724/1000000, d_loss: -60.787513732910156,  g_loss: 84.98605346679688\n",
            "Training epoch 1725/1000000, d_loss: -181.44268798828125,  g_loss: -2.2658958435058594\n",
            "Training epoch 1726/1000000, d_loss: -96.4883041381836,  g_loss: -52.957855224609375\n",
            "Training epoch 1727/1000000, d_loss: -117.53105163574219,  g_loss: 204.35643005371094\n",
            "Training epoch 1728/1000000, d_loss: -552.665283203125,  g_loss: -176.2787322998047\n",
            "Training epoch 1729/1000000, d_loss: -57.20109558105469,  g_loss: 97.97683715820312\n",
            "Training epoch 1730/1000000, d_loss: -225.97698974609375,  g_loss: 274.4679260253906\n",
            "Training epoch 1731/1000000, d_loss: -105.30528259277344,  g_loss: 115.39764404296875\n",
            "Training epoch 1732/1000000, d_loss: -234.85401916503906,  g_loss: -8.119758605957031\n",
            "Training epoch 1733/1000000, d_loss: 21.618858337402344,  g_loss: 98.63023376464844\n",
            "Training epoch 1734/1000000, d_loss: -32.13957595825195,  g_loss: 94.21714782714844\n",
            "Training epoch 1735/1000000, d_loss: -1343.7723388671875,  g_loss: -638.875732421875\n",
            "Training epoch 1736/1000000, d_loss: 125.50166320800781,  g_loss: -170.21804809570312\n",
            "Training epoch 1737/1000000, d_loss: -667.9072265625,  g_loss: -374.65667724609375\n",
            "Training epoch 1738/1000000, d_loss: 154.25601196289062,  g_loss: -202.8766326904297\n",
            "Training epoch 1739/1000000, d_loss: -39.03484344482422,  g_loss: 15.913249969482422\n",
            "Training epoch 1740/1000000, d_loss: -148.59698486328125,  g_loss: 156.94346618652344\n",
            "Training epoch 1741/1000000, d_loss: -106.70932006835938,  g_loss: 30.04104232788086\n",
            "Training epoch 1742/1000000, d_loss: -74.55889129638672,  g_loss: 27.748512268066406\n",
            "Training epoch 1743/1000000, d_loss: -119.3168716430664,  g_loss: 164.6414337158203\n",
            "Training epoch 1744/1000000, d_loss: -43.39373779296875,  g_loss: 91.7652359008789\n",
            "Training epoch 1745/1000000, d_loss: 3.1029701232910156,  g_loss: -13.197805404663086\n",
            "Training epoch 1746/1000000, d_loss: -129.56517028808594,  g_loss: -32.659175872802734\n",
            "Training epoch 1747/1000000, d_loss: -45.874549865722656,  g_loss: -23.849462509155273\n",
            "Training epoch 1748/1000000, d_loss: -20.129535675048828,  g_loss: -2.010861396789551\n",
            "Training epoch 1749/1000000, d_loss: -650.1982421875,  g_loss: -225.2047119140625\n",
            "Training epoch 1750/1000000, d_loss: 63.31338882446289,  g_loss: 108.58833312988281\n",
            "Training epoch 1751/1000000, d_loss: -115.04785919189453,  g_loss: 153.10865783691406\n",
            "Training epoch 1752/1000000, d_loss: -105.59951782226562,  g_loss: 130.77890014648438\n",
            "Training epoch 1753/1000000, d_loss: -117.55255889892578,  g_loss: 384.06536865234375\n",
            "Training epoch 1754/1000000, d_loss: -171.43411254882812,  g_loss: 125.05892944335938\n",
            "Training epoch 1755/1000000, d_loss: -91.53372192382812,  g_loss: 99.25126647949219\n",
            "Training epoch 1756/1000000, d_loss: -296.9570617675781,  g_loss: 29.415781021118164\n",
            "Training epoch 1757/1000000, d_loss: -144.55906677246094,  g_loss: 10.90203857421875\n",
            "Training epoch 1758/1000000, d_loss: -120.83073425292969,  g_loss: 191.45045471191406\n",
            "Training epoch 1759/1000000, d_loss: -87.91887664794922,  g_loss: 25.251977920532227\n",
            "Training epoch 1760/1000000, d_loss: -173.29452514648438,  g_loss: 82.7153091430664\n",
            "Training epoch 1761/1000000, d_loss: -134.9208984375,  g_loss: 175.237548828125\n",
            "Training epoch 1762/1000000, d_loss: -113.07396697998047,  g_loss: 103.97517395019531\n",
            "Training epoch 1763/1000000, d_loss: -94.09949493408203,  g_loss: -47.51487731933594\n",
            "Training epoch 1764/1000000, d_loss: -201.6654052734375,  g_loss: 16.277042388916016\n",
            "Training epoch 1765/1000000, d_loss: -265.6293640136719,  g_loss: -54.609825134277344\n",
            "Training epoch 1766/1000000, d_loss: -225.04759216308594,  g_loss: 8.089239120483398\n",
            "Training epoch 1767/1000000, d_loss: -358.08465576171875,  g_loss: -172.46685791015625\n",
            "Training epoch 1768/1000000, d_loss: -138.75099182128906,  g_loss: 178.48526000976562\n",
            "Training epoch 1769/1000000, d_loss: -170.75741577148438,  g_loss: 94.68061065673828\n",
            "Training epoch 1770/1000000, d_loss: -89.5045394897461,  g_loss: 114.01343536376953\n",
            "Training epoch 1771/1000000, d_loss: -47.566078186035156,  g_loss: 215.27493286132812\n",
            "Training epoch 1772/1000000, d_loss: -143.41754150390625,  g_loss: 344.9850158691406\n",
            "Training epoch 1773/1000000, d_loss: -437.9808044433594,  g_loss: -21.005977630615234\n",
            "Training epoch 1774/1000000, d_loss: -119.68466186523438,  g_loss: -2.0283775329589844\n",
            "Training epoch 1775/1000000, d_loss: -32.967994689941406,  g_loss: 22.85647964477539\n",
            "Training epoch 1776/1000000, d_loss: -92.71094512939453,  g_loss: 26.101181030273438\n",
            "Training epoch 1777/1000000, d_loss: -147.74502563476562,  g_loss: 53.874267578125\n",
            "Training epoch 1778/1000000, d_loss: -90.77196502685547,  g_loss: -8.753715515136719\n",
            "Training epoch 1779/1000000, d_loss: -43.82649230957031,  g_loss: 33.450347900390625\n",
            "Training epoch 1780/1000000, d_loss: -178.98387145996094,  g_loss: 22.80562400817871\n",
            "Training epoch 1781/1000000, d_loss: -27.671443939208984,  g_loss: 49.070106506347656\n",
            "Training epoch 1782/1000000, d_loss: -85.41148376464844,  g_loss: 64.24778747558594\n",
            "Training epoch 1783/1000000, d_loss: -57.45119094848633,  g_loss: 52.09376525878906\n",
            "Training epoch 1784/1000000, d_loss: -104.32244110107422,  g_loss: -25.13697052001953\n",
            "Training epoch 1785/1000000, d_loss: -64.44786834716797,  g_loss: -77.41770935058594\n",
            "Training epoch 1786/1000000, d_loss: -131.49220275878906,  g_loss: -88.7535629272461\n",
            "Training epoch 1787/1000000, d_loss: -1086.512451171875,  g_loss: -594.5179443359375\n",
            "Training epoch 1788/1000000, d_loss: 227.6890106201172,  g_loss: 44.41188430786133\n",
            "Training epoch 1789/1000000, d_loss: 55.23148727416992,  g_loss: 216.3046875\n",
            "Training epoch 1790/1000000, d_loss: -226.62814331054688,  g_loss: 661.6702880859375\n",
            "Training epoch 1791/1000000, d_loss: -190.19873046875,  g_loss: 351.579345703125\n",
            "Training epoch 1792/1000000, d_loss: -72.58502197265625,  g_loss: 147.9241943359375\n",
            "Training epoch 1793/1000000, d_loss: -321.204345703125,  g_loss: -106.98388671875\n",
            "Training epoch 1794/1000000, d_loss: -23.45317840576172,  g_loss: 34.23154067993164\n",
            "Training epoch 1795/1000000, d_loss: -212.1639404296875,  g_loss: 145.53500366210938\n",
            "Training epoch 1796/1000000, d_loss: 77.3943099975586,  g_loss: 158.2848663330078\n",
            "Training epoch 1797/1000000, d_loss: -156.44911193847656,  g_loss: 284.0184020996094\n",
            "Training epoch 1798/1000000, d_loss: -220.49624633789062,  g_loss: 297.0229187011719\n",
            "Training epoch 1799/1000000, d_loss: -95.8896713256836,  g_loss: 299.37445068359375\n",
            "Training epoch 1800/1000000, d_loss: -145.9078369140625,  g_loss: 127.470703125\n",
            "Training epoch 1801/1000000, d_loss: -368.4897155761719,  g_loss: -77.30714416503906\n",
            "Training epoch 1802/1000000, d_loss: -64.5130615234375,  g_loss: -97.748046875\n",
            "Training epoch 1803/1000000, d_loss: -121.29182434082031,  g_loss: 221.75575256347656\n",
            "Training epoch 1804/1000000, d_loss: -72.83984375,  g_loss: 60.75539016723633\n",
            "Training epoch 1805/1000000, d_loss: -213.91848754882812,  g_loss: 384.6437683105469\n",
            "Training epoch 1806/1000000, d_loss: 39.04371643066406,  g_loss: -53.412025451660156\n",
            "Training epoch 1807/1000000, d_loss: -182.2857208251953,  g_loss: -83.55311584472656\n",
            "Training epoch 1808/1000000, d_loss: -114.40834045410156,  g_loss: -24.932912826538086\n",
            "Training epoch 1809/1000000, d_loss: -92.0247573852539,  g_loss: -69.4670181274414\n",
            "Training epoch 1810/1000000, d_loss: -49.44925308227539,  g_loss: -56.429237365722656\n",
            "Training epoch 1811/1000000, d_loss: -66.47598266601562,  g_loss: -79.45530700683594\n",
            "Training epoch 1812/1000000, d_loss: -72.19567108154297,  g_loss: -38.75475311279297\n",
            "Training epoch 1813/1000000, d_loss: -88.83850860595703,  g_loss: 43.85944747924805\n",
            "Training epoch 1814/1000000, d_loss: -50.930816650390625,  g_loss: 112.09756469726562\n",
            "Training epoch 1815/1000000, d_loss: -71.583740234375,  g_loss: 66.79351806640625\n",
            "Training epoch 1816/1000000, d_loss: -54.28404235839844,  g_loss: 10.012367248535156\n",
            "Training epoch 1817/1000000, d_loss: -141.6669921875,  g_loss: -16.849990844726562\n",
            "Training epoch 1818/1000000, d_loss: -495.21588134765625,  g_loss: -188.80191040039062\n",
            "Training epoch 1819/1000000, d_loss: -59.290653228759766,  g_loss: -110.34788513183594\n",
            "Training epoch 1820/1000000, d_loss: -42.323402404785156,  g_loss: -150.5840301513672\n",
            "Training epoch 1821/1000000, d_loss: -69.39151000976562,  g_loss: -36.87422561645508\n",
            "Training epoch 1822/1000000, d_loss: -95.85015869140625,  g_loss: -61.363494873046875\n",
            "Training epoch 1823/1000000, d_loss: -66.43424987792969,  g_loss: -85.87750244140625\n",
            "Training epoch 1824/1000000, d_loss: -798.4104614257812,  g_loss: -302.909423828125\n",
            "Training epoch 1825/1000000, d_loss: -3.395050048828125,  g_loss: -90.8366470336914\n",
            "Training epoch 1826/1000000, d_loss: -35.5347900390625,  g_loss: -24.683025360107422\n",
            "Training epoch 1827/1000000, d_loss: -61.04711151123047,  g_loss: -13.970331192016602\n",
            "Training epoch 1828/1000000, d_loss: -20.254104614257812,  g_loss: 43.06227111816406\n",
            "Training epoch 1829/1000000, d_loss: -81.36900329589844,  g_loss: 160.3614959716797\n",
            "Training epoch 1830/1000000, d_loss: -175.96609497070312,  g_loss: -32.899627685546875\n",
            "Training epoch 1831/1000000, d_loss: -162.36318969726562,  g_loss: -88.70380401611328\n",
            "Training epoch 1832/1000000, d_loss: -184.8118896484375,  g_loss: -128.5409698486328\n",
            "Training epoch 1833/1000000, d_loss: -67.84877014160156,  g_loss: 37.696048736572266\n",
            "Training epoch 1834/1000000, d_loss: -161.89988708496094,  g_loss: 124.80056762695312\n",
            "Training epoch 1835/1000000, d_loss: -103.86845397949219,  g_loss: 14.66383171081543\n",
            "Training epoch 1836/1000000, d_loss: -292.8455505371094,  g_loss: -107.21910095214844\n",
            "Training epoch 1837/1000000, d_loss: -477.51116943359375,  g_loss: -326.87646484375\n",
            "Training epoch 1838/1000000, d_loss: -90.60185241699219,  g_loss: 305.902587890625\n",
            "Training epoch 1839/1000000, d_loss: -45.888519287109375,  g_loss: 136.04632568359375\n",
            "Training epoch 1840/1000000, d_loss: -120.53499603271484,  g_loss: 185.24813842773438\n",
            "Training epoch 1841/1000000, d_loss: -262.738525390625,  g_loss: 242.80917358398438\n",
            "Training epoch 1842/1000000, d_loss: -321.69927978515625,  g_loss: 28.648395538330078\n",
            "Training epoch 1843/1000000, d_loss: 16.83660125732422,  g_loss: 41.08226776123047\n",
            "Training epoch 1844/1000000, d_loss: -47.85281753540039,  g_loss: 129.01171875\n",
            "Training epoch 1845/1000000, d_loss: -172.0058135986328,  g_loss: 273.8431701660156\n",
            "Training epoch 1846/1000000, d_loss: -90.91329193115234,  g_loss: 221.85496520996094\n",
            "Training epoch 1847/1000000, d_loss: -44.811946868896484,  g_loss: 142.86953735351562\n",
            "Training epoch 1848/1000000, d_loss: -1.1901817321777344,  g_loss: 140.06341552734375\n",
            "Training epoch 1849/1000000, d_loss: -134.235595703125,  g_loss: 23.606721878051758\n",
            "Training epoch 1850/1000000, d_loss: -141.7274932861328,  g_loss: 57.80717468261719\n",
            "Training epoch 1851/1000000, d_loss: -49.249717712402344,  g_loss: 104.09083557128906\n",
            "Training epoch 1852/1000000, d_loss: -226.10121154785156,  g_loss: -49.2716178894043\n",
            "Training epoch 1853/1000000, d_loss: -89.95700073242188,  g_loss: -43.886253356933594\n",
            "Training epoch 1854/1000000, d_loss: -40.18931198120117,  g_loss: -37.35310363769531\n",
            "Training epoch 1855/1000000, d_loss: -307.9285888671875,  g_loss: -255.70059204101562\n",
            "Training epoch 1856/1000000, d_loss: 18.54701042175293,  g_loss: 63.429595947265625\n",
            "Training epoch 1857/1000000, d_loss: -100.55903625488281,  g_loss: 43.10469436645508\n",
            "Training epoch 1858/1000000, d_loss: -119.71871948242188,  g_loss: -35.41416931152344\n",
            "Training epoch 1859/1000000, d_loss: -156.7064666748047,  g_loss: 207.39254760742188\n",
            "Training epoch 1860/1000000, d_loss: -173.9978485107422,  g_loss: 146.94268798828125\n",
            "Training epoch 1861/1000000, d_loss: -199.30152893066406,  g_loss: 183.48159790039062\n",
            "Training epoch 1862/1000000, d_loss: -41.874996185302734,  g_loss: 199.03440856933594\n",
            "Training epoch 1863/1000000, d_loss: -21.306970596313477,  g_loss: 86.51483154296875\n",
            "Training epoch 1864/1000000, d_loss: -23.404951095581055,  g_loss: 84.73297119140625\n",
            "Training epoch 1865/1000000, d_loss: -324.3254699707031,  g_loss: -36.644134521484375\n",
            "Training epoch 1866/1000000, d_loss: -228.72247314453125,  g_loss: -74.13949584960938\n",
            "Training epoch 1867/1000000, d_loss: -350.58819580078125,  g_loss: -117.14082336425781\n",
            "Training epoch 1868/1000000, d_loss: -360.95806884765625,  g_loss: -300.2843322753906\n",
            "Training epoch 1869/1000000, d_loss: -65.75125122070312,  g_loss: -136.67807006835938\n",
            "Training epoch 1870/1000000, d_loss: -120.54991149902344,  g_loss: -70.00757598876953\n",
            "Training epoch 1871/1000000, d_loss: -32.303131103515625,  g_loss: -17.045312881469727\n",
            "Training epoch 1872/1000000, d_loss: -34.076988220214844,  g_loss: 66.09415435791016\n",
            "Training epoch 1873/1000000, d_loss: -51.46502685546875,  g_loss: 20.707252502441406\n",
            "Training epoch 1874/1000000, d_loss: -15.230667114257812,  g_loss: 46.04529571533203\n",
            "Training epoch 1875/1000000, d_loss: 15.201446533203125,  g_loss: 66.66209411621094\n",
            "Training epoch 1876/1000000, d_loss: -158.81304931640625,  g_loss: -68.32933807373047\n",
            "Training epoch 1877/1000000, d_loss: -127.23788452148438,  g_loss: 107.42268371582031\n",
            "Training epoch 1878/1000000, d_loss: -189.31573486328125,  g_loss: 18.110864639282227\n",
            "Training epoch 1879/1000000, d_loss: -17.825748443603516,  g_loss: 56.823944091796875\n",
            "Training epoch 1880/1000000, d_loss: -193.28411865234375,  g_loss: -86.70030975341797\n",
            "Training epoch 1881/1000000, d_loss: -51.68266677856445,  g_loss: 46.32051086425781\n",
            "Training epoch 1882/1000000, d_loss: -315.94970703125,  g_loss: -110.18218231201172\n",
            "Training epoch 1883/1000000, d_loss: -68.6777572631836,  g_loss: 68.64725494384766\n",
            "Training epoch 1884/1000000, d_loss: -576.02978515625,  g_loss: -246.83363342285156\n",
            "Training epoch 1885/1000000, d_loss: -6.491729736328125,  g_loss: 166.62258911132812\n",
            "Training epoch 1886/1000000, d_loss: -68.8318862915039,  g_loss: 207.310302734375\n",
            "Training epoch 1887/1000000, d_loss: -128.24777221679688,  g_loss: 41.795310974121094\n",
            "Training epoch 1888/1000000, d_loss: -76.06304931640625,  g_loss: 115.51806640625\n",
            "Training epoch 1889/1000000, d_loss: -254.90386962890625,  g_loss: 50.847259521484375\n",
            "Training epoch 1890/1000000, d_loss: -181.3658447265625,  g_loss: 8.654476165771484\n",
            "Training epoch 1891/1000000, d_loss: -177.85256958007812,  g_loss: 151.36842346191406\n",
            "Training epoch 1892/1000000, d_loss: -286.50677490234375,  g_loss: 164.2697296142578\n",
            "Training epoch 1893/1000000, d_loss: 0.08929443359375,  g_loss: 22.3504638671875\n",
            "Training epoch 1894/1000000, d_loss: -34.00090026855469,  g_loss: 98.17539978027344\n",
            "Training epoch 1895/1000000, d_loss: -71.03832244873047,  g_loss: 110.4169921875\n",
            "Training epoch 1896/1000000, d_loss: -33.93219757080078,  g_loss: 98.88984680175781\n",
            "Training epoch 1897/1000000, d_loss: -91.18730163574219,  g_loss: 104.92894744873047\n",
            "Training epoch 1898/1000000, d_loss: -9.955963134765625,  g_loss: 133.56869506835938\n",
            "Training epoch 1899/1000000, d_loss: -48.517784118652344,  g_loss: 129.76882934570312\n",
            "Training epoch 1900/1000000, d_loss: -167.8751678466797,  g_loss: 245.9120330810547\n",
            "Training epoch 1901/1000000, d_loss: -202.88409423828125,  g_loss: 41.418025970458984\n",
            "Training epoch 1902/1000000, d_loss: -169.1088409423828,  g_loss: -20.342130661010742\n",
            "Training epoch 1903/1000000, d_loss: -301.5633544921875,  g_loss: -46.302635192871094\n",
            "Training epoch 1904/1000000, d_loss: -33.515525817871094,  g_loss: 52.836891174316406\n",
            "Training epoch 1905/1000000, d_loss: -55.92432403564453,  g_loss: 84.65121459960938\n",
            "Training epoch 1906/1000000, d_loss: -267.4613952636719,  g_loss: 43.53245544433594\n",
            "Training epoch 1907/1000000, d_loss: -103.70948791503906,  g_loss: 50.621315002441406\n",
            "Training epoch 1908/1000000, d_loss: -36.12396240234375,  g_loss: 117.67694854736328\n",
            "Training epoch 1909/1000000, d_loss: -79.0164794921875,  g_loss: 22.94760513305664\n",
            "Training epoch 1910/1000000, d_loss: -11.387917518615723,  g_loss: -8.912372589111328\n",
            "Training epoch 1911/1000000, d_loss: -82.47953796386719,  g_loss: 13.362329483032227\n",
            "Training epoch 1912/1000000, d_loss: -189.8543701171875,  g_loss: -152.34027099609375\n",
            "Training epoch 1913/1000000, d_loss: -116.57798767089844,  g_loss: 323.93157958984375\n",
            "Training epoch 1914/1000000, d_loss: -447.70440673828125,  g_loss: -113.21986389160156\n",
            "Training epoch 1915/1000000, d_loss: -646.6336669921875,  g_loss: -192.59884643554688\n",
            "Training epoch 1916/1000000, d_loss: 20.30181884765625,  g_loss: -102.54219055175781\n",
            "Training epoch 1917/1000000, d_loss: -110.94943237304688,  g_loss: -155.4486083984375\n",
            "Training epoch 1918/1000000, d_loss: -107.85980987548828,  g_loss: -82.75335693359375\n",
            "Training epoch 1919/1000000, d_loss: -13.613876342773438,  g_loss: -79.65693664550781\n",
            "Training epoch 1920/1000000, d_loss: -59.13068389892578,  g_loss: 16.270761489868164\n",
            "Training epoch 1921/1000000, d_loss: -73.63545989990234,  g_loss: 67.32516479492188\n",
            "Training epoch 1922/1000000, d_loss: -84.23916625976562,  g_loss: -22.702472686767578\n",
            "Training epoch 1923/1000000, d_loss: -180.5891571044922,  g_loss: -35.804508209228516\n",
            "Training epoch 1924/1000000, d_loss: -24.092126846313477,  g_loss: -55.61357116699219\n",
            "Training epoch 1925/1000000, d_loss: -428.2597351074219,  g_loss: -249.84463500976562\n",
            "Training epoch 1926/1000000, d_loss: -19.27933120727539,  g_loss: -59.539608001708984\n",
            "Training epoch 1927/1000000, d_loss: -14.908279418945312,  g_loss: -104.60902404785156\n",
            "Training epoch 1928/1000000, d_loss: -64.77146911621094,  g_loss: -92.54171752929688\n",
            "Training epoch 1929/1000000, d_loss: 8.820253372192383,  g_loss: -66.79898834228516\n",
            "Training epoch 1930/1000000, d_loss: -22.74142837524414,  g_loss: -39.8757438659668\n",
            "Training epoch 1931/1000000, d_loss: -42.42490005493164,  g_loss: -32.79084777832031\n",
            "Training epoch 1932/1000000, d_loss: -132.09902954101562,  g_loss: 7.296909332275391\n",
            "Training epoch 1933/1000000, d_loss: -34.515357971191406,  g_loss: 87.99154663085938\n",
            "Training epoch 1934/1000000, d_loss: -68.62793731689453,  g_loss: 15.25224494934082\n",
            "Training epoch 1935/1000000, d_loss: -16.325958251953125,  g_loss: 10.150545120239258\n",
            "Training epoch 1936/1000000, d_loss: -2.28326416015625,  g_loss: 37.07794189453125\n",
            "Training epoch 1937/1000000, d_loss: -91.04051208496094,  g_loss: 11.695510864257812\n",
            "Training epoch 1938/1000000, d_loss: -725.3260498046875,  g_loss: -306.1700439453125\n",
            "Training epoch 1939/1000000, d_loss: -32.14101791381836,  g_loss: -2.2685165405273438\n",
            "Training epoch 1940/1000000, d_loss: -41.932167053222656,  g_loss: -14.589731216430664\n",
            "Training epoch 1941/1000000, d_loss: -68.85582733154297,  g_loss: -20.45532989501953\n",
            "Training epoch 1942/1000000, d_loss: -139.18792724609375,  g_loss: 130.39224243164062\n",
            "Training epoch 1943/1000000, d_loss: -67.1941146850586,  g_loss: 160.34005737304688\n",
            "Training epoch 1944/1000000, d_loss: 11.667407989501953,  g_loss: 96.34004211425781\n",
            "Training epoch 1945/1000000, d_loss: -362.35382080078125,  g_loss: 29.382537841796875\n",
            "Training epoch 1946/1000000, d_loss: -57.557071685791016,  g_loss: -35.36255645751953\n",
            "Training epoch 1947/1000000, d_loss: -104.00772857666016,  g_loss: 301.9432373046875\n",
            "Training epoch 1948/1000000, d_loss: -235.80148315429688,  g_loss: -11.226255416870117\n",
            "Training epoch 1949/1000000, d_loss: -5.508800506591797,  g_loss: 52.222652435302734\n",
            "Training epoch 1950/1000000, d_loss: 804.951904296875,  g_loss: -69.68589782714844\n",
            "Training epoch 1951/1000000, d_loss: 90.39906311035156,  g_loss: -86.91543579101562\n",
            "Training epoch 1952/1000000, d_loss: -9.304100036621094,  g_loss: 156.03060913085938\n",
            "Training epoch 1953/1000000, d_loss: -11.611732482910156,  g_loss: 147.80068969726562\n",
            "Training epoch 1954/1000000, d_loss: -63.441654205322266,  g_loss: 81.00410461425781\n",
            "Training epoch 1955/1000000, d_loss: -63.69847106933594,  g_loss: 40.83951187133789\n",
            "Training epoch 1956/1000000, d_loss: -31.847349166870117,  g_loss: -56.8641357421875\n",
            "Training epoch 1957/1000000, d_loss: -178.16250610351562,  g_loss: -63.13698196411133\n",
            "Training epoch 1958/1000000, d_loss: -97.0057601928711,  g_loss: -128.77410888671875\n",
            "Training epoch 1959/1000000, d_loss: -52.771385192871094,  g_loss: 87.07855987548828\n",
            "Training epoch 1960/1000000, d_loss: -33.31244659423828,  g_loss: 71.14443969726562\n",
            "Training epoch 1961/1000000, d_loss: -83.99147033691406,  g_loss: 5.217776298522949\n",
            "Training epoch 1962/1000000, d_loss: -72.52693176269531,  g_loss: -22.924663543701172\n",
            "Training epoch 1963/1000000, d_loss: -112.30641174316406,  g_loss: 27.805959701538086\n",
            "Training epoch 1964/1000000, d_loss: -143.51939392089844,  g_loss: 13.460668563842773\n",
            "Training epoch 1965/1000000, d_loss: 14.686273574829102,  g_loss: 50.482505798339844\n",
            "Training epoch 1966/1000000, d_loss: -78.7138671875,  g_loss: 78.89369201660156\n",
            "Training epoch 1967/1000000, d_loss: -28.5755615234375,  g_loss: 67.058349609375\n",
            "Training epoch 1968/1000000, d_loss: 34.34956359863281,  g_loss: 64.22299194335938\n",
            "Training epoch 1969/1000000, d_loss: -86.86398315429688,  g_loss: -3.1783480644226074\n",
            "Training epoch 1970/1000000, d_loss: -101.56365966796875,  g_loss: 169.60215759277344\n",
            "Training epoch 1971/1000000, d_loss: -199.61599731445312,  g_loss: -75.06755065917969\n",
            "Training epoch 1972/1000000, d_loss: -57.73747634887695,  g_loss: -58.5068473815918\n",
            "Training epoch 1973/1000000, d_loss: -18.013269424438477,  g_loss: -1.4834281206130981\n",
            "Training epoch 1974/1000000, d_loss: -59.675838470458984,  g_loss: 79.99314880371094\n",
            "Training epoch 1975/1000000, d_loss: -54.84739685058594,  g_loss: 28.24779510498047\n",
            "Training epoch 1976/1000000, d_loss: -167.33843994140625,  g_loss: -46.54466247558594\n",
            "Training epoch 1977/1000000, d_loss: -56.19886016845703,  g_loss: 32.61622619628906\n",
            "Training epoch 1978/1000000, d_loss: -608.6756591796875,  g_loss: -180.42910766601562\n",
            "Training epoch 1979/1000000, d_loss: 64.91943359375,  g_loss: -49.23009490966797\n",
            "Training epoch 1980/1000000, d_loss: -11.069515228271484,  g_loss: -81.84369659423828\n",
            "Training epoch 1981/1000000, d_loss: -105.3734130859375,  g_loss: -15.860867500305176\n",
            "Training epoch 1982/1000000, d_loss: -95.28274536132812,  g_loss: -50.68826675415039\n",
            "Training epoch 1983/1000000, d_loss: -69.5820083618164,  g_loss: -53.52690887451172\n",
            "Training epoch 1984/1000000, d_loss: -39.1606559753418,  g_loss: -55.539737701416016\n",
            "Training epoch 1985/1000000, d_loss: -52.32668685913086,  g_loss: -42.59431838989258\n",
            "Training epoch 1986/1000000, d_loss: -37.60883331298828,  g_loss: -19.18649673461914\n",
            "Training epoch 1987/1000000, d_loss: -76.17324829101562,  g_loss: -17.933597564697266\n",
            "Training epoch 1988/1000000, d_loss: -118.84051513671875,  g_loss: 10.859905242919922\n",
            "Training epoch 1989/1000000, d_loss: -39.202362060546875,  g_loss: 24.35940170288086\n",
            "Training epoch 1990/1000000, d_loss: 65.24163818359375,  g_loss: 21.678268432617188\n",
            "Training epoch 1991/1000000, d_loss: -90.51271057128906,  g_loss: 63.58576965332031\n",
            "Training epoch 1992/1000000, d_loss: -30.537879943847656,  g_loss: 6.833081245422363\n",
            "Training epoch 1993/1000000, d_loss: -74.37448120117188,  g_loss: 78.58802795410156\n",
            "Training epoch 1994/1000000, d_loss: -75.09884643554688,  g_loss: -26.93623161315918\n",
            "Training epoch 1995/1000000, d_loss: -263.2690734863281,  g_loss: -123.02066040039062\n",
            "Training epoch 1996/1000000, d_loss: -466.9913330078125,  g_loss: -155.62796020507812\n",
            "Training epoch 1997/1000000, d_loss: -25.018491744995117,  g_loss: -66.09658813476562\n",
            "Training epoch 1998/1000000, d_loss: -17.76957893371582,  g_loss: -0.43940162658691406\n",
            "Training epoch 1999/1000000, d_loss: -94.66920471191406,  g_loss: 74.063720703125\n",
            "Training epoch 2000/1000000, d_loss: -48.400123596191406,  g_loss: -43.40097427368164\n",
            "Training epoch 2001/1000000, d_loss: -83.99246215820312,  g_loss: 92.94097137451172\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 11/11 [00:00<00:00, 729.17it/s]\n",
            "Meshing: 100%|██████████| 1483/1483 [00:00<00:00, 5716.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_2001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_2001/assets\n",
            "Training epoch 2002/1000000, d_loss: -83.66452026367188,  g_loss: 87.77813720703125\n",
            "Training epoch 2003/1000000, d_loss: -57.374176025390625,  g_loss: 267.0378723144531\n",
            "Training epoch 2004/1000000, d_loss: -90.18025970458984,  g_loss: 144.28211975097656\n",
            "Training epoch 2005/1000000, d_loss: -13.448186874389648,  g_loss: 80.7264404296875\n",
            "Training epoch 2006/1000000, d_loss: -51.84103775024414,  g_loss: 122.19056701660156\n",
            "Training epoch 2007/1000000, d_loss: -70.60542297363281,  g_loss: 84.66967010498047\n",
            "Training epoch 2008/1000000, d_loss: -154.05477905273438,  g_loss: 81.13507080078125\n",
            "Training epoch 2009/1000000, d_loss: -202.40579223632812,  g_loss: -74.3924560546875\n",
            "Training epoch 2010/1000000, d_loss: -103.53746032714844,  g_loss: -59.070068359375\n",
            "Training epoch 2011/1000000, d_loss: -91.43766021728516,  g_loss: 97.16470336914062\n",
            "Training epoch 2012/1000000, d_loss: -71.6278305053711,  g_loss: 69.90522766113281\n",
            "Training epoch 2013/1000000, d_loss: -29.01085662841797,  g_loss: 63.24291229248047\n",
            "Training epoch 2014/1000000, d_loss: -130.29629516601562,  g_loss: 10.631513595581055\n",
            "Training epoch 2015/1000000, d_loss: 0.9521293640136719,  g_loss: -7.231022357940674\n",
            "Training epoch 2016/1000000, d_loss: -36.96464920043945,  g_loss: 92.69752502441406\n",
            "Training epoch 2017/1000000, d_loss: -40.56975173950195,  g_loss: 54.48314666748047\n",
            "Training epoch 2018/1000000, d_loss: -192.4657440185547,  g_loss: -44.64668655395508\n",
            "Training epoch 2019/1000000, d_loss: -17.617759704589844,  g_loss: 47.265892028808594\n",
            "Training epoch 2020/1000000, d_loss: -43.52260971069336,  g_loss: 38.25020217895508\n",
            "Training epoch 2021/1000000, d_loss: -88.2948226928711,  g_loss: -34.223907470703125\n",
            "Training epoch 2022/1000000, d_loss: -118.24042510986328,  g_loss: 166.0330047607422\n",
            "Training epoch 2023/1000000, d_loss: -57.79829406738281,  g_loss: 19.349842071533203\n",
            "Training epoch 2024/1000000, d_loss: -98.88512420654297,  g_loss: 27.74239158630371\n",
            "Training epoch 2025/1000000, d_loss: -1182.77978515625,  g_loss: -431.7229309082031\n",
            "Training epoch 2026/1000000, d_loss: -121.98704528808594,  g_loss: -186.038330078125\n",
            "Training epoch 2027/1000000, d_loss: -168.80926513671875,  g_loss: -134.670166015625\n",
            "Training epoch 2028/1000000, d_loss: -98.21624755859375,  g_loss: -243.09344482421875\n",
            "Training epoch 2029/1000000, d_loss: -98.62301635742188,  g_loss: -137.26239013671875\n",
            "Training epoch 2030/1000000, d_loss: 149.7251739501953,  g_loss: -54.91703796386719\n",
            "Training epoch 2031/1000000, d_loss: -13.033527374267578,  g_loss: -6.213356971740723\n",
            "Training epoch 2032/1000000, d_loss: -92.38166809082031,  g_loss: -32.6005859375\n",
            "Training epoch 2033/1000000, d_loss: -121.16836547851562,  g_loss: 160.53024291992188\n",
            "Training epoch 2034/1000000, d_loss: -163.4884490966797,  g_loss: 113.47134399414062\n",
            "Training epoch 2035/1000000, d_loss: -109.68696594238281,  g_loss: 180.1981658935547\n",
            "Training epoch 2036/1000000, d_loss: -52.351966857910156,  g_loss: 130.73660278320312\n",
            "Training epoch 2037/1000000, d_loss: -59.62782287597656,  g_loss: 16.49791717529297\n",
            "Training epoch 2038/1000000, d_loss: -391.1460876464844,  g_loss: -179.18826293945312\n",
            "Training epoch 2039/1000000, d_loss: 17.23599624633789,  g_loss: -129.53549194335938\n",
            "Training epoch 2040/1000000, d_loss: -9.121513366699219,  g_loss: 44.750797271728516\n",
            "Training epoch 2041/1000000, d_loss: -89.01324462890625,  g_loss: 19.367786407470703\n",
            "Training epoch 2042/1000000, d_loss: -140.95901489257812,  g_loss: 220.2328338623047\n",
            "Training epoch 2043/1000000, d_loss: -56.37582778930664,  g_loss: 47.374977111816406\n",
            "Training epoch 2044/1000000, d_loss: -136.9671630859375,  g_loss: 84.44304656982422\n",
            "Training epoch 2045/1000000, d_loss: -33.94822692871094,  g_loss: 162.87271118164062\n",
            "Training epoch 2046/1000000, d_loss: -140.58901977539062,  g_loss: -50.84636306762695\n",
            "Training epoch 2047/1000000, d_loss: -2.5575485229492188,  g_loss: 28.204378128051758\n",
            "Training epoch 2048/1000000, d_loss: -54.95208740234375,  g_loss: 96.40620422363281\n",
            "Training epoch 2049/1000000, d_loss: -50.417789459228516,  g_loss: 57.08289337158203\n",
            "Training epoch 2050/1000000, d_loss: -93.17160034179688,  g_loss: 117.06065368652344\n",
            "Training epoch 2051/1000000, d_loss: -76.90679931640625,  g_loss: 45.55365753173828\n",
            "Training epoch 2052/1000000, d_loss: -72.53115844726562,  g_loss: 36.92159652709961\n",
            "Training epoch 2053/1000000, d_loss: -69.7646255493164,  g_loss: 30.270668029785156\n",
            "Training epoch 2054/1000000, d_loss: -79.26354217529297,  g_loss: 56.22119903564453\n",
            "Training epoch 2055/1000000, d_loss: -122.12285614013672,  g_loss: -14.568232536315918\n",
            "Training epoch 2056/1000000, d_loss: -717.7069091796875,  g_loss: -221.68328857421875\n",
            "Training epoch 2057/1000000, d_loss: 230.95068359375,  g_loss: -113.83844757080078\n",
            "Training epoch 2058/1000000, d_loss: -80.72847747802734,  g_loss: 12.432573318481445\n",
            "Training epoch 2059/1000000, d_loss: -95.85957336425781,  g_loss: -8.315237045288086\n",
            "Training epoch 2060/1000000, d_loss: 79.31222534179688,  g_loss: 49.824466705322266\n",
            "Training epoch 2061/1000000, d_loss: -98.55230712890625,  g_loss: 56.872947692871094\n",
            "Training epoch 2062/1000000, d_loss: -87.72028350830078,  g_loss: 65.0367431640625\n",
            "Training epoch 2063/1000000, d_loss: -6.788460731506348,  g_loss: 110.12672424316406\n",
            "Training epoch 2064/1000000, d_loss: -30.204879760742188,  g_loss: 92.92042541503906\n",
            "Training epoch 2065/1000000, d_loss: -39.936668395996094,  g_loss: 30.64590835571289\n",
            "Training epoch 2066/1000000, d_loss: -79.77428436279297,  g_loss: 136.933837890625\n",
            "Training epoch 2067/1000000, d_loss: -107.11944580078125,  g_loss: 74.10619354248047\n",
            "Training epoch 2068/1000000, d_loss: -76.9535140991211,  g_loss: 69.11416625976562\n",
            "Training epoch 2069/1000000, d_loss: -187.97618103027344,  g_loss: 58.44997787475586\n",
            "Training epoch 2070/1000000, d_loss: -42.12858581542969,  g_loss: 78.57573699951172\n",
            "Training epoch 2071/1000000, d_loss: -95.75823211669922,  g_loss: 86.5884017944336\n",
            "Training epoch 2072/1000000, d_loss: -108.31243896484375,  g_loss: 40.5447998046875\n",
            "Training epoch 2073/1000000, d_loss: -226.4888916015625,  g_loss: -181.0807647705078\n",
            "Training epoch 2074/1000000, d_loss: 98.79801940917969,  g_loss: 46.118743896484375\n",
            "Training epoch 2075/1000000, d_loss: -41.79653549194336,  g_loss: 33.00001525878906\n",
            "Training epoch 2076/1000000, d_loss: -868.1694946289062,  g_loss: -437.62823486328125\n",
            "Training epoch 2077/1000000, d_loss: -127.44281005859375,  g_loss: -57.026710510253906\n",
            "Training epoch 2078/1000000, d_loss: 51.245643615722656,  g_loss: -34.99748611450195\n",
            "Training epoch 2079/1000000, d_loss: -38.85493087768555,  g_loss: 0.5884952545166016\n",
            "Training epoch 2080/1000000, d_loss: -68.72503662109375,  g_loss: 43.606353759765625\n",
            "Training epoch 2081/1000000, d_loss: -125.88560485839844,  g_loss: 38.42180633544922\n",
            "Training epoch 2082/1000000, d_loss: -123.15083312988281,  g_loss: 59.87662124633789\n",
            "Training epoch 2083/1000000, d_loss: -175.7126922607422,  g_loss: -18.23654556274414\n",
            "Training epoch 2084/1000000, d_loss: -118.96483612060547,  g_loss: 154.39688110351562\n",
            "Training epoch 2085/1000000, d_loss: -156.49383544921875,  g_loss: 46.827117919921875\n",
            "Training epoch 2086/1000000, d_loss: -123.22537231445312,  g_loss: -4.808781623840332\n",
            "Training epoch 2087/1000000, d_loss: -161.82455444335938,  g_loss: 34.808509826660156\n",
            "Training epoch 2088/1000000, d_loss: -301.3091735839844,  g_loss: -203.93212890625\n",
            "Training epoch 2089/1000000, d_loss: -258.8961181640625,  g_loss: 484.34405517578125\n",
            "Training epoch 2090/1000000, d_loss: -137.02349853515625,  g_loss: 328.9559326171875\n",
            "Training epoch 2091/1000000, d_loss: -13.56097412109375,  g_loss: 156.16079711914062\n",
            "Training epoch 2092/1000000, d_loss: -140.86195373535156,  g_loss: 187.41262817382812\n",
            "Training epoch 2093/1000000, d_loss: -149.67721557617188,  g_loss: 145.5367431640625\n",
            "Training epoch 2094/1000000, d_loss: -52.68273162841797,  g_loss: 75.624755859375\n",
            "Training epoch 2095/1000000, d_loss: -811.3656005859375,  g_loss: -286.63043212890625\n",
            "Training epoch 2096/1000000, d_loss: -88.98379516601562,  g_loss: 164.41761779785156\n",
            "Training epoch 2097/1000000, d_loss: -51.15596008300781,  g_loss: 160.2644805908203\n",
            "Training epoch 2098/1000000, d_loss: -4.85391902923584,  g_loss: 171.86874389648438\n",
            "Training epoch 2099/1000000, d_loss: -145.15040588378906,  g_loss: 378.0982666015625\n",
            "Training epoch 2100/1000000, d_loss: -10.394744873046875,  g_loss: 168.4046630859375\n",
            "Training epoch 2101/1000000, d_loss: -92.59700775146484,  g_loss: 291.60784912109375\n",
            "Training epoch 2102/1000000, d_loss: -32.03380584716797,  g_loss: 207.28790283203125\n",
            "Training epoch 2103/1000000, d_loss: -76.40238952636719,  g_loss: 224.8955078125\n",
            "Training epoch 2104/1000000, d_loss: -67.86259460449219,  g_loss: 242.78512573242188\n",
            "Training epoch 2105/1000000, d_loss: -80.0115966796875,  g_loss: 275.7941589355469\n",
            "Training epoch 2106/1000000, d_loss: -66.89037322998047,  g_loss: 283.3414611816406\n",
            "Training epoch 2107/1000000, d_loss: -29.984130859375,  g_loss: 216.97894287109375\n",
            "Training epoch 2108/1000000, d_loss: -232.5229034423828,  g_loss: 390.9105529785156\n",
            "Training epoch 2109/1000000, d_loss: -159.1559600830078,  g_loss: 112.70063781738281\n",
            "Training epoch 2110/1000000, d_loss: -48.72325134277344,  g_loss: 198.97323608398438\n",
            "Training epoch 2111/1000000, d_loss: -3.0380172729492188,  g_loss: 213.3290252685547\n",
            "Training epoch 2112/1000000, d_loss: -91.19429016113281,  g_loss: 242.1526641845703\n",
            "Training epoch 2113/1000000, d_loss: -370.26141357421875,  g_loss: 53.701263427734375\n",
            "Training epoch 2114/1000000, d_loss: -56.27699661254883,  g_loss: -1.6687278747558594\n",
            "Training epoch 2115/1000000, d_loss: -63.48086166381836,  g_loss: 142.93031311035156\n",
            "Training epoch 2116/1000000, d_loss: -92.86668395996094,  g_loss: 160.8603515625\n",
            "Training epoch 2117/1000000, d_loss: -119.88438415527344,  g_loss: 24.257123947143555\n",
            "Training epoch 2118/1000000, d_loss: 8.159969329833984,  g_loss: 209.50253295898438\n",
            "Training epoch 2119/1000000, d_loss: -198.6733856201172,  g_loss: 21.09783935546875\n",
            "Training epoch 2120/1000000, d_loss: -42.275508880615234,  g_loss: 15.76380729675293\n",
            "Training epoch 2121/1000000, d_loss: -75.96489715576172,  g_loss: 104.76396179199219\n",
            "Training epoch 2122/1000000, d_loss: -112.23405456542969,  g_loss: 54.77215576171875\n",
            "Training epoch 2123/1000000, d_loss: -172.83306884765625,  g_loss: -4.5421857833862305\n",
            "Training epoch 2124/1000000, d_loss: -51.73976516723633,  g_loss: 116.1789779663086\n",
            "Training epoch 2125/1000000, d_loss: -97.55447387695312,  g_loss: 130.0077667236328\n",
            "Training epoch 2126/1000000, d_loss: 63.026763916015625,  g_loss: 20.359153747558594\n",
            "Training epoch 2127/1000000, d_loss: -231.26498413085938,  g_loss: 69.52984619140625\n",
            "Training epoch 2128/1000000, d_loss: -507.75091552734375,  g_loss: -320.5789489746094\n",
            "Training epoch 2129/1000000, d_loss: -192.22581481933594,  g_loss: -92.31866455078125\n",
            "Training epoch 2130/1000000, d_loss: 51.32217788696289,  g_loss: 16.035858154296875\n",
            "Training epoch 2131/1000000, d_loss: -1029.3203125,  g_loss: -353.921142578125\n",
            "Training epoch 2132/1000000, d_loss: -1005.533203125,  g_loss: -997.8555908203125\n",
            "Training epoch 2133/1000000, d_loss: 267.13470458984375,  g_loss: 9.907010078430176\n",
            "Training epoch 2134/1000000, d_loss: -171.68865966796875,  g_loss: 112.50382995605469\n",
            "Training epoch 2135/1000000, d_loss: -238.1702880859375,  g_loss: 242.949462890625\n",
            "Training epoch 2136/1000000, d_loss: -92.86434173583984,  g_loss: 165.00262451171875\n",
            "Training epoch 2137/1000000, d_loss: -239.96072387695312,  g_loss: 104.98043823242188\n",
            "Training epoch 2138/1000000, d_loss: -71.2838134765625,  g_loss: 169.99256896972656\n",
            "Training epoch 2139/1000000, d_loss: -109.6056137084961,  g_loss: 353.17822265625\n",
            "Training epoch 2140/1000000, d_loss: -50.51350402832031,  g_loss: 62.66587829589844\n",
            "Training epoch 2141/1000000, d_loss: -58.215946197509766,  g_loss: 56.275089263916016\n",
            "Training epoch 2142/1000000, d_loss: -34.65650177001953,  g_loss: 143.85574340820312\n",
            "Training epoch 2143/1000000, d_loss: 33.430702209472656,  g_loss: 27.734811782836914\n",
            "Training epoch 2144/1000000, d_loss: -72.1166763305664,  g_loss: 165.99501037597656\n",
            "Training epoch 2145/1000000, d_loss: -153.14698791503906,  g_loss: 69.00730895996094\n",
            "Training epoch 2146/1000000, d_loss: -89.32240295410156,  g_loss: 122.47361755371094\n",
            "Training epoch 2147/1000000, d_loss: -46.96129608154297,  g_loss: 115.31571197509766\n",
            "Training epoch 2148/1000000, d_loss: -6.474082946777344,  g_loss: 87.99562072753906\n",
            "Training epoch 2149/1000000, d_loss: -98.07467651367188,  g_loss: 59.16211700439453\n",
            "Training epoch 2150/1000000, d_loss: -96.49205780029297,  g_loss: 191.88006591796875\n",
            "Training epoch 2151/1000000, d_loss: -145.783447265625,  g_loss: 107.58587646484375\n",
            "Training epoch 2152/1000000, d_loss: -359.9710693359375,  g_loss: -52.44478988647461\n",
            "Training epoch 2153/1000000, d_loss: -209.818359375,  g_loss: -47.055198669433594\n",
            "Training epoch 2154/1000000, d_loss: -63.660125732421875,  g_loss: -10.073399543762207\n",
            "Training epoch 2155/1000000, d_loss: -83.25930786132812,  g_loss: -6.209342956542969\n",
            "Training epoch 2156/1000000, d_loss: 12.431194305419922,  g_loss: 28.12274742126465\n",
            "Training epoch 2157/1000000, d_loss: -102.65272521972656,  g_loss: 165.2933349609375\n",
            "Training epoch 2158/1000000, d_loss: -58.45947265625,  g_loss: 80.71039581298828\n",
            "Training epoch 2159/1000000, d_loss: -336.04681396484375,  g_loss: -104.22853088378906\n",
            "Training epoch 2160/1000000, d_loss: -15.977996826171875,  g_loss: -69.82581329345703\n",
            "Training epoch 2161/1000000, d_loss: -225.6543731689453,  g_loss: -42.17112731933594\n",
            "Training epoch 2162/1000000, d_loss: -47.459129333496094,  g_loss: 10.952012062072754\n",
            "Training epoch 2163/1000000, d_loss: -82.11712646484375,  g_loss: 153.5608673095703\n",
            "Training epoch 2164/1000000, d_loss: -144.4821319580078,  g_loss: -16.750993728637695\n",
            "Training epoch 2165/1000000, d_loss: -116.83526611328125,  g_loss: 148.14959716796875\n",
            "Training epoch 2166/1000000, d_loss: 24.898906707763672,  g_loss: 156.3225860595703\n",
            "Training epoch 2167/1000000, d_loss: -148.3350830078125,  g_loss: 50.49116897583008\n",
            "Training epoch 2168/1000000, d_loss: -134.00477600097656,  g_loss: 73.31961059570312\n",
            "Training epoch 2169/1000000, d_loss: -70.10259246826172,  g_loss: 46.65620422363281\n",
            "Training epoch 2170/1000000, d_loss: -332.9409484863281,  g_loss: -67.62625122070312\n",
            "Training epoch 2171/1000000, d_loss: -278.3669128417969,  g_loss: -201.1466827392578\n",
            "Training epoch 2172/1000000, d_loss: -482.3659973144531,  g_loss: -605.8681030273438\n",
            "Training epoch 2173/1000000, d_loss: 57.1903076171875,  g_loss: 93.73390197753906\n",
            "Training epoch 2174/1000000, d_loss: -14.874649047851562,  g_loss: 7.691802024841309\n",
            "Training epoch 2175/1000000, d_loss: -83.50981903076172,  g_loss: -82.93516540527344\n",
            "Training epoch 2176/1000000, d_loss: 11.912750244140625,  g_loss: 54.79536437988281\n",
            "Training epoch 2177/1000000, d_loss: -112.18276977539062,  g_loss: 222.73292541503906\n",
            "Training epoch 2178/1000000, d_loss: -50.471858978271484,  g_loss: 274.8460998535156\n",
            "Training epoch 2179/1000000, d_loss: -183.44200134277344,  g_loss: 345.02655029296875\n",
            "Training epoch 2180/1000000, d_loss: -35.72161865234375,  g_loss: 308.486572265625\n",
            "Training epoch 2181/1000000, d_loss: -73.36593627929688,  g_loss: 163.858154296875\n",
            "Training epoch 2182/1000000, d_loss: -58.2618293762207,  g_loss: 171.24996948242188\n",
            "Training epoch 2183/1000000, d_loss: -107.98567199707031,  g_loss: 325.17303466796875\n",
            "Training epoch 2184/1000000, d_loss: -142.16946411132812,  g_loss: 108.08612823486328\n",
            "Training epoch 2185/1000000, d_loss: -411.9588623046875,  g_loss: -77.38741302490234\n",
            "Training epoch 2186/1000000, d_loss: -744.9205322265625,  g_loss: -474.40606689453125\n",
            "Training epoch 2187/1000000, d_loss: -328.58087158203125,  g_loss: -194.4596710205078\n",
            "Training epoch 2188/1000000, d_loss: -27.331811904907227,  g_loss: -199.63552856445312\n",
            "Training epoch 2189/1000000, d_loss: 2.8439712524414062,  g_loss: -38.190704345703125\n",
            "Training epoch 2190/1000000, d_loss: -73.287109375,  g_loss: -2.6138267517089844\n",
            "Training epoch 2191/1000000, d_loss: -156.59832763671875,  g_loss: -126.45265197753906\n",
            "Training epoch 2192/1000000, d_loss: -54.356197357177734,  g_loss: -60.79905319213867\n",
            "Training epoch 2193/1000000, d_loss: -139.37823486328125,  g_loss: 60.57598876953125\n",
            "Training epoch 2194/1000000, d_loss: -181.58248901367188,  g_loss: -27.478546142578125\n",
            "Training epoch 2195/1000000, d_loss: -130.5879364013672,  g_loss: 55.41358184814453\n",
            "Training epoch 2196/1000000, d_loss: -96.65042877197266,  g_loss: -21.447620391845703\n",
            "Training epoch 2197/1000000, d_loss: -154.00323486328125,  g_loss: -69.85758972167969\n",
            "Training epoch 2198/1000000, d_loss: -112.02476501464844,  g_loss: 168.8050994873047\n",
            "Training epoch 2199/1000000, d_loss: -52.71379089355469,  g_loss: 101.98768615722656\n",
            "Training epoch 2200/1000000, d_loss: -97.26394653320312,  g_loss: 173.79922485351562\n",
            "Training epoch 2201/1000000, d_loss: 7.3380126953125,  g_loss: 66.58264923095703\n",
            "Training epoch 2202/1000000, d_loss: -8.222915649414062,  g_loss: 115.14138793945312\n",
            "Training epoch 2203/1000000, d_loss: -87.21517944335938,  g_loss: 54.063899993896484\n",
            "Training epoch 2204/1000000, d_loss: 21.486148834228516,  g_loss: 102.00519561767578\n",
            "Training epoch 2205/1000000, d_loss: -224.5882568359375,  g_loss: -5.242301940917969\n",
            "Training epoch 2206/1000000, d_loss: -56.83619689941406,  g_loss: -108.10369873046875\n",
            "Training epoch 2207/1000000, d_loss: -3.5521697998046875,  g_loss: -88.75909423828125\n",
            "Training epoch 2208/1000000, d_loss: -72.01924896240234,  g_loss: -29.53156280517578\n",
            "Training epoch 2209/1000000, d_loss: -156.6566162109375,  g_loss: 127.13585662841797\n",
            "Training epoch 2210/1000000, d_loss: 1.6840991973876953,  g_loss: 69.87987518310547\n",
            "Training epoch 2211/1000000, d_loss: -199.35269165039062,  g_loss: -63.09931182861328\n",
            "Training epoch 2212/1000000, d_loss: -224.18344116210938,  g_loss: -103.67113494873047\n",
            "Training epoch 2213/1000000, d_loss: -153.94386291503906,  g_loss: 288.6737060546875\n",
            "Training epoch 2214/1000000, d_loss: -49.864784240722656,  g_loss: 69.23981475830078\n",
            "Training epoch 2215/1000000, d_loss: -2.1587657928466797,  g_loss: 144.724853515625\n",
            "Training epoch 2216/1000000, d_loss: -672.2877807617188,  g_loss: -214.8839874267578\n",
            "Training epoch 2217/1000000, d_loss: -0.2966499328613281,  g_loss: 89.37091064453125\n",
            "Training epoch 2218/1000000, d_loss: -135.84356689453125,  g_loss: 168.9375\n",
            "Training epoch 2219/1000000, d_loss: -149.72869873046875,  g_loss: 23.732872009277344\n",
            "Training epoch 2220/1000000, d_loss: -57.934783935546875,  g_loss: 57.98292541503906\n",
            "Training epoch 2221/1000000, d_loss: -99.89198303222656,  g_loss: 194.9824676513672\n",
            "Training epoch 2222/1000000, d_loss: -114.07720947265625,  g_loss: 123.90662384033203\n",
            "Training epoch 2223/1000000, d_loss: -62.25200653076172,  g_loss: 133.9329071044922\n",
            "Training epoch 2224/1000000, d_loss: 226.02328491210938,  g_loss: 179.35658264160156\n",
            "Training epoch 2225/1000000, d_loss: -43.96588897705078,  g_loss: 230.9835968017578\n",
            "Training epoch 2226/1000000, d_loss: -72.47698211669922,  g_loss: 238.95501708984375\n",
            "Training epoch 2227/1000000, d_loss: -58.729190826416016,  g_loss: 202.6995849609375\n",
            "Training epoch 2228/1000000, d_loss: -147.03939819335938,  g_loss: 366.9942626953125\n",
            "Training epoch 2229/1000000, d_loss: -113.90309143066406,  g_loss: 229.83042907714844\n",
            "Training epoch 2230/1000000, d_loss: -4.739662170410156,  g_loss: 187.7157745361328\n",
            "Training epoch 2231/1000000, d_loss: -176.55613708496094,  g_loss: 158.644775390625\n",
            "Training epoch 2232/1000000, d_loss: -29.777851104736328,  g_loss: 188.0881805419922\n",
            "Training epoch 2233/1000000, d_loss: -286.36737060546875,  g_loss: 187.61077880859375\n",
            "Training epoch 2234/1000000, d_loss: -52.25232696533203,  g_loss: 172.3314208984375\n",
            "Training epoch 2235/1000000, d_loss: -247.55599975585938,  g_loss: 81.98704528808594\n",
            "Training epoch 2236/1000000, d_loss: -238.92922973632812,  g_loss: 57.490272521972656\n",
            "Training epoch 2237/1000000, d_loss: -157.86856079101562,  g_loss: 344.1204833984375\n",
            "Training epoch 2238/1000000, d_loss: -314.635986328125,  g_loss: 0.40044498443603516\n",
            "Training epoch 2239/1000000, d_loss: 56.11768341064453,  g_loss: 57.825496673583984\n",
            "Training epoch 2240/1000000, d_loss: -19.715871810913086,  g_loss: 86.24871826171875\n",
            "Training epoch 2241/1000000, d_loss: -74.65836334228516,  g_loss: 59.34109115600586\n",
            "Training epoch 2242/1000000, d_loss: -25.791940689086914,  g_loss: 58.614654541015625\n",
            "Training epoch 2243/1000000, d_loss: -87.30764770507812,  g_loss: 37.890872955322266\n",
            "Training epoch 2244/1000000, d_loss: -21.40459632873535,  g_loss: 86.83922576904297\n",
            "Training epoch 2245/1000000, d_loss: -92.38663482666016,  g_loss: 43.40772247314453\n",
            "Training epoch 2246/1000000, d_loss: -182.35955810546875,  g_loss: 21.285085678100586\n",
            "Training epoch 2247/1000000, d_loss: -142.7803955078125,  g_loss: 81.54193115234375\n",
            "Training epoch 2248/1000000, d_loss: -76.05026245117188,  g_loss: 101.26068115234375\n",
            "Training epoch 2249/1000000, d_loss: -323.1057434082031,  g_loss: -71.76335906982422\n",
            "Training epoch 2250/1000000, d_loss: -40.955711364746094,  g_loss: 73.69468688964844\n",
            "Training epoch 2251/1000000, d_loss: -242.90296936035156,  g_loss: -41.735748291015625\n",
            "Training epoch 2252/1000000, d_loss: -297.3979187011719,  g_loss: 21.08019256591797\n",
            "Training epoch 2253/1000000, d_loss: -32.426734924316406,  g_loss: 29.440753936767578\n",
            "Training epoch 2254/1000000, d_loss: -15.934799194335938,  g_loss: 32.89299392700195\n",
            "Training epoch 2255/1000000, d_loss: -212.96206665039062,  g_loss: 11.672209739685059\n",
            "Training epoch 2256/1000000, d_loss: -69.6248550415039,  g_loss: 9.981950759887695\n",
            "Training epoch 2257/1000000, d_loss: -93.80484008789062,  g_loss: 7.385984420776367\n",
            "Training epoch 2258/1000000, d_loss: -437.8216857910156,  g_loss: -197.5615234375\n",
            "Training epoch 2259/1000000, d_loss: -356.1493835449219,  g_loss: -276.319580078125\n",
            "Training epoch 2260/1000000, d_loss: -78.32843017578125,  g_loss: 91.84410095214844\n",
            "Training epoch 2261/1000000, d_loss: -607.3997192382812,  g_loss: -42.82949447631836\n",
            "Training epoch 2262/1000000, d_loss: 12.535255432128906,  g_loss: 111.94656372070312\n",
            "Training epoch 2263/1000000, d_loss: -108.09481811523438,  g_loss: 160.2156219482422\n",
            "Training epoch 2264/1000000, d_loss: -120.08976745605469,  g_loss: 271.2933044433594\n",
            "Training epoch 2265/1000000, d_loss: -55.556373596191406,  g_loss: 55.31666946411133\n",
            "Training epoch 2266/1000000, d_loss: 4.51397705078125,  g_loss: 134.20306396484375\n",
            "Training epoch 2267/1000000, d_loss: -159.2473602294922,  g_loss: 185.68243408203125\n",
            "Training epoch 2268/1000000, d_loss: -92.71666717529297,  g_loss: 140.60281372070312\n",
            "Training epoch 2269/1000000, d_loss: -474.34442138671875,  g_loss: -76.5742416381836\n",
            "Training epoch 2270/1000000, d_loss: -265.83197021484375,  g_loss: -45.32554626464844\n",
            "Training epoch 2271/1000000, d_loss: -229.74961853027344,  g_loss: -71.3763656616211\n",
            "Training epoch 2272/1000000, d_loss: -49.16986083984375,  g_loss: 29.287029266357422\n",
            "Training epoch 2273/1000000, d_loss: -48.00141906738281,  g_loss: 115.50703430175781\n",
            "Training epoch 2274/1000000, d_loss: 14.493804931640625,  g_loss: 122.89690399169922\n",
            "Training epoch 2275/1000000, d_loss: -103.88370513916016,  g_loss: 175.7152862548828\n",
            "Training epoch 2276/1000000, d_loss: -297.30096435546875,  g_loss: -99.10274505615234\n",
            "Training epoch 2277/1000000, d_loss: -58.54267883300781,  g_loss: 12.240545272827148\n",
            "Training epoch 2278/1000000, d_loss: -99.37239074707031,  g_loss: 49.08717346191406\n",
            "Training epoch 2279/1000000, d_loss: -60.03275680541992,  g_loss: 70.10585021972656\n",
            "Training epoch 2280/1000000, d_loss: -744.598388671875,  g_loss: -244.848876953125\n",
            "Training epoch 2281/1000000, d_loss: -444.01678466796875,  g_loss: -232.2108612060547\n",
            "Training epoch 2282/1000000, d_loss: -25.904891967773438,  g_loss: 12.644426345825195\n",
            "Training epoch 2283/1000000, d_loss: -22.73310089111328,  g_loss: -7.872049331665039\n",
            "Training epoch 2284/1000000, d_loss: -80.24270629882812,  g_loss: 33.30323028564453\n",
            "Training epoch 2285/1000000, d_loss: -80.86875915527344,  g_loss: 47.517578125\n",
            "Training epoch 2286/1000000, d_loss: -96.63847351074219,  g_loss: 46.75322723388672\n",
            "Training epoch 2287/1000000, d_loss: -83.13471984863281,  g_loss: 70.1397476196289\n",
            "Training epoch 2288/1000000, d_loss: -155.7609100341797,  g_loss: -65.59598541259766\n",
            "Training epoch 2289/1000000, d_loss: -89.21337890625,  g_loss: -39.991119384765625\n",
            "Training epoch 2290/1000000, d_loss: -156.54110717773438,  g_loss: 71.52096557617188\n",
            "Training epoch 2291/1000000, d_loss: -240.78311157226562,  g_loss: -96.38363647460938\n",
            "Training epoch 2292/1000000, d_loss: -16.561813354492188,  g_loss: -41.67205047607422\n",
            "Training epoch 2293/1000000, d_loss: -53.05663299560547,  g_loss: 34.78800964355469\n",
            "Training epoch 2294/1000000, d_loss: -157.4380645751953,  g_loss: 74.21063232421875\n",
            "Training epoch 2295/1000000, d_loss: -62.35877990722656,  g_loss: -13.154887199401855\n",
            "Training epoch 2296/1000000, d_loss: 12.689873695373535,  g_loss: 0.8907549381256104\n",
            "Training epoch 2297/1000000, d_loss: -84.767578125,  g_loss: 68.25907135009766\n",
            "Training epoch 2298/1000000, d_loss: -57.14661407470703,  g_loss: 63.01652908325195\n",
            "Training epoch 2299/1000000, d_loss: 4.832454681396484,  g_loss: -2.87503719329834\n",
            "Training epoch 2300/1000000, d_loss: -138.96559143066406,  g_loss: -45.144493103027344\n",
            "Training epoch 2301/1000000, d_loss: -323.890380859375,  g_loss: -173.7712860107422\n",
            "Training epoch 2302/1000000, d_loss: -489.1211242675781,  g_loss: -441.7496643066406\n",
            "Training epoch 2303/1000000, d_loss: 40.87132263183594,  g_loss: 17.278154373168945\n",
            "Training epoch 2304/1000000, d_loss: -72.45503234863281,  g_loss: -26.45073890686035\n",
            "Training epoch 2305/1000000, d_loss: -82.38870239257812,  g_loss: 6.473842620849609\n",
            "Training epoch 2306/1000000, d_loss: -126.56499481201172,  g_loss: 59.245399475097656\n",
            "Training epoch 2307/1000000, d_loss: -688.2508544921875,  g_loss: -99.21717834472656\n",
            "Training epoch 2308/1000000, d_loss: -376.8122863769531,  g_loss: -146.10006713867188\n",
            "Training epoch 2309/1000000, d_loss: -127.33010864257812,  g_loss: 168.6474609375\n",
            "Training epoch 2310/1000000, d_loss: -271.70989990234375,  g_loss: 248.74708557128906\n",
            "Training epoch 2311/1000000, d_loss: -42.949974060058594,  g_loss: 59.05863952636719\n",
            "Training epoch 2312/1000000, d_loss: -58.30941390991211,  g_loss: 89.57196044921875\n",
            "Training epoch 2313/1000000, d_loss: -34.88250732421875,  g_loss: 73.70706176757812\n",
            "Training epoch 2314/1000000, d_loss: -99.38106536865234,  g_loss: 73.38044738769531\n",
            "Training epoch 2315/1000000, d_loss: -105.17893981933594,  g_loss: 72.47712707519531\n",
            "Training epoch 2316/1000000, d_loss: -508.3192138671875,  g_loss: -180.335205078125\n",
            "Training epoch 2317/1000000, d_loss: -101.2645263671875,  g_loss: -67.39183807373047\n",
            "Training epoch 2318/1000000, d_loss: -680.1915283203125,  g_loss: -487.31982421875\n",
            "Training epoch 2319/1000000, d_loss: -14.169380187988281,  g_loss: 208.54776000976562\n",
            "Training epoch 2320/1000000, d_loss: -156.33984375,  g_loss: 109.79731750488281\n",
            "Training epoch 2321/1000000, d_loss: -266.0550537109375,  g_loss: 141.1058349609375\n",
            "Training epoch 2322/1000000, d_loss: -100.47132873535156,  g_loss: 207.58578491210938\n",
            "Training epoch 2323/1000000, d_loss: -170.9580078125,  g_loss: 333.498291015625\n",
            "Training epoch 2324/1000000, d_loss: -238.19979858398438,  g_loss: 355.8922119140625\n",
            "Training epoch 2325/1000000, d_loss: -123.68559265136719,  g_loss: 245.22683715820312\n",
            "Training epoch 2326/1000000, d_loss: -57.82670211791992,  g_loss: 204.9622802734375\n",
            "Training epoch 2327/1000000, d_loss: 25.90977668762207,  g_loss: 173.70562744140625\n",
            "Training epoch 2328/1000000, d_loss: -57.27892303466797,  g_loss: 121.01608276367188\n",
            "Training epoch 2329/1000000, d_loss: -94.06391906738281,  g_loss: 134.97181701660156\n",
            "Training epoch 2330/1000000, d_loss: -493.8841552734375,  g_loss: -36.94071960449219\n",
            "Training epoch 2331/1000000, d_loss: -102.96232604980469,  g_loss: -2.6508560180664062\n",
            "Training epoch 2332/1000000, d_loss: -226.89572143554688,  g_loss: -26.948856353759766\n",
            "Training epoch 2333/1000000, d_loss: -61.706520080566406,  g_loss: -107.99028015136719\n",
            "Training epoch 2334/1000000, d_loss: -71.63642883300781,  g_loss: 21.93248748779297\n",
            "Training epoch 2335/1000000, d_loss: -273.94232177734375,  g_loss: -163.5176544189453\n",
            "Training epoch 2336/1000000, d_loss: -63.588844299316406,  g_loss: 65.72026062011719\n",
            "Training epoch 2337/1000000, d_loss: 69.62063598632812,  g_loss: 113.80975341796875\n",
            "Training epoch 2338/1000000, d_loss: -193.82839965820312,  g_loss: 80.4346694946289\n",
            "Training epoch 2339/1000000, d_loss: -111.65338134765625,  g_loss: 222.30838012695312\n",
            "Training epoch 2340/1000000, d_loss: -130.5037078857422,  g_loss: 202.08544921875\n",
            "Training epoch 2341/1000000, d_loss: -353.69793701171875,  g_loss: -44.52067947387695\n",
            "Training epoch 2342/1000000, d_loss: 60.2115478515625,  g_loss: 33.82100296020508\n",
            "Training epoch 2343/1000000, d_loss: -268.2710266113281,  g_loss: -38.50919723510742\n",
            "Training epoch 2344/1000000, d_loss: 18.913818359375,  g_loss: 24.006858825683594\n",
            "Training epoch 2345/1000000, d_loss: -101.40093994140625,  g_loss: 109.11489868164062\n",
            "Training epoch 2346/1000000, d_loss: -332.1374206542969,  g_loss: -48.202537536621094\n",
            "Training epoch 2347/1000000, d_loss: 34.007972717285156,  g_loss: 74.48320770263672\n",
            "Training epoch 2348/1000000, d_loss: -211.120849609375,  g_loss: 54.28965759277344\n",
            "Training epoch 2349/1000000, d_loss: -66.8048324584961,  g_loss: 100.9200210571289\n",
            "Training epoch 2350/1000000, d_loss: -130.25173950195312,  g_loss: 113.4526596069336\n",
            "Training epoch 2351/1000000, d_loss: -1.0040397644042969,  g_loss: 197.59007263183594\n",
            "Training epoch 2352/1000000, d_loss: -49.96721649169922,  g_loss: 203.26300048828125\n",
            "Training epoch 2353/1000000, d_loss: -30.80051040649414,  g_loss: 99.7945327758789\n",
            "Training epoch 2354/1000000, d_loss: -488.0802001953125,  g_loss: -219.36390686035156\n",
            "Training epoch 2355/1000000, d_loss: 32.84019470214844,  g_loss: 62.64396286010742\n",
            "Training epoch 2356/1000000, d_loss: -108.57003021240234,  g_loss: 52.447105407714844\n",
            "Training epoch 2357/1000000, d_loss: -140.93170166015625,  g_loss: 10.221543312072754\n",
            "Training epoch 2358/1000000, d_loss: -110.47674560546875,  g_loss: 151.40176391601562\n",
            "Training epoch 2359/1000000, d_loss: -73.16255950927734,  g_loss: 136.8675079345703\n",
            "Training epoch 2360/1000000, d_loss: -91.70063781738281,  g_loss: 82.60184478759766\n",
            "Training epoch 2361/1000000, d_loss: -269.72943115234375,  g_loss: 7.4995903968811035\n",
            "Training epoch 2362/1000000, d_loss: -149.07386779785156,  g_loss: 20.519195556640625\n",
            "Training epoch 2363/1000000, d_loss: 3.829172134399414,  g_loss: 8.920659065246582\n",
            "Training epoch 2364/1000000, d_loss: -109.91797637939453,  g_loss: 20.340408325195312\n",
            "Training epoch 2365/1000000, d_loss: -262.8179016113281,  g_loss: -3.1559677124023438\n",
            "Training epoch 2366/1000000, d_loss: -74.93585205078125,  g_loss: 9.865776062011719\n",
            "Training epoch 2367/1000000, d_loss: -128.86279296875,  g_loss: 24.039308547973633\n",
            "Training epoch 2368/1000000, d_loss: -23.238723754882812,  g_loss: 96.68655395507812\n",
            "Training epoch 2369/1000000, d_loss: -35.91458511352539,  g_loss: 125.28755950927734\n",
            "Training epoch 2370/1000000, d_loss: -106.9521484375,  g_loss: 128.02627563476562\n",
            "Training epoch 2371/1000000, d_loss: -7.046058654785156,  g_loss: 30.507585525512695\n",
            "Training epoch 2372/1000000, d_loss: -131.1177978515625,  g_loss: 70.36820983886719\n",
            "Training epoch 2373/1000000, d_loss: -148.4196319580078,  g_loss: 124.4089126586914\n",
            "Training epoch 2374/1000000, d_loss: -284.8066101074219,  g_loss: 7.232593536376953\n",
            "Training epoch 2375/1000000, d_loss: -38.640140533447266,  g_loss: -19.55084228515625\n",
            "Training epoch 2376/1000000, d_loss: -331.92724609375,  g_loss: -323.5900573730469\n",
            "Training epoch 2377/1000000, d_loss: 51.05396270751953,  g_loss: -168.11563110351562\n",
            "Training epoch 2378/1000000, d_loss: 45.561187744140625,  g_loss: -7.36614990234375\n",
            "Training epoch 2379/1000000, d_loss: -30.41848373413086,  g_loss: 50.57855224609375\n",
            "Training epoch 2380/1000000, d_loss: -169.43637084960938,  g_loss: 92.4148941040039\n",
            "Training epoch 2381/1000000, d_loss: -207.76710510253906,  g_loss: 119.22308349609375\n",
            "Training epoch 2382/1000000, d_loss: -35.81120681762695,  g_loss: 149.77349853515625\n",
            "Training epoch 2383/1000000, d_loss: -138.35739135742188,  g_loss: 234.39089965820312\n",
            "Training epoch 2384/1000000, d_loss: -163.44871520996094,  g_loss: 65.11373901367188\n",
            "Training epoch 2385/1000000, d_loss: -484.65545654296875,  g_loss: -156.050537109375\n",
            "Training epoch 2386/1000000, d_loss: -236.11061096191406,  g_loss: -219.79913330078125\n",
            "Training epoch 2387/1000000, d_loss: -54.83061218261719,  g_loss: 317.1053466796875\n",
            "Training epoch 2388/1000000, d_loss: -50.77108383178711,  g_loss: 388.0721435546875\n",
            "Training epoch 2389/1000000, d_loss: -65.3106918334961,  g_loss: 483.9755859375\n",
            "Training epoch 2390/1000000, d_loss: -132.9029998779297,  g_loss: 406.8626708984375\n",
            "Training epoch 2391/1000000, d_loss: -118.65061950683594,  g_loss: 120.343505859375\n",
            "Training epoch 2392/1000000, d_loss: -113.65666961669922,  g_loss: 193.91732788085938\n",
            "Training epoch 2393/1000000, d_loss: -62.06243896484375,  g_loss: 260.93975830078125\n",
            "Training epoch 2394/1000000, d_loss: -83.67030334472656,  g_loss: 227.07150268554688\n",
            "Training epoch 2395/1000000, d_loss: -37.31523895263672,  g_loss: 138.02914428710938\n",
            "Training epoch 2396/1000000, d_loss: -67.73282623291016,  g_loss: 140.1544189453125\n",
            "Training epoch 2397/1000000, d_loss: -384.73919677734375,  g_loss: 5.537873268127441\n",
            "Training epoch 2398/1000000, d_loss: -127.49200439453125,  g_loss: -153.95797729492188\n",
            "Training epoch 2399/1000000, d_loss: -31.980037689208984,  g_loss: 51.77698516845703\n",
            "Training epoch 2400/1000000, d_loss: -70.11363220214844,  g_loss: 63.52608108520508\n",
            "Training epoch 2401/1000000, d_loss: -102.96460723876953,  g_loss: 100.78852081298828\n",
            "Training epoch 2402/1000000, d_loss: -146.0184326171875,  g_loss: 12.744861602783203\n",
            "Training epoch 2403/1000000, d_loss: -65.32683563232422,  g_loss: 106.09471893310547\n",
            "Training epoch 2404/1000000, d_loss: -156.31219482421875,  g_loss: 125.7796630859375\n",
            "Training epoch 2405/1000000, d_loss: -133.98956298828125,  g_loss: -48.71782684326172\n",
            "Training epoch 2406/1000000, d_loss: -117.0323257446289,  g_loss: 55.524566650390625\n",
            "Training epoch 2407/1000000, d_loss: 28.373964309692383,  g_loss: 72.91577911376953\n",
            "Training epoch 2408/1000000, d_loss: -74.66340637207031,  g_loss: 136.31271362304688\n",
            "Training epoch 2409/1000000, d_loss: -89.62309265136719,  g_loss: 120.22702026367188\n",
            "Training epoch 2410/1000000, d_loss: -100.10456848144531,  g_loss: 20.217254638671875\n",
            "Training epoch 2411/1000000, d_loss: -33.59104919433594,  g_loss: 21.80173683166504\n",
            "Training epoch 2412/1000000, d_loss: -301.5511169433594,  g_loss: -136.6931610107422\n",
            "Training epoch 2413/1000000, d_loss: -32.360252380371094,  g_loss: 8.151424407958984\n",
            "Training epoch 2414/1000000, d_loss: -90.42349243164062,  g_loss: 134.49517822265625\n",
            "Training epoch 2415/1000000, d_loss: -109.73316955566406,  g_loss: 132.8263702392578\n",
            "Training epoch 2416/1000000, d_loss: -171.7782745361328,  g_loss: 246.6397705078125\n",
            "Training epoch 2417/1000000, d_loss: -57.930877685546875,  g_loss: 64.86121368408203\n",
            "Training epoch 2418/1000000, d_loss: -97.79231262207031,  g_loss: 21.058080673217773\n",
            "Training epoch 2419/1000000, d_loss: -180.77450561523438,  g_loss: -76.15798950195312\n",
            "Training epoch 2420/1000000, d_loss: -695.8994750976562,  g_loss: -351.3243408203125\n",
            "Training epoch 2421/1000000, d_loss: 36.640472412109375,  g_loss: 86.38380432128906\n",
            "Training epoch 2422/1000000, d_loss: -301.26019287109375,  g_loss: 54.53862762451172\n",
            "Training epoch 2423/1000000, d_loss: -42.37308883666992,  g_loss: 214.38250732421875\n",
            "Training epoch 2424/1000000, d_loss: -116.71771240234375,  g_loss: 121.29287719726562\n",
            "Training epoch 2425/1000000, d_loss: 33.21662521362305,  g_loss: 219.46539306640625\n",
            "Training epoch 2426/1000000, d_loss: -110.60875701904297,  g_loss: 236.01553344726562\n",
            "Training epoch 2427/1000000, d_loss: -112.02568054199219,  g_loss: 220.99566650390625\n",
            "Training epoch 2428/1000000, d_loss: -350.438720703125,  g_loss: 846.64599609375\n",
            "Training epoch 2429/1000000, d_loss: 234.4344482421875,  g_loss: 224.15118408203125\n",
            "Training epoch 2430/1000000, d_loss: 1.4019317626953125,  g_loss: 108.79519653320312\n",
            "Training epoch 2431/1000000, d_loss: -57.829750061035156,  g_loss: -22.493921279907227\n",
            "Training epoch 2432/1000000, d_loss: -77.55451202392578,  g_loss: 61.333702087402344\n",
            "Training epoch 2433/1000000, d_loss: -68.07339477539062,  g_loss: 61.450172424316406\n",
            "Training epoch 2434/1000000, d_loss: -44.40029525756836,  g_loss: 62.85260009765625\n",
            "Training epoch 2435/1000000, d_loss: -336.29541015625,  g_loss: -175.86865234375\n",
            "Training epoch 2436/1000000, d_loss: -195.25306701660156,  g_loss: -98.45397186279297\n",
            "Training epoch 2437/1000000, d_loss: -241.22271728515625,  g_loss: -78.7908935546875\n",
            "Training epoch 2438/1000000, d_loss: -180.24002075195312,  g_loss: -232.9241485595703\n",
            "Training epoch 2439/1000000, d_loss: -128.70709228515625,  g_loss: 72.11939239501953\n",
            "Training epoch 2440/1000000, d_loss: -230.40509033203125,  g_loss: -51.00309371948242\n",
            "Training epoch 2441/1000000, d_loss: 40.081687927246094,  g_loss: -24.517000198364258\n",
            "Training epoch 2442/1000000, d_loss: -22.93616485595703,  g_loss: 210.20257568359375\n",
            "Training epoch 2443/1000000, d_loss: -227.44577026367188,  g_loss: 437.823974609375\n",
            "Training epoch 2444/1000000, d_loss: -140.06150817871094,  g_loss: 395.67449951171875\n",
            "Training epoch 2445/1000000, d_loss: -100.41910552978516,  g_loss: 343.35748291015625\n",
            "Training epoch 2446/1000000, d_loss: -80.80677795410156,  g_loss: 108.1331787109375\n",
            "Training epoch 2447/1000000, d_loss: -496.2132568359375,  g_loss: -66.9621810913086\n",
            "Training epoch 2448/1000000, d_loss: -240.92356872558594,  g_loss: -257.9631652832031\n",
            "Training epoch 2449/1000000, d_loss: -13.3040771484375,  g_loss: -45.240482330322266\n",
            "Training epoch 2450/1000000, d_loss: -69.21854400634766,  g_loss: -100.48431396484375\n",
            "Training epoch 2451/1000000, d_loss: -83.19246673583984,  g_loss: -78.085693359375\n",
            "Training epoch 2452/1000000, d_loss: -66.35302734375,  g_loss: -112.96403503417969\n",
            "Training epoch 2453/1000000, d_loss: -121.1326904296875,  g_loss: -76.71952819824219\n",
            "Training epoch 2454/1000000, d_loss: -116.10478973388672,  g_loss: -127.8182373046875\n",
            "Training epoch 2455/1000000, d_loss: -305.00860595703125,  g_loss: -215.80624389648438\n",
            "Training epoch 2456/1000000, d_loss: -767.5284423828125,  g_loss: -657.1913452148438\n",
            "Training epoch 2457/1000000, d_loss: -179.3552703857422,  g_loss: -34.48918914794922\n",
            "Training epoch 2458/1000000, d_loss: -121.6950912475586,  g_loss: 60.64656066894531\n",
            "Training epoch 2459/1000000, d_loss: -14.937761306762695,  g_loss: 61.242149353027344\n",
            "Training epoch 2460/1000000, d_loss: 31.091888427734375,  g_loss: 66.74684143066406\n",
            "Training epoch 2461/1000000, d_loss: -162.61578369140625,  g_loss: 119.70454406738281\n",
            "Training epoch 2462/1000000, d_loss: -75.7201919555664,  g_loss: 251.70547485351562\n",
            "Training epoch 2463/1000000, d_loss: -86.809326171875,  g_loss: 237.3419189453125\n",
            "Training epoch 2464/1000000, d_loss: -235.04013061523438,  g_loss: 286.36627197265625\n",
            "Training epoch 2465/1000000, d_loss: -123.21882629394531,  g_loss: 280.6494140625\n",
            "Training epoch 2466/1000000, d_loss: -51.530113220214844,  g_loss: 152.147705078125\n",
            "Training epoch 2467/1000000, d_loss: -745.2622680664062,  g_loss: -104.25433349609375\n",
            "Training epoch 2468/1000000, d_loss: -97.93328857421875,  g_loss: -33.857330322265625\n",
            "Training epoch 2469/1000000, d_loss: -130.23831176757812,  g_loss: -130.09825134277344\n",
            "Training epoch 2470/1000000, d_loss: -64.23812866210938,  g_loss: -25.576169967651367\n",
            "Training epoch 2471/1000000, d_loss: -559.4337158203125,  g_loss: -169.58311462402344\n",
            "Training epoch 2472/1000000, d_loss: 157.72549438476562,  g_loss: -60.54266357421875\n",
            "Training epoch 2473/1000000, d_loss: -53.702484130859375,  g_loss: -4.5145263671875\n",
            "Training epoch 2474/1000000, d_loss: -110.77824401855469,  g_loss: -8.021315574645996\n",
            "Training epoch 2475/1000000, d_loss: -77.67565155029297,  g_loss: 31.16277503967285\n",
            "Training epoch 2476/1000000, d_loss: -329.012939453125,  g_loss: -177.67628479003906\n",
            "Training epoch 2477/1000000, d_loss: -96.3982162475586,  g_loss: -16.838462829589844\n",
            "Training epoch 2478/1000000, d_loss: -100.97564697265625,  g_loss: -91.52714538574219\n",
            "Training epoch 2479/1000000, d_loss: -148.49920654296875,  g_loss: 64.86167907714844\n",
            "Training epoch 2480/1000000, d_loss: -289.1119689941406,  g_loss: -103.54757690429688\n",
            "Training epoch 2481/1000000, d_loss: -110.12520599365234,  g_loss: 23.73058319091797\n",
            "Training epoch 2482/1000000, d_loss: -239.87583923339844,  g_loss: 56.53171920776367\n",
            "Training epoch 2483/1000000, d_loss: -127.90367126464844,  g_loss: 169.37966918945312\n",
            "Training epoch 2484/1000000, d_loss: -81.91496276855469,  g_loss: -21.46331214904785\n",
            "Training epoch 2485/1000000, d_loss: -107.85301208496094,  g_loss: 107.95431518554688\n",
            "Training epoch 2486/1000000, d_loss: -7.630607604980469,  g_loss: -61.732765197753906\n",
            "Training epoch 2487/1000000, d_loss: -42.17863845825195,  g_loss: -34.538246154785156\n",
            "Training epoch 2488/1000000, d_loss: -195.35147094726562,  g_loss: -61.372779846191406\n",
            "Training epoch 2489/1000000, d_loss: -127.46699523925781,  g_loss: 62.64463806152344\n",
            "Training epoch 2490/1000000, d_loss: -28.003589630126953,  g_loss: -24.20022964477539\n",
            "Training epoch 2491/1000000, d_loss: -102.06224822998047,  g_loss: -36.85045623779297\n",
            "Training epoch 2492/1000000, d_loss: -574.0308837890625,  g_loss: -247.92129516601562\n",
            "Training epoch 2493/1000000, d_loss: -286.2867736816406,  g_loss: -48.49303436279297\n",
            "Training epoch 2494/1000000, d_loss: -35.04217529296875,  g_loss: -62.730072021484375\n",
            "Training epoch 2495/1000000, d_loss: -12.090481758117676,  g_loss: -149.18389892578125\n",
            "Training epoch 2496/1000000, d_loss: -48.52789306640625,  g_loss: -87.1656723022461\n",
            "Training epoch 2497/1000000, d_loss: -106.70046997070312,  g_loss: -184.85491943359375\n",
            "Training epoch 2498/1000000, d_loss: -98.89091491699219,  g_loss: -177.03143310546875\n",
            "Training epoch 2499/1000000, d_loss: -9.719196319580078,  g_loss: -116.03557586669922\n",
            "Training epoch 2500/1000000, d_loss: -87.15576934814453,  g_loss: -76.66703033447266\n",
            "Training epoch 2501/1000000, d_loss: 25.923107147216797,  g_loss: -48.76122283935547\n",
            "Training epoch 2502/1000000, d_loss: -118.60785675048828,  g_loss: -60.874473571777344\n",
            "Training epoch 2503/1000000, d_loss: -243.73458862304688,  g_loss: -85.13746643066406\n",
            "Training epoch 2504/1000000, d_loss: -242.66851806640625,  g_loss: -142.8731689453125\n",
            "Training epoch 2505/1000000, d_loss: -87.18305206298828,  g_loss: -17.80837631225586\n",
            "Training epoch 2506/1000000, d_loss: -54.52786636352539,  g_loss: -68.0002670288086\n",
            "Training epoch 2507/1000000, d_loss: -7.116491317749023,  g_loss: -158.754150390625\n",
            "Training epoch 2508/1000000, d_loss: -39.98358917236328,  g_loss: -100.93907928466797\n",
            "Training epoch 2509/1000000, d_loss: 96.16169738769531,  g_loss: -68.33502197265625\n",
            "Training epoch 2510/1000000, d_loss: -102.42288970947266,  g_loss: -13.785345077514648\n",
            "Training epoch 2511/1000000, d_loss: -177.69778442382812,  g_loss: -75.54174041748047\n",
            "Training epoch 2512/1000000, d_loss: -81.94552612304688,  g_loss: -21.26504135131836\n",
            "Training epoch 2513/1000000, d_loss: -8.486045837402344,  g_loss: -78.3944091796875\n",
            "Training epoch 2514/1000000, d_loss: -30.224658966064453,  g_loss: -105.44241333007812\n",
            "Training epoch 2515/1000000, d_loss: -398.0330505371094,  g_loss: -173.49392700195312\n",
            "Training epoch 2516/1000000, d_loss: -23.508872985839844,  g_loss: -14.398260116577148\n",
            "Training epoch 2517/1000000, d_loss: -361.2012939453125,  g_loss: -39.584266662597656\n",
            "Training epoch 2518/1000000, d_loss: -61.96635437011719,  g_loss: 32.575660705566406\n",
            "Training epoch 2519/1000000, d_loss: -33.2451286315918,  g_loss: 78.68488311767578\n",
            "Training epoch 2520/1000000, d_loss: -85.30379486083984,  g_loss: 180.0211181640625\n",
            "Training epoch 2521/1000000, d_loss: -68.02729797363281,  g_loss: 105.31365966796875\n",
            "Training epoch 2522/1000000, d_loss: -156.439453125,  g_loss: 79.08818054199219\n",
            "Training epoch 2523/1000000, d_loss: -271.3455505371094,  g_loss: -79.79193115234375\n",
            "Training epoch 2524/1000000, d_loss: -100.95130920410156,  g_loss: -48.736289978027344\n",
            "Training epoch 2525/1000000, d_loss: 15.327621459960938,  g_loss: -30.83139419555664\n",
            "Training epoch 2526/1000000, d_loss: -72.09822082519531,  g_loss: 106.36428833007812\n",
            "Training epoch 2527/1000000, d_loss: -60.97114562988281,  g_loss: 183.1547088623047\n",
            "Training epoch 2528/1000000, d_loss: -27.56504249572754,  g_loss: 50.93606185913086\n",
            "Training epoch 2529/1000000, d_loss: -152.17127990722656,  g_loss: 101.02948760986328\n",
            "Training epoch 2530/1000000, d_loss: -20.920209884643555,  g_loss: 171.87451171875\n",
            "Training epoch 2531/1000000, d_loss: -49.077213287353516,  g_loss: 165.32077026367188\n",
            "Training epoch 2532/1000000, d_loss: -120.30201721191406,  g_loss: 174.4528045654297\n",
            "Training epoch 2533/1000000, d_loss: -115.30374145507812,  g_loss: 93.9324951171875\n",
            "Training epoch 2534/1000000, d_loss: -102.88090515136719,  g_loss: 17.60403060913086\n",
            "Training epoch 2535/1000000, d_loss: -37.03058624267578,  g_loss: 17.655902862548828\n",
            "Training epoch 2536/1000000, d_loss: -121.32872772216797,  g_loss: -78.23497009277344\n",
            "Training epoch 2537/1000000, d_loss: -141.38577270507812,  g_loss: -62.62442398071289\n",
            "Training epoch 2538/1000000, d_loss: 31.58535385131836,  g_loss: -1.910634994506836\n",
            "Training epoch 2539/1000000, d_loss: -52.0157585144043,  g_loss: 22.470285415649414\n",
            "Training epoch 2540/1000000, d_loss: -186.25064086914062,  g_loss: -44.6699333190918\n",
            "Training epoch 2541/1000000, d_loss: -1.875,  g_loss: -60.60588836669922\n",
            "Training epoch 2542/1000000, d_loss: -75.71051025390625,  g_loss: -54.18012237548828\n",
            "Training epoch 2543/1000000, d_loss: -368.17132568359375,  g_loss: -122.98260498046875\n",
            "Training epoch 2544/1000000, d_loss: 31.888324737548828,  g_loss: -80.83707427978516\n",
            "Training epoch 2545/1000000, d_loss: -245.559814453125,  g_loss: -171.6977081298828\n",
            "Training epoch 2546/1000000, d_loss: -79.14500427246094,  g_loss: 71.93426513671875\n",
            "Training epoch 2547/1000000, d_loss: -112.85243225097656,  g_loss: 169.02188110351562\n",
            "Training epoch 2548/1000000, d_loss: -80.79126739501953,  g_loss: 52.38276290893555\n",
            "Training epoch 2549/1000000, d_loss: -466.8370361328125,  g_loss: -196.98828125\n",
            "Training epoch 2550/1000000, d_loss: 53.56309509277344,  g_loss: 32.00088119506836\n",
            "Training epoch 2551/1000000, d_loss: -117.72083282470703,  g_loss: 22.654260635375977\n",
            "Training epoch 2552/1000000, d_loss: -143.38922119140625,  g_loss: -8.564516067504883\n",
            "Training epoch 2553/1000000, d_loss: -91.82723236083984,  g_loss: -28.495927810668945\n",
            "Training epoch 2554/1000000, d_loss: -51.903587341308594,  g_loss: -2.1651687622070312\n",
            "Training epoch 2555/1000000, d_loss: -1.5836677551269531,  g_loss: 37.83603286743164\n",
            "Training epoch 2556/1000000, d_loss: -32.080299377441406,  g_loss: 111.26164245605469\n",
            "Training epoch 2557/1000000, d_loss: -14.474990844726562,  g_loss: 12.773275375366211\n",
            "Training epoch 2558/1000000, d_loss: -571.5643310546875,  g_loss: -117.73246765136719\n",
            "Training epoch 2559/1000000, d_loss: 88.85955810546875,  g_loss: -10.752464294433594\n",
            "Training epoch 2560/1000000, d_loss: -71.25348663330078,  g_loss: 1.0010261535644531\n",
            "Training epoch 2561/1000000, d_loss: -85.27204895019531,  g_loss: -13.08320426940918\n",
            "Training epoch 2562/1000000, d_loss: -85.48983764648438,  g_loss: 94.6868896484375\n",
            "Training epoch 2563/1000000, d_loss: -183.03152465820312,  g_loss: 66.86891174316406\n",
            "Training epoch 2564/1000000, d_loss: -152.28045654296875,  g_loss: 38.84662628173828\n",
            "Training epoch 2565/1000000, d_loss: -216.2679901123047,  g_loss: -77.7912826538086\n",
            "Training epoch 2566/1000000, d_loss: -212.66932678222656,  g_loss: -134.32452392578125\n",
            "Training epoch 2567/1000000, d_loss: 47.21287536621094,  g_loss: 117.7852783203125\n",
            "Training epoch 2568/1000000, d_loss: -180.34762573242188,  g_loss: 304.1177978515625\n",
            "Training epoch 2569/1000000, d_loss: -67.34495544433594,  g_loss: 112.82640075683594\n",
            "Training epoch 2570/1000000, d_loss: -210.42379760742188,  g_loss: 303.8759460449219\n",
            "Training epoch 2571/1000000, d_loss: -160.2528076171875,  g_loss: 24.8637638092041\n",
            "Training epoch 2572/1000000, d_loss: -96.34310913085938,  g_loss: 36.376747131347656\n",
            "Training epoch 2573/1000000, d_loss: -261.0478210449219,  g_loss: -82.25294494628906\n",
            "Training epoch 2574/1000000, d_loss: -141.89639282226562,  g_loss: 34.45940399169922\n",
            "Training epoch 2575/1000000, d_loss: -291.5440673828125,  g_loss: 347.5222473144531\n",
            "Training epoch 2576/1000000, d_loss: -149.51910400390625,  g_loss: 143.25747680664062\n",
            "Training epoch 2577/1000000, d_loss: 1.6361770629882812,  g_loss: 90.06163024902344\n",
            "Training epoch 2578/1000000, d_loss: -210.3253173828125,  g_loss: 150.8966522216797\n",
            "Training epoch 2579/1000000, d_loss: -68.69720458984375,  g_loss: 85.15357208251953\n",
            "Training epoch 2580/1000000, d_loss: -180.21697998046875,  g_loss: -66.47515106201172\n",
            "Training epoch 2581/1000000, d_loss: -96.04374694824219,  g_loss: 107.23062133789062\n",
            "Training epoch 2582/1000000, d_loss: -175.33221435546875,  g_loss: -126.03325653076172\n",
            "Training epoch 2583/1000000, d_loss: -162.31695556640625,  g_loss: 89.85394287109375\n",
            "Training epoch 2584/1000000, d_loss: -172.28610229492188,  g_loss: -137.25482177734375\n",
            "Training epoch 2585/1000000, d_loss: -111.51823425292969,  g_loss: -126.53422546386719\n",
            "Training epoch 2586/1000000, d_loss: -207.60787963867188,  g_loss: -104.982421875\n",
            "Training epoch 2587/1000000, d_loss: -141.7307891845703,  g_loss: 152.56761169433594\n",
            "Training epoch 2588/1000000, d_loss: -348.82989501953125,  g_loss: -141.89845275878906\n",
            "Training epoch 2589/1000000, d_loss: -116.21847534179688,  g_loss: 156.00802612304688\n",
            "Training epoch 2590/1000000, d_loss: -855.845947265625,  g_loss: -386.72491455078125\n",
            "Training epoch 2591/1000000, d_loss: 67.44441986083984,  g_loss: 75.9345703125\n",
            "Training epoch 2592/1000000, d_loss: -92.11310577392578,  g_loss: 185.09280395507812\n",
            "Training epoch 2593/1000000, d_loss: -127.49686431884766,  g_loss: 236.90858459472656\n",
            "Training epoch 2594/1000000, d_loss: -135.42738342285156,  g_loss: 330.1913146972656\n",
            "Training epoch 2595/1000000, d_loss: -87.8204345703125,  g_loss: 236.34259033203125\n",
            "Training epoch 2596/1000000, d_loss: -85.55096435546875,  g_loss: 237.90643310546875\n",
            "Training epoch 2597/1000000, d_loss: 64.42619323730469,  g_loss: 102.91974639892578\n",
            "Training epoch 2598/1000000, d_loss: -36.3884162902832,  g_loss: 84.53572082519531\n",
            "Training epoch 2599/1000000, d_loss: -154.96719360351562,  g_loss: -59.32023620605469\n",
            "Training epoch 2600/1000000, d_loss: -301.6797180175781,  g_loss: -159.25808715820312\n",
            "Training epoch 2601/1000000, d_loss: -25.76352310180664,  g_loss: 91.95661926269531\n",
            "Training epoch 2602/1000000, d_loss: -95.02291870117188,  g_loss: 136.66554260253906\n",
            "Training epoch 2603/1000000, d_loss: -82.32304382324219,  g_loss: 5.592347621917725\n",
            "Training epoch 2604/1000000, d_loss: -119.54324340820312,  g_loss: 28.29770278930664\n",
            "Training epoch 2605/1000000, d_loss: -221.05126953125,  g_loss: 86.78556823730469\n",
            "Training epoch 2606/1000000, d_loss: -297.4382629394531,  g_loss: -261.5281677246094\n",
            "Training epoch 2607/1000000, d_loss: -240.74710083007812,  g_loss: -5.8058390617370605\n",
            "Training epoch 2608/1000000, d_loss: -176.25860595703125,  g_loss: 367.33203125\n",
            "Training epoch 2609/1000000, d_loss: -111.76953125,  g_loss: -40.20520782470703\n",
            "Training epoch 2610/1000000, d_loss: -834.795166015625,  g_loss: -594.3795166015625\n",
            "Training epoch 2611/1000000, d_loss: 77.17813110351562,  g_loss: -182.91004943847656\n",
            "Training epoch 2612/1000000, d_loss: -50.299259185791016,  g_loss: 53.1666259765625\n",
            "Training epoch 2613/1000000, d_loss: -179.1890869140625,  g_loss: -51.32490539550781\n",
            "Training epoch 2614/1000000, d_loss: -57.73394775390625,  g_loss: 104.08929443359375\n",
            "Training epoch 2615/1000000, d_loss: -166.45025634765625,  g_loss: 151.2993927001953\n",
            "Training epoch 2616/1000000, d_loss: -125.23738098144531,  g_loss: 96.76451873779297\n",
            "Training epoch 2617/1000000, d_loss: 48.58586120605469,  g_loss: 23.050708770751953\n",
            "Training epoch 2618/1000000, d_loss: -86.12825775146484,  g_loss: 20.288803100585938\n",
            "Training epoch 2619/1000000, d_loss: -226.67657470703125,  g_loss: -51.41694259643555\n",
            "Training epoch 2620/1000000, d_loss: -34.146812438964844,  g_loss: -6.448531150817871\n",
            "Training epoch 2621/1000000, d_loss: -163.2520751953125,  g_loss: 73.88581848144531\n",
            "Training epoch 2622/1000000, d_loss: -284.3123474121094,  g_loss: 107.35015869140625\n",
            "Training epoch 2623/1000000, d_loss: 28.314208984375,  g_loss: 144.62974548339844\n",
            "Training epoch 2624/1000000, d_loss: 2.849081039428711,  g_loss: 95.15283966064453\n",
            "Training epoch 2625/1000000, d_loss: -29.871225357055664,  g_loss: 75.12144470214844\n",
            "Training epoch 2626/1000000, d_loss: -84.50557708740234,  g_loss: 151.4567413330078\n",
            "Training epoch 2627/1000000, d_loss: -76.52906799316406,  g_loss: 121.75205993652344\n",
            "Training epoch 2628/1000000, d_loss: -38.95599365234375,  g_loss: 68.5426254272461\n",
            "Training epoch 2629/1000000, d_loss: -21.73678970336914,  g_loss: 61.226009368896484\n",
            "Training epoch 2630/1000000, d_loss: -39.41547393798828,  g_loss: 62.775726318359375\n",
            "Training epoch 2631/1000000, d_loss: -502.27093505859375,  g_loss: -114.92559814453125\n",
            "Training epoch 2632/1000000, d_loss: -601.2815551757812,  g_loss: -65.32569885253906\n",
            "Training epoch 2633/1000000, d_loss: -25.350418090820312,  g_loss: -113.02250671386719\n",
            "Training epoch 2634/1000000, d_loss: 0.8541393280029297,  g_loss: -122.4120864868164\n",
            "Training epoch 2635/1000000, d_loss: -76.53573608398438,  g_loss: 39.20171356201172\n",
            "Training epoch 2636/1000000, d_loss: -73.973876953125,  g_loss: 12.091710090637207\n",
            "Training epoch 2637/1000000, d_loss: 19.81523895263672,  g_loss: 6.188224792480469\n",
            "Training epoch 2638/1000000, d_loss: -14.44760513305664,  g_loss: 42.90504837036133\n",
            "Training epoch 2639/1000000, d_loss: -90.55754089355469,  g_loss: -40.91564178466797\n",
            "Training epoch 2640/1000000, d_loss: -27.304306030273438,  g_loss: 19.098976135253906\n",
            "Training epoch 2641/1000000, d_loss: -47.9779052734375,  g_loss: 14.64094066619873\n",
            "Training epoch 2642/1000000, d_loss: -63.42644119262695,  g_loss: 45.62993621826172\n",
            "Training epoch 2643/1000000, d_loss: -284.99224853515625,  g_loss: -69.79562377929688\n",
            "Training epoch 2644/1000000, d_loss: -141.31129455566406,  g_loss: -109.45272827148438\n",
            "Training epoch 2645/1000000, d_loss: -82.59932708740234,  g_loss: -38.056800842285156\n",
            "Training epoch 2646/1000000, d_loss: -0.15749549865722656,  g_loss: 34.44172286987305\n",
            "Training epoch 2647/1000000, d_loss: -111.65579223632812,  g_loss: 44.039146423339844\n",
            "Training epoch 2648/1000000, d_loss: -130.8573760986328,  g_loss: -65.78451538085938\n",
            "Training epoch 2649/1000000, d_loss: -58.2119026184082,  g_loss: -17.91782569885254\n",
            "Training epoch 2650/1000000, d_loss: -260.3649597167969,  g_loss: -136.64276123046875\n",
            "Training epoch 2651/1000000, d_loss: -64.39708709716797,  g_loss: -51.24248504638672\n",
            "Training epoch 2652/1000000, d_loss: -82.50430297851562,  g_loss: -11.585000991821289\n",
            "Training epoch 2653/1000000, d_loss: -35.138431549072266,  g_loss: -10.764642715454102\n",
            "Training epoch 2654/1000000, d_loss: -140.83865356445312,  g_loss: -59.313541412353516\n",
            "Training epoch 2655/1000000, d_loss: -42.732322692871094,  g_loss: -9.347175598144531\n",
            "Training epoch 2656/1000000, d_loss: -149.4193572998047,  g_loss: -45.40964126586914\n",
            "Training epoch 2657/1000000, d_loss: -359.1216735839844,  g_loss: -197.61029052734375\n",
            "Training epoch 2658/1000000, d_loss: 37.68144226074219,  g_loss: 90.59070587158203\n",
            "Training epoch 2659/1000000, d_loss: -65.94235229492188,  g_loss: 58.586143493652344\n",
            "Training epoch 2660/1000000, d_loss: -20.913036346435547,  g_loss: 79.11552429199219\n",
            "Training epoch 2661/1000000, d_loss: -14.81779670715332,  g_loss: 148.84280395507812\n",
            "Training epoch 2662/1000000, d_loss: -279.47564697265625,  g_loss: 35.94142150878906\n",
            "Training epoch 2663/1000000, d_loss: -167.2716827392578,  g_loss: -44.69267654418945\n",
            "Training epoch 2664/1000000, d_loss: -42.907466888427734,  g_loss: 61.4661865234375\n",
            "Training epoch 2665/1000000, d_loss: 5.00909423828125,  g_loss: 123.5831069946289\n",
            "Training epoch 2666/1000000, d_loss: -46.042518615722656,  g_loss: 30.677412033081055\n",
            "Training epoch 2667/1000000, d_loss: -72.79498291015625,  g_loss: 116.21476745605469\n",
            "Training epoch 2668/1000000, d_loss: -51.66704559326172,  g_loss: 44.30015182495117\n",
            "Training epoch 2669/1000000, d_loss: 7.525852203369141,  g_loss: -22.524246215820312\n",
            "Training epoch 2670/1000000, d_loss: -148.84133911132812,  g_loss: -37.60765838623047\n",
            "Training epoch 2671/1000000, d_loss: -114.81587219238281,  g_loss: 41.116127014160156\n",
            "Training epoch 2672/1000000, d_loss: -101.96247863769531,  g_loss: 8.832454681396484\n",
            "Training epoch 2673/1000000, d_loss: -89.68595886230469,  g_loss: -18.350433349609375\n",
            "Training epoch 2674/1000000, d_loss: -199.66026306152344,  g_loss: -90.56172943115234\n",
            "Training epoch 2675/1000000, d_loss: -104.02086639404297,  g_loss: 99.5566635131836\n",
            "Training epoch 2676/1000000, d_loss: -96.94416809082031,  g_loss: 199.01744079589844\n",
            "Training epoch 2677/1000000, d_loss: -28.149490356445312,  g_loss: 182.2498779296875\n",
            "Training epoch 2678/1000000, d_loss: -17.932050704956055,  g_loss: 48.80414581298828\n",
            "Training epoch 2679/1000000, d_loss: -42.46315383911133,  g_loss: 16.81814193725586\n",
            "Training epoch 2680/1000000, d_loss: -57.66761779785156,  g_loss: -13.476449012756348\n",
            "Training epoch 2681/1000000, d_loss: -245.44935607910156,  g_loss: -64.86325073242188\n",
            "Training epoch 2682/1000000, d_loss: -218.89959716796875,  g_loss: -172.33529663085938\n",
            "Training epoch 2683/1000000, d_loss: -81.54586791992188,  g_loss: -178.93563842773438\n",
            "Training epoch 2684/1000000, d_loss: -487.00238037109375,  g_loss: -204.3777618408203\n",
            "Training epoch 2685/1000000, d_loss: -592.603515625,  g_loss: -284.1739196777344\n",
            "Training epoch 2686/1000000, d_loss: 6.944942474365234,  g_loss: -63.520877838134766\n",
            "Training epoch 2687/1000000, d_loss: -90.53788757324219,  g_loss: -117.794921875\n",
            "Training epoch 2688/1000000, d_loss: 36.77447509765625,  g_loss: 17.92364501953125\n",
            "Training epoch 2689/1000000, d_loss: -69.83832550048828,  g_loss: 41.67524337768555\n",
            "Training epoch 2690/1000000, d_loss: -92.01001739501953,  g_loss: 98.72943115234375\n",
            "Training epoch 2691/1000000, d_loss: -80.1961441040039,  g_loss: 54.53205108642578\n",
            "Training epoch 2692/1000000, d_loss: -94.15031433105469,  g_loss: 59.41886901855469\n",
            "Training epoch 2693/1000000, d_loss: -98.85140228271484,  g_loss: 77.50215148925781\n",
            "Training epoch 2694/1000000, d_loss: -34.13847732543945,  g_loss: 101.70762634277344\n",
            "Training epoch 2695/1000000, d_loss: -89.41291809082031,  g_loss: 53.14299011230469\n",
            "Training epoch 2696/1000000, d_loss: -177.3977813720703,  g_loss: -45.06121826171875\n",
            "Training epoch 2697/1000000, d_loss: -99.4278564453125,  g_loss: 116.20684814453125\n",
            "Training epoch 2698/1000000, d_loss: -39.285037994384766,  g_loss: 198.32501220703125\n",
            "Training epoch 2699/1000000, d_loss: -51.9576530456543,  g_loss: 141.6074981689453\n",
            "Training epoch 2700/1000000, d_loss: -117.01437377929688,  g_loss: 228.23582458496094\n",
            "Training epoch 2701/1000000, d_loss: -23.556522369384766,  g_loss: 74.57808685302734\n",
            "Training epoch 2702/1000000, d_loss: -81.3521499633789,  g_loss: 57.551719665527344\n",
            "Training epoch 2703/1000000, d_loss: -481.9638977050781,  g_loss: -54.95566940307617\n",
            "Training epoch 2704/1000000, d_loss: -283.56585693359375,  g_loss: -50.053321838378906\n",
            "Training epoch 2705/1000000, d_loss: -37.54902267456055,  g_loss: 47.278053283691406\n",
            "Training epoch 2706/1000000, d_loss: -33.507354736328125,  g_loss: 82.19406127929688\n",
            "Training epoch 2707/1000000, d_loss: -9.810647964477539,  g_loss: 119.7738037109375\n",
            "Training epoch 2708/1000000, d_loss: -46.551734924316406,  g_loss: 78.13972473144531\n",
            "Training epoch 2709/1000000, d_loss: -333.2271728515625,  g_loss: -123.16595458984375\n",
            "Training epoch 2710/1000000, d_loss: -63.69112014770508,  g_loss: -75.2435302734375\n",
            "Training epoch 2711/1000000, d_loss: -118.90800476074219,  g_loss: -166.36541748046875\n",
            "Training epoch 2712/1000000, d_loss: -52.11444854736328,  g_loss: -29.776111602783203\n",
            "Training epoch 2713/1000000, d_loss: -193.05679321289062,  g_loss: 137.43382263183594\n",
            "Training epoch 2714/1000000, d_loss: -9.05410385131836,  g_loss: -111.40568542480469\n",
            "Training epoch 2715/1000000, d_loss: -127.37664794921875,  g_loss: -98.97167205810547\n",
            "Training epoch 2716/1000000, d_loss: -258.8348083496094,  g_loss: -240.64541625976562\n",
            "Training epoch 2717/1000000, d_loss: -205.64263916015625,  g_loss: 131.2014617919922\n",
            "Training epoch 2718/1000000, d_loss: -51.795162200927734,  g_loss: 58.858665466308594\n",
            "Training epoch 2719/1000000, d_loss: -79.16576385498047,  g_loss: -9.268654823303223\n",
            "Training epoch 2720/1000000, d_loss: -158.2747039794922,  g_loss: -87.1089096069336\n",
            "Training epoch 2721/1000000, d_loss: -737.5105590820312,  g_loss: -331.07342529296875\n",
            "Training epoch 2722/1000000, d_loss: -27.6817626953125,  g_loss: -25.742568969726562\n",
            "Training epoch 2723/1000000, d_loss: -379.3505859375,  g_loss: -70.78514099121094\n",
            "Training epoch 2724/1000000, d_loss: 21.963102340698242,  g_loss: -34.10712814331055\n",
            "Training epoch 2725/1000000, d_loss: -365.1545104980469,  g_loss: -130.2027130126953\n",
            "Training epoch 2726/1000000, d_loss: 91.91024017333984,  g_loss: 14.170305252075195\n",
            "Training epoch 2727/1000000, d_loss: -95.10812377929688,  g_loss: 194.31787109375\n",
            "Training epoch 2728/1000000, d_loss: -133.01026916503906,  g_loss: 139.08273315429688\n",
            "Training epoch 2729/1000000, d_loss: -123.45118713378906,  g_loss: 87.4483413696289\n",
            "Training epoch 2730/1000000, d_loss: -77.03265380859375,  g_loss: 220.92874145507812\n",
            "Training epoch 2731/1000000, d_loss: -86.68258666992188,  g_loss: 127.70428466796875\n",
            "Training epoch 2732/1000000, d_loss: 11.98931884765625,  g_loss: 69.18541717529297\n",
            "Training epoch 2733/1000000, d_loss: -43.430599212646484,  g_loss: 16.455480575561523\n",
            "Training epoch 2734/1000000, d_loss: -48.998043060302734,  g_loss: 60.348140716552734\n",
            "Training epoch 2735/1000000, d_loss: -37.74449920654297,  g_loss: -35.199241638183594\n",
            "Training epoch 2736/1000000, d_loss: -95.6860122680664,  g_loss: 14.009838104248047\n",
            "Training epoch 2737/1000000, d_loss: -291.4814453125,  g_loss: 27.561016082763672\n",
            "Training epoch 2738/1000000, d_loss: -52.63036346435547,  g_loss: 65.57501220703125\n",
            "Training epoch 2739/1000000, d_loss: -415.2519226074219,  g_loss: -51.89856719970703\n",
            "Training epoch 2740/1000000, d_loss: -120.33689880371094,  g_loss: -46.155967712402344\n",
            "Training epoch 2741/1000000, d_loss: -35.160560607910156,  g_loss: 103.07469177246094\n",
            "Training epoch 2742/1000000, d_loss: -75.48468780517578,  g_loss: 90.00032043457031\n",
            "Training epoch 2743/1000000, d_loss: -192.36489868164062,  g_loss: 105.73825073242188\n",
            "Training epoch 2744/1000000, d_loss: -161.7626953125,  g_loss: 30.353891372680664\n",
            "Training epoch 2745/1000000, d_loss: -56.44633865356445,  g_loss: 98.7107162475586\n",
            "Training epoch 2746/1000000, d_loss: -82.60673522949219,  g_loss: 24.273067474365234\n",
            "Training epoch 2747/1000000, d_loss: -95.14466094970703,  g_loss: 5.559795379638672\n",
            "Training epoch 2748/1000000, d_loss: -97.94963073730469,  g_loss: 36.4158935546875\n",
            "Training epoch 2749/1000000, d_loss: -555.8995361328125,  g_loss: -169.52191162109375\n",
            "Training epoch 2750/1000000, d_loss: -65.06765747070312,  g_loss: 15.188974380493164\n",
            "Training epoch 2751/1000000, d_loss: -208.0184326171875,  g_loss: -60.73334503173828\n",
            "Training epoch 2752/1000000, d_loss: -124.13514709472656,  g_loss: 112.073974609375\n",
            "Training epoch 2753/1000000, d_loss: -126.04829406738281,  g_loss: 134.89410400390625\n",
            "Training epoch 2754/1000000, d_loss: -93.71292877197266,  g_loss: 140.25296020507812\n",
            "Training epoch 2755/1000000, d_loss: -132.4091033935547,  g_loss: 145.4221649169922\n",
            "Training epoch 2756/1000000, d_loss: -47.80576705932617,  g_loss: -3.9066834449768066\n",
            "Training epoch 2757/1000000, d_loss: -307.2745361328125,  g_loss: -114.78571319580078\n",
            "Training epoch 2758/1000000, d_loss: -141.3550567626953,  g_loss: -114.77685546875\n",
            "Training epoch 2759/1000000, d_loss: -87.97174835205078,  g_loss: 86.08665466308594\n",
            "Training epoch 2760/1000000, d_loss: -204.6160430908203,  g_loss: -28.61906623840332\n",
            "Training epoch 2761/1000000, d_loss: -39.55583953857422,  g_loss: 71.14382934570312\n",
            "Training epoch 2762/1000000, d_loss: -86.56996154785156,  g_loss: 196.6125946044922\n",
            "Training epoch 2763/1000000, d_loss: 14.010162353515625,  g_loss: 86.23699188232422\n",
            "Training epoch 2764/1000000, d_loss: -199.920654296875,  g_loss: 27.02556610107422\n",
            "Training epoch 2765/1000000, d_loss: -61.4770622253418,  g_loss: 41.76833724975586\n",
            "Training epoch 2766/1000000, d_loss: -110.39468383789062,  g_loss: 29.112131118774414\n",
            "Training epoch 2767/1000000, d_loss: -118.97355651855469,  g_loss: 52.28681945800781\n",
            "Training epoch 2768/1000000, d_loss: -178.17359924316406,  g_loss: -74.014404296875\n",
            "Training epoch 2769/1000000, d_loss: -67.35203552246094,  g_loss: 114.84124755859375\n",
            "Training epoch 2770/1000000, d_loss: -190.22042846679688,  g_loss: 48.4631233215332\n",
            "Training epoch 2771/1000000, d_loss: -193.29124450683594,  g_loss: 14.361968994140625\n",
            "Training epoch 2772/1000000, d_loss: -37.469017028808594,  g_loss: 181.357421875\n",
            "Training epoch 2773/1000000, d_loss: -89.87289428710938,  g_loss: 212.47677612304688\n",
            "Training epoch 2774/1000000, d_loss: -182.7228240966797,  g_loss: 388.1600341796875\n",
            "Training epoch 2775/1000000, d_loss: -148.9250030517578,  g_loss: 210.9371337890625\n",
            "Training epoch 2776/1000000, d_loss: -122.11577606201172,  g_loss: 76.53506469726562\n",
            "Training epoch 2777/1000000, d_loss: -208.68551635742188,  g_loss: -39.45237350463867\n",
            "Training epoch 2778/1000000, d_loss: -39.489646911621094,  g_loss: 48.483516693115234\n",
            "Training epoch 2779/1000000, d_loss: -46.15119934082031,  g_loss: 97.13226318359375\n",
            "Training epoch 2780/1000000, d_loss: -21.65904998779297,  g_loss: 158.02349853515625\n",
            "Training epoch 2781/1000000, d_loss: -60.24713134765625,  g_loss: 212.26470947265625\n",
            "Training epoch 2782/1000000, d_loss: -211.9437255859375,  g_loss: 189.1493377685547\n",
            "Training epoch 2783/1000000, d_loss: -120.11344909667969,  g_loss: -46.80197525024414\n",
            "Training epoch 2784/1000000, d_loss: -7.720115661621094,  g_loss: -96.4802017211914\n",
            "Training epoch 2785/1000000, d_loss: -389.42486572265625,  g_loss: -381.70721435546875\n",
            "Training epoch 2786/1000000, d_loss: -181.72659301757812,  g_loss: -179.68923950195312\n",
            "Training epoch 2787/1000000, d_loss: -123.23986053466797,  g_loss: -15.441120147705078\n",
            "Training epoch 2788/1000000, d_loss: -59.78346252441406,  g_loss: 143.2363739013672\n",
            "Training epoch 2789/1000000, d_loss: -26.331737518310547,  g_loss: 91.41963958740234\n",
            "Training epoch 2790/1000000, d_loss: -198.03305053710938,  g_loss: 247.61920166015625\n",
            "Training epoch 2791/1000000, d_loss: -201.3842315673828,  g_loss: 168.94041442871094\n",
            "Training epoch 2792/1000000, d_loss: -14.579580307006836,  g_loss: 35.59486389160156\n",
            "Training epoch 2793/1000000, d_loss: -157.26930236816406,  g_loss: 64.1583251953125\n",
            "Training epoch 2794/1000000, d_loss: -38.3350715637207,  g_loss: -124.95140075683594\n",
            "Training epoch 2795/1000000, d_loss: -119.82545471191406,  g_loss: -19.14269256591797\n",
            "Training epoch 2796/1000000, d_loss: -98.65005493164062,  g_loss: -32.10508728027344\n",
            "Training epoch 2797/1000000, d_loss: -131.38275146484375,  g_loss: -120.95732116699219\n",
            "Training epoch 2798/1000000, d_loss: -112.20016479492188,  g_loss: -129.739990234375\n",
            "Training epoch 2799/1000000, d_loss: -457.8777770996094,  g_loss: -475.8973083496094\n",
            "Training epoch 2800/1000000, d_loss: -78.99937438964844,  g_loss: 1.0804252624511719\n",
            "Training epoch 2801/1000000, d_loss: -141.66358947753906,  g_loss: -13.122776985168457\n",
            "Training epoch 2802/1000000, d_loss: -164.0375213623047,  g_loss: -8.979549407958984\n",
            "Training epoch 2803/1000000, d_loss: -89.27799987792969,  g_loss: -81.86751556396484\n",
            "Training epoch 2804/1000000, d_loss: -38.79338073730469,  g_loss: 13.036052703857422\n",
            "Training epoch 2805/1000000, d_loss: -42.00372314453125,  g_loss: 57.742095947265625\n",
            "Training epoch 2806/1000000, d_loss: -45.02327346801758,  g_loss: 30.09356689453125\n",
            "Training epoch 2807/1000000, d_loss: -211.56915283203125,  g_loss: 100.47856140136719\n",
            "Training epoch 2808/1000000, d_loss: -48.77758026123047,  g_loss: 93.54244232177734\n",
            "Training epoch 2809/1000000, d_loss: -106.81307983398438,  g_loss: 86.93548583984375\n",
            "Training epoch 2810/1000000, d_loss: -26.14812469482422,  g_loss: 116.43362426757812\n",
            "Training epoch 2811/1000000, d_loss: -2.284626007080078,  g_loss: 166.886962890625\n",
            "Training epoch 2812/1000000, d_loss: -278.0757751464844,  g_loss: -19.155506134033203\n",
            "Training epoch 2813/1000000, d_loss: -549.3040771484375,  g_loss: -278.8998107910156\n",
            "Training epoch 2814/1000000, d_loss: -41.046043395996094,  g_loss: -66.12789154052734\n",
            "Training epoch 2815/1000000, d_loss: -22.450260162353516,  g_loss: 52.070960998535156\n",
            "Training epoch 2816/1000000, d_loss: -201.36593627929688,  g_loss: -10.909814834594727\n",
            "Training epoch 2817/1000000, d_loss: -92.2984619140625,  g_loss: 19.157211303710938\n",
            "Training epoch 2818/1000000, d_loss: -252.0569305419922,  g_loss: -230.5835723876953\n",
            "Training epoch 2819/1000000, d_loss: -45.86052703857422,  g_loss: 92.95669555664062\n",
            "Training epoch 2820/1000000, d_loss: -153.25970458984375,  g_loss: -60.25193786621094\n",
            "Training epoch 2821/1000000, d_loss: -44.915245056152344,  g_loss: 101.19985961914062\n",
            "Training epoch 2822/1000000, d_loss: -152.82472229003906,  g_loss: -27.606325149536133\n",
            "Training epoch 2823/1000000, d_loss: -11.697519302368164,  g_loss: -56.5225715637207\n",
            "Training epoch 2824/1000000, d_loss: -132.2337646484375,  g_loss: -0.4026813507080078\n",
            "Training epoch 2825/1000000, d_loss: -31.045764923095703,  g_loss: 98.87726593017578\n",
            "Training epoch 2826/1000000, d_loss: 14.004335403442383,  g_loss: 129.47689819335938\n",
            "Training epoch 2827/1000000, d_loss: 66.41459655761719,  g_loss: 0.4836139678955078\n",
            "Training epoch 2828/1000000, d_loss: -34.949005126953125,  g_loss: 56.73271560668945\n",
            "Training epoch 2829/1000000, d_loss: -209.68211364746094,  g_loss: -103.53203582763672\n",
            "Training epoch 2830/1000000, d_loss: -226.53915405273438,  g_loss: -69.71465301513672\n",
            "Training epoch 2831/1000000, d_loss: -192.05123901367188,  g_loss: 1.1232795715332031\n",
            "Training epoch 2832/1000000, d_loss: -203.70462036132812,  g_loss: 77.26287841796875\n",
            "Training epoch 2833/1000000, d_loss: -212.01487731933594,  g_loss: 108.48457336425781\n",
            "Training epoch 2834/1000000, d_loss: -223.06988525390625,  g_loss: -100.73762512207031\n",
            "Training epoch 2835/1000000, d_loss: -100.8141860961914,  g_loss: 138.49545288085938\n",
            "Training epoch 2836/1000000, d_loss: -255.15188598632812,  g_loss: 41.58407211303711\n",
            "Training epoch 2837/1000000, d_loss: -129.8077850341797,  g_loss: 122.97464752197266\n",
            "Training epoch 2838/1000000, d_loss: -37.69104766845703,  g_loss: 134.3789825439453\n",
            "Training epoch 2839/1000000, d_loss: -215.25526428222656,  g_loss: 131.6444854736328\n",
            "Training epoch 2840/1000000, d_loss: -21.93496322631836,  g_loss: 21.257617950439453\n",
            "Training epoch 2841/1000000, d_loss: -104.1663589477539,  g_loss: 53.06166076660156\n",
            "Training epoch 2842/1000000, d_loss: -121.47679138183594,  g_loss: 74.03311920166016\n",
            "Training epoch 2843/1000000, d_loss: -225.39146423339844,  g_loss: 85.28068542480469\n",
            "Training epoch 2844/1000000, d_loss: -286.02093505859375,  g_loss: -48.985504150390625\n",
            "Training epoch 2845/1000000, d_loss: 29.652729034423828,  g_loss: 186.4344482421875\n",
            "Training epoch 2846/1000000, d_loss: -34.13108444213867,  g_loss: 228.0986328125\n",
            "Training epoch 2847/1000000, d_loss: -406.3130798339844,  g_loss: 1.6437206268310547\n",
            "Training epoch 2848/1000000, d_loss: -64.398681640625,  g_loss: 78.73562622070312\n",
            "Training epoch 2849/1000000, d_loss: -433.31829833984375,  g_loss: -98.39115142822266\n",
            "Training epoch 2850/1000000, d_loss: -45.68656921386719,  g_loss: -83.86930084228516\n",
            "Training epoch 2851/1000000, d_loss: -184.37355041503906,  g_loss: -43.83552551269531\n",
            "Training epoch 2852/1000000, d_loss: -224.5897674560547,  g_loss: -63.93550491333008\n",
            "Training epoch 2853/1000000, d_loss: -57.46807098388672,  g_loss: 89.75323486328125\n",
            "Training epoch 2854/1000000, d_loss: -138.86337280273438,  g_loss: 73.84937286376953\n",
            "Training epoch 2855/1000000, d_loss: -130.2329559326172,  g_loss: 50.7032585144043\n",
            "Training epoch 2856/1000000, d_loss: -57.22947311401367,  g_loss: 161.5228271484375\n",
            "Training epoch 2857/1000000, d_loss: -55.94781494140625,  g_loss: 102.00190734863281\n",
            "Training epoch 2858/1000000, d_loss: -182.58587646484375,  g_loss: 217.4853515625\n",
            "Training epoch 2859/1000000, d_loss: -87.26469421386719,  g_loss: 136.67041015625\n",
            "Training epoch 2860/1000000, d_loss: -90.90850830078125,  g_loss: -12.488275527954102\n",
            "Training epoch 2861/1000000, d_loss: -111.26701354980469,  g_loss: -36.85821533203125\n",
            "Training epoch 2862/1000000, d_loss: -90.6676025390625,  g_loss: 78.18650817871094\n",
            "Training epoch 2863/1000000, d_loss: -5.280350685119629,  g_loss: 62.03404998779297\n",
            "Training epoch 2864/1000000, d_loss: -103.19093322753906,  g_loss: 49.82714080810547\n",
            "Training epoch 2865/1000000, d_loss: -95.7144775390625,  g_loss: 172.4226531982422\n",
            "Training epoch 2866/1000000, d_loss: -85.33009338378906,  g_loss: 87.37242126464844\n",
            "Training epoch 2867/1000000, d_loss: 4.224939346313477,  g_loss: 72.53640747070312\n",
            "Training epoch 2868/1000000, d_loss: -40.04742431640625,  g_loss: 31.56266212463379\n",
            "Training epoch 2869/1000000, d_loss: -158.67431640625,  g_loss: 31.896400451660156\n",
            "Training epoch 2870/1000000, d_loss: -81.71617126464844,  g_loss: 72.40739440917969\n",
            "Training epoch 2871/1000000, d_loss: -34.684043884277344,  g_loss: -19.602949142456055\n",
            "Training epoch 2872/1000000, d_loss: -169.52264404296875,  g_loss: 42.619483947753906\n",
            "Training epoch 2873/1000000, d_loss: 25.2354736328125,  g_loss: 74.50120544433594\n",
            "Training epoch 2874/1000000, d_loss: -21.171905517578125,  g_loss: 135.53802490234375\n",
            "Training epoch 2875/1000000, d_loss: -7.622882843017578,  g_loss: 88.75399780273438\n",
            "Training epoch 2876/1000000, d_loss: -8.529231071472168,  g_loss: 74.1080551147461\n",
            "Training epoch 2877/1000000, d_loss: 0.03397369384765625,  g_loss: 54.668617248535156\n",
            "Training epoch 2878/1000000, d_loss: -32.51776123046875,  g_loss: 32.605140686035156\n",
            "Training epoch 2879/1000000, d_loss: -134.19775390625,  g_loss: 26.85028839111328\n",
            "Training epoch 2880/1000000, d_loss: -36.57793045043945,  g_loss: 168.96490478515625\n",
            "Training epoch 2881/1000000, d_loss: -60.206634521484375,  g_loss: 124.12349700927734\n",
            "Training epoch 2882/1000000, d_loss: -73.75717163085938,  g_loss: 74.3821029663086\n",
            "Training epoch 2883/1000000, d_loss: -177.67108154296875,  g_loss: -18.79258155822754\n",
            "Training epoch 2884/1000000, d_loss: -89.29899597167969,  g_loss: 34.93034362792969\n",
            "Training epoch 2885/1000000, d_loss: -40.51831817626953,  g_loss: 59.877960205078125\n",
            "Training epoch 2886/1000000, d_loss: -101.24870300292969,  g_loss: 118.28793334960938\n",
            "Training epoch 2887/1000000, d_loss: -84.4271469116211,  g_loss: 42.35580062866211\n",
            "Training epoch 2888/1000000, d_loss: -63.33061218261719,  g_loss: 0.25598716735839844\n",
            "Training epoch 2889/1000000, d_loss: -47.099273681640625,  g_loss: 71.17919921875\n",
            "Training epoch 2890/1000000, d_loss: -10.659452438354492,  g_loss: 112.53782653808594\n",
            "Training epoch 2891/1000000, d_loss: -44.29820251464844,  g_loss: 70.9166488647461\n",
            "Training epoch 2892/1000000, d_loss: -321.2930603027344,  g_loss: -160.48077392578125\n",
            "Training epoch 2893/1000000, d_loss: -60.36345291137695,  g_loss: 48.84869384765625\n",
            "Training epoch 2894/1000000, d_loss: -38.72351837158203,  g_loss: 107.7277603149414\n",
            "Training epoch 2895/1000000, d_loss: -112.653564453125,  g_loss: 37.86603546142578\n",
            "Training epoch 2896/1000000, d_loss: -199.42568969726562,  g_loss: 66.98910522460938\n",
            "Training epoch 2897/1000000, d_loss: -592.44140625,  g_loss: -166.052978515625\n",
            "Training epoch 2898/1000000, d_loss: 93.80555725097656,  g_loss: -25.099456787109375\n",
            "Training epoch 2899/1000000, d_loss: -109.76821899414062,  g_loss: 24.85491180419922\n",
            "Training epoch 2900/1000000, d_loss: -95.56694030761719,  g_loss: 71.69139862060547\n",
            "Training epoch 2901/1000000, d_loss: -18.618738174438477,  g_loss: 47.190155029296875\n",
            "Training epoch 2902/1000000, d_loss: -83.02937316894531,  g_loss: 163.09930419921875\n",
            "Training epoch 2903/1000000, d_loss: -169.50628662109375,  g_loss: 108.18476867675781\n",
            "Training epoch 2904/1000000, d_loss: -186.5699005126953,  g_loss: 0.8050289154052734\n",
            "Training epoch 2905/1000000, d_loss: -231.48196411132812,  g_loss: -48.525146484375\n",
            "Training epoch 2906/1000000, d_loss: -83.74815368652344,  g_loss: 35.28636932373047\n",
            "Training epoch 2907/1000000, d_loss: -110.7101058959961,  g_loss: 67.2424087524414\n",
            "Training epoch 2908/1000000, d_loss: -665.399169921875,  g_loss: -305.2178649902344\n",
            "Training epoch 2909/1000000, d_loss: -31.461397171020508,  g_loss: 55.60983657836914\n",
            "Training epoch 2910/1000000, d_loss: -284.7093505859375,  g_loss: 424.19561767578125\n",
            "Training epoch 2911/1000000, d_loss: -389.48907470703125,  g_loss: 679.7850341796875\n",
            "Training epoch 2912/1000000, d_loss: -238.47279357910156,  g_loss: 279.8940734863281\n",
            "Training epoch 2913/1000000, d_loss: -139.97198486328125,  g_loss: 324.4128723144531\n",
            "Training epoch 2914/1000000, d_loss: -19.24215316772461,  g_loss: 145.6712646484375\n",
            "Training epoch 2915/1000000, d_loss: -122.51850891113281,  g_loss: 4.7827606201171875\n",
            "Training epoch 2916/1000000, d_loss: -64.32810974121094,  g_loss: 29.97569465637207\n",
            "Training epoch 2917/1000000, d_loss: -360.7682189941406,  g_loss: -65.15652465820312\n",
            "Training epoch 2918/1000000, d_loss: -540.2910766601562,  g_loss: -660.19140625\n",
            "Training epoch 2919/1000000, d_loss: -102.24152374267578,  g_loss: -58.54524230957031\n",
            "Training epoch 2920/1000000, d_loss: -20.145301818847656,  g_loss: 60.57549285888672\n",
            "Training epoch 2921/1000000, d_loss: -27.595035552978516,  g_loss: 34.20982360839844\n",
            "Training epoch 2922/1000000, d_loss: 4.0523223876953125,  g_loss: 60.02432632446289\n",
            "Training epoch 2923/1000000, d_loss: -67.061767578125,  g_loss: 95.2629165649414\n",
            "Training epoch 2924/1000000, d_loss: -76.9438247680664,  g_loss: 54.567562103271484\n",
            "Training epoch 2925/1000000, d_loss: -69.8863525390625,  g_loss: 135.59219360351562\n",
            "Training epoch 2926/1000000, d_loss: -83.26643371582031,  g_loss: 101.60928344726562\n",
            "Training epoch 2927/1000000, d_loss: 91.45547485351562,  g_loss: 121.59595489501953\n",
            "Training epoch 2928/1000000, d_loss: -66.7457046508789,  g_loss: 91.96905517578125\n",
            "Training epoch 2929/1000000, d_loss: -395.6387023925781,  g_loss: -27.586301803588867\n",
            "Training epoch 2930/1000000, d_loss: -38.0135498046875,  g_loss: 75.87095642089844\n",
            "Training epoch 2931/1000000, d_loss: -3.5268630981445312,  g_loss: 61.90142822265625\n",
            "Training epoch 2932/1000000, d_loss: -73.34671783447266,  g_loss: 64.23176574707031\n",
            "Training epoch 2933/1000000, d_loss: -28.760372161865234,  g_loss: 25.86722183227539\n",
            "Training epoch 2934/1000000, d_loss: -1075.0198974609375,  g_loss: -299.41259765625\n",
            "Training epoch 2935/1000000, d_loss: 8.3707275390625,  g_loss: -118.99300384521484\n",
            "Training epoch 2936/1000000, d_loss: -44.37648010253906,  g_loss: -17.007450103759766\n",
            "Training epoch 2937/1000000, d_loss: 77.67510223388672,  g_loss: 49.6889533996582\n",
            "Training epoch 2938/1000000, d_loss: -81.94913482666016,  g_loss: -0.9994430541992188\n",
            "Training epoch 2939/1000000, d_loss: 7.355365753173828,  g_loss: 168.35443115234375\n",
            "Training epoch 2940/1000000, d_loss: 14.175628662109375,  g_loss: 126.52151489257812\n",
            "Training epoch 2941/1000000, d_loss: -158.35096740722656,  g_loss: 328.6312255859375\n",
            "Training epoch 2942/1000000, d_loss: -57.40119934082031,  g_loss: 154.7711181640625\n",
            "Training epoch 2943/1000000, d_loss: -63.922691345214844,  g_loss: 270.7780456542969\n",
            "Training epoch 2944/1000000, d_loss: -138.22665405273438,  g_loss: 125.19119262695312\n",
            "Training epoch 2945/1000000, d_loss: -243.14659118652344,  g_loss: 171.20968627929688\n",
            "Training epoch 2946/1000000, d_loss: 79.46369171142578,  g_loss: 179.67239379882812\n",
            "Training epoch 2947/1000000, d_loss: -137.52914428710938,  g_loss: 274.5919189453125\n",
            "Training epoch 2948/1000000, d_loss: -74.70918273925781,  g_loss: 204.9235076904297\n",
            "Training epoch 2949/1000000, d_loss: -96.02240753173828,  g_loss: 204.48521423339844\n",
            "Training epoch 2950/1000000, d_loss: -112.55945587158203,  g_loss: 114.80781555175781\n",
            "Training epoch 2951/1000000, d_loss: -95.63619232177734,  g_loss: 100.84837341308594\n",
            "Training epoch 2952/1000000, d_loss: -56.561038970947266,  g_loss: 90.43307495117188\n",
            "Training epoch 2953/1000000, d_loss: -77.13567352294922,  g_loss: 94.76203155517578\n",
            "Training epoch 2954/1000000, d_loss: -34.278018951416016,  g_loss: 89.28014373779297\n",
            "Training epoch 2955/1000000, d_loss: -264.1076965332031,  g_loss: 37.62450408935547\n",
            "Training epoch 2956/1000000, d_loss: -102.68376922607422,  g_loss: 101.97535705566406\n",
            "Training epoch 2957/1000000, d_loss: -45.6982421875,  g_loss: 62.915531158447266\n",
            "Training epoch 2958/1000000, d_loss: -92.81055450439453,  g_loss: 92.8004150390625\n",
            "Training epoch 2959/1000000, d_loss: 1.0738964080810547,  g_loss: 105.7010498046875\n",
            "Training epoch 2960/1000000, d_loss: -105.73603057861328,  g_loss: 70.59115600585938\n",
            "Training epoch 2961/1000000, d_loss: -198.86221313476562,  g_loss: 33.45907974243164\n",
            "Training epoch 2962/1000000, d_loss: -62.534706115722656,  g_loss: 2.974855422973633\n",
            "Training epoch 2963/1000000, d_loss: -38.16116714477539,  g_loss: 78.66907501220703\n",
            "Training epoch 2964/1000000, d_loss: -2.1881961822509766,  g_loss: 61.17707824707031\n",
            "Training epoch 2965/1000000, d_loss: -28.938608169555664,  g_loss: 85.35942077636719\n",
            "Training epoch 2966/1000000, d_loss: -33.60094451904297,  g_loss: 75.59434509277344\n",
            "Training epoch 2967/1000000, d_loss: -779.6461181640625,  g_loss: -263.9893798828125\n",
            "Training epoch 2968/1000000, d_loss: 53.779541015625,  g_loss: -126.9144287109375\n",
            "Training epoch 2969/1000000, d_loss: -82.38789367675781,  g_loss: -66.73765563964844\n",
            "Training epoch 2970/1000000, d_loss: -90.59806823730469,  g_loss: -129.13316345214844\n",
            "Training epoch 2971/1000000, d_loss: -19.242719650268555,  g_loss: -84.7116470336914\n",
            "Training epoch 2972/1000000, d_loss: 17.559547424316406,  g_loss: -34.7901611328125\n",
            "Training epoch 2973/1000000, d_loss: -10.324216842651367,  g_loss: -19.739604949951172\n",
            "Training epoch 2974/1000000, d_loss: -27.350215911865234,  g_loss: 30.634294509887695\n",
            "Training epoch 2975/1000000, d_loss: -545.01220703125,  g_loss: -195.02670288085938\n",
            "Training epoch 2976/1000000, d_loss: 0.43964385986328125,  g_loss: -59.90534973144531\n",
            "Training epoch 2977/1000000, d_loss: -137.24288940429688,  g_loss: -67.40985107421875\n",
            "Training epoch 2978/1000000, d_loss: 18.578262329101562,  g_loss: -64.01469421386719\n",
            "Training epoch 2979/1000000, d_loss: -116.60751342773438,  g_loss: -60.935455322265625\n",
            "Training epoch 2980/1000000, d_loss: -15.179769515991211,  g_loss: -133.14305114746094\n",
            "Training epoch 2981/1000000, d_loss: -84.03408813476562,  g_loss: -74.65001678466797\n",
            "Training epoch 2982/1000000, d_loss: 4.516380310058594,  g_loss: -98.34837341308594\n",
            "Training epoch 2983/1000000, d_loss: -33.76834487915039,  g_loss: -58.0966796875\n",
            "Training epoch 2984/1000000, d_loss: -345.296630859375,  g_loss: -98.02558135986328\n",
            "Training epoch 2985/1000000, d_loss: -33.16727066040039,  g_loss: 110.52845764160156\n",
            "Training epoch 2986/1000000, d_loss: -266.3067932128906,  g_loss: 170.2176055908203\n",
            "Training epoch 2987/1000000, d_loss: -117.57009887695312,  g_loss: 268.693359375\n",
            "Training epoch 2988/1000000, d_loss: 6.1730194091796875,  g_loss: -34.5498161315918\n",
            "Training epoch 2989/1000000, d_loss: -44.85457992553711,  g_loss: 19.29144287109375\n",
            "Training epoch 2990/1000000, d_loss: -78.18274688720703,  g_loss: -1.1728343963623047\n",
            "Training epoch 2991/1000000, d_loss: -287.14227294921875,  g_loss: -67.68162536621094\n",
            "Training epoch 2992/1000000, d_loss: -4.042381286621094,  g_loss: -25.645231246948242\n",
            "Training epoch 2993/1000000, d_loss: -60.179386138916016,  g_loss: 49.053504943847656\n",
            "Training epoch 2994/1000000, d_loss: -118.53842163085938,  g_loss: -29.967391967773438\n",
            "Training epoch 2995/1000000, d_loss: 4.6361236572265625,  g_loss: 147.66961669921875\n",
            "Training epoch 2996/1000000, d_loss: -261.0489807128906,  g_loss: -22.104145050048828\n",
            "Training epoch 2997/1000000, d_loss: -180.98117065429688,  g_loss: 21.0585880279541\n",
            "Training epoch 2998/1000000, d_loss: -174.45675659179688,  g_loss: 213.3816375732422\n",
            "Training epoch 2999/1000000, d_loss: -58.41627502441406,  g_loss: 35.889747619628906\n",
            "Training epoch 3000/1000000, d_loss: -30.900161743164062,  g_loss: 110.90119934082031\n",
            "Training epoch 3001/1000000, d_loss: -31.424301147460938,  g_loss: 48.37348556518555\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 24/24 [00:00<00:00, 421.34it/s]\n",
            "Meshing: 100%|██████████| 4798/4798 [00:00<00:00, 5371.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_3001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_3001/assets\n",
            "Training epoch 3002/1000000, d_loss: -276.8318786621094,  g_loss: -64.28682708740234\n",
            "Training epoch 3003/1000000, d_loss: -186.2008056640625,  g_loss: -21.63463592529297\n",
            "Training epoch 3004/1000000, d_loss: -3.141629219055176,  g_loss: 98.27996826171875\n",
            "Training epoch 3005/1000000, d_loss: -89.38653564453125,  g_loss: 27.170095443725586\n",
            "Training epoch 3006/1000000, d_loss: -53.594940185546875,  g_loss: 60.168067932128906\n",
            "Training epoch 3007/1000000, d_loss: -241.3889923095703,  g_loss: -17.445314407348633\n",
            "Training epoch 3008/1000000, d_loss: -255.19915771484375,  g_loss: -105.47856140136719\n",
            "Training epoch 3009/1000000, d_loss: -142.64926147460938,  g_loss: -177.04977416992188\n",
            "Training epoch 3010/1000000, d_loss: -21.004493713378906,  g_loss: 173.89956665039062\n",
            "Training epoch 3011/1000000, d_loss: -97.19291687011719,  g_loss: 276.5003662109375\n",
            "Training epoch 3012/1000000, d_loss: -372.8050842285156,  g_loss: 229.57078552246094\n",
            "Training epoch 3013/1000000, d_loss: -129.98594665527344,  g_loss: 168.40304565429688\n",
            "Training epoch 3014/1000000, d_loss: -170.56907653808594,  g_loss: 35.514705657958984\n",
            "Training epoch 3015/1000000, d_loss: -349.1570739746094,  g_loss: -251.79745483398438\n",
            "Training epoch 3016/1000000, d_loss: 101.99749755859375,  g_loss: 81.17061614990234\n",
            "Training epoch 3017/1000000, d_loss: -222.09683227539062,  g_loss: 408.67327880859375\n",
            "Training epoch 3018/1000000, d_loss: -236.996826171875,  g_loss: 394.90948486328125\n",
            "Training epoch 3019/1000000, d_loss: -254.82814025878906,  g_loss: 27.543886184692383\n",
            "Training epoch 3020/1000000, d_loss: -89.45397186279297,  g_loss: 38.44425964355469\n",
            "Training epoch 3021/1000000, d_loss: -182.49130249023438,  g_loss: 483.73370361328125\n",
            "Training epoch 3022/1000000, d_loss: -58.69281005859375,  g_loss: 129.88900756835938\n",
            "Training epoch 3023/1000000, d_loss: -120.1492691040039,  g_loss: 19.111175537109375\n",
            "Training epoch 3024/1000000, d_loss: -79.2601547241211,  g_loss: 45.45066833496094\n",
            "Training epoch 3025/1000000, d_loss: -154.77474975585938,  g_loss: 166.52432250976562\n",
            "Training epoch 3026/1000000, d_loss: -147.402099609375,  g_loss: 74.37763977050781\n",
            "Training epoch 3027/1000000, d_loss: -57.28416442871094,  g_loss: 163.5238037109375\n",
            "Training epoch 3028/1000000, d_loss: -206.26876831054688,  g_loss: -46.246131896972656\n",
            "Training epoch 3029/1000000, d_loss: -39.05205535888672,  g_loss: -92.36894226074219\n",
            "Training epoch 3030/1000000, d_loss: -445.2879638671875,  g_loss: -229.2508544921875\n",
            "Training epoch 3031/1000000, d_loss: -61.438865661621094,  g_loss: -23.002674102783203\n",
            "Training epoch 3032/1000000, d_loss: -40.51692199707031,  g_loss: -27.89052963256836\n",
            "Training epoch 3033/1000000, d_loss: -153.18765258789062,  g_loss: -11.12844467163086\n",
            "Training epoch 3034/1000000, d_loss: 29.49325180053711,  g_loss: 46.85832214355469\n",
            "Training epoch 3035/1000000, d_loss: 53.76038360595703,  g_loss: 4.914400100708008\n",
            "Training epoch 3036/1000000, d_loss: -88.9375,  g_loss: 31.491037368774414\n",
            "Training epoch 3037/1000000, d_loss: -68.03253173828125,  g_loss: -30.27309226989746\n",
            "Training epoch 3038/1000000, d_loss: -50.875877380371094,  g_loss: -5.55070686340332\n",
            "Training epoch 3039/1000000, d_loss: -44.8831787109375,  g_loss: 18.603694915771484\n",
            "Training epoch 3040/1000000, d_loss: -134.8541259765625,  g_loss: -10.71420669555664\n",
            "Training epoch 3041/1000000, d_loss: -35.8363151550293,  g_loss: 35.631656646728516\n",
            "Training epoch 3042/1000000, d_loss: -664.8734741210938,  g_loss: -187.8594512939453\n",
            "Training epoch 3043/1000000, d_loss: 83.26417541503906,  g_loss: -51.36163330078125\n",
            "Training epoch 3044/1000000, d_loss: -91.75011444091797,  g_loss: -3.3240909576416016\n",
            "Training epoch 3045/1000000, d_loss: -155.83956909179688,  g_loss: 3.5116920471191406\n",
            "Training epoch 3046/1000000, d_loss: -190.14614868164062,  g_loss: -143.84646606445312\n",
            "Training epoch 3047/1000000, d_loss: -265.0866394042969,  g_loss: 570.0562744140625\n",
            "Training epoch 3048/1000000, d_loss: -241.29164123535156,  g_loss: -60.09052276611328\n",
            "Training epoch 3049/1000000, d_loss: -54.087310791015625,  g_loss: -65.61669158935547\n",
            "Training epoch 3050/1000000, d_loss: -41.162620544433594,  g_loss: -39.17803955078125\n",
            "Training epoch 3051/1000000, d_loss: 10.521732330322266,  g_loss: 15.708707809448242\n",
            "Training epoch 3052/1000000, d_loss: -196.42613220214844,  g_loss: -137.89859008789062\n",
            "Training epoch 3053/1000000, d_loss: -515.5713500976562,  g_loss: -243.82696533203125\n",
            "Training epoch 3054/1000000, d_loss: -89.35321044921875,  g_loss: -55.84709930419922\n",
            "Training epoch 3055/1000000, d_loss: -41.244667053222656,  g_loss: -41.30492401123047\n",
            "Training epoch 3056/1000000, d_loss: -13.317695617675781,  g_loss: -85.37910461425781\n",
            "Training epoch 3057/1000000, d_loss: 5.7132415771484375,  g_loss: 32.317832946777344\n",
            "Training epoch 3058/1000000, d_loss: -56.146942138671875,  g_loss: -23.33356285095215\n",
            "Training epoch 3059/1000000, d_loss: -125.5063705444336,  g_loss: -34.01979446411133\n",
            "Training epoch 3060/1000000, d_loss: -326.26483154296875,  g_loss: -148.86849975585938\n",
            "Training epoch 3061/1000000, d_loss: -46.96302032470703,  g_loss: 47.65913391113281\n",
            "Training epoch 3062/1000000, d_loss: -71.53887939453125,  g_loss: 102.59078979492188\n",
            "Training epoch 3063/1000000, d_loss: -117.3155746459961,  g_loss: 141.2877197265625\n",
            "Training epoch 3064/1000000, d_loss: -128.40968322753906,  g_loss: -2.201446533203125\n",
            "Training epoch 3065/1000000, d_loss: -88.8998031616211,  g_loss: -15.792882919311523\n",
            "Training epoch 3066/1000000, d_loss: -549.2802124023438,  g_loss: -202.0032958984375\n",
            "Training epoch 3067/1000000, d_loss: 1.686859130859375,  g_loss: 9.671464920043945\n",
            "Training epoch 3068/1000000, d_loss: -57.15544128417969,  g_loss: -14.661155700683594\n",
            "Training epoch 3069/1000000, d_loss: -42.45640563964844,  g_loss: 82.84660339355469\n",
            "Training epoch 3070/1000000, d_loss: -76.28119659423828,  g_loss: 70.8939208984375\n",
            "Training epoch 3071/1000000, d_loss: -77.36831665039062,  g_loss: 6.3211822509765625\n",
            "Training epoch 3072/1000000, d_loss: -128.03147888183594,  g_loss: 22.655345916748047\n",
            "Training epoch 3073/1000000, d_loss: -310.6159362792969,  g_loss: -153.55685424804688\n",
            "Training epoch 3074/1000000, d_loss: -68.21773529052734,  g_loss: -11.362250328063965\n",
            "Training epoch 3075/1000000, d_loss: -55.30722427368164,  g_loss: 137.84530639648438\n",
            "Training epoch 3076/1000000, d_loss: -141.37612915039062,  g_loss: 182.2921142578125\n",
            "Training epoch 3077/1000000, d_loss: -187.65518188476562,  g_loss: -23.241607666015625\n",
            "Training epoch 3078/1000000, d_loss: -34.980384826660156,  g_loss: 39.110408782958984\n",
            "Training epoch 3079/1000000, d_loss: -91.59607696533203,  g_loss: 45.283817291259766\n",
            "Training epoch 3080/1000000, d_loss: -57.486454010009766,  g_loss: 10.252548217773438\n",
            "Training epoch 3081/1000000, d_loss: -55.9940185546875,  g_loss: -78.67282104492188\n",
            "Training epoch 3082/1000000, d_loss: -117.78678131103516,  g_loss: 53.46696472167969\n",
            "Training epoch 3083/1000000, d_loss: -141.49586486816406,  g_loss: 33.17485046386719\n",
            "Training epoch 3084/1000000, d_loss: -358.296875,  g_loss: -143.9803466796875\n",
            "Training epoch 3085/1000000, d_loss: -197.3629608154297,  g_loss: -101.18634796142578\n",
            "Training epoch 3086/1000000, d_loss: -17.862977981567383,  g_loss: 22.908370971679688\n",
            "Training epoch 3087/1000000, d_loss: -102.62461853027344,  g_loss: 55.0641975402832\n",
            "Training epoch 3088/1000000, d_loss: -27.68354034423828,  g_loss: 25.07554054260254\n",
            "Training epoch 3089/1000000, d_loss: -147.74449157714844,  g_loss: -64.70545959472656\n",
            "Training epoch 3090/1000000, d_loss: -46.741390228271484,  g_loss: 48.66983413696289\n",
            "Training epoch 3091/1000000, d_loss: -120.71467590332031,  g_loss: 12.501871109008789\n",
            "Training epoch 3092/1000000, d_loss: -120.04296112060547,  g_loss: 1.1076440811157227\n",
            "Training epoch 3093/1000000, d_loss: -486.1807861328125,  g_loss: -322.60284423828125\n",
            "Training epoch 3094/1000000, d_loss: -35.00259780883789,  g_loss: 193.19317626953125\n",
            "Training epoch 3095/1000000, d_loss: -228.341796875,  g_loss: 603.2620849609375\n",
            "Training epoch 3096/1000000, d_loss: -55.77631759643555,  g_loss: 316.83441162109375\n",
            "Training epoch 3097/1000000, d_loss: -79.6428451538086,  g_loss: 299.2488098144531\n",
            "Training epoch 3098/1000000, d_loss: -43.40416717529297,  g_loss: 257.9432373046875\n",
            "Training epoch 3099/1000000, d_loss: -66.63951873779297,  g_loss: 99.73541259765625\n",
            "Training epoch 3100/1000000, d_loss: -2.60333251953125,  g_loss: 98.68571472167969\n",
            "Training epoch 3101/1000000, d_loss: -412.7253112792969,  g_loss: -30.666719436645508\n",
            "Training epoch 3102/1000000, d_loss: -242.2393798828125,  g_loss: -74.60963439941406\n",
            "Training epoch 3103/1000000, d_loss: -174.70166015625,  g_loss: 27.377708435058594\n",
            "Training epoch 3104/1000000, d_loss: -124.46532440185547,  g_loss: -16.05325698852539\n",
            "Training epoch 3105/1000000, d_loss: 86.4922866821289,  g_loss: 43.98192596435547\n",
            "Training epoch 3106/1000000, d_loss: -93.08804321289062,  g_loss: 133.50120544433594\n",
            "Training epoch 3107/1000000, d_loss: -29.384536743164062,  g_loss: 145.28915405273438\n",
            "Training epoch 3108/1000000, d_loss: -109.47200012207031,  g_loss: 348.643310546875\n",
            "Training epoch 3109/1000000, d_loss: -139.46060180664062,  g_loss: 395.5010681152344\n",
            "Training epoch 3110/1000000, d_loss: -57.355072021484375,  g_loss: 17.58783531188965\n",
            "Training epoch 3111/1000000, d_loss: -79.31023406982422,  g_loss: 99.3051528930664\n",
            "Training epoch 3112/1000000, d_loss: -257.8549499511719,  g_loss: -33.52389144897461\n",
            "Training epoch 3113/1000000, d_loss: -20.034385681152344,  g_loss: -14.123699188232422\n",
            "Training epoch 3114/1000000, d_loss: -84.21382141113281,  g_loss: 64.51893615722656\n",
            "Training epoch 3115/1000000, d_loss: -85.03355407714844,  g_loss: 87.70645904541016\n",
            "Training epoch 3116/1000000, d_loss: -491.5753479003906,  g_loss: -113.15945434570312\n",
            "Training epoch 3117/1000000, d_loss: -18.07314682006836,  g_loss: 59.01555252075195\n",
            "Training epoch 3118/1000000, d_loss: -34.14164352416992,  g_loss: -6.0050153732299805\n",
            "Training epoch 3119/1000000, d_loss: 33.587703704833984,  g_loss: -6.139555931091309\n",
            "Training epoch 3120/1000000, d_loss: -463.05194091796875,  g_loss: -100.90287017822266\n",
            "Training epoch 3121/1000000, d_loss: -370.9439697265625,  g_loss: -105.45614624023438\n",
            "Training epoch 3122/1000000, d_loss: 27.7096004486084,  g_loss: -62.00883865356445\n",
            "Training epoch 3123/1000000, d_loss: -31.629955291748047,  g_loss: -28.81200408935547\n",
            "Training epoch 3124/1000000, d_loss: -40.942604064941406,  g_loss: 89.49833679199219\n",
            "Training epoch 3125/1000000, d_loss: -133.0876922607422,  g_loss: 30.789424896240234\n",
            "Training epoch 3126/1000000, d_loss: -166.67935180664062,  g_loss: 104.79780578613281\n",
            "Training epoch 3127/1000000, d_loss: -116.58894348144531,  g_loss: 1.7973060607910156\n",
            "Training epoch 3128/1000000, d_loss: -55.22201156616211,  g_loss: 130.21522521972656\n",
            "Training epoch 3129/1000000, d_loss: -92.12559509277344,  g_loss: 64.77865600585938\n",
            "Training epoch 3130/1000000, d_loss: -49.62916564941406,  g_loss: 122.71527099609375\n",
            "Training epoch 3131/1000000, d_loss: -108.26432800292969,  g_loss: 73.17295837402344\n",
            "Training epoch 3132/1000000, d_loss: -188.1935577392578,  g_loss: -2.733663558959961\n",
            "Training epoch 3133/1000000, d_loss: -9.082778930664062,  g_loss: 72.2461929321289\n",
            "Training epoch 3134/1000000, d_loss: -148.98440551757812,  g_loss: -43.127254486083984\n",
            "Training epoch 3135/1000000, d_loss: -489.697265625,  g_loss: -310.2454528808594\n",
            "Training epoch 3136/1000000, d_loss: -32.76407241821289,  g_loss: 137.8782958984375\n",
            "Training epoch 3137/1000000, d_loss: -92.05208587646484,  g_loss: 135.8973388671875\n",
            "Training epoch 3138/1000000, d_loss: 74.48011016845703,  g_loss: 99.25447845458984\n",
            "Training epoch 3139/1000000, d_loss: -70.32186126708984,  g_loss: 149.71209716796875\n",
            "Training epoch 3140/1000000, d_loss: -36.799041748046875,  g_loss: 138.88307189941406\n",
            "Training epoch 3141/1000000, d_loss: 12.7237548828125,  g_loss: 143.50047302246094\n",
            "Training epoch 3142/1000000, d_loss: -116.8988265991211,  g_loss: 179.53611755371094\n",
            "Training epoch 3143/1000000, d_loss: -84.90029907226562,  g_loss: 149.03250122070312\n",
            "Training epoch 3144/1000000, d_loss: -56.05944061279297,  g_loss: 216.85525512695312\n",
            "Training epoch 3145/1000000, d_loss: -52.1724853515625,  g_loss: 124.1974105834961\n",
            "Training epoch 3146/1000000, d_loss: -22.625946044921875,  g_loss: 170.97296142578125\n",
            "Training epoch 3147/1000000, d_loss: -198.02879333496094,  g_loss: 59.046104431152344\n",
            "Training epoch 3148/1000000, d_loss: -47.570838928222656,  g_loss: 173.25608825683594\n",
            "Training epoch 3149/1000000, d_loss: -362.17803955078125,  g_loss: 1.1131305694580078\n",
            "Training epoch 3150/1000000, d_loss: -298.5573425292969,  g_loss: -62.413475036621094\n",
            "Training epoch 3151/1000000, d_loss: -154.00306701660156,  g_loss: 137.62168884277344\n",
            "Training epoch 3152/1000000, d_loss: 852.9738159179688,  g_loss: 112.6156997680664\n",
            "Training epoch 3153/1000000, d_loss: -62.306884765625,  g_loss: -48.748191833496094\n",
            "Training epoch 3154/1000000, d_loss: -77.34857177734375,  g_loss: 75.20414733886719\n",
            "Training epoch 3155/1000000, d_loss: -125.62169647216797,  g_loss: 144.09375\n",
            "Training epoch 3156/1000000, d_loss: -119.62539672851562,  g_loss: 162.57626342773438\n",
            "Training epoch 3157/1000000, d_loss: -224.76229858398438,  g_loss: -24.376258850097656\n",
            "Training epoch 3158/1000000, d_loss: -213.51661682128906,  g_loss: 63.9355354309082\n",
            "Training epoch 3159/1000000, d_loss: -101.2185287475586,  g_loss: 139.21412658691406\n",
            "Training epoch 3160/1000000, d_loss: -106.04434204101562,  g_loss: 212.2813720703125\n",
            "Training epoch 3161/1000000, d_loss: -54.73088073730469,  g_loss: 104.05671691894531\n",
            "Training epoch 3162/1000000, d_loss: -111.7173080444336,  g_loss: 228.8972930908203\n",
            "Training epoch 3163/1000000, d_loss: -29.559059143066406,  g_loss: 17.83761978149414\n",
            "Training epoch 3164/1000000, d_loss: -42.35474395751953,  g_loss: -37.84547424316406\n",
            "Training epoch 3165/1000000, d_loss: -146.49169921875,  g_loss: -15.441375732421875\n",
            "Training epoch 3166/1000000, d_loss: -45.30646896362305,  g_loss: 79.07784271240234\n",
            "Training epoch 3167/1000000, d_loss: -87.8575439453125,  g_loss: -79.15861511230469\n",
            "Training epoch 3168/1000000, d_loss: -46.20649719238281,  g_loss: 9.025062561035156\n",
            "Training epoch 3169/1000000, d_loss: -420.4510192871094,  g_loss: -152.7974090576172\n",
            "Training epoch 3170/1000000, d_loss: -72.44805145263672,  g_loss: 109.40738677978516\n",
            "Training epoch 3171/1000000, d_loss: -39.45177459716797,  g_loss: 190.0811767578125\n",
            "Training epoch 3172/1000000, d_loss: -49.73670959472656,  g_loss: 110.6302490234375\n",
            "Training epoch 3173/1000000, d_loss: -107.61080169677734,  g_loss: 42.326416015625\n",
            "Training epoch 3174/1000000, d_loss: -181.52725219726562,  g_loss: -58.082000732421875\n",
            "Training epoch 3175/1000000, d_loss: -112.22771453857422,  g_loss: -53.67137908935547\n",
            "Training epoch 3176/1000000, d_loss: -451.181640625,  g_loss: -195.56028747558594\n",
            "Training epoch 3177/1000000, d_loss: -36.319793701171875,  g_loss: -66.8987045288086\n",
            "Training epoch 3178/1000000, d_loss: -322.8123779296875,  g_loss: -76.52068328857422\n",
            "Training epoch 3179/1000000, d_loss: -42.23213195800781,  g_loss: 18.34981918334961\n",
            "Training epoch 3180/1000000, d_loss: -217.1035614013672,  g_loss: -133.6521453857422\n",
            "Training epoch 3181/1000000, d_loss: -211.39065551757812,  g_loss: -286.26800537109375\n",
            "Training epoch 3182/1000000, d_loss: -178.80911254882812,  g_loss: 100.89730834960938\n",
            "Training epoch 3183/1000000, d_loss: -160.17703247070312,  g_loss: 211.48382568359375\n",
            "Training epoch 3184/1000000, d_loss: -349.7204284667969,  g_loss: 872.8541259765625\n",
            "Training epoch 3185/1000000, d_loss: -169.58941650390625,  g_loss: 249.5120849609375\n",
            "Training epoch 3186/1000000, d_loss: -84.54330444335938,  g_loss: 128.06243896484375\n",
            "Training epoch 3187/1000000, d_loss: -59.33497619628906,  g_loss: 230.39117431640625\n",
            "Training epoch 3188/1000000, d_loss: -71.60367584228516,  g_loss: 2.316486358642578\n",
            "Training epoch 3189/1000000, d_loss: -214.99729919433594,  g_loss: -122.53836059570312\n",
            "Training epoch 3190/1000000, d_loss: -85.60969543457031,  g_loss: 16.454639434814453\n",
            "Training epoch 3191/1000000, d_loss: -83.26028442382812,  g_loss: 23.248611450195312\n",
            "Training epoch 3192/1000000, d_loss: -510.5728759765625,  g_loss: -161.61163330078125\n",
            "Training epoch 3193/1000000, d_loss: -5.401084899902344,  g_loss: 47.85088348388672\n",
            "Training epoch 3194/1000000, d_loss: 30.952741622924805,  g_loss: 87.83313751220703\n",
            "Training epoch 3195/1000000, d_loss: -26.67430877685547,  g_loss: 45.981380462646484\n",
            "Training epoch 3196/1000000, d_loss: -79.04855346679688,  g_loss: 95.81077575683594\n",
            "Training epoch 3197/1000000, d_loss: -166.47811889648438,  g_loss: 122.77438354492188\n",
            "Training epoch 3198/1000000, d_loss: -124.91887664794922,  g_loss: 92.8326644897461\n",
            "Training epoch 3199/1000000, d_loss: -51.237205505371094,  g_loss: 38.738914489746094\n",
            "Training epoch 3200/1000000, d_loss: -85.17574310302734,  g_loss: 86.61259460449219\n",
            "Training epoch 3201/1000000, d_loss: -185.8598175048828,  g_loss: -46.78939437866211\n",
            "Training epoch 3202/1000000, d_loss: -77.10894775390625,  g_loss: -52.512577056884766\n",
            "Training epoch 3203/1000000, d_loss: -61.068328857421875,  g_loss: 64.38404846191406\n",
            "Training epoch 3204/1000000, d_loss: -253.20750427246094,  g_loss: -175.75247192382812\n",
            "Training epoch 3205/1000000, d_loss: -17.52254867553711,  g_loss: 65.7773666381836\n",
            "Training epoch 3206/1000000, d_loss: -292.830810546875,  g_loss: -134.68991088867188\n",
            "Training epoch 3207/1000000, d_loss: -41.14319610595703,  g_loss: -5.3893232345581055\n",
            "Training epoch 3208/1000000, d_loss: -62.59978485107422,  g_loss: 45.40688705444336\n",
            "Training epoch 3209/1000000, d_loss: -135.0328369140625,  g_loss: 74.92656707763672\n",
            "Training epoch 3210/1000000, d_loss: -38.92503356933594,  g_loss: 17.892322540283203\n",
            "Training epoch 3211/1000000, d_loss: -90.47057342529297,  g_loss: 226.56661987304688\n",
            "Training epoch 3212/1000000, d_loss: -224.564697265625,  g_loss: 41.400272369384766\n",
            "Training epoch 3213/1000000, d_loss: -306.1314392089844,  g_loss: -63.61262512207031\n",
            "Training epoch 3214/1000000, d_loss: 90.18719482421875,  g_loss: 120.03401947021484\n",
            "Training epoch 3215/1000000, d_loss: -72.74304962158203,  g_loss: 4.257385730743408\n",
            "Training epoch 3216/1000000, d_loss: -64.79710388183594,  g_loss: 110.94207000732422\n",
            "Training epoch 3217/1000000, d_loss: -381.3096008300781,  g_loss: 9.082916259765625\n",
            "Training epoch 3218/1000000, d_loss: -288.3212890625,  g_loss: -65.89874267578125\n",
            "Training epoch 3219/1000000, d_loss: -74.97247314453125,  g_loss: 183.2220458984375\n",
            "Training epoch 3220/1000000, d_loss: -139.63241577148438,  g_loss: 227.2224884033203\n",
            "Training epoch 3221/1000000, d_loss: -62.70360565185547,  g_loss: 118.2366943359375\n",
            "Training epoch 3222/1000000, d_loss: -55.79105758666992,  g_loss: 177.8359832763672\n",
            "Training epoch 3223/1000000, d_loss: -271.4751892089844,  g_loss: 52.43952178955078\n",
            "Training epoch 3224/1000000, d_loss: -441.5296630859375,  g_loss: -375.9554443359375\n",
            "Training epoch 3225/1000000, d_loss: 4521.69873046875,  g_loss: 168.1181182861328\n",
            "Training epoch 3226/1000000, d_loss: -141.68209838867188,  g_loss: 365.95648193359375\n",
            "Training epoch 3227/1000000, d_loss: -115.02427673339844,  g_loss: 211.807373046875\n",
            "Training epoch 3228/1000000, d_loss: -126.92398071289062,  g_loss: 290.8218078613281\n",
            "Training epoch 3229/1000000, d_loss: -218.85797119140625,  g_loss: 370.96307373046875\n",
            "Training epoch 3230/1000000, d_loss: -37.454898834228516,  g_loss: 148.43072509765625\n",
            "Training epoch 3231/1000000, d_loss: -30.14144515991211,  g_loss: 144.82501220703125\n",
            "Training epoch 3232/1000000, d_loss: -244.03314208984375,  g_loss: 116.4062728881836\n",
            "Training epoch 3233/1000000, d_loss: -9.697868347167969,  g_loss: 105.86800384521484\n",
            "Training epoch 3234/1000000, d_loss: -142.29779052734375,  g_loss: 87.76226806640625\n",
            "Training epoch 3235/1000000, d_loss: -677.88037109375,  g_loss: -183.02401733398438\n",
            "Training epoch 3236/1000000, d_loss: -152.3154296875,  g_loss: -225.9750213623047\n",
            "Training epoch 3237/1000000, d_loss: -130.37924194335938,  g_loss: -290.398193359375\n",
            "Training epoch 3238/1000000, d_loss: -56.54473876953125,  g_loss: -107.70285034179688\n",
            "Training epoch 3239/1000000, d_loss: -111.29456329345703,  g_loss: -2.5265846252441406\n",
            "Training epoch 3240/1000000, d_loss: -128.55191040039062,  g_loss: 259.84710693359375\n",
            "Training epoch 3241/1000000, d_loss: -171.59146118164062,  g_loss: 72.85150146484375\n",
            "Training epoch 3242/1000000, d_loss: -178.31369018554688,  g_loss: 85.10235595703125\n",
            "Training epoch 3243/1000000, d_loss: -133.76373291015625,  g_loss: 226.8333740234375\n",
            "Training epoch 3244/1000000, d_loss: -186.91293334960938,  g_loss: 356.36981201171875\n",
            "Training epoch 3245/1000000, d_loss: -763.0813598632812,  g_loss: -425.2492980957031\n",
            "Training epoch 3246/1000000, d_loss: -107.80296325683594,  g_loss: -28.774641036987305\n",
            "Training epoch 3247/1000000, d_loss: -109.52021026611328,  g_loss: 236.9581298828125\n",
            "Training epoch 3248/1000000, d_loss: -159.16030883789062,  g_loss: 323.355712890625\n",
            "Training epoch 3249/1000000, d_loss: -229.31228637695312,  g_loss: 362.9549560546875\n",
            "Training epoch 3250/1000000, d_loss: -290.09368896484375,  g_loss: -167.21011352539062\n",
            "Training epoch 3251/1000000, d_loss: -158.66720581054688,  g_loss: 120.41065216064453\n",
            "Training epoch 3252/1000000, d_loss: -72.6581802368164,  g_loss: 311.7532958984375\n",
            "Training epoch 3253/1000000, d_loss: -412.7423400878906,  g_loss: 781.6107177734375\n",
            "Training epoch 3254/1000000, d_loss: 91.68310546875,  g_loss: 197.70779418945312\n",
            "Training epoch 3255/1000000, d_loss: -96.96697235107422,  g_loss: 220.61688232421875\n",
            "Training epoch 3256/1000000, d_loss: -137.27923583984375,  g_loss: 107.44780731201172\n",
            "Training epoch 3257/1000000, d_loss: -268.449951171875,  g_loss: -42.64313507080078\n",
            "Training epoch 3258/1000000, d_loss: -87.33993530273438,  g_loss: 68.81045532226562\n",
            "Training epoch 3259/1000000, d_loss: -120.75199890136719,  g_loss: 121.17784881591797\n",
            "Training epoch 3260/1000000, d_loss: -56.65904998779297,  g_loss: 175.85714721679688\n",
            "Training epoch 3261/1000000, d_loss: -138.66192626953125,  g_loss: 163.81466674804688\n",
            "Training epoch 3262/1000000, d_loss: -76.75509643554688,  g_loss: 33.66419219970703\n",
            "Training epoch 3263/1000000, d_loss: -74.74681854248047,  g_loss: 14.401773452758789\n",
            "Training epoch 3264/1000000, d_loss: -56.319190979003906,  g_loss: 0.8010463714599609\n",
            "Training epoch 3265/1000000, d_loss: -442.37213134765625,  g_loss: -181.43148803710938\n",
            "Training epoch 3266/1000000, d_loss: -156.9404296875,  g_loss: 137.03822326660156\n",
            "Training epoch 3267/1000000, d_loss: -75.85775756835938,  g_loss: 27.243972778320312\n",
            "Training epoch 3268/1000000, d_loss: -123.59407043457031,  g_loss: 82.73748779296875\n",
            "Training epoch 3269/1000000, d_loss: -22.824115753173828,  g_loss: 35.47081756591797\n",
            "Training epoch 3270/1000000, d_loss: 38.31719207763672,  g_loss: 75.12076568603516\n",
            "Training epoch 3271/1000000, d_loss: 619.3035888671875,  g_loss: 289.04864501953125\n",
            "Training epoch 3272/1000000, d_loss: -11.670761108398438,  g_loss: 209.05929565429688\n",
            "Training epoch 3273/1000000, d_loss: -23.939464569091797,  g_loss: 191.72958374023438\n",
            "Training epoch 3274/1000000, d_loss: -95.72489929199219,  g_loss: 151.1016082763672\n",
            "Training epoch 3275/1000000, d_loss: -415.71954345703125,  g_loss: 91.25640869140625\n",
            "Training epoch 3276/1000000, d_loss: -48.060482025146484,  g_loss: 52.714332580566406\n",
            "Training epoch 3277/1000000, d_loss: -8.418058395385742,  g_loss: 31.1783504486084\n",
            "Training epoch 3278/1000000, d_loss: -78.36151885986328,  g_loss: -10.406745910644531\n",
            "Training epoch 3279/1000000, d_loss: -290.3114929199219,  g_loss: -15.30985164642334\n",
            "Training epoch 3280/1000000, d_loss: -29.72125816345215,  g_loss: 70.52508544921875\n",
            "Training epoch 3281/1000000, d_loss: -33.720664978027344,  g_loss: 94.41181945800781\n",
            "Training epoch 3282/1000000, d_loss: -99.86445617675781,  g_loss: 75.35122680664062\n",
            "Training epoch 3283/1000000, d_loss: -213.85018920898438,  g_loss: -17.059707641601562\n",
            "Training epoch 3284/1000000, d_loss: -231.42996215820312,  g_loss: -56.073692321777344\n",
            "Training epoch 3285/1000000, d_loss: -44.087013244628906,  g_loss: -78.0079116821289\n",
            "Training epoch 3286/1000000, d_loss: -130.01446533203125,  g_loss: -75.74205017089844\n",
            "Training epoch 3287/1000000, d_loss: -28.091373443603516,  g_loss: 52.759544372558594\n",
            "Training epoch 3288/1000000, d_loss: -158.07676696777344,  g_loss: -97.23550415039062\n",
            "Training epoch 3289/1000000, d_loss: -131.97409057617188,  g_loss: 107.22223663330078\n",
            "Training epoch 3290/1000000, d_loss: -32.64789962768555,  g_loss: 47.09254455566406\n",
            "Training epoch 3291/1000000, d_loss: -182.8988037109375,  g_loss: -79.45201110839844\n",
            "Training epoch 3292/1000000, d_loss: -119.23845672607422,  g_loss: -103.0835952758789\n",
            "Training epoch 3293/1000000, d_loss: -119.96367645263672,  g_loss: -37.859519958496094\n",
            "Training epoch 3294/1000000, d_loss: 3.9208593368530273,  g_loss: 55.45914077758789\n",
            "Training epoch 3295/1000000, d_loss: -93.98536682128906,  g_loss: -54.15396499633789\n",
            "Training epoch 3296/1000000, d_loss: -469.87689208984375,  g_loss: -461.3908996582031\n",
            "Training epoch 3297/1000000, d_loss: -48.146278381347656,  g_loss: 307.5524597167969\n",
            "Training epoch 3298/1000000, d_loss: -191.80294799804688,  g_loss: 101.71228790283203\n",
            "Training epoch 3299/1000000, d_loss: 51.710052490234375,  g_loss: 102.57989501953125\n",
            "Training epoch 3300/1000000, d_loss: -72.35730743408203,  g_loss: 297.08184814453125\n",
            "Training epoch 3301/1000000, d_loss: -137.71322631835938,  g_loss: 212.79327392578125\n",
            "Training epoch 3302/1000000, d_loss: -122.71088409423828,  g_loss: 218.28512573242188\n",
            "Training epoch 3303/1000000, d_loss: -499.3756103515625,  g_loss: -17.38973045349121\n",
            "Training epoch 3304/1000000, d_loss: 92.63816833496094,  g_loss: 266.14422607421875\n",
            "Training epoch 3305/1000000, d_loss: -68.79631042480469,  g_loss: 228.08889770507812\n",
            "Training epoch 3306/1000000, d_loss: -47.51163101196289,  g_loss: 185.86013793945312\n",
            "Training epoch 3307/1000000, d_loss: -270.1658630371094,  g_loss: 147.48760986328125\n",
            "Training epoch 3308/1000000, d_loss: -65.76492309570312,  g_loss: 148.08966064453125\n",
            "Training epoch 3309/1000000, d_loss: -131.31544494628906,  g_loss: 177.7081756591797\n",
            "Training epoch 3310/1000000, d_loss: -34.59959030151367,  g_loss: 135.178466796875\n",
            "Training epoch 3311/1000000, d_loss: -76.7275390625,  g_loss: 169.02719116210938\n",
            "Training epoch 3312/1000000, d_loss: -744.903076171875,  g_loss: 51.289955139160156\n",
            "Training epoch 3313/1000000, d_loss: -99.36768341064453,  g_loss: 136.27731323242188\n",
            "Training epoch 3314/1000000, d_loss: -93.13238525390625,  g_loss: 391.96246337890625\n",
            "Training epoch 3315/1000000, d_loss: -128.05221557617188,  g_loss: 443.4622802734375\n",
            "Training epoch 3316/1000000, d_loss: -167.08970642089844,  g_loss: 248.68345642089844\n",
            "Training epoch 3317/1000000, d_loss: -214.5043487548828,  g_loss: 121.0876235961914\n",
            "Training epoch 3318/1000000, d_loss: 17.20763397216797,  g_loss: 125.5807876586914\n",
            "Training epoch 3319/1000000, d_loss: -103.4819107055664,  g_loss: 232.39471435546875\n",
            "Training epoch 3320/1000000, d_loss: -66.33692169189453,  g_loss: 166.98257446289062\n",
            "Training epoch 3321/1000000, d_loss: -100.36769104003906,  g_loss: 37.94734191894531\n",
            "Training epoch 3322/1000000, d_loss: 71.77310180664062,  g_loss: 352.0374755859375\n",
            "Training epoch 3323/1000000, d_loss: -133.39700317382812,  g_loss: 205.3712158203125\n",
            "Training epoch 3324/1000000, d_loss: -124.49565124511719,  g_loss: 38.3923454284668\n",
            "Training epoch 3325/1000000, d_loss: -169.5561065673828,  g_loss: 114.63526916503906\n",
            "Training epoch 3326/1000000, d_loss: -57.195823669433594,  g_loss: 140.6499786376953\n",
            "Training epoch 3327/1000000, d_loss: -54.1876220703125,  g_loss: 414.6595153808594\n",
            "Training epoch 3328/1000000, d_loss: -40.23920440673828,  g_loss: 198.19699096679688\n",
            "Training epoch 3329/1000000, d_loss: -41.77272033691406,  g_loss: 159.69692993164062\n",
            "Training epoch 3330/1000000, d_loss: -213.34854125976562,  g_loss: 193.87619018554688\n",
            "Training epoch 3331/1000000, d_loss: -201.9196319580078,  g_loss: 194.41464233398438\n",
            "Training epoch 3332/1000000, d_loss: -33.907814025878906,  g_loss: 164.0311279296875\n",
            "Training epoch 3333/1000000, d_loss: -5.910402297973633,  g_loss: 169.8402099609375\n",
            "Training epoch 3334/1000000, d_loss: -148.06539916992188,  g_loss: 250.3965606689453\n",
            "Training epoch 3335/1000000, d_loss: -47.48480987548828,  g_loss: 150.75576782226562\n",
            "Training epoch 3336/1000000, d_loss: -88.37359619140625,  g_loss: 162.90289306640625\n",
            "Training epoch 3337/1000000, d_loss: -137.41940307617188,  g_loss: 136.61666870117188\n",
            "Training epoch 3338/1000000, d_loss: 4.87245512008667,  g_loss: 127.47282409667969\n",
            "Training epoch 3339/1000000, d_loss: 43.630027770996094,  g_loss: 161.47509765625\n",
            "Training epoch 3340/1000000, d_loss: -71.50289916992188,  g_loss: 149.80419921875\n",
            "Training epoch 3341/1000000, d_loss: -129.32269287109375,  g_loss: -44.44020080566406\n",
            "Training epoch 3342/1000000, d_loss: -110.08710479736328,  g_loss: 126.98951721191406\n",
            "Training epoch 3343/1000000, d_loss: -89.14578247070312,  g_loss: 258.015869140625\n",
            "Training epoch 3344/1000000, d_loss: -67.3552474975586,  g_loss: 129.9078369140625\n",
            "Training epoch 3345/1000000, d_loss: -399.62738037109375,  g_loss: -141.37005615234375\n",
            "Training epoch 3346/1000000, d_loss: 27.833106994628906,  g_loss: -143.14161682128906\n",
            "Training epoch 3347/1000000, d_loss: 5.878959655761719,  g_loss: -132.06906127929688\n",
            "Training epoch 3348/1000000, d_loss: -175.39857482910156,  g_loss: -76.29373168945312\n",
            "Training epoch 3349/1000000, d_loss: -28.41006088256836,  g_loss: -0.8682937622070312\n",
            "Training epoch 3350/1000000, d_loss: -36.395057678222656,  g_loss: -50.96512985229492\n",
            "Training epoch 3351/1000000, d_loss: -66.71289825439453,  g_loss: -95.16996765136719\n",
            "Training epoch 3352/1000000, d_loss: -141.7650146484375,  g_loss: -88.14631652832031\n",
            "Training epoch 3353/1000000, d_loss: -136.74049377441406,  g_loss: -27.66072654724121\n",
            "Training epoch 3354/1000000, d_loss: -132.51405334472656,  g_loss: -85.39086151123047\n",
            "Training epoch 3355/1000000, d_loss: -146.63461303710938,  g_loss: -67.57467651367188\n",
            "Training epoch 3356/1000000, d_loss: -271.4178466796875,  g_loss: -151.66873168945312\n",
            "Training epoch 3357/1000000, d_loss: -363.7364196777344,  g_loss: -148.09658813476562\n",
            "Training epoch 3358/1000000, d_loss: -250.0522918701172,  g_loss: -263.996337890625\n",
            "Training epoch 3359/1000000, d_loss: 49.890174865722656,  g_loss: -241.7117156982422\n",
            "Training epoch 3360/1000000, d_loss: -339.30999755859375,  g_loss: -334.32049560546875\n",
            "Training epoch 3361/1000000, d_loss: 6.821807861328125,  g_loss: -474.798095703125\n",
            "Training epoch 3362/1000000, d_loss: 59.28644943237305,  g_loss: 121.97068786621094\n",
            "Training epoch 3363/1000000, d_loss: -106.58808898925781,  g_loss: 227.72850036621094\n",
            "Training epoch 3364/1000000, d_loss: -76.50785827636719,  g_loss: 229.49136352539062\n",
            "Training epoch 3365/1000000, d_loss: -302.7142028808594,  g_loss: -53.42558670043945\n",
            "Training epoch 3366/1000000, d_loss: -101.48609161376953,  g_loss: 31.931934356689453\n",
            "Training epoch 3367/1000000, d_loss: -33.50495147705078,  g_loss: 25.499422073364258\n",
            "Training epoch 3368/1000000, d_loss: -166.21229553222656,  g_loss: 33.45557403564453\n",
            "Training epoch 3369/1000000, d_loss: -143.8255615234375,  g_loss: 128.60800170898438\n",
            "Training epoch 3370/1000000, d_loss: -47.03165817260742,  g_loss: 114.0223159790039\n",
            "Training epoch 3371/1000000, d_loss: -87.275634765625,  g_loss: 51.73869323730469\n",
            "Training epoch 3372/1000000, d_loss: -20.28387451171875,  g_loss: 86.6346435546875\n",
            "Training epoch 3373/1000000, d_loss: -121.09803771972656,  g_loss: 24.050033569335938\n",
            "Training epoch 3374/1000000, d_loss: -30.741348266601562,  g_loss: 142.85931396484375\n",
            "Training epoch 3375/1000000, d_loss: -311.97796630859375,  g_loss: 62.38800811767578\n",
            "Training epoch 3376/1000000, d_loss: -135.528564453125,  g_loss: 356.4590759277344\n",
            "Training epoch 3377/1000000, d_loss: -19.11121368408203,  g_loss: 158.11050415039062\n",
            "Training epoch 3378/1000000, d_loss: -147.44869995117188,  g_loss: 66.23481750488281\n",
            "Training epoch 3379/1000000, d_loss: -434.47784423828125,  g_loss: -40.00248336791992\n",
            "Training epoch 3380/1000000, d_loss: -104.96997833251953,  g_loss: 21.451499938964844\n",
            "Training epoch 3381/1000000, d_loss: 11.896476745605469,  g_loss: -69.66091918945312\n",
            "Training epoch 3382/1000000, d_loss: -33.547855377197266,  g_loss: -58.933128356933594\n",
            "Training epoch 3383/1000000, d_loss: 11.928695678710938,  g_loss: -103.93425750732422\n",
            "Training epoch 3384/1000000, d_loss: -69.3472671508789,  g_loss: -23.50665855407715\n",
            "Training epoch 3385/1000000, d_loss: -127.05309295654297,  g_loss: -68.86064910888672\n",
            "Training epoch 3386/1000000, d_loss: -18.405364990234375,  g_loss: 7.527803421020508\n",
            "Training epoch 3387/1000000, d_loss: -110.26154327392578,  g_loss: 15.115650177001953\n",
            "Training epoch 3388/1000000, d_loss: -229.68434143066406,  g_loss: -62.1917610168457\n",
            "Training epoch 3389/1000000, d_loss: -234.06092834472656,  g_loss: -48.90098571777344\n",
            "Training epoch 3390/1000000, d_loss: -87.74923706054688,  g_loss: 0.36284637451171875\n",
            "Training epoch 3391/1000000, d_loss: -1012.1405029296875,  g_loss: -391.71612548828125\n",
            "Training epoch 3392/1000000, d_loss: 146.17723083496094,  g_loss: -104.59274291992188\n",
            "Training epoch 3393/1000000, d_loss: -29.471237182617188,  g_loss: -116.79264831542969\n",
            "Training epoch 3394/1000000, d_loss: -414.001220703125,  g_loss: -210.44497680664062\n",
            "Training epoch 3395/1000000, d_loss: 124.28851318359375,  g_loss: -110.37549591064453\n",
            "Training epoch 3396/1000000, d_loss: -198.85147094726562,  g_loss: -42.08653259277344\n",
            "Training epoch 3397/1000000, d_loss: -88.06599426269531,  g_loss: 35.534366607666016\n",
            "Training epoch 3398/1000000, d_loss: -262.0199279785156,  g_loss: -151.0726776123047\n",
            "Training epoch 3399/1000000, d_loss: -65.17268371582031,  g_loss: -87.26588439941406\n",
            "Training epoch 3400/1000000, d_loss: -53.71861267089844,  g_loss: 70.85261535644531\n",
            "Training epoch 3401/1000000, d_loss: -174.26246643066406,  g_loss: 125.36570739746094\n",
            "Training epoch 3402/1000000, d_loss: -60.229087829589844,  g_loss: 50.584468841552734\n",
            "Training epoch 3403/1000000, d_loss: -84.09798431396484,  g_loss: -45.747901916503906\n",
            "Training epoch 3404/1000000, d_loss: 418.0517883300781,  g_loss: 201.04080200195312\n",
            "Training epoch 3405/1000000, d_loss: -51.549285888671875,  g_loss: 209.62515258789062\n",
            "Training epoch 3406/1000000, d_loss: -148.9674835205078,  g_loss: 129.48687744140625\n",
            "Training epoch 3407/1000000, d_loss: -60.42134094238281,  g_loss: 99.866455078125\n",
            "Training epoch 3408/1000000, d_loss: -769.5448608398438,  g_loss: -165.43316650390625\n",
            "Training epoch 3409/1000000, d_loss: 6.912043571472168,  g_loss: -84.35211181640625\n",
            "Training epoch 3410/1000000, d_loss: -188.80770874023438,  g_loss: 161.97244262695312\n",
            "Training epoch 3411/1000000, d_loss: -73.03277587890625,  g_loss: 58.8031120300293\n",
            "Training epoch 3412/1000000, d_loss: -37.26329040527344,  g_loss: 114.194091796875\n",
            "Training epoch 3413/1000000, d_loss: -94.22985076904297,  g_loss: 29.0451717376709\n",
            "Training epoch 3414/1000000, d_loss: -181.64976501464844,  g_loss: -94.84698486328125\n",
            "Training epoch 3415/1000000, d_loss: -79.80633544921875,  g_loss: 38.41575622558594\n",
            "Training epoch 3416/1000000, d_loss: -43.16718673706055,  g_loss: 177.2874755859375\n",
            "Training epoch 3417/1000000, d_loss: -107.55520629882812,  g_loss: 81.42272186279297\n",
            "Training epoch 3418/1000000, d_loss: -139.3805389404297,  g_loss: 58.71836471557617\n",
            "Training epoch 3419/1000000, d_loss: -100.89420318603516,  g_loss: 20.32293128967285\n",
            "Training epoch 3420/1000000, d_loss: -245.23875427246094,  g_loss: -161.94970703125\n",
            "Training epoch 3421/1000000, d_loss: -316.79022216796875,  g_loss: -399.3327331542969\n",
            "Training epoch 3422/1000000, d_loss: -103.5793685913086,  g_loss: 201.8137969970703\n",
            "Training epoch 3423/1000000, d_loss: -183.1531524658203,  g_loss: 183.2760009765625\n",
            "Training epoch 3424/1000000, d_loss: -191.84864807128906,  g_loss: 328.40765380859375\n",
            "Training epoch 3425/1000000, d_loss: -274.3432312011719,  g_loss: 18.685935974121094\n",
            "Training epoch 3426/1000000, d_loss: -28.560903549194336,  g_loss: 82.36034393310547\n",
            "Training epoch 3427/1000000, d_loss: -654.832275390625,  g_loss: -136.4072265625\n",
            "Training epoch 3428/1000000, d_loss: 69.43801879882812,  g_loss: 107.0214614868164\n",
            "Training epoch 3429/1000000, d_loss: -512.0087280273438,  g_loss: -108.54353332519531\n",
            "Training epoch 3430/1000000, d_loss: 21.8443603515625,  g_loss: 237.1083526611328\n",
            "Training epoch 3431/1000000, d_loss: 17.080039978027344,  g_loss: 253.0350341796875\n",
            "Training epoch 3432/1000000, d_loss: -56.31557083129883,  g_loss: 167.79937744140625\n",
            "Training epoch 3433/1000000, d_loss: -197.11656188964844,  g_loss: 163.27926635742188\n",
            "Training epoch 3434/1000000, d_loss: -112.041259765625,  g_loss: 156.3761749267578\n",
            "Training epoch 3435/1000000, d_loss: -119.23048400878906,  g_loss: 118.18196868896484\n",
            "Training epoch 3436/1000000, d_loss: -272.3631896972656,  g_loss: 184.63815307617188\n",
            "Training epoch 3437/1000000, d_loss: 38.05534362792969,  g_loss: 215.66404724121094\n",
            "Training epoch 3438/1000000, d_loss: -140.79226684570312,  g_loss: 142.97576904296875\n",
            "Training epoch 3439/1000000, d_loss: -265.9142150878906,  g_loss: 505.09271240234375\n",
            "Training epoch 3440/1000000, d_loss: -176.00387573242188,  g_loss: 67.74629211425781\n",
            "Training epoch 3441/1000000, d_loss: -109.19290161132812,  g_loss: 265.6681823730469\n",
            "Training epoch 3442/1000000, d_loss: -77.85763549804688,  g_loss: 188.44996643066406\n",
            "Training epoch 3443/1000000, d_loss: -106.78333282470703,  g_loss: 186.68292236328125\n",
            "Training epoch 3444/1000000, d_loss: -171.72735595703125,  g_loss: 220.62149047851562\n",
            "Training epoch 3445/1000000, d_loss: -0.9559173583984375,  g_loss: 268.1527099609375\n",
            "Training epoch 3446/1000000, d_loss: -54.193267822265625,  g_loss: 237.6211395263672\n",
            "Training epoch 3447/1000000, d_loss: -149.320556640625,  g_loss: 231.6299591064453\n",
            "Training epoch 3448/1000000, d_loss: -68.91194152832031,  g_loss: 197.63388061523438\n",
            "Training epoch 3449/1000000, d_loss: -308.2688293457031,  g_loss: 84.71359252929688\n",
            "Training epoch 3450/1000000, d_loss: -700.3211669921875,  g_loss: -133.25411987304688\n",
            "Training epoch 3451/1000000, d_loss: -19.4156551361084,  g_loss: 134.24974060058594\n",
            "Training epoch 3452/1000000, d_loss: -52.36016845703125,  g_loss: 232.2918701171875\n",
            "Training epoch 3453/1000000, d_loss: 23.43842315673828,  g_loss: 227.65567016601562\n",
            "Training epoch 3454/1000000, d_loss: -117.63458251953125,  g_loss: 195.5533905029297\n",
            "Training epoch 3455/1000000, d_loss: -88.68096923828125,  g_loss: 182.979736328125\n",
            "Training epoch 3456/1000000, d_loss: -218.00811767578125,  g_loss: 632.962890625\n",
            "Training epoch 3457/1000000, d_loss: -158.8134307861328,  g_loss: 358.13238525390625\n",
            "Training epoch 3458/1000000, d_loss: -391.814697265625,  g_loss: -42.58655548095703\n",
            "Training epoch 3459/1000000, d_loss: 153.09063720703125,  g_loss: 77.3790512084961\n",
            "Training epoch 3460/1000000, d_loss: -43.748146057128906,  g_loss: 142.51998901367188\n",
            "Training epoch 3461/1000000, d_loss: -96.14238739013672,  g_loss: 95.0492935180664\n",
            "Training epoch 3462/1000000, d_loss: -449.1873474121094,  g_loss: -50.8945426940918\n",
            "Training epoch 3463/1000000, d_loss: -146.04849243164062,  g_loss: 32.55774688720703\n",
            "Training epoch 3464/1000000, d_loss: -44.169769287109375,  g_loss: 85.63661193847656\n",
            "Training epoch 3465/1000000, d_loss: -87.21100616455078,  g_loss: 163.62261962890625\n",
            "Training epoch 3466/1000000, d_loss: -78.51509857177734,  g_loss: 95.4616470336914\n",
            "Training epoch 3467/1000000, d_loss: -48.91057205200195,  g_loss: 62.84217834472656\n",
            "Training epoch 3468/1000000, d_loss: -72.8014144897461,  g_loss: 213.23312377929688\n",
            "Training epoch 3469/1000000, d_loss: -196.096435546875,  g_loss: 157.28173828125\n",
            "Training epoch 3470/1000000, d_loss: -285.51165771484375,  g_loss: 63.34729766845703\n",
            "Training epoch 3471/1000000, d_loss: -531.895263671875,  g_loss: -47.18822479248047\n",
            "Training epoch 3472/1000000, d_loss: -473.81439208984375,  g_loss: -179.56024169921875\n",
            "Training epoch 3473/1000000, d_loss: 178.46925354003906,  g_loss: 206.62367248535156\n",
            "Training epoch 3474/1000000, d_loss: -277.48602294921875,  g_loss: 108.4243392944336\n",
            "Training epoch 3475/1000000, d_loss: -268.0528869628906,  g_loss: 146.01976013183594\n",
            "Training epoch 3476/1000000, d_loss: 84.1569595336914,  g_loss: 90.57514953613281\n",
            "Training epoch 3477/1000000, d_loss: -78.45390319824219,  g_loss: 63.43571472167969\n",
            "Training epoch 3478/1000000, d_loss: -7.9970703125,  g_loss: 207.19540405273438\n",
            "Training epoch 3479/1000000, d_loss: 16.939239501953125,  g_loss: 148.0280303955078\n",
            "Training epoch 3480/1000000, d_loss: -23.910558700561523,  g_loss: 88.60767364501953\n",
            "Training epoch 3481/1000000, d_loss: -107.4317626953125,  g_loss: 242.33099365234375\n",
            "Training epoch 3482/1000000, d_loss: -72.63394927978516,  g_loss: 166.85714721679688\n",
            "Training epoch 3483/1000000, d_loss: -125.43211364746094,  g_loss: 159.81964111328125\n",
            "Training epoch 3484/1000000, d_loss: -66.42003631591797,  g_loss: 66.98710632324219\n",
            "Training epoch 3485/1000000, d_loss: 47.37413787841797,  g_loss: 155.63339233398438\n",
            "Training epoch 3486/1000000, d_loss: -144.7330322265625,  g_loss: 185.97677612304688\n",
            "Training epoch 3487/1000000, d_loss: -29.68683624267578,  g_loss: 94.83622741699219\n",
            "Training epoch 3488/1000000, d_loss: -56.49634552001953,  g_loss: 224.43499755859375\n",
            "Training epoch 3489/1000000, d_loss: -102.9061279296875,  g_loss: 111.74269104003906\n",
            "Training epoch 3490/1000000, d_loss: -38.078453063964844,  g_loss: 131.92428588867188\n",
            "Training epoch 3491/1000000, d_loss: -310.1614685058594,  g_loss: 80.02534484863281\n",
            "Training epoch 3492/1000000, d_loss: -80.36970520019531,  g_loss: 194.4691925048828\n",
            "Training epoch 3493/1000000, d_loss: -76.93013000488281,  g_loss: 25.633586883544922\n",
            "Training epoch 3494/1000000, d_loss: -44.098121643066406,  g_loss: 18.52143096923828\n",
            "Training epoch 3495/1000000, d_loss: -83.50453186035156,  g_loss: 101.14753723144531\n",
            "Training epoch 3496/1000000, d_loss: -198.281005859375,  g_loss: 23.305511474609375\n",
            "Training epoch 3497/1000000, d_loss: -78.6506576538086,  g_loss: 223.69960021972656\n",
            "Training epoch 3498/1000000, d_loss: -16.963272094726562,  g_loss: 143.31069946289062\n",
            "Training epoch 3499/1000000, d_loss: -38.746517181396484,  g_loss: 163.460205078125\n",
            "Training epoch 3500/1000000, d_loss: -78.39788818359375,  g_loss: 151.67623901367188\n",
            "Training epoch 3501/1000000, d_loss: -162.75704956054688,  g_loss: 51.04826354980469\n",
            "Training epoch 3502/1000000, d_loss: -63.12266540527344,  g_loss: 209.25936889648438\n",
            "Training epoch 3503/1000000, d_loss: -19.27785873413086,  g_loss: 226.7152099609375\n",
            "Training epoch 3504/1000000, d_loss: -35.207183837890625,  g_loss: 171.179931640625\n",
            "Training epoch 3505/1000000, d_loss: 6.523451805114746,  g_loss: 163.2177734375\n",
            "Training epoch 3506/1000000, d_loss: -334.742431640625,  g_loss: 70.40617370605469\n",
            "Training epoch 3507/1000000, d_loss: -93.40459442138672,  g_loss: 147.82830810546875\n",
            "Training epoch 3508/1000000, d_loss: -165.85546875,  g_loss: 72.47090911865234\n",
            "Training epoch 3509/1000000, d_loss: -56.83753204345703,  g_loss: 87.73551940917969\n",
            "Training epoch 3510/1000000, d_loss: -47.47148132324219,  g_loss: 86.769775390625\n",
            "Training epoch 3511/1000000, d_loss: -11.826244354248047,  g_loss: 127.13880157470703\n",
            "Training epoch 3512/1000000, d_loss: -109.15750122070312,  g_loss: 167.70315551757812\n",
            "Training epoch 3513/1000000, d_loss: 206.1317901611328,  g_loss: 156.93194580078125\n",
            "Training epoch 3514/1000000, d_loss: -86.8966064453125,  g_loss: 132.9390411376953\n",
            "Training epoch 3515/1000000, d_loss: -132.7132568359375,  g_loss: 37.93257522583008\n",
            "Training epoch 3516/1000000, d_loss: -66.7886962890625,  g_loss: 137.5335693359375\n",
            "Training epoch 3517/1000000, d_loss: -204.5020294189453,  g_loss: -45.5090446472168\n",
            "Training epoch 3518/1000000, d_loss: 38.283729553222656,  g_loss: 48.33160400390625\n",
            "Training epoch 3519/1000000, d_loss: 19.4554443359375,  g_loss: 82.13665008544922\n",
            "Training epoch 3520/1000000, d_loss: -94.6044921875,  g_loss: 42.038265228271484\n",
            "Training epoch 3521/1000000, d_loss: -146.53201293945312,  g_loss: 67.310546875\n",
            "Training epoch 3522/1000000, d_loss: -53.0068473815918,  g_loss: 124.13099670410156\n",
            "Training epoch 3523/1000000, d_loss: -100.46919250488281,  g_loss: 68.51180267333984\n",
            "Training epoch 3524/1000000, d_loss: -82.03138732910156,  g_loss: 175.41854858398438\n",
            "Training epoch 3525/1000000, d_loss: -110.58557891845703,  g_loss: 212.4422607421875\n",
            "Training epoch 3526/1000000, d_loss: -78.80748748779297,  g_loss: 81.37478637695312\n",
            "Training epoch 3527/1000000, d_loss: -44.6192626953125,  g_loss: 67.92247772216797\n",
            "Training epoch 3528/1000000, d_loss: -57.668033599853516,  g_loss: 41.83139419555664\n",
            "Training epoch 3529/1000000, d_loss: -151.69357299804688,  g_loss: -64.11317443847656\n",
            "Training epoch 3530/1000000, d_loss: -93.70999145507812,  g_loss: -33.83087158203125\n",
            "Training epoch 3531/1000000, d_loss: 19.524436950683594,  g_loss: 98.60041809082031\n",
            "Training epoch 3532/1000000, d_loss: 1841.662109375,  g_loss: 126.26811218261719\n",
            "Training epoch 3533/1000000, d_loss: -55.348106384277344,  g_loss: 108.31660461425781\n",
            "Training epoch 3534/1000000, d_loss: -160.165283203125,  g_loss: 116.37940216064453\n",
            "Training epoch 3535/1000000, d_loss: -67.51985168457031,  g_loss: 186.49395751953125\n",
            "Training epoch 3536/1000000, d_loss: -31.847200393676758,  g_loss: 148.97613525390625\n",
            "Training epoch 3537/1000000, d_loss: -147.72178649902344,  g_loss: 140.5488739013672\n",
            "Training epoch 3538/1000000, d_loss: -89.79100036621094,  g_loss: 142.31121826171875\n",
            "Training epoch 3539/1000000, d_loss: -120.59994506835938,  g_loss: 94.68508911132812\n",
            "Training epoch 3540/1000000, d_loss: -11.442058563232422,  g_loss: 54.15367126464844\n",
            "Training epoch 3541/1000000, d_loss: -108.8919906616211,  g_loss: 86.4043960571289\n",
            "Training epoch 3542/1000000, d_loss: -33.702659606933594,  g_loss: 82.33525085449219\n",
            "Training epoch 3543/1000000, d_loss: -111.13168334960938,  g_loss: -21.72616195678711\n",
            "Training epoch 3544/1000000, d_loss: -167.37025451660156,  g_loss: 189.66336059570312\n",
            "Training epoch 3545/1000000, d_loss: -158.42987060546875,  g_loss: 174.1282958984375\n",
            "Training epoch 3546/1000000, d_loss: 14.169317245483398,  g_loss: 213.23974609375\n",
            "Training epoch 3547/1000000, d_loss: -68.73077392578125,  g_loss: 296.0742492675781\n",
            "Training epoch 3548/1000000, d_loss: -187.99813842773438,  g_loss: 124.95280456542969\n",
            "Training epoch 3549/1000000, d_loss: -51.819190979003906,  g_loss: 64.78101348876953\n",
            "Training epoch 3550/1000000, d_loss: -96.55357360839844,  g_loss: 131.71090698242188\n",
            "Training epoch 3551/1000000, d_loss: -234.29071044921875,  g_loss: 65.38951110839844\n",
            "Training epoch 3552/1000000, d_loss: -257.6114196777344,  g_loss: 14.28497314453125\n",
            "Training epoch 3553/1000000, d_loss: -212.49705505371094,  g_loss: 60.51620101928711\n",
            "Training epoch 3554/1000000, d_loss: 23.3031005859375,  g_loss: 218.84820556640625\n",
            "Training epoch 3555/1000000, d_loss: -108.46015930175781,  g_loss: 388.62530517578125\n",
            "Training epoch 3556/1000000, d_loss: -219.57327270507812,  g_loss: 624.4552612304688\n",
            "Training epoch 3557/1000000, d_loss: -105.49354553222656,  g_loss: 421.31982421875\n",
            "Training epoch 3558/1000000, d_loss: -74.92738342285156,  g_loss: 334.66424560546875\n",
            "Training epoch 3559/1000000, d_loss: -319.5262451171875,  g_loss: 123.17207336425781\n",
            "Training epoch 3560/1000000, d_loss: -186.9631805419922,  g_loss: 181.7740936279297\n",
            "Training epoch 3561/1000000, d_loss: -186.91384887695312,  g_loss: 74.33123016357422\n",
            "Training epoch 3562/1000000, d_loss: 202.1508026123047,  g_loss: 249.95425415039062\n",
            "Training epoch 3563/1000000, d_loss: -101.02796936035156,  g_loss: 196.83035278320312\n",
            "Training epoch 3564/1000000, d_loss: -106.1917724609375,  g_loss: 214.12155151367188\n",
            "Training epoch 3565/1000000, d_loss: -99.13430786132812,  g_loss: 123.94931030273438\n",
            "Training epoch 3566/1000000, d_loss: -104.5858154296875,  g_loss: 243.33767700195312\n",
            "Training epoch 3567/1000000, d_loss: -279.0195617675781,  g_loss: 125.51106262207031\n",
            "Training epoch 3568/1000000, d_loss: 48.321380615234375,  g_loss: 94.84583282470703\n",
            "Training epoch 3569/1000000, d_loss: -45.53095245361328,  g_loss: 112.90833282470703\n",
            "Training epoch 3570/1000000, d_loss: -32.63274383544922,  g_loss: 146.52264404296875\n",
            "Training epoch 3571/1000000, d_loss: -137.5140838623047,  g_loss: 68.88801574707031\n",
            "Training epoch 3572/1000000, d_loss: -87.12123107910156,  g_loss: 63.41693878173828\n",
            "Training epoch 3573/1000000, d_loss: -226.01797485351562,  g_loss: 58.61499786376953\n",
            "Training epoch 3574/1000000, d_loss: -190.0328826904297,  g_loss: -45.80705642700195\n",
            "Training epoch 3575/1000000, d_loss: -107.49279022216797,  g_loss: 330.53912353515625\n",
            "Training epoch 3576/1000000, d_loss: -167.6731719970703,  g_loss: 271.11236572265625\n",
            "Training epoch 3577/1000000, d_loss: -50.959625244140625,  g_loss: 349.2850341796875\n",
            "Training epoch 3578/1000000, d_loss: -37.951759338378906,  g_loss: 142.78610229492188\n",
            "Training epoch 3579/1000000, d_loss: -65.50843811035156,  g_loss: -5.662162780761719\n",
            "Training epoch 3580/1000000, d_loss: -117.00359344482422,  g_loss: 119.74734497070312\n",
            "Training epoch 3581/1000000, d_loss: -102.73969268798828,  g_loss: 28.037334442138672\n",
            "Training epoch 3582/1000000, d_loss: -76.9585952758789,  g_loss: 0.36685943603515625\n",
            "Training epoch 3583/1000000, d_loss: -104.24987030029297,  g_loss: 78.11093139648438\n",
            "Training epoch 3584/1000000, d_loss: -72.43075561523438,  g_loss: 21.303558349609375\n",
            "Training epoch 3585/1000000, d_loss: 11.439342498779297,  g_loss: -25.905670166015625\n",
            "Training epoch 3586/1000000, d_loss: -127.01698303222656,  g_loss: -42.75752258300781\n",
            "Training epoch 3587/1000000, d_loss: -47.780792236328125,  g_loss: 17.750885009765625\n",
            "Training epoch 3588/1000000, d_loss: -13.84386157989502,  g_loss: -34.47761154174805\n",
            "Training epoch 3589/1000000, d_loss: -37.81201934814453,  g_loss: -33.238746643066406\n",
            "Training epoch 3590/1000000, d_loss: -128.37823486328125,  g_loss: -0.5738315582275391\n",
            "Training epoch 3591/1000000, d_loss: -569.125244140625,  g_loss: -216.2209014892578\n",
            "Training epoch 3592/1000000, d_loss: -67.01509094238281,  g_loss: -176.00730895996094\n",
            "Training epoch 3593/1000000, d_loss: -73.54529571533203,  g_loss: -140.43382263183594\n",
            "Training epoch 3594/1000000, d_loss: -50.53392028808594,  g_loss: -168.40806579589844\n",
            "Training epoch 3595/1000000, d_loss: 67.96014404296875,  g_loss: 4.24487829208374\n",
            "Training epoch 3596/1000000, d_loss: -199.898193359375,  g_loss: -78.0910873413086\n",
            "Training epoch 3597/1000000, d_loss: -103.29908752441406,  g_loss: 33.28076171875\n",
            "Training epoch 3598/1000000, d_loss: -255.26466369628906,  g_loss: 112.8020248413086\n",
            "Training epoch 3599/1000000, d_loss: -125.06524658203125,  g_loss: -101.42819213867188\n",
            "Training epoch 3600/1000000, d_loss: -12.081390380859375,  g_loss: 22.52498435974121\n",
            "Training epoch 3601/1000000, d_loss: -71.18899536132812,  g_loss: -49.64471435546875\n",
            "Training epoch 3602/1000000, d_loss: -127.88824462890625,  g_loss: -37.83843231201172\n",
            "Training epoch 3603/1000000, d_loss: -60.14356231689453,  g_loss: -94.64443969726562\n",
            "Training epoch 3604/1000000, d_loss: -800.614990234375,  g_loss: -481.84808349609375\n",
            "Training epoch 3605/1000000, d_loss: 185.86032104492188,  g_loss: -70.19784545898438\n",
            "Training epoch 3606/1000000, d_loss: 72.45964813232422,  g_loss: -56.45796585083008\n",
            "Training epoch 3607/1000000, d_loss: -49.95237731933594,  g_loss: 4.4821648597717285\n",
            "Training epoch 3608/1000000, d_loss: -14.789180755615234,  g_loss: -42.500999450683594\n",
            "Training epoch 3609/1000000, d_loss: -113.10364532470703,  g_loss: -19.876995086669922\n",
            "Training epoch 3610/1000000, d_loss: -81.76359558105469,  g_loss: 26.5693416595459\n",
            "Training epoch 3611/1000000, d_loss: -130.91339111328125,  g_loss: -22.766254425048828\n",
            "Training epoch 3612/1000000, d_loss: -85.20532989501953,  g_loss: 25.385784149169922\n",
            "Training epoch 3613/1000000, d_loss: -91.95484161376953,  g_loss: -5.650032043457031\n",
            "Training epoch 3614/1000000, d_loss: -284.132080078125,  g_loss: 5.91883659362793\n",
            "Training epoch 3615/1000000, d_loss: -92.30377960205078,  g_loss: 45.16046905517578\n",
            "Training epoch 3616/1000000, d_loss: -125.80024719238281,  g_loss: 78.7724609375\n",
            "Training epoch 3617/1000000, d_loss: -195.3780517578125,  g_loss: 357.89324951171875\n",
            "Training epoch 3618/1000000, d_loss: -38.458885192871094,  g_loss: 193.17202758789062\n",
            "Training epoch 3619/1000000, d_loss: -242.90676879882812,  g_loss: 512.8934936523438\n",
            "Training epoch 3620/1000000, d_loss: -32.83808135986328,  g_loss: 171.119140625\n",
            "Training epoch 3621/1000000, d_loss: -38.437828063964844,  g_loss: 197.75418090820312\n",
            "Training epoch 3622/1000000, d_loss: 18.127723693847656,  g_loss: 90.35150146484375\n",
            "Training epoch 3623/1000000, d_loss: -27.628829956054688,  g_loss: 16.908769607543945\n",
            "Training epoch 3624/1000000, d_loss: -272.7850341796875,  g_loss: -45.61918640136719\n",
            "Training epoch 3625/1000000, d_loss: -27.532840728759766,  g_loss: 92.21549987792969\n",
            "Training epoch 3626/1000000, d_loss: -41.42560577392578,  g_loss: 181.2703399658203\n",
            "Training epoch 3627/1000000, d_loss: -33.12518310546875,  g_loss: 154.5623321533203\n",
            "Training epoch 3628/1000000, d_loss: -29.90806007385254,  g_loss: 91.88453674316406\n",
            "Training epoch 3629/1000000, d_loss: -33.501869201660156,  g_loss: 101.4581527709961\n",
            "Training epoch 3630/1000000, d_loss: -10.332103729248047,  g_loss: 74.02388000488281\n",
            "Training epoch 3631/1000000, d_loss: -127.31193542480469,  g_loss: 78.13424682617188\n",
            "Training epoch 3632/1000000, d_loss: -11.591777801513672,  g_loss: 80.43885803222656\n",
            "Training epoch 3633/1000000, d_loss: -48.705482482910156,  g_loss: 115.98593139648438\n",
            "Training epoch 3634/1000000, d_loss: -81.90673828125,  g_loss: 74.03837585449219\n",
            "Training epoch 3635/1000000, d_loss: -133.4390106201172,  g_loss: 35.095703125\n",
            "Training epoch 3636/1000000, d_loss: -82.30604553222656,  g_loss: 46.37266540527344\n",
            "Training epoch 3637/1000000, d_loss: -57.80401611328125,  g_loss: 49.31029510498047\n",
            "Training epoch 3638/1000000, d_loss: -119.80233764648438,  g_loss: 2.147718906402588\n",
            "Training epoch 3639/1000000, d_loss: -138.74127197265625,  g_loss: 31.144737243652344\n",
            "Training epoch 3640/1000000, d_loss: -66.25679779052734,  g_loss: 130.39889526367188\n",
            "Training epoch 3641/1000000, d_loss: -38.282440185546875,  g_loss: 116.7272720336914\n",
            "Training epoch 3642/1000000, d_loss: -736.5181884765625,  g_loss: -123.59765625\n",
            "Training epoch 3643/1000000, d_loss: -137.84962463378906,  g_loss: -58.19707489013672\n",
            "Training epoch 3644/1000000, d_loss: -53.15438461303711,  g_loss: -5.850986480712891\n",
            "Training epoch 3645/1000000, d_loss: -83.65888214111328,  g_loss: 21.20958709716797\n",
            "Training epoch 3646/1000000, d_loss: -117.62989044189453,  g_loss: -18.099079132080078\n",
            "Training epoch 3647/1000000, d_loss: -791.3807373046875,  g_loss: -211.59515380859375\n",
            "Training epoch 3648/1000000, d_loss: -274.14593505859375,  g_loss: -179.18356323242188\n",
            "Training epoch 3649/1000000, d_loss: -109.54456329345703,  g_loss: -10.584412574768066\n",
            "Training epoch 3650/1000000, d_loss: -169.1827392578125,  g_loss: -73.47266387939453\n",
            "Training epoch 3651/1000000, d_loss: -382.70648193359375,  g_loss: -92.99015045166016\n",
            "Training epoch 3652/1000000, d_loss: -27.32498550415039,  g_loss: 3.4749698638916016\n",
            "Training epoch 3653/1000000, d_loss: -78.61974334716797,  g_loss: 55.384742736816406\n",
            "Training epoch 3654/1000000, d_loss: -406.31182861328125,  g_loss: 421.4354553222656\n",
            "Training epoch 3655/1000000, d_loss: -36.75994110107422,  g_loss: -164.66567993164062\n",
            "Training epoch 3656/1000000, d_loss: -38.094505310058594,  g_loss: -89.078857421875\n",
            "Training epoch 3657/1000000, d_loss: 2.1777801513671875,  g_loss: -132.38046264648438\n",
            "Training epoch 3658/1000000, d_loss: -49.67917251586914,  g_loss: -127.24256134033203\n",
            "Training epoch 3659/1000000, d_loss: -143.23931884765625,  g_loss: -194.1650390625\n",
            "Training epoch 3660/1000000, d_loss: -187.45872497558594,  g_loss: -239.0765838623047\n",
            "Training epoch 3661/1000000, d_loss: -89.41565704345703,  g_loss: -236.6720733642578\n",
            "Training epoch 3662/1000000, d_loss: -25.49269676208496,  g_loss: -22.328350067138672\n",
            "Training epoch 3663/1000000, d_loss: -122.85868835449219,  g_loss: 102.92146301269531\n",
            "Training epoch 3664/1000000, d_loss: -164.63906860351562,  g_loss: 56.877891540527344\n",
            "Training epoch 3665/1000000, d_loss: -195.0075225830078,  g_loss: -73.6432113647461\n",
            "Training epoch 3666/1000000, d_loss: -124.74595642089844,  g_loss: 68.63520812988281\n",
            "Training epoch 3667/1000000, d_loss: 103.49324035644531,  g_loss: 99.02153015136719\n",
            "Training epoch 3668/1000000, d_loss: -140.26669311523438,  g_loss: 115.87593078613281\n",
            "Training epoch 3669/1000000, d_loss: -141.73782348632812,  g_loss: 30.96135902404785\n",
            "Training epoch 3670/1000000, d_loss: -485.6145935058594,  g_loss: 1.7684850692749023\n",
            "Training epoch 3671/1000000, d_loss: -574.641357421875,  g_loss: -347.45318603515625\n",
            "Training epoch 3672/1000000, d_loss: 100.50979614257812,  g_loss: 282.12371826171875\n",
            "Training epoch 3673/1000000, d_loss: -57.78129577636719,  g_loss: 216.3684539794922\n",
            "Training epoch 3674/1000000, d_loss: -451.4276428222656,  g_loss: 39.731571197509766\n",
            "Training epoch 3675/1000000, d_loss: -76.49226379394531,  g_loss: 6.309297561645508\n",
            "Training epoch 3676/1000000, d_loss: -132.81561279296875,  g_loss: 40.46924591064453\n",
            "Training epoch 3677/1000000, d_loss: -252.99172973632812,  g_loss: -102.07386779785156\n",
            "Training epoch 3678/1000000, d_loss: -104.25052642822266,  g_loss: 149.81607055664062\n",
            "Training epoch 3679/1000000, d_loss: 4.5254669189453125,  g_loss: 172.6100311279297\n",
            "Training epoch 3680/1000000, d_loss: -309.7342834472656,  g_loss: 170.80445861816406\n",
            "Training epoch 3681/1000000, d_loss: -376.6582336425781,  g_loss: 838.2637329101562\n",
            "Training epoch 3682/1000000, d_loss: -188.58642578125,  g_loss: -28.9455623626709\n",
            "Training epoch 3683/1000000, d_loss: 174.93699645996094,  g_loss: -97.96913146972656\n",
            "Training epoch 3684/1000000, d_loss: -146.11683654785156,  g_loss: 75.47634887695312\n",
            "Training epoch 3685/1000000, d_loss: -203.81744384765625,  g_loss: 28.876758575439453\n",
            "Training epoch 3686/1000000, d_loss: -61.75877380371094,  g_loss: 114.64724731445312\n",
            "Training epoch 3687/1000000, d_loss: -142.97132873535156,  g_loss: 11.02252197265625\n",
            "Training epoch 3688/1000000, d_loss: -96.75057983398438,  g_loss: 124.82962036132812\n",
            "Training epoch 3689/1000000, d_loss: -218.34487915039062,  g_loss: -20.01953125\n",
            "Training epoch 3690/1000000, d_loss: -280.8811340332031,  g_loss: -57.73550796508789\n",
            "Training epoch 3691/1000000, d_loss: -245.9308624267578,  g_loss: -68.35539245605469\n",
            "Training epoch 3692/1000000, d_loss: -201.20477294921875,  g_loss: -67.22842407226562\n",
            "Training epoch 3693/1000000, d_loss: -97.58334350585938,  g_loss: -109.27782440185547\n",
            "Training epoch 3694/1000000, d_loss: -113.62775421142578,  g_loss: -83.1081314086914\n",
            "Training epoch 3695/1000000, d_loss: -23.725830078125,  g_loss: 98.87733459472656\n",
            "Training epoch 3696/1000000, d_loss: -87.339111328125,  g_loss: 80.19715881347656\n",
            "Training epoch 3697/1000000, d_loss: 11.822059631347656,  g_loss: 113.27869415283203\n",
            "Training epoch 3698/1000000, d_loss: -48.392704010009766,  g_loss: 158.32666015625\n",
            "Training epoch 3699/1000000, d_loss: -128.0054168701172,  g_loss: 219.58236694335938\n",
            "Training epoch 3700/1000000, d_loss: -25.055030822753906,  g_loss: 127.28876495361328\n",
            "Training epoch 3701/1000000, d_loss: -135.30712890625,  g_loss: 82.24309539794922\n",
            "Training epoch 3702/1000000, d_loss: -31.757678985595703,  g_loss: 70.021728515625\n",
            "Training epoch 3703/1000000, d_loss: -158.79649353027344,  g_loss: 60.510108947753906\n",
            "Training epoch 3704/1000000, d_loss: -39.601905822753906,  g_loss: 87.11811828613281\n",
            "Training epoch 3705/1000000, d_loss: -14.40057373046875,  g_loss: 84.48381805419922\n",
            "Training epoch 3706/1000000, d_loss: -603.4692993164062,  g_loss: -136.9110565185547\n",
            "Training epoch 3707/1000000, d_loss: -89.37559509277344,  g_loss: -128.2763214111328\n",
            "Training epoch 3708/1000000, d_loss: -175.77032470703125,  g_loss: 169.35565185546875\n",
            "Training epoch 3709/1000000, d_loss: 77.32574462890625,  g_loss: 272.29254150390625\n",
            "Training epoch 3710/1000000, d_loss: -142.04513549804688,  g_loss: 278.7156982421875\n",
            "Training epoch 3711/1000000, d_loss: -260.0893859863281,  g_loss: 88.49761962890625\n",
            "Training epoch 3712/1000000, d_loss: -43.1309928894043,  g_loss: 165.08932495117188\n",
            "Training epoch 3713/1000000, d_loss: -185.71060180664062,  g_loss: 60.927207946777344\n",
            "Training epoch 3714/1000000, d_loss: -12.173385620117188,  g_loss: 215.42669677734375\n",
            "Training epoch 3715/1000000, d_loss: -150.3264617919922,  g_loss: 164.42642211914062\n",
            "Training epoch 3716/1000000, d_loss: -43.15652847290039,  g_loss: 101.09878540039062\n",
            "Training epoch 3717/1000000, d_loss: -77.69471740722656,  g_loss: 101.83541870117188\n",
            "Training epoch 3718/1000000, d_loss: -352.6044006347656,  g_loss: -62.50804138183594\n",
            "Training epoch 3719/1000000, d_loss: -46.13230514526367,  g_loss: 61.3890495300293\n",
            "Training epoch 3720/1000000, d_loss: -127.39825439453125,  g_loss: 111.0465087890625\n",
            "Training epoch 3721/1000000, d_loss: -114.61763763427734,  g_loss: 71.80712127685547\n",
            "Training epoch 3722/1000000, d_loss: -215.30763244628906,  g_loss: -3.7295217514038086\n",
            "Training epoch 3723/1000000, d_loss: -733.4373779296875,  g_loss: -392.57928466796875\n",
            "Training epoch 3724/1000000, d_loss: 65.84825134277344,  g_loss: -68.29470825195312\n",
            "Training epoch 3725/1000000, d_loss: 38.772735595703125,  g_loss: -86.09716796875\n",
            "Training epoch 3726/1000000, d_loss: -31.963598251342773,  g_loss: 31.208633422851562\n",
            "Training epoch 3727/1000000, d_loss: -75.17253112792969,  g_loss: 136.5726776123047\n",
            "Training epoch 3728/1000000, d_loss: -118.0827407836914,  g_loss: 149.278076171875\n",
            "Training epoch 3729/1000000, d_loss: -231.72366333007812,  g_loss: 100.37980651855469\n",
            "Training epoch 3730/1000000, d_loss: -84.40897369384766,  g_loss: 54.11174774169922\n",
            "Training epoch 3731/1000000, d_loss: -36.567501068115234,  g_loss: 120.02037048339844\n",
            "Training epoch 3732/1000000, d_loss: -31.331789016723633,  g_loss: 46.63715362548828\n",
            "Training epoch 3733/1000000, d_loss: -140.57937622070312,  g_loss: 291.5525207519531\n",
            "Training epoch 3734/1000000, d_loss: -29.828189849853516,  g_loss: 270.18145751953125\n",
            "Training epoch 3735/1000000, d_loss: -338.700927734375,  g_loss: -31.981306076049805\n",
            "Training epoch 3736/1000000, d_loss: -61.032196044921875,  g_loss: -43.47626876831055\n",
            "Training epoch 3737/1000000, d_loss: -94.71917724609375,  g_loss: 37.392086029052734\n",
            "Training epoch 3738/1000000, d_loss: -135.96026611328125,  g_loss: 21.488359451293945\n",
            "Training epoch 3739/1000000, d_loss: -273.6009826660156,  g_loss: -67.47976684570312\n",
            "Training epoch 3740/1000000, d_loss: -230.79583740234375,  g_loss: -108.57411193847656\n",
            "Training epoch 3741/1000000, d_loss: -99.26567077636719,  g_loss: -66.04525756835938\n",
            "Training epoch 3742/1000000, d_loss: -89.25496673583984,  g_loss: 72.22856903076172\n",
            "Training epoch 3743/1000000, d_loss: -67.27528381347656,  g_loss: 45.24050521850586\n",
            "Training epoch 3744/1000000, d_loss: 22.088237762451172,  g_loss: 59.10468673706055\n",
            "Training epoch 3745/1000000, d_loss: -302.6062316894531,  g_loss: -216.833251953125\n",
            "Training epoch 3746/1000000, d_loss: -88.10337829589844,  g_loss: 81.65214538574219\n",
            "Training epoch 3747/1000000, d_loss: -47.13407897949219,  g_loss: 46.67184066772461\n",
            "Training epoch 3748/1000000, d_loss: -54.48945236206055,  g_loss: 108.74608612060547\n",
            "Training epoch 3749/1000000, d_loss: -149.3552703857422,  g_loss: 50.91010284423828\n",
            "Training epoch 3750/1000000, d_loss: -17.937728881835938,  g_loss: 50.1886100769043\n",
            "Training epoch 3751/1000000, d_loss: 0.2672290802001953,  g_loss: 129.6116485595703\n",
            "Training epoch 3752/1000000, d_loss: -43.979530334472656,  g_loss: 35.843414306640625\n",
            "Training epoch 3753/1000000, d_loss: -224.34121704101562,  g_loss: 23.090070724487305\n",
            "Training epoch 3754/1000000, d_loss: -61.80964279174805,  g_loss: 88.82138061523438\n",
            "Training epoch 3755/1000000, d_loss: -136.09259033203125,  g_loss: 94.83654022216797\n",
            "Training epoch 3756/1000000, d_loss: -64.08680725097656,  g_loss: 53.43089294433594\n",
            "Training epoch 3757/1000000, d_loss: -133.91151428222656,  g_loss: 71.21931457519531\n",
            "Training epoch 3758/1000000, d_loss: -50.927669525146484,  g_loss: 29.481544494628906\n",
            "Training epoch 3759/1000000, d_loss: -36.26374053955078,  g_loss: 33.26559829711914\n",
            "Training epoch 3760/1000000, d_loss: -168.1362762451172,  g_loss: 96.72486877441406\n",
            "Training epoch 3761/1000000, d_loss: -40.40843963623047,  g_loss: 73.97975158691406\n",
            "Training epoch 3762/1000000, d_loss: -33.66130828857422,  g_loss: 114.55186462402344\n",
            "Training epoch 3763/1000000, d_loss: 16.891357421875,  g_loss: 30.40160369873047\n",
            "Training epoch 3764/1000000, d_loss: 30.48704719543457,  g_loss: 54.258243560791016\n",
            "Training epoch 3765/1000000, d_loss: -146.27444458007812,  g_loss: -63.29414749145508\n",
            "Training epoch 3766/1000000, d_loss: -70.63975524902344,  g_loss: 78.74777221679688\n",
            "Training epoch 3767/1000000, d_loss: -107.32766723632812,  g_loss: 191.591064453125\n",
            "Training epoch 3768/1000000, d_loss: -207.129150390625,  g_loss: 6.579462051391602\n",
            "Training epoch 3769/1000000, d_loss: -66.35370635986328,  g_loss: 51.106048583984375\n",
            "Training epoch 3770/1000000, d_loss: -797.658935546875,  g_loss: -138.07557678222656\n",
            "Training epoch 3771/1000000, d_loss: -50.18583679199219,  g_loss: -47.288963317871094\n",
            "Training epoch 3772/1000000, d_loss: -100.1050033569336,  g_loss: -19.795761108398438\n",
            "Training epoch 3773/1000000, d_loss: -204.2151336669922,  g_loss: -20.860111236572266\n",
            "Training epoch 3774/1000000, d_loss: -102.77111053466797,  g_loss: 103.31680297851562\n",
            "Training epoch 3775/1000000, d_loss: -80.14372253417969,  g_loss: 121.8382339477539\n",
            "Training epoch 3776/1000000, d_loss: -80.36740112304688,  g_loss: 38.9581184387207\n",
            "Training epoch 3777/1000000, d_loss: -4.8580780029296875,  g_loss: 47.25530242919922\n",
            "Training epoch 3778/1000000, d_loss: -105.28477478027344,  g_loss: 28.946578979492188\n",
            "Training epoch 3779/1000000, d_loss: -38.48793411254883,  g_loss: 119.263427734375\n",
            "Training epoch 3780/1000000, d_loss: -86.17219543457031,  g_loss: 267.5670166015625\n",
            "Training epoch 3781/1000000, d_loss: 20.575042724609375,  g_loss: 145.67335510253906\n",
            "Training epoch 3782/1000000, d_loss: 52.857177734375,  g_loss: 169.79730224609375\n",
            "Training epoch 3783/1000000, d_loss: -49.35132598876953,  g_loss: 171.44183349609375\n",
            "Training epoch 3784/1000000, d_loss: -58.68584442138672,  g_loss: 111.96484375\n",
            "Training epoch 3785/1000000, d_loss: -60.86918640136719,  g_loss: 97.26266479492188\n",
            "Training epoch 3786/1000000, d_loss: -121.07577514648438,  g_loss: 151.01531982421875\n",
            "Training epoch 3787/1000000, d_loss: -16.330305099487305,  g_loss: 105.62577056884766\n",
            "Training epoch 3788/1000000, d_loss: -531.1381225585938,  g_loss: -37.9533805847168\n",
            "Training epoch 3789/1000000, d_loss: -100.97996520996094,  g_loss: -1.5941295623779297\n",
            "Training epoch 3790/1000000, d_loss: -9.585012435913086,  g_loss: 122.23191833496094\n",
            "Training epoch 3791/1000000, d_loss: -163.46163940429688,  g_loss: 55.101768493652344\n",
            "Training epoch 3792/1000000, d_loss: -131.6396942138672,  g_loss: 122.24833679199219\n",
            "Training epoch 3793/1000000, d_loss: -26.69769859313965,  g_loss: 111.43955993652344\n",
            "Training epoch 3794/1000000, d_loss: -370.1363220214844,  g_loss: -116.01359558105469\n",
            "Training epoch 3795/1000000, d_loss: -13.073246002197266,  g_loss: 20.956069946289062\n",
            "Training epoch 3796/1000000, d_loss: -123.90042114257812,  g_loss: 57.58440017700195\n",
            "Training epoch 3797/1000000, d_loss: -56.77765655517578,  g_loss: 0.4022064208984375\n",
            "Training epoch 3798/1000000, d_loss: -22.444936752319336,  g_loss: 52.49267578125\n",
            "Training epoch 3799/1000000, d_loss: -95.83453369140625,  g_loss: 138.4464874267578\n",
            "Training epoch 3800/1000000, d_loss: -164.69570922851562,  g_loss: 52.819854736328125\n",
            "Training epoch 3801/1000000, d_loss: -100.06422424316406,  g_loss: 4.146477222442627\n",
            "Training epoch 3802/1000000, d_loss: -30.681884765625,  g_loss: 33.80870819091797\n",
            "Training epoch 3803/1000000, d_loss: -95.70709228515625,  g_loss: 27.87143325805664\n",
            "Training epoch 3804/1000000, d_loss: -109.6889877319336,  g_loss: 79.03260803222656\n",
            "Training epoch 3805/1000000, d_loss: -661.3096313476562,  g_loss: -307.70343017578125\n",
            "Training epoch 3806/1000000, d_loss: -41.746768951416016,  g_loss: 118.17183685302734\n",
            "Training epoch 3807/1000000, d_loss: -119.87744903564453,  g_loss: 235.55267333984375\n",
            "Training epoch 3808/1000000, d_loss: -89.4389419555664,  g_loss: 168.034912109375\n",
            "Training epoch 3809/1000000, d_loss: -137.52810668945312,  g_loss: 341.6026611328125\n",
            "Training epoch 3810/1000000, d_loss: -24.761201858520508,  g_loss: 284.76763916015625\n",
            "Training epoch 3811/1000000, d_loss: -154.6071014404297,  g_loss: 51.13385009765625\n",
            "Training epoch 3812/1000000, d_loss: -135.8810272216797,  g_loss: 217.96334838867188\n",
            "Training epoch 3813/1000000, d_loss: -198.21588134765625,  g_loss: 4.895906448364258\n",
            "Training epoch 3814/1000000, d_loss: -42.12987518310547,  g_loss: 61.00180435180664\n",
            "Training epoch 3815/1000000, d_loss: -892.856201171875,  g_loss: -392.6591796875\n",
            "Training epoch 3816/1000000, d_loss: -59.689178466796875,  g_loss: 26.487564086914062\n",
            "Training epoch 3817/1000000, d_loss: -399.47296142578125,  g_loss: -151.78160095214844\n",
            "Training epoch 3818/1000000, d_loss: -93.70864868164062,  g_loss: 38.30548858642578\n",
            "Training epoch 3819/1000000, d_loss: -108.80078125,  g_loss: 171.32107543945312\n",
            "Training epoch 3820/1000000, d_loss: -323.45458984375,  g_loss: 30.533226013183594\n",
            "Training epoch 3821/1000000, d_loss: 17.386566162109375,  g_loss: 145.31640625\n",
            "Training epoch 3822/1000000, d_loss: -153.95504760742188,  g_loss: 352.786376953125\n",
            "Training epoch 3823/1000000, d_loss: -266.64752197265625,  g_loss: 539.2587280273438\n",
            "Training epoch 3824/1000000, d_loss: 18.609039306640625,  g_loss: 208.76718139648438\n",
            "Training epoch 3825/1000000, d_loss: -222.80844116210938,  g_loss: 174.11614990234375\n",
            "Training epoch 3826/1000000, d_loss: -127.26747131347656,  g_loss: 124.70423889160156\n",
            "Training epoch 3827/1000000, d_loss: -1.7961158752441406,  g_loss: 122.9014892578125\n",
            "Training epoch 3828/1000000, d_loss: -13.434219360351562,  g_loss: 134.92092895507812\n",
            "Training epoch 3829/1000000, d_loss: -118.96656799316406,  g_loss: 103.40604400634766\n",
            "Training epoch 3830/1000000, d_loss: -121.08151245117188,  g_loss: -11.9111909866333\n",
            "Training epoch 3831/1000000, d_loss: -28.895915985107422,  g_loss: -78.3774642944336\n",
            "Training epoch 3832/1000000, d_loss: -66.5931396484375,  g_loss: -26.32468605041504\n",
            "Training epoch 3833/1000000, d_loss: -120.76032257080078,  g_loss: 90.45547485351562\n",
            "Training epoch 3834/1000000, d_loss: -153.1243438720703,  g_loss: 74.3775405883789\n",
            "Training epoch 3835/1000000, d_loss: -111.11890411376953,  g_loss: 27.95833396911621\n",
            "Training epoch 3836/1000000, d_loss: -498.98089599609375,  g_loss: -257.08447265625\n",
            "Training epoch 3837/1000000, d_loss: -144.1016082763672,  g_loss: -168.602783203125\n",
            "Training epoch 3838/1000000, d_loss: -799.1630249023438,  g_loss: -678.4532470703125\n",
            "Training epoch 3839/1000000, d_loss: 73.0627670288086,  g_loss: -94.30706787109375\n",
            "Training epoch 3840/1000000, d_loss: 81.0748291015625,  g_loss: 73.68551635742188\n",
            "Training epoch 3841/1000000, d_loss: -168.07752990722656,  g_loss: 239.26644897460938\n",
            "Training epoch 3842/1000000, d_loss: -109.13150024414062,  g_loss: 448.9393615722656\n",
            "Training epoch 3843/1000000, d_loss: -347.13427734375,  g_loss: 1039.50830078125\n",
            "Training epoch 3844/1000000, d_loss: -192.2923583984375,  g_loss: 277.2193908691406\n",
            "Training epoch 3845/1000000, d_loss: -35.16660690307617,  g_loss: 245.1741943359375\n",
            "Training epoch 3846/1000000, d_loss: -158.7178497314453,  g_loss: 226.31393432617188\n",
            "Training epoch 3847/1000000, d_loss: -138.7505340576172,  g_loss: 310.83721923828125\n",
            "Training epoch 3848/1000000, d_loss: -32.943023681640625,  g_loss: 232.2853240966797\n",
            "Training epoch 3849/1000000, d_loss: -29.401260375976562,  g_loss: 91.29324340820312\n",
            "Training epoch 3850/1000000, d_loss: -334.6985778808594,  g_loss: -8.989789009094238\n",
            "Training epoch 3851/1000000, d_loss: 20.808692932128906,  g_loss: 69.44308471679688\n",
            "Training epoch 3852/1000000, d_loss: -591.7049560546875,  g_loss: -280.3927917480469\n",
            "Training epoch 3853/1000000, d_loss: -92.5937271118164,  g_loss: -33.11334228515625\n",
            "Training epoch 3854/1000000, d_loss: -393.5774841308594,  g_loss: -80.29763793945312\n",
            "Training epoch 3855/1000000, d_loss: -55.20781326293945,  g_loss: -180.01214599609375\n",
            "Training epoch 3856/1000000, d_loss: -296.802978515625,  g_loss: -140.2455596923828\n",
            "Training epoch 3857/1000000, d_loss: -496.40771484375,  g_loss: -71.61932373046875\n",
            "Training epoch 3858/1000000, d_loss: 9.365055084228516,  g_loss: -159.03140258789062\n",
            "Training epoch 3859/1000000, d_loss: -611.217041015625,  g_loss: 1347.7308349609375\n",
            "Training epoch 3860/1000000, d_loss: -55.93901824951172,  g_loss: -171.84063720703125\n",
            "Training epoch 3861/1000000, d_loss: -174.88720703125,  g_loss: -170.9637451171875\n",
            "Training epoch 3862/1000000, d_loss: -26.822769165039062,  g_loss: -135.9783477783203\n",
            "Training epoch 3863/1000000, d_loss: -206.96661376953125,  g_loss: -108.59869384765625\n",
            "Training epoch 3864/1000000, d_loss: 44.60728454589844,  g_loss: -83.25543212890625\n",
            "Training epoch 3865/1000000, d_loss: -94.53392028808594,  g_loss: -206.69229125976562\n",
            "Training epoch 3866/1000000, d_loss: -59.60358810424805,  g_loss: -63.584510803222656\n",
            "Training epoch 3867/1000000, d_loss: -49.63086700439453,  g_loss: -147.508056640625\n",
            "Training epoch 3868/1000000, d_loss: -61.845802307128906,  g_loss: -43.186546325683594\n",
            "Training epoch 3869/1000000, d_loss: 8.256629943847656,  g_loss: 77.14244079589844\n",
            "Training epoch 3870/1000000, d_loss: -96.9489974975586,  g_loss: 31.108196258544922\n",
            "Training epoch 3871/1000000, d_loss: 59.778526306152344,  g_loss: 69.75778198242188\n",
            "Training epoch 3872/1000000, d_loss: -47.36328125,  g_loss: 54.28713607788086\n",
            "Training epoch 3873/1000000, d_loss: -139.22430419921875,  g_loss: 115.91116333007812\n",
            "Training epoch 3874/1000000, d_loss: -235.85308837890625,  g_loss: 309.4024353027344\n",
            "Training epoch 3875/1000000, d_loss: -157.0240020751953,  g_loss: -8.546497344970703\n",
            "Training epoch 3876/1000000, d_loss: -66.6950912475586,  g_loss: 35.360984802246094\n",
            "Training epoch 3877/1000000, d_loss: 27.14056396484375,  g_loss: 43.57496643066406\n",
            "Training epoch 3878/1000000, d_loss: -77.1005859375,  g_loss: 14.160762786865234\n",
            "Training epoch 3879/1000000, d_loss: -134.4533233642578,  g_loss: -36.588951110839844\n",
            "Training epoch 3880/1000000, d_loss: -162.19200134277344,  g_loss: -38.76062774658203\n",
            "Training epoch 3881/1000000, d_loss: 23.35433006286621,  g_loss: -3.887423515319824\n",
            "Training epoch 3882/1000000, d_loss: -132.504638671875,  g_loss: 47.522056579589844\n",
            "Training epoch 3883/1000000, d_loss: 67.55331420898438,  g_loss: 39.51567840576172\n",
            "Training epoch 3884/1000000, d_loss: -75.90107727050781,  g_loss: 118.33927917480469\n",
            "Training epoch 3885/1000000, d_loss: -147.2993621826172,  g_loss: 51.88468933105469\n",
            "Training epoch 3886/1000000, d_loss: -67.76486206054688,  g_loss: 6.144890785217285\n",
            "Training epoch 3887/1000000, d_loss: -95.1949234008789,  g_loss: 59.56365966796875\n",
            "Training epoch 3888/1000000, d_loss: -113.50663757324219,  g_loss: 5.515653610229492\n",
            "Training epoch 3889/1000000, d_loss: -110.88783264160156,  g_loss: 66.22930908203125\n",
            "Training epoch 3890/1000000, d_loss: -63.92356872558594,  g_loss: 43.015098571777344\n",
            "Training epoch 3891/1000000, d_loss: -109.64789581298828,  g_loss: 169.90101623535156\n",
            "Training epoch 3892/1000000, d_loss: -292.40582275390625,  g_loss: 37.612396240234375\n",
            "Training epoch 3893/1000000, d_loss: -6.7498884201049805,  g_loss: 105.40216064453125\n",
            "Training epoch 3894/1000000, d_loss: -71.4389877319336,  g_loss: 68.47347259521484\n",
            "Training epoch 3895/1000000, d_loss: -64.49999237060547,  g_loss: 44.746337890625\n",
            "Training epoch 3896/1000000, d_loss: -191.04019165039062,  g_loss: 23.823917388916016\n",
            "Training epoch 3897/1000000, d_loss: -153.04644775390625,  g_loss: 225.88388061523438\n",
            "Training epoch 3898/1000000, d_loss: -113.0825424194336,  g_loss: 49.139034271240234\n",
            "Training epoch 3899/1000000, d_loss: -42.833984375,  g_loss: 94.54933166503906\n",
            "Training epoch 3900/1000000, d_loss: -190.7967071533203,  g_loss: 99.79983520507812\n",
            "Training epoch 3901/1000000, d_loss: 54.495262145996094,  g_loss: 194.8199005126953\n",
            "Training epoch 3902/1000000, d_loss: -109.72418975830078,  g_loss: 168.987060546875\n",
            "Training epoch 3903/1000000, d_loss: -82.96636199951172,  g_loss: 103.65768432617188\n",
            "Training epoch 3904/1000000, d_loss: -81.55693054199219,  g_loss: 239.61273193359375\n",
            "Training epoch 3905/1000000, d_loss: -129.8812255859375,  g_loss: 77.54293060302734\n",
            "Training epoch 3906/1000000, d_loss: 13.720340728759766,  g_loss: 95.31786346435547\n",
            "Training epoch 3907/1000000, d_loss: -235.08407592773438,  g_loss: 24.159912109375\n",
            "Training epoch 3908/1000000, d_loss: 21.34567642211914,  g_loss: 102.55308532714844\n",
            "Training epoch 3909/1000000, d_loss: -158.64886474609375,  g_loss: 84.4327392578125\n",
            "Training epoch 3910/1000000, d_loss: -147.4572296142578,  g_loss: 9.90760612487793\n",
            "Training epoch 3911/1000000, d_loss: 20.53887939453125,  g_loss: 160.277587890625\n",
            "Training epoch 3912/1000000, d_loss: -134.45863342285156,  g_loss: 160.5452117919922\n",
            "Training epoch 3913/1000000, d_loss: -127.42070770263672,  g_loss: 142.1410675048828\n",
            "Training epoch 3914/1000000, d_loss: -41.706321716308594,  g_loss: 123.55166625976562\n",
            "Training epoch 3915/1000000, d_loss: -111.27495574951172,  g_loss: 200.41415405273438\n",
            "Training epoch 3916/1000000, d_loss: -28.68463706970215,  g_loss: 138.2111358642578\n",
            "Training epoch 3917/1000000, d_loss: -125.92265319824219,  g_loss: 149.36331176757812\n",
            "Training epoch 3918/1000000, d_loss: -83.60645294189453,  g_loss: 358.8141784667969\n",
            "Training epoch 3919/1000000, d_loss: -88.29740905761719,  g_loss: 145.6998748779297\n",
            "Training epoch 3920/1000000, d_loss: -124.5892333984375,  g_loss: 132.03363037109375\n",
            "Training epoch 3921/1000000, d_loss: -111.498291015625,  g_loss: 276.5055847167969\n",
            "Training epoch 3922/1000000, d_loss: -177.5712890625,  g_loss: 80.68197631835938\n",
            "Training epoch 3923/1000000, d_loss: -84.79450988769531,  g_loss: 84.04808044433594\n",
            "Training epoch 3924/1000000, d_loss: -19.801559448242188,  g_loss: 67.3619155883789\n",
            "Training epoch 3925/1000000, d_loss: -260.82000732421875,  g_loss: 54.79844665527344\n",
            "Training epoch 3926/1000000, d_loss: -108.65568542480469,  g_loss: 109.05281066894531\n",
            "Training epoch 3927/1000000, d_loss: -5.037818908691406,  g_loss: -38.17433166503906\n",
            "Training epoch 3928/1000000, d_loss: -478.9302673339844,  g_loss: -136.35977172851562\n",
            "Training epoch 3929/1000000, d_loss: -1215.429931640625,  g_loss: -1010.0454711914062\n",
            "Training epoch 3930/1000000, d_loss: 477.2255859375,  g_loss: -372.207275390625\n",
            "Training epoch 3931/1000000, d_loss: 393.27886962890625,  g_loss: -197.5418701171875\n",
            "Training epoch 3932/1000000, d_loss: 421.95599365234375,  g_loss: -157.0694580078125\n",
            "Training epoch 3933/1000000, d_loss: 57.330352783203125,  g_loss: 265.85394287109375\n",
            "Training epoch 3934/1000000, d_loss: -407.20965576171875,  g_loss: 511.2753601074219\n",
            "Training epoch 3935/1000000, d_loss: -885.3767700195312,  g_loss: 1380.5885009765625\n",
            "Training epoch 3936/1000000, d_loss: -97.86719512939453,  g_loss: 659.3475341796875\n",
            "Training epoch 3937/1000000, d_loss: -120.83464813232422,  g_loss: -180.284912109375\n",
            "Training epoch 3938/1000000, d_loss: -33.77296829223633,  g_loss: -97.95855712890625\n",
            "Training epoch 3939/1000000, d_loss: -31.562870025634766,  g_loss: -49.87449645996094\n",
            "Training epoch 3940/1000000, d_loss: 43.73777389526367,  g_loss: -24.05870819091797\n",
            "Training epoch 3941/1000000, d_loss: -167.4879913330078,  g_loss: 7.294229507446289\n",
            "Training epoch 3942/1000000, d_loss: -21.522132873535156,  g_loss: 81.4037094116211\n",
            "Training epoch 3943/1000000, d_loss: -54.18491744995117,  g_loss: 16.747421264648438\n",
            "Training epoch 3944/1000000, d_loss: -54.44922637939453,  g_loss: -29.684059143066406\n",
            "Training epoch 3945/1000000, d_loss: -109.52381134033203,  g_loss: 5.181806564331055\n",
            "Training epoch 3946/1000000, d_loss: -281.3835144042969,  g_loss: 32.701087951660156\n",
            "Training epoch 3947/1000000, d_loss: -59.039886474609375,  g_loss: -67.8285903930664\n",
            "Training epoch 3948/1000000, d_loss: -64.2164306640625,  g_loss: -37.34512710571289\n",
            "Training epoch 3949/1000000, d_loss: -84.08170318603516,  g_loss: -4.435535430908203\n",
            "Training epoch 3950/1000000, d_loss: -146.234375,  g_loss: 104.39635467529297\n",
            "Training epoch 3951/1000000, d_loss: 29.71738052368164,  g_loss: -31.64604949951172\n",
            "Training epoch 3952/1000000, d_loss: -11.427005767822266,  g_loss: 77.8176498413086\n",
            "Training epoch 3953/1000000, d_loss: -170.72467041015625,  g_loss: 60.31074523925781\n",
            "Training epoch 3954/1000000, d_loss: -86.2675552368164,  g_loss: 204.50669860839844\n",
            "Training epoch 3955/1000000, d_loss: -98.71155548095703,  g_loss: 189.22891235351562\n",
            "Training epoch 3956/1000000, d_loss: -128.16299438476562,  g_loss: 26.373395919799805\n",
            "Training epoch 3957/1000000, d_loss: 74.16752624511719,  g_loss: 124.57411193847656\n",
            "Training epoch 3958/1000000, d_loss: -51.275779724121094,  g_loss: 88.40064239501953\n",
            "Training epoch 3959/1000000, d_loss: -91.95065307617188,  g_loss: 122.0902099609375\n",
            "Training epoch 3960/1000000, d_loss: -123.15235900878906,  g_loss: 108.21788024902344\n",
            "Training epoch 3961/1000000, d_loss: -180.9387664794922,  g_loss: 154.5936279296875\n",
            "Training epoch 3962/1000000, d_loss: -127.08920288085938,  g_loss: 59.6119499206543\n",
            "Training epoch 3963/1000000, d_loss: -847.332275390625,  g_loss: -342.5634765625\n",
            "Training epoch 3964/1000000, d_loss: 412.5538330078125,  g_loss: -25.323287963867188\n",
            "Training epoch 3965/1000000, d_loss: 79.97750854492188,  g_loss: 11.02855110168457\n",
            "Training epoch 3966/1000000, d_loss: -46.443817138671875,  g_loss: 179.64480590820312\n",
            "Training epoch 3967/1000000, d_loss: -213.71182250976562,  g_loss: 296.0235595703125\n",
            "Training epoch 3968/1000000, d_loss: -79.5284652709961,  g_loss: 303.591552734375\n",
            "Training epoch 3969/1000000, d_loss: -184.20408630371094,  g_loss: 375.5820617675781\n",
            "Training epoch 3970/1000000, d_loss: -223.97845458984375,  g_loss: 524.9943237304688\n",
            "Training epoch 3971/1000000, d_loss: -123.01472473144531,  g_loss: 307.9178161621094\n",
            "Training epoch 3972/1000000, d_loss: -146.70108032226562,  g_loss: 222.12802124023438\n",
            "Training epoch 3973/1000000, d_loss: -144.91607666015625,  g_loss: 175.99923706054688\n",
            "Training epoch 3974/1000000, d_loss: -42.265357971191406,  g_loss: 143.50936889648438\n",
            "Training epoch 3975/1000000, d_loss: 13.559764862060547,  g_loss: 54.98457336425781\n",
            "Training epoch 3976/1000000, d_loss: -170.93551635742188,  g_loss: 111.2886962890625\n",
            "Training epoch 3977/1000000, d_loss: -310.3567199707031,  g_loss: 13.770315170288086\n",
            "Training epoch 3978/1000000, d_loss: -183.0731964111328,  g_loss: -155.86366271972656\n",
            "Training epoch 3979/1000000, d_loss: 16.43918800354004,  g_loss: -46.202247619628906\n",
            "Training epoch 3980/1000000, d_loss: -150.48544311523438,  g_loss: 363.8543395996094\n",
            "Training epoch 3981/1000000, d_loss: -164.98028564453125,  g_loss: 157.05075073242188\n",
            "Training epoch 3982/1000000, d_loss: -2.449211597442627,  g_loss: 159.83547973632812\n",
            "Training epoch 3983/1000000, d_loss: -243.01828002929688,  g_loss: 83.69374084472656\n",
            "Training epoch 3984/1000000, d_loss: -255.74281311035156,  g_loss: 326.083984375\n",
            "Training epoch 3985/1000000, d_loss: -68.48969268798828,  g_loss: 270.8052062988281\n",
            "Training epoch 3986/1000000, d_loss: -138.428955078125,  g_loss: 59.334808349609375\n",
            "Training epoch 3987/1000000, d_loss: -96.7477798461914,  g_loss: -43.27149963378906\n",
            "Training epoch 3988/1000000, d_loss: -476.1094665527344,  g_loss: -172.4235076904297\n",
            "Training epoch 3989/1000000, d_loss: -135.92257690429688,  g_loss: -152.03286743164062\n",
            "Training epoch 3990/1000000, d_loss: -77.96216583251953,  g_loss: 96.1441879272461\n",
            "Training epoch 3991/1000000, d_loss: -332.1457824707031,  g_loss: -18.158981323242188\n",
            "Training epoch 3992/1000000, d_loss: -2.6827926635742188,  g_loss: 68.53376007080078\n",
            "Training epoch 3993/1000000, d_loss: -89.300537109375,  g_loss: 33.645790100097656\n",
            "Training epoch 3994/1000000, d_loss: -76.38737487792969,  g_loss: 77.9041519165039\n",
            "Training epoch 3995/1000000, d_loss: -77.20194244384766,  g_loss: 82.25165557861328\n",
            "Training epoch 3996/1000000, d_loss: -269.09234619140625,  g_loss: -21.353164672851562\n",
            "Training epoch 3997/1000000, d_loss: -212.0802459716797,  g_loss: -131.5858917236328\n",
            "Training epoch 3998/1000000, d_loss: -112.39283752441406,  g_loss: 8.806524276733398\n",
            "Training epoch 3999/1000000, d_loss: -6.987850189208984,  g_loss: 150.22640991210938\n",
            "Training epoch 4000/1000000, d_loss: -142.49691772460938,  g_loss: 84.69779968261719\n",
            "Training epoch 4001/1000000, d_loss: -205.2990264892578,  g_loss: 90.38036346435547\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 62/62 [00:00<00:00, 161.72it/s]\n",
            "Meshing: 100%|██████████| 27869/27869 [00:09<00:00, 2824.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_4001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_4001/assets\n",
            "Training epoch 4002/1000000, d_loss: -227.877197265625,  g_loss: 21.788555145263672\n",
            "Training epoch 4003/1000000, d_loss: -135.0448455810547,  g_loss: 208.802978515625\n",
            "Training epoch 4004/1000000, d_loss: -424.0396728515625,  g_loss: -199.60650634765625\n",
            "Training epoch 4005/1000000, d_loss: -131.01290893554688,  g_loss: 70.72145080566406\n",
            "Training epoch 4006/1000000, d_loss: 16.034793853759766,  g_loss: 32.888206481933594\n",
            "Training epoch 4007/1000000, d_loss: -113.82140350341797,  g_loss: 85.71295166015625\n",
            "Training epoch 4008/1000000, d_loss: -150.1630859375,  g_loss: 21.623695373535156\n",
            "Training epoch 4009/1000000, d_loss: -88.56426239013672,  g_loss: 10.673484802246094\n",
            "Training epoch 4010/1000000, d_loss: -115.76573181152344,  g_loss: 189.31927490234375\n",
            "Training epoch 4011/1000000, d_loss: -391.7478332519531,  g_loss: 60.98191452026367\n",
            "Training epoch 4012/1000000, d_loss: -751.7268676757812,  g_loss: -513.9641723632812\n",
            "Training epoch 4013/1000000, d_loss: 21.605484008789062,  g_loss: -329.5359191894531\n",
            "Training epoch 4014/1000000, d_loss: -148.70077514648438,  g_loss: -309.86260986328125\n",
            "Training epoch 4015/1000000, d_loss: 284.6514892578125,  g_loss: -103.44869232177734\n",
            "Training epoch 4016/1000000, d_loss: 12.487486839294434,  g_loss: -61.73159408569336\n",
            "Training epoch 4017/1000000, d_loss: -177.44573974609375,  g_loss: 27.761173248291016\n",
            "Training epoch 4018/1000000, d_loss: -23.778778076171875,  g_loss: -68.57638549804688\n",
            "Training epoch 4019/1000000, d_loss: -17.107946395874023,  g_loss: -80.64393615722656\n",
            "Training epoch 4020/1000000, d_loss: -119.88765716552734,  g_loss: 79.27474975585938\n",
            "Training epoch 4021/1000000, d_loss: -91.51902770996094,  g_loss: 169.73255920410156\n",
            "Training epoch 4022/1000000, d_loss: -235.43533325195312,  g_loss: 245.2550048828125\n",
            "Training epoch 4023/1000000, d_loss: -122.55027770996094,  g_loss: 186.377685546875\n",
            "Training epoch 4024/1000000, d_loss: -126.02833557128906,  g_loss: 179.24868774414062\n",
            "Training epoch 4025/1000000, d_loss: -207.89944458007812,  g_loss: 0.747889518737793\n",
            "Training epoch 4026/1000000, d_loss: -227.3046875,  g_loss: 74.95285034179688\n",
            "Training epoch 4027/1000000, d_loss: -86.57469940185547,  g_loss: 32.748748779296875\n",
            "Training epoch 4028/1000000, d_loss: -98.41021728515625,  g_loss: 173.97518920898438\n",
            "Training epoch 4029/1000000, d_loss: -256.702880859375,  g_loss: 86.02435302734375\n",
            "Training epoch 4030/1000000, d_loss: -314.9028625488281,  g_loss: -64.74081420898438\n",
            "Training epoch 4031/1000000, d_loss: -1.5496139526367188,  g_loss: -59.155460357666016\n",
            "Training epoch 4032/1000000, d_loss: -206.7758331298828,  g_loss: -164.85304260253906\n",
            "Training epoch 4033/1000000, d_loss: -103.56493377685547,  g_loss: 85.85338592529297\n",
            "Training epoch 4034/1000000, d_loss: -56.8045654296875,  g_loss: 94.89421081542969\n",
            "Training epoch 4035/1000000, d_loss: -42.49848937988281,  g_loss: 239.51698303222656\n",
            "Training epoch 4036/1000000, d_loss: -14.990936279296875,  g_loss: 155.24017333984375\n",
            "Training epoch 4037/1000000, d_loss: -211.81573486328125,  g_loss: 145.9169464111328\n",
            "Training epoch 4038/1000000, d_loss: -123.69026184082031,  g_loss: -62.84244918823242\n",
            "Training epoch 4039/1000000, d_loss: -97.92523193359375,  g_loss: 33.58381652832031\n",
            "Training epoch 4040/1000000, d_loss: -96.013916015625,  g_loss: 139.30062866210938\n",
            "Training epoch 4041/1000000, d_loss: -300.8138732910156,  g_loss: -55.95768737792969\n",
            "Training epoch 4042/1000000, d_loss: -87.0284423828125,  g_loss: -23.42350959777832\n",
            "Training epoch 4043/1000000, d_loss: -24.74482536315918,  g_loss: 74.51075744628906\n",
            "Training epoch 4044/1000000, d_loss: -106.01881408691406,  g_loss: 118.51741027832031\n",
            "Training epoch 4045/1000000, d_loss: -166.47216796875,  g_loss: 91.72726440429688\n",
            "Training epoch 4046/1000000, d_loss: -91.53205871582031,  g_loss: 114.93590545654297\n",
            "Training epoch 4047/1000000, d_loss: -114.78346252441406,  g_loss: 157.52069091796875\n",
            "Training epoch 4048/1000000, d_loss: -299.0020751953125,  g_loss: 42.366607666015625\n",
            "Training epoch 4049/1000000, d_loss: -264.1154479980469,  g_loss: 119.04624938964844\n",
            "Training epoch 4050/1000000, d_loss: -21.702552795410156,  g_loss: 109.07622528076172\n",
            "Training epoch 4051/1000000, d_loss: -132.17555236816406,  g_loss: 86.2458724975586\n",
            "Training epoch 4052/1000000, d_loss: -267.5084228515625,  g_loss: 79.11070251464844\n",
            "Training epoch 4053/1000000, d_loss: -153.2006072998047,  g_loss: -10.997901916503906\n",
            "Training epoch 4054/1000000, d_loss: -96.44037628173828,  g_loss: 28.677284240722656\n",
            "Training epoch 4055/1000000, d_loss: -54.05345916748047,  g_loss: 127.14408874511719\n",
            "Training epoch 4056/1000000, d_loss: -115.49956512451172,  g_loss: 75.21171569824219\n",
            "Training epoch 4057/1000000, d_loss: -118.02017211914062,  g_loss: 175.37344360351562\n",
            "Training epoch 4058/1000000, d_loss: -107.84960174560547,  g_loss: 4.403594970703125\n",
            "Training epoch 4059/1000000, d_loss: -235.169189453125,  g_loss: 22.887371063232422\n",
            "Training epoch 4060/1000000, d_loss: -20.248498916625977,  g_loss: -39.50238037109375\n",
            "Training epoch 4061/1000000, d_loss: -36.84498977661133,  g_loss: 64.4737319946289\n",
            "Training epoch 4062/1000000, d_loss: -18.270893096923828,  g_loss: 29.218994140625\n",
            "Training epoch 4063/1000000, d_loss: -900.6641235351562,  g_loss: -153.996337890625\n",
            "Training epoch 4064/1000000, d_loss: -49.4480094909668,  g_loss: -105.53768157958984\n",
            "Training epoch 4065/1000000, d_loss: -41.0318603515625,  g_loss: -36.301326751708984\n",
            "Training epoch 4066/1000000, d_loss: 26.083984375,  g_loss: 7.829228401184082\n",
            "Training epoch 4067/1000000, d_loss: -142.0616912841797,  g_loss: 52.420501708984375\n",
            "Training epoch 4068/1000000, d_loss: -237.33612060546875,  g_loss: -49.74668502807617\n",
            "Training epoch 4069/1000000, d_loss: 55.04170608520508,  g_loss: -123.88703918457031\n",
            "Training epoch 4070/1000000, d_loss: -95.56111907958984,  g_loss: -99.6734619140625\n",
            "Training epoch 4071/1000000, d_loss: -91.02799987792969,  g_loss: -51.05713653564453\n",
            "Training epoch 4072/1000000, d_loss: -18.382198333740234,  g_loss: 87.39893341064453\n",
            "Training epoch 4073/1000000, d_loss: 20.964019775390625,  g_loss: -41.667789459228516\n",
            "Training epoch 4074/1000000, d_loss: -243.7736358642578,  g_loss: -45.473182678222656\n",
            "Training epoch 4075/1000000, d_loss: -48.923561096191406,  g_loss: -1.8930282592773438\n",
            "Training epoch 4076/1000000, d_loss: -24.37010383605957,  g_loss: 62.64128494262695\n",
            "Training epoch 4077/1000000, d_loss: -79.26309967041016,  g_loss: 0.3854093551635742\n",
            "Training epoch 4078/1000000, d_loss: -135.53671264648438,  g_loss: 16.010496139526367\n",
            "Training epoch 4079/1000000, d_loss: -440.138671875,  g_loss: -138.31826782226562\n",
            "Training epoch 4080/1000000, d_loss: -44.34041213989258,  g_loss: 38.52068328857422\n",
            "Training epoch 4081/1000000, d_loss: -33.611087799072266,  g_loss: -1.0948240756988525\n",
            "Training epoch 4082/1000000, d_loss: -37.94610595703125,  g_loss: 51.824317932128906\n",
            "Training epoch 4083/1000000, d_loss: -159.73728942871094,  g_loss: -20.868684768676758\n",
            "Training epoch 4084/1000000, d_loss: -63.34148406982422,  g_loss: 67.09329223632812\n",
            "Training epoch 4085/1000000, d_loss: -174.36569213867188,  g_loss: 60.170326232910156\n",
            "Training epoch 4086/1000000, d_loss: -37.17815399169922,  g_loss: 94.91475677490234\n",
            "Training epoch 4087/1000000, d_loss: -260.3317565917969,  g_loss: 55.162811279296875\n",
            "Training epoch 4088/1000000, d_loss: -58.24159240722656,  g_loss: 135.35301208496094\n",
            "Training epoch 4089/1000000, d_loss: -81.8406982421875,  g_loss: 90.078857421875\n",
            "Training epoch 4090/1000000, d_loss: -88.73255920410156,  g_loss: 148.8915252685547\n",
            "Training epoch 4091/1000000, d_loss: -90.32537841796875,  g_loss: 43.09119415283203\n",
            "Training epoch 4092/1000000, d_loss: -88.73674774169922,  g_loss: 18.441762924194336\n",
            "Training epoch 4093/1000000, d_loss: -200.88449096679688,  g_loss: 135.07418823242188\n",
            "Training epoch 4094/1000000, d_loss: -98.26074981689453,  g_loss: 181.45944213867188\n",
            "Training epoch 4095/1000000, d_loss: -104.11931610107422,  g_loss: 109.29320526123047\n",
            "Training epoch 4096/1000000, d_loss: -337.76385498046875,  g_loss: -41.89163589477539\n",
            "Training epoch 4097/1000000, d_loss: -153.121826171875,  g_loss: -31.07561492919922\n",
            "Training epoch 4098/1000000, d_loss: -43.9437255859375,  g_loss: 19.14769172668457\n",
            "Training epoch 4099/1000000, d_loss: -503.0923156738281,  g_loss: -204.90577697753906\n",
            "Training epoch 4100/1000000, d_loss: -236.79652404785156,  g_loss: -21.960559844970703\n",
            "Training epoch 4101/1000000, d_loss: -138.0135040283203,  g_loss: 125.7766342163086\n",
            "Training epoch 4102/1000000, d_loss: -244.41600036621094,  g_loss: 420.8863830566406\n",
            "Training epoch 4103/1000000, d_loss: -3.5227508544921875,  g_loss: 383.7749938964844\n",
            "Training epoch 4104/1000000, d_loss: -119.04314422607422,  g_loss: 205.00103759765625\n",
            "Training epoch 4105/1000000, d_loss: -111.96287536621094,  g_loss: 316.59027099609375\n",
            "Training epoch 4106/1000000, d_loss: -61.735511779785156,  g_loss: 212.3135223388672\n",
            "Training epoch 4107/1000000, d_loss: -153.6839141845703,  g_loss: 261.718017578125\n",
            "Training epoch 4108/1000000, d_loss: -470.41680908203125,  g_loss: -71.3642349243164\n",
            "Training epoch 4109/1000000, d_loss: -67.73969268798828,  g_loss: 104.88191986083984\n",
            "Training epoch 4110/1000000, d_loss: -86.41777038574219,  g_loss: 71.55113220214844\n",
            "Training epoch 4111/1000000, d_loss: -8.515287399291992,  g_loss: 77.2452392578125\n",
            "Training epoch 4112/1000000, d_loss: -87.80252075195312,  g_loss: 83.58424377441406\n",
            "Training epoch 4113/1000000, d_loss: -57.35365295410156,  g_loss: 204.30137634277344\n",
            "Training epoch 4114/1000000, d_loss: -78.78797149658203,  g_loss: 121.43266296386719\n",
            "Training epoch 4115/1000000, d_loss: -269.9664306640625,  g_loss: 80.82659149169922\n",
            "Training epoch 4116/1000000, d_loss: -46.52161407470703,  g_loss: 150.4031982421875\n",
            "Training epoch 4117/1000000, d_loss: -14.700157165527344,  g_loss: 63.3661003112793\n",
            "Training epoch 4118/1000000, d_loss: -543.1770629882812,  g_loss: -120.09129333496094\n",
            "Training epoch 4119/1000000, d_loss: -123.82839965820312,  g_loss: 64.41041564941406\n",
            "Training epoch 4120/1000000, d_loss: -267.27362060546875,  g_loss: 29.935771942138672\n",
            "Training epoch 4121/1000000, d_loss: 14.600692749023438,  g_loss: -55.90989685058594\n",
            "Training epoch 4122/1000000, d_loss: -113.11551666259766,  g_loss: 87.1392822265625\n",
            "Training epoch 4123/1000000, d_loss: -87.55420684814453,  g_loss: -29.09400749206543\n",
            "Training epoch 4124/1000000, d_loss: -495.6551513671875,  g_loss: -128.5786590576172\n",
            "Training epoch 4125/1000000, d_loss: 59.69007110595703,  g_loss: 48.69903564453125\n",
            "Training epoch 4126/1000000, d_loss: -14.486785888671875,  g_loss: 70.70038604736328\n",
            "Training epoch 4127/1000000, d_loss: -133.97840881347656,  g_loss: 125.81163024902344\n",
            "Training epoch 4128/1000000, d_loss: -5.926368713378906,  g_loss: 145.07772827148438\n",
            "Training epoch 4129/1000000, d_loss: -70.80473327636719,  g_loss: 58.48307800292969\n",
            "Training epoch 4130/1000000, d_loss: -152.64639282226562,  g_loss: 68.46724700927734\n",
            "Training epoch 4131/1000000, d_loss: -148.11163330078125,  g_loss: 16.82707977294922\n",
            "Training epoch 4132/1000000, d_loss: -76.9052734375,  g_loss: 76.7485122680664\n",
            "Training epoch 4133/1000000, d_loss: -63.4174690246582,  g_loss: 55.827213287353516\n",
            "Training epoch 4134/1000000, d_loss: -135.9397430419922,  g_loss: 178.891357421875\n",
            "Training epoch 4135/1000000, d_loss: -115.59253692626953,  g_loss: 68.26554870605469\n",
            "Training epoch 4136/1000000, d_loss: -48.475433349609375,  g_loss: 108.59513854980469\n",
            "Training epoch 4137/1000000, d_loss: -354.0600280761719,  g_loss: 369.647216796875\n",
            "Training epoch 4138/1000000, d_loss: -47.01041030883789,  g_loss: -4.143601417541504\n",
            "Training epoch 4139/1000000, d_loss: -42.86540222167969,  g_loss: 5.310070037841797\n",
            "Training epoch 4140/1000000, d_loss: -100.14834594726562,  g_loss: 39.21881866455078\n",
            "Training epoch 4141/1000000, d_loss: -110.86485290527344,  g_loss: 22.856727600097656\n",
            "Training epoch 4142/1000000, d_loss: -96.83251953125,  g_loss: 103.81983947753906\n",
            "Training epoch 4143/1000000, d_loss: -370.48858642578125,  g_loss: -188.34963989257812\n",
            "Training epoch 4144/1000000, d_loss: -2466.930419921875,  g_loss: -880.5538330078125\n",
            "Training epoch 4145/1000000, d_loss: 806.061767578125,  g_loss: -329.26300048828125\n",
            "Training epoch 4146/1000000, d_loss: 115.31159973144531,  g_loss: -417.17193603515625\n",
            "Training epoch 4147/1000000, d_loss: 1419.4569091796875,  g_loss: -126.8404541015625\n",
            "Training epoch 4148/1000000, d_loss: 458.6077880859375,  g_loss: -120.43881225585938\n",
            "Training epoch 4149/1000000, d_loss: -74.88350677490234,  g_loss: 108.00525665283203\n",
            "Training epoch 4150/1000000, d_loss: 134.32135009765625,  g_loss: 429.875\n",
            "Training epoch 4151/1000000, d_loss: 13.814170837402344,  g_loss: 60.15899658203125\n",
            "Training epoch 4152/1000000, d_loss: -126.89627075195312,  g_loss: 30.693199157714844\n",
            "Training epoch 4153/1000000, d_loss: -288.7406311035156,  g_loss: -30.30819320678711\n",
            "Training epoch 4154/1000000, d_loss: -208.95147705078125,  g_loss: 17.525907516479492\n",
            "Training epoch 4155/1000000, d_loss: -147.2183837890625,  g_loss: 125.35289764404297\n",
            "Training epoch 4156/1000000, d_loss: -80.57872009277344,  g_loss: 132.58631896972656\n",
            "Training epoch 4157/1000000, d_loss: -158.91339111328125,  g_loss: 390.65380859375\n",
            "Training epoch 4158/1000000, d_loss: -62.01483917236328,  g_loss: 38.33122634887695\n",
            "Training epoch 4159/1000000, d_loss: -44.789466857910156,  g_loss: 36.75876998901367\n",
            "Training epoch 4160/1000000, d_loss: 2.6458816528320312,  g_loss: -41.75804138183594\n",
            "Training epoch 4161/1000000, d_loss: -241.6039581298828,  g_loss: -10.69442081451416\n",
            "Training epoch 4162/1000000, d_loss: 2.303190231323242,  g_loss: 58.850135803222656\n",
            "Training epoch 4163/1000000, d_loss: -78.34634399414062,  g_loss: 112.12005615234375\n",
            "Training epoch 4164/1000000, d_loss: -139.67868041992188,  g_loss: 107.51115417480469\n",
            "Training epoch 4165/1000000, d_loss: -65.57484436035156,  g_loss: 78.64306640625\n",
            "Training epoch 4166/1000000, d_loss: 9.139305114746094,  g_loss: 59.39591979980469\n",
            "Training epoch 4167/1000000, d_loss: 13.347650527954102,  g_loss: -16.412586212158203\n",
            "Training epoch 4168/1000000, d_loss: -110.64336395263672,  g_loss: 62.638545989990234\n",
            "Training epoch 4169/1000000, d_loss: 35.211143493652344,  g_loss: 98.78849029541016\n",
            "Training epoch 4170/1000000, d_loss: -14.72361946105957,  g_loss: 119.43495178222656\n",
            "Training epoch 4171/1000000, d_loss: -359.69366455078125,  g_loss: -12.277755737304688\n",
            "Training epoch 4172/1000000, d_loss: -787.6571655273438,  g_loss: -290.7021789550781\n",
            "Training epoch 4173/1000000, d_loss: 73.32792663574219,  g_loss: -96.82972717285156\n",
            "Training epoch 4174/1000000, d_loss: -221.11318969726562,  g_loss: -54.83704376220703\n",
            "Training epoch 4175/1000000, d_loss: -497.5347595214844,  g_loss: -94.37057495117188\n",
            "Training epoch 4176/1000000, d_loss: -65.95649719238281,  g_loss: 35.44990921020508\n",
            "Training epoch 4177/1000000, d_loss: -79.89608001708984,  g_loss: 102.95327758789062\n",
            "Training epoch 4178/1000000, d_loss: -136.8836212158203,  g_loss: 245.84019470214844\n",
            "Training epoch 4179/1000000, d_loss: -273.3407897949219,  g_loss: 417.37689208984375\n",
            "Training epoch 4180/1000000, d_loss: -58.397682189941406,  g_loss: 93.75904083251953\n",
            "Training epoch 4181/1000000, d_loss: -125.46000671386719,  g_loss: 264.65313720703125\n",
            "Training epoch 4182/1000000, d_loss: -26.027484893798828,  g_loss: 143.50418090820312\n",
            "Training epoch 4183/1000000, d_loss: -98.66487121582031,  g_loss: 235.19924926757812\n",
            "Training epoch 4184/1000000, d_loss: -116.8070297241211,  g_loss: 166.77792358398438\n",
            "Training epoch 4185/1000000, d_loss: -93.89324951171875,  g_loss: 180.99807739257812\n",
            "Training epoch 4186/1000000, d_loss: -461.0069580078125,  g_loss: -3.427248001098633\n",
            "Training epoch 4187/1000000, d_loss: 45.045982360839844,  g_loss: 26.49448013305664\n",
            "Training epoch 4188/1000000, d_loss: -247.73861694335938,  g_loss: -25.6784610748291\n",
            "Training epoch 4189/1000000, d_loss: -110.78997802734375,  g_loss: -33.012943267822266\n",
            "Training epoch 4190/1000000, d_loss: -47.499061584472656,  g_loss: -3.8296213150024414\n",
            "Training epoch 4191/1000000, d_loss: -65.1165771484375,  g_loss: 5.505768299102783\n",
            "Training epoch 4192/1000000, d_loss: -79.0241928100586,  g_loss: 30.686697006225586\n",
            "Training epoch 4193/1000000, d_loss: -84.2516860961914,  g_loss: 14.494078636169434\n",
            "Training epoch 4194/1000000, d_loss: -232.8235321044922,  g_loss: -155.32131958007812\n",
            "Training epoch 4195/1000000, d_loss: -342.20941162109375,  g_loss: -41.76789855957031\n",
            "Training epoch 4196/1000000, d_loss: -64.92632293701172,  g_loss: 69.2310791015625\n",
            "Training epoch 4197/1000000, d_loss: -59.98163604736328,  g_loss: 111.4391098022461\n",
            "Training epoch 4198/1000000, d_loss: -275.7872314453125,  g_loss: -79.35884094238281\n",
            "Training epoch 4199/1000000, d_loss: -101.95829772949219,  g_loss: 86.70235443115234\n",
            "Training epoch 4200/1000000, d_loss: -110.71453094482422,  g_loss: -41.33937072753906\n",
            "Training epoch 4201/1000000, d_loss: -307.8087463378906,  g_loss: 256.848388671875\n",
            "Training epoch 4202/1000000, d_loss: -33.85697937011719,  g_loss: -78.04988098144531\n",
            "Training epoch 4203/1000000, d_loss: -115.7716064453125,  g_loss: 64.57714080810547\n",
            "Training epoch 4204/1000000, d_loss: -192.75123596191406,  g_loss: -64.08414459228516\n",
            "Training epoch 4205/1000000, d_loss: -20.359590530395508,  g_loss: 6.134603023529053\n",
            "Training epoch 4206/1000000, d_loss: -481.15185546875,  g_loss: -125.10680389404297\n",
            "Training epoch 4207/1000000, d_loss: -328.1639404296875,  g_loss: -44.1820068359375\n",
            "Training epoch 4208/1000000, d_loss: -25.583730697631836,  g_loss: 61.874351501464844\n",
            "Training epoch 4209/1000000, d_loss: -358.8966369628906,  g_loss: 19.194852828979492\n",
            "Training epoch 4210/1000000, d_loss: -103.96598815917969,  g_loss: 167.17575073242188\n",
            "Training epoch 4211/1000000, d_loss: -135.11703491210938,  g_loss: -59.41545104980469\n",
            "Training epoch 4212/1000000, d_loss: -33.7263069152832,  g_loss: 43.01869201660156\n",
            "Training epoch 4213/1000000, d_loss: -378.87890625,  g_loss: 338.3599548339844\n",
            "Training epoch 4214/1000000, d_loss: 76.55352020263672,  g_loss: -10.914691925048828\n",
            "Training epoch 4215/1000000, d_loss: -30.489879608154297,  g_loss: 38.2854118347168\n",
            "Training epoch 4216/1000000, d_loss: -42.14271545410156,  g_loss: 23.937084197998047\n",
            "Training epoch 4217/1000000, d_loss: 16.280065536499023,  g_loss: 38.92369079589844\n",
            "Training epoch 4218/1000000, d_loss: 5.475353240966797,  g_loss: 25.929166793823242\n",
            "Training epoch 4219/1000000, d_loss: -54.91542053222656,  g_loss: 0.7893772125244141\n",
            "Training epoch 4220/1000000, d_loss: -32.42730712890625,  g_loss: 80.62826538085938\n",
            "Training epoch 4221/1000000, d_loss: 1332.3974609375,  g_loss: 49.66748046875\n",
            "Training epoch 4222/1000000, d_loss: -64.89635467529297,  g_loss: 111.2216796875\n",
            "Training epoch 4223/1000000, d_loss: -73.55585479736328,  g_loss: 106.68388366699219\n",
            "Training epoch 4224/1000000, d_loss: -32.863494873046875,  g_loss: 39.47079086303711\n",
            "Training epoch 4225/1000000, d_loss: -45.6706428527832,  g_loss: 37.4201545715332\n",
            "Training epoch 4226/1000000, d_loss: -137.1621856689453,  g_loss: -2.8777284622192383\n",
            "Training epoch 4227/1000000, d_loss: -149.6955108642578,  g_loss: -36.7027473449707\n",
            "Training epoch 4228/1000000, d_loss: -33.53712844848633,  g_loss: 61.38910675048828\n",
            "Training epoch 4229/1000000, d_loss: -322.2744140625,  g_loss: -1.837730884552002\n",
            "Training epoch 4230/1000000, d_loss: -288.0940246582031,  g_loss: -329.0194091796875\n",
            "Training epoch 4231/1000000, d_loss: -71.5914306640625,  g_loss: 21.648571014404297\n",
            "Training epoch 4232/1000000, d_loss: -82.42129516601562,  g_loss: 97.82579040527344\n",
            "Training epoch 4233/1000000, d_loss: -119.05754852294922,  g_loss: 7.901050567626953\n",
            "Training epoch 4234/1000000, d_loss: -77.8861083984375,  g_loss: 104.03047180175781\n",
            "Training epoch 4235/1000000, d_loss: -359.90313720703125,  g_loss: 19.583995819091797\n",
            "Training epoch 4236/1000000, d_loss: -981.23291015625,  g_loss: -543.0698852539062\n",
            "Training epoch 4237/1000000, d_loss: 238.8166046142578,  g_loss: -89.86713409423828\n",
            "Training epoch 4238/1000000, d_loss: -79.82120513916016,  g_loss: 103.52118682861328\n",
            "Training epoch 4239/1000000, d_loss: -235.56155395507812,  g_loss: 353.8143310546875\n",
            "Training epoch 4240/1000000, d_loss: -352.16534423828125,  g_loss: 464.91632080078125\n",
            "Training epoch 4241/1000000, d_loss: -176.48641967773438,  g_loss: 293.0014343261719\n",
            "Training epoch 4242/1000000, d_loss: 20.959096908569336,  g_loss: 140.9248046875\n",
            "Training epoch 4243/1000000, d_loss: -4.172512054443359,  g_loss: 94.83320617675781\n",
            "Training epoch 4244/1000000, d_loss: -105.68438720703125,  g_loss: 105.88856506347656\n",
            "Training epoch 4245/1000000, d_loss: -917.302490234375,  g_loss: -312.0843200683594\n",
            "Training epoch 4246/1000000, d_loss: -14.131782531738281,  g_loss: -48.12733459472656\n",
            "Training epoch 4247/1000000, d_loss: -411.36822509765625,  g_loss: -57.24017333984375\n",
            "Training epoch 4248/1000000, d_loss: -167.43685913085938,  g_loss: 118.28128051757812\n",
            "Training epoch 4249/1000000, d_loss: -252.0553741455078,  g_loss: 479.24462890625\n",
            "Training epoch 4250/1000000, d_loss: -80.93583679199219,  g_loss: 234.4006805419922\n",
            "Training epoch 4251/1000000, d_loss: -268.46002197265625,  g_loss: 637.4090576171875\n",
            "Training epoch 4252/1000000, d_loss: -93.4363784790039,  g_loss: 227.1381378173828\n",
            "Training epoch 4253/1000000, d_loss: 9.965906143188477,  g_loss: 252.79718017578125\n",
            "Training epoch 4254/1000000, d_loss: -95.91580200195312,  g_loss: 391.4087829589844\n",
            "Training epoch 4255/1000000, d_loss: -154.29754638671875,  g_loss: 127.54823303222656\n",
            "Training epoch 4256/1000000, d_loss: -44.61820983886719,  g_loss: 222.92599487304688\n",
            "Training epoch 4257/1000000, d_loss: -89.52455139160156,  g_loss: 174.61526489257812\n",
            "Training epoch 4258/1000000, d_loss: -260.5056457519531,  g_loss: 316.9585266113281\n",
            "Training epoch 4259/1000000, d_loss: -34.219818115234375,  g_loss: 185.02688598632812\n",
            "Training epoch 4260/1000000, d_loss: -9.842910766601562,  g_loss: 173.21084594726562\n",
            "Training epoch 4261/1000000, d_loss: -117.82738494873047,  g_loss: 176.7225341796875\n",
            "Training epoch 4262/1000000, d_loss: -25.59893035888672,  g_loss: 96.22978210449219\n",
            "Training epoch 4263/1000000, d_loss: -105.11944580078125,  g_loss: 95.13249206542969\n",
            "Training epoch 4264/1000000, d_loss: -188.8298797607422,  g_loss: 141.76075744628906\n",
            "Training epoch 4265/1000000, d_loss: -248.2379913330078,  g_loss: 95.57466125488281\n",
            "Training epoch 4266/1000000, d_loss: -72.83458709716797,  g_loss: 40.64398956298828\n",
            "Training epoch 4267/1000000, d_loss: 6.222866058349609,  g_loss: 106.03821563720703\n",
            "Training epoch 4268/1000000, d_loss: -39.15094757080078,  g_loss: 198.8347930908203\n",
            "Training epoch 4269/1000000, d_loss: -80.12805938720703,  g_loss: 138.65185546875\n",
            "Training epoch 4270/1000000, d_loss: -27.503402709960938,  g_loss: 99.73733520507812\n",
            "Training epoch 4271/1000000, d_loss: -109.28392028808594,  g_loss: 33.067962646484375\n",
            "Training epoch 4272/1000000, d_loss: -191.01397705078125,  g_loss: 76.58272552490234\n",
            "Training epoch 4273/1000000, d_loss: 106.02273559570312,  g_loss: 113.26319885253906\n",
            "Training epoch 4274/1000000, d_loss: -46.50636291503906,  g_loss: 79.20547485351562\n",
            "Training epoch 4275/1000000, d_loss: -15.292476654052734,  g_loss: 129.01040649414062\n",
            "Training epoch 4276/1000000, d_loss: -171.57713317871094,  g_loss: 14.514997482299805\n",
            "Training epoch 4277/1000000, d_loss: -257.2014465332031,  g_loss: -34.57744216918945\n",
            "Training epoch 4278/1000000, d_loss: -58.416255950927734,  g_loss: 81.75031280517578\n",
            "Training epoch 4279/1000000, d_loss: -106.10932922363281,  g_loss: 131.80776977539062\n",
            "Training epoch 4280/1000000, d_loss: -108.81658172607422,  g_loss: 98.70571899414062\n",
            "Training epoch 4281/1000000, d_loss: -438.302734375,  g_loss: -49.864437103271484\n",
            "Training epoch 4282/1000000, d_loss: -5.535186767578125,  g_loss: 79.8585433959961\n",
            "Training epoch 4283/1000000, d_loss: -75.25971984863281,  g_loss: 36.40046691894531\n",
            "Training epoch 4284/1000000, d_loss: -40.943565368652344,  g_loss: -15.862581253051758\n",
            "Training epoch 4285/1000000, d_loss: -57.002960205078125,  g_loss: 1.4309864044189453\n",
            "Training epoch 4286/1000000, d_loss: -42.523006439208984,  g_loss: 44.00450134277344\n",
            "Training epoch 4287/1000000, d_loss: -46.538047790527344,  g_loss: -4.96934700012207\n",
            "Training epoch 4288/1000000, d_loss: -122.74483489990234,  g_loss: 108.20690155029297\n",
            "Training epoch 4289/1000000, d_loss: -85.05953979492188,  g_loss: 48.96485900878906\n",
            "Training epoch 4290/1000000, d_loss: -194.52548217773438,  g_loss: 79.09207916259766\n",
            "Training epoch 4291/1000000, d_loss: -161.41806030273438,  g_loss: 134.38490295410156\n",
            "Training epoch 4292/1000000, d_loss: -156.712158203125,  g_loss: 175.93014526367188\n",
            "Training epoch 4293/1000000, d_loss: -54.35749816894531,  g_loss: 167.63320922851562\n",
            "Training epoch 4294/1000000, d_loss: -32.322845458984375,  g_loss: 158.9580841064453\n",
            "Training epoch 4295/1000000, d_loss: 12.939566612243652,  g_loss: 126.7769775390625\n",
            "Training epoch 4296/1000000, d_loss: -111.18028259277344,  g_loss: 99.15972900390625\n",
            "Training epoch 4297/1000000, d_loss: -378.3042297363281,  g_loss: 56.00362777709961\n",
            "Training epoch 4298/1000000, d_loss: -25.30319595336914,  g_loss: 117.67227172851562\n",
            "Training epoch 4299/1000000, d_loss: -30.655481338500977,  g_loss: 119.45100402832031\n",
            "Training epoch 4300/1000000, d_loss: -497.49285888671875,  g_loss: -102.90682983398438\n",
            "Training epoch 4301/1000000, d_loss: -1287.738525390625,  g_loss: -260.6624755859375\n",
            "Training epoch 4302/1000000, d_loss: -189.715576171875,  g_loss: 2.7691879272460938\n",
            "Training epoch 4303/1000000, d_loss: 16.51287078857422,  g_loss: 135.97337341308594\n",
            "Training epoch 4304/1000000, d_loss: 0.72357177734375,  g_loss: 185.22593688964844\n",
            "Training epoch 4305/1000000, d_loss: -46.31813049316406,  g_loss: 196.6577606201172\n",
            "Training epoch 4306/1000000, d_loss: -91.29476928710938,  g_loss: 364.79022216796875\n",
            "Training epoch 4307/1000000, d_loss: -30.818918228149414,  g_loss: 291.76129150390625\n",
            "Training epoch 4308/1000000, d_loss: 3.1143417358398438,  g_loss: 183.11061096191406\n",
            "Training epoch 4309/1000000, d_loss: -30.213884353637695,  g_loss: 226.42333984375\n",
            "Training epoch 4310/1000000, d_loss: -16.842247009277344,  g_loss: 210.16513061523438\n",
            "Training epoch 4311/1000000, d_loss: 13.266658782958984,  g_loss: 215.01873779296875\n",
            "Training epoch 4312/1000000, d_loss: -424.88812255859375,  g_loss: 25.57998275756836\n",
            "Training epoch 4313/1000000, d_loss: 20.99947166442871,  g_loss: 85.38845825195312\n",
            "Training epoch 4314/1000000, d_loss: -85.62321472167969,  g_loss: 102.70838928222656\n",
            "Training epoch 4315/1000000, d_loss: -85.16609954833984,  g_loss: 92.00752258300781\n",
            "Training epoch 4316/1000000, d_loss: -276.34942626953125,  g_loss: -125.36842346191406\n",
            "Training epoch 4317/1000000, d_loss: -222.794921875,  g_loss: 516.8973388671875\n",
            "Training epoch 4318/1000000, d_loss: -606.678466796875,  g_loss: 825.7577514648438\n",
            "Training epoch 4319/1000000, d_loss: -372.92706298828125,  g_loss: 216.2186737060547\n",
            "Training epoch 4320/1000000, d_loss: -19.546039581298828,  g_loss: 213.5316162109375\n",
            "Training epoch 4321/1000000, d_loss: -410.2231750488281,  g_loss: -187.02249145507812\n",
            "Training epoch 4322/1000000, d_loss: -18.103221893310547,  g_loss: 11.186256408691406\n",
            "Training epoch 4323/1000000, d_loss: -155.83935546875,  g_loss: 171.61868286132812\n",
            "Training epoch 4324/1000000, d_loss: -274.23687744140625,  g_loss: 540.1670532226562\n",
            "Training epoch 4325/1000000, d_loss: -155.82862854003906,  g_loss: 262.7471008300781\n",
            "Training epoch 4326/1000000, d_loss: -147.78977966308594,  g_loss: 629.8070068359375\n",
            "Training epoch 4327/1000000, d_loss: -26.105531692504883,  g_loss: 305.9619445800781\n",
            "Training epoch 4328/1000000, d_loss: -117.08838653564453,  g_loss: 291.43695068359375\n",
            "Training epoch 4329/1000000, d_loss: -158.84315490722656,  g_loss: 113.71529388427734\n",
            "Training epoch 4330/1000000, d_loss: 43.34538650512695,  g_loss: -7.647700309753418\n",
            "Training epoch 4331/1000000, d_loss: -85.46234130859375,  g_loss: 79.74330139160156\n",
            "Training epoch 4332/1000000, d_loss: 21.35744285583496,  g_loss: 39.528114318847656\n",
            "Training epoch 4333/1000000, d_loss: -137.9034881591797,  g_loss: 18.32726287841797\n",
            "Training epoch 4334/1000000, d_loss: -433.2080993652344,  g_loss: -111.14392852783203\n",
            "Training epoch 4335/1000000, d_loss: -139.89892578125,  g_loss: -192.184326171875\n",
            "Training epoch 4336/1000000, d_loss: -46.638404846191406,  g_loss: 19.657611846923828\n",
            "Training epoch 4337/1000000, d_loss: -12.859500885009766,  g_loss: 27.487815856933594\n",
            "Training epoch 4338/1000000, d_loss: -58.45442199707031,  g_loss: 96.95525360107422\n",
            "Training epoch 4339/1000000, d_loss: -64.13993835449219,  g_loss: 130.1256561279297\n",
            "Training epoch 4340/1000000, d_loss: 10.024948120117188,  g_loss: 164.58360290527344\n",
            "Training epoch 4341/1000000, d_loss: -124.10627746582031,  g_loss: 130.45947265625\n",
            "Training epoch 4342/1000000, d_loss: -321.44390869140625,  g_loss: -63.242393493652344\n",
            "Training epoch 4343/1000000, d_loss: -318.6715087890625,  g_loss: 98.70398712158203\n",
            "Training epoch 4344/1000000, d_loss: -500.9515380859375,  g_loss: -12.84922981262207\n",
            "Training epoch 4345/1000000, d_loss: 9.995506286621094,  g_loss: 22.76767349243164\n",
            "Training epoch 4346/1000000, d_loss: 6.351192474365234,  g_loss: -29.40574836730957\n",
            "Training epoch 4347/1000000, d_loss: -118.99435424804688,  g_loss: 14.529684066772461\n",
            "Training epoch 4348/1000000, d_loss: -102.06605529785156,  g_loss: 50.04473876953125\n",
            "Training epoch 4349/1000000, d_loss: -44.758460998535156,  g_loss: -28.12394142150879\n",
            "Training epoch 4350/1000000, d_loss: -37.590606689453125,  g_loss: 33.793060302734375\n",
            "Training epoch 4351/1000000, d_loss: -295.67218017578125,  g_loss: -48.32115173339844\n",
            "Training epoch 4352/1000000, d_loss: -835.1519775390625,  g_loss: -324.8150939941406\n",
            "Training epoch 4353/1000000, d_loss: 878.3296508789062,  g_loss: 32.913631439208984\n",
            "Training epoch 4354/1000000, d_loss: -37.0498161315918,  g_loss: 88.3658676147461\n",
            "Training epoch 4355/1000000, d_loss: -95.61260223388672,  g_loss: 99.395263671875\n",
            "Training epoch 4356/1000000, d_loss: -94.1103286743164,  g_loss: 76.01660919189453\n",
            "Training epoch 4357/1000000, d_loss: -262.4263610839844,  g_loss: 221.34561157226562\n",
            "Training epoch 4358/1000000, d_loss: -421.9481201171875,  g_loss: -48.370765686035156\n",
            "Training epoch 4359/1000000, d_loss: -86.38086700439453,  g_loss: 86.1853256225586\n",
            "Training epoch 4360/1000000, d_loss: -102.82627868652344,  g_loss: -46.648773193359375\n",
            "Training epoch 4361/1000000, d_loss: -21.084739685058594,  g_loss: -47.079856872558594\n",
            "Training epoch 4362/1000000, d_loss: -152.54605102539062,  g_loss: -76.23779296875\n",
            "Training epoch 4363/1000000, d_loss: -158.34786987304688,  g_loss: 3.78151798248291\n",
            "Training epoch 4364/1000000, d_loss: -87.23925018310547,  g_loss: -4.254740238189697\n",
            "Training epoch 4365/1000000, d_loss: -377.7608337402344,  g_loss: -144.84927368164062\n",
            "Training epoch 4366/1000000, d_loss: 11.544967651367188,  g_loss: 0.4822521209716797\n",
            "Training epoch 4367/1000000, d_loss: 63.02351379394531,  g_loss: 48.704200744628906\n",
            "Training epoch 4368/1000000, d_loss: -134.75914001464844,  g_loss: 59.09272766113281\n",
            "Training epoch 4369/1000000, d_loss: -102.07948303222656,  g_loss: 38.947750091552734\n",
            "Training epoch 4370/1000000, d_loss: -188.22776794433594,  g_loss: 189.54360961914062\n",
            "Training epoch 4371/1000000, d_loss: -58.677223205566406,  g_loss: -26.103191375732422\n",
            "Training epoch 4372/1000000, d_loss: -240.36911010742188,  g_loss: -116.73617553710938\n",
            "Training epoch 4373/1000000, d_loss: -9.361492156982422,  g_loss: 60.12629699707031\n",
            "Training epoch 4374/1000000, d_loss: -26.70656967163086,  g_loss: 133.2605743408203\n",
            "Training epoch 4375/1000000, d_loss: -71.63143920898438,  g_loss: 56.55450439453125\n",
            "Training epoch 4376/1000000, d_loss: -87.68731689453125,  g_loss: 82.07234191894531\n",
            "Training epoch 4377/1000000, d_loss: -639.0088500976562,  g_loss: -252.3170623779297\n",
            "Training epoch 4378/1000000, d_loss: -37.60411834716797,  g_loss: 20.7347469329834\n",
            "Training epoch 4379/1000000, d_loss: -146.82383728027344,  g_loss: -11.133042335510254\n",
            "Training epoch 4380/1000000, d_loss: -62.077796936035156,  g_loss: 102.98513793945312\n",
            "Training epoch 4381/1000000, d_loss: -42.02294921875,  g_loss: 41.557464599609375\n",
            "Training epoch 4382/1000000, d_loss: -94.94678497314453,  g_loss: 172.2082977294922\n",
            "Training epoch 4383/1000000, d_loss: -240.81094360351562,  g_loss: 10.781591415405273\n",
            "Training epoch 4384/1000000, d_loss: 12.987824440002441,  g_loss: 27.31109619140625\n",
            "Training epoch 4385/1000000, d_loss: -8.16110610961914,  g_loss: 80.01168823242188\n",
            "Training epoch 4386/1000000, d_loss: -23.417497634887695,  g_loss: 50.15889358520508\n",
            "Training epoch 4387/1000000, d_loss: -42.13237762451172,  g_loss: 4.183403015136719\n",
            "Training epoch 4388/1000000, d_loss: -99.38802337646484,  g_loss: 38.31418991088867\n",
            "Training epoch 4389/1000000, d_loss: -98.59832000732422,  g_loss: 46.21802520751953\n",
            "Training epoch 4390/1000000, d_loss: -37.37293243408203,  g_loss: 19.299060821533203\n",
            "Training epoch 4391/1000000, d_loss: 0.8107147216796875,  g_loss: 67.40531921386719\n",
            "Training epoch 4392/1000000, d_loss: -92.15718078613281,  g_loss: 8.647006034851074\n",
            "Training epoch 4393/1000000, d_loss: -50.54175567626953,  g_loss: 82.51848602294922\n",
            "Training epoch 4394/1000000, d_loss: -98.42894744873047,  g_loss: -26.766725540161133\n",
            "Training epoch 4395/1000000, d_loss: -43.344295501708984,  g_loss: 8.422310829162598\n",
            "Training epoch 4396/1000000, d_loss: -116.9631576538086,  g_loss: 21.850727081298828\n",
            "Training epoch 4397/1000000, d_loss: -122.9183578491211,  g_loss: 57.59798049926758\n",
            "Training epoch 4398/1000000, d_loss: -650.915771484375,  g_loss: -185.0540313720703\n",
            "Training epoch 4399/1000000, d_loss: -104.62673950195312,  g_loss: -357.6994934082031\n",
            "Training epoch 4400/1000000, d_loss: -52.92769241333008,  g_loss: -111.75326538085938\n",
            "Training epoch 4401/1000000, d_loss: -84.45194244384766,  g_loss: -48.592506408691406\n",
            "Training epoch 4402/1000000, d_loss: -60.930755615234375,  g_loss: 16.035625457763672\n",
            "Training epoch 4403/1000000, d_loss: -179.3560791015625,  g_loss: -158.89144897460938\n",
            "Training epoch 4404/1000000, d_loss: 35.43491744995117,  g_loss: -62.02165222167969\n",
            "Training epoch 4405/1000000, d_loss: 14.5328369140625,  g_loss: -3.7595043182373047\n",
            "Training epoch 4406/1000000, d_loss: -90.8157958984375,  g_loss: -15.62607479095459\n",
            "Training epoch 4407/1000000, d_loss: -191.6434326171875,  g_loss: -126.9024429321289\n",
            "Training epoch 4408/1000000, d_loss: -85.27610778808594,  g_loss: 24.675809860229492\n",
            "Training epoch 4409/1000000, d_loss: -103.99325561523438,  g_loss: 15.615974426269531\n",
            "Training epoch 4410/1000000, d_loss: -146.87835693359375,  g_loss: 250.52267456054688\n",
            "Training epoch 4411/1000000, d_loss: -378.28973388671875,  g_loss: 493.6389465332031\n",
            "Training epoch 4412/1000000, d_loss: -103.03379821777344,  g_loss: -131.1348114013672\n",
            "Training epoch 4413/1000000, d_loss: -64.06153106689453,  g_loss: -132.15255737304688\n",
            "Training epoch 4414/1000000, d_loss: -366.50146484375,  g_loss: -381.9483947753906\n",
            "Training epoch 4415/1000000, d_loss: -9.467214584350586,  g_loss: -119.91404724121094\n",
            "Training epoch 4416/1000000, d_loss: -61.399208068847656,  g_loss: -189.9921875\n",
            "Training epoch 4417/1000000, d_loss: -49.96905517578125,  g_loss: -144.19613647460938\n",
            "Training epoch 4418/1000000, d_loss: -8.002033233642578,  g_loss: -52.11405563354492\n",
            "Training epoch 4419/1000000, d_loss: -190.36459350585938,  g_loss: -99.80484008789062\n",
            "Training epoch 4420/1000000, d_loss: -377.12713623046875,  g_loss: -131.5609588623047\n",
            "Training epoch 4421/1000000, d_loss: -151.429443359375,  g_loss: -79.81634521484375\n",
            "Training epoch 4422/1000000, d_loss: -244.2470703125,  g_loss: -189.57696533203125\n",
            "Training epoch 4423/1000000, d_loss: -96.39024353027344,  g_loss: -5.813764572143555\n",
            "Training epoch 4424/1000000, d_loss: -47.55570983886719,  g_loss: 35.90755844116211\n",
            "Training epoch 4425/1000000, d_loss: -120.7948989868164,  g_loss: 76.48307800292969\n",
            "Training epoch 4426/1000000, d_loss: 1064.77685546875,  g_loss: 119.1409683227539\n",
            "Training epoch 4427/1000000, d_loss: -3.2490768432617188,  g_loss: 168.50457763671875\n",
            "Training epoch 4428/1000000, d_loss: -96.33441162109375,  g_loss: 181.81951904296875\n",
            "Training epoch 4429/1000000, d_loss: -40.36016845703125,  g_loss: 161.2047882080078\n",
            "Training epoch 4430/1000000, d_loss: -67.43273162841797,  g_loss: 157.26373291015625\n",
            "Training epoch 4431/1000000, d_loss: -136.6943817138672,  g_loss: 132.12237548828125\n",
            "Training epoch 4432/1000000, d_loss: 13.800910949707031,  g_loss: 130.76181030273438\n",
            "Training epoch 4433/1000000, d_loss: -67.40328979492188,  g_loss: 123.50605773925781\n",
            "Training epoch 4434/1000000, d_loss: -21.65161895751953,  g_loss: 89.92655944824219\n",
            "Training epoch 4435/1000000, d_loss: -92.7052230834961,  g_loss: 115.82556915283203\n",
            "Training epoch 4436/1000000, d_loss: -188.01904296875,  g_loss: 15.68463134765625\n",
            "Training epoch 4437/1000000, d_loss: -117.58912658691406,  g_loss: 93.70333862304688\n",
            "Training epoch 4438/1000000, d_loss: -550.7935180664062,  g_loss: 54.09675216674805\n",
            "Training epoch 4439/1000000, d_loss: -18.562931060791016,  g_loss: 58.4439697265625\n",
            "Training epoch 4440/1000000, d_loss: -55.83417892456055,  g_loss: 58.345462799072266\n",
            "Training epoch 4441/1000000, d_loss: -209.37867736816406,  g_loss: -6.154998779296875\n",
            "Training epoch 4442/1000000, d_loss: -346.3988342285156,  g_loss: -42.16279220581055\n",
            "Training epoch 4443/1000000, d_loss: -93.79643249511719,  g_loss: 160.00213623046875\n",
            "Training epoch 4444/1000000, d_loss: -467.4728088378906,  g_loss: -4.547294616699219\n",
            "Training epoch 4445/1000000, d_loss: -424.25738525390625,  g_loss: -122.61758422851562\n",
            "Training epoch 4446/1000000, d_loss: -316.5362548828125,  g_loss: 290.69329833984375\n",
            "Training epoch 4447/1000000, d_loss: -297.0153503417969,  g_loss: 475.1783447265625\n",
            "Training epoch 4448/1000000, d_loss: -46.50061798095703,  g_loss: 73.17903137207031\n",
            "Training epoch 4449/1000000, d_loss: -279.95538330078125,  g_loss: 90.96572875976562\n",
            "Training epoch 4450/1000000, d_loss: -85.96781158447266,  g_loss: 79.04679870605469\n",
            "Training epoch 4451/1000000, d_loss: -363.5725402832031,  g_loss: 388.34698486328125\n",
            "Training epoch 4452/1000000, d_loss: -87.24568176269531,  g_loss: 14.949870109558105\n",
            "Training epoch 4453/1000000, d_loss: 12.938468933105469,  g_loss: 36.51410675048828\n",
            "Training epoch 4454/1000000, d_loss: 4.233253479003906,  g_loss: 46.38152313232422\n",
            "Training epoch 4455/1000000, d_loss: -16.074031829833984,  g_loss: -8.092964172363281\n",
            "Training epoch 4456/1000000, d_loss: -63.62242126464844,  g_loss: 172.1802978515625\n",
            "Training epoch 4457/1000000, d_loss: -19.199548721313477,  g_loss: 56.70088195800781\n",
            "Training epoch 4458/1000000, d_loss: -21.90700340270996,  g_loss: 76.49833679199219\n",
            "Training epoch 4459/1000000, d_loss: -120.06665802001953,  g_loss: 52.17247009277344\n",
            "Training epoch 4460/1000000, d_loss: -57.785858154296875,  g_loss: 17.60158920288086\n",
            "Training epoch 4461/1000000, d_loss: -207.34622192382812,  g_loss: -4.197537422180176\n",
            "Training epoch 4462/1000000, d_loss: -31.673030853271484,  g_loss: 41.74146270751953\n",
            "Training epoch 4463/1000000, d_loss: -399.3310546875,  g_loss: -99.02377319335938\n",
            "Training epoch 4464/1000000, d_loss: -18.31328582763672,  g_loss: 25.536331176757812\n",
            "Training epoch 4465/1000000, d_loss: -153.5807342529297,  g_loss: 27.436555862426758\n",
            "Training epoch 4466/1000000, d_loss: -267.2107238769531,  g_loss: -132.23548889160156\n",
            "Training epoch 4467/1000000, d_loss: -114.23054504394531,  g_loss: -48.11456298828125\n",
            "Training epoch 4468/1000000, d_loss: 2.126384735107422,  g_loss: 56.660377502441406\n",
            "Training epoch 4469/1000000, d_loss: -96.2421646118164,  g_loss: 75.87748718261719\n",
            "Training epoch 4470/1000000, d_loss: -93.32967376708984,  g_loss: 105.77874755859375\n",
            "Training epoch 4471/1000000, d_loss: -48.44044494628906,  g_loss: 108.61738586425781\n",
            "Training epoch 4472/1000000, d_loss: -51.93887710571289,  g_loss: 81.33432006835938\n",
            "Training epoch 4473/1000000, d_loss: -144.2295379638672,  g_loss: 184.60321044921875\n",
            "Training epoch 4474/1000000, d_loss: -7.807168960571289,  g_loss: 45.5301513671875\n",
            "Training epoch 4475/1000000, d_loss: -471.6379089355469,  g_loss: -101.84165954589844\n",
            "Training epoch 4476/1000000, d_loss: -59.102596282958984,  g_loss: -21.961130142211914\n",
            "Training epoch 4477/1000000, d_loss: -25.991607666015625,  g_loss: 51.148380279541016\n",
            "Training epoch 4478/1000000, d_loss: -763.1864013671875,  g_loss: -118.69644165039062\n",
            "Training epoch 4479/1000000, d_loss: -0.95196533203125,  g_loss: -131.05441284179688\n",
            "Training epoch 4480/1000000, d_loss: -179.62074279785156,  g_loss: -18.657390594482422\n",
            "Training epoch 4481/1000000, d_loss: -60.64240646362305,  g_loss: 130.21371459960938\n",
            "Training epoch 4482/1000000, d_loss: -27.938833236694336,  g_loss: 86.95536804199219\n",
            "Training epoch 4483/1000000, d_loss: -144.94332885742188,  g_loss: 219.61187744140625\n",
            "Training epoch 4484/1000000, d_loss: -49.86341094970703,  g_loss: 16.26388168334961\n",
            "Training epoch 4485/1000000, d_loss: -186.88687133789062,  g_loss: -89.44910430908203\n",
            "Training epoch 4486/1000000, d_loss: -118.2499771118164,  g_loss: 8.398052215576172\n",
            "Training epoch 4487/1000000, d_loss: -114.53363037109375,  g_loss: -49.277320861816406\n",
            "Training epoch 4488/1000000, d_loss: -57.94428253173828,  g_loss: -46.000823974609375\n",
            "Training epoch 4489/1000000, d_loss: -147.8289794921875,  g_loss: 285.45941162109375\n",
            "Training epoch 4490/1000000, d_loss: -140.9761505126953,  g_loss: 522.2184448242188\n",
            "Training epoch 4491/1000000, d_loss: -81.03656005859375,  g_loss: 95.17569732666016\n",
            "Training epoch 4492/1000000, d_loss: -43.99603271484375,  g_loss: 60.07283020019531\n",
            "Training epoch 4493/1000000, d_loss: -103.4573745727539,  g_loss: 95.57351684570312\n",
            "Training epoch 4494/1000000, d_loss: -140.8371124267578,  g_loss: 157.16506958007812\n",
            "Training epoch 4495/1000000, d_loss: -57.62773895263672,  g_loss: 121.75086975097656\n",
            "Training epoch 4496/1000000, d_loss: -107.8802261352539,  g_loss: 28.583885192871094\n",
            "Training epoch 4497/1000000, d_loss: -39.944068908691406,  g_loss: 52.548004150390625\n",
            "Training epoch 4498/1000000, d_loss: -91.39945983886719,  g_loss: 104.43762969970703\n",
            "Training epoch 4499/1000000, d_loss: -8.151931762695312,  g_loss: 35.37096405029297\n",
            "Training epoch 4500/1000000, d_loss: -699.4649047851562,  g_loss: -137.5325927734375\n",
            "Training epoch 4501/1000000, d_loss: -40.00485610961914,  g_loss: -89.6888656616211\n",
            "Training epoch 4502/1000000, d_loss: -173.010498046875,  g_loss: -175.4415283203125\n",
            "Training epoch 4503/1000000, d_loss: 297.0577087402344,  g_loss: -63.18684005737305\n",
            "Training epoch 4504/1000000, d_loss: -280.74200439453125,  g_loss: -212.71514892578125\n",
            "Training epoch 4505/1000000, d_loss: -14.907447814941406,  g_loss: -98.73860931396484\n",
            "Training epoch 4506/1000000, d_loss: -45.68114471435547,  g_loss: -19.111154556274414\n",
            "Training epoch 4507/1000000, d_loss: -55.86993408203125,  g_loss: -37.263370513916016\n",
            "Training epoch 4508/1000000, d_loss: -47.19598388671875,  g_loss: -9.082491874694824\n",
            "Training epoch 4509/1000000, d_loss: -92.39521789550781,  g_loss: -2.32456111907959\n",
            "Training epoch 4510/1000000, d_loss: -50.38154983520508,  g_loss: -53.20221710205078\n",
            "Training epoch 4511/1000000, d_loss: -59.07392120361328,  g_loss: 29.805477142333984\n",
            "Training epoch 4512/1000000, d_loss: -255.40972900390625,  g_loss: -44.768531799316406\n",
            "Training epoch 4513/1000000, d_loss: 51.11377716064453,  g_loss: 32.5533447265625\n",
            "Training epoch 4514/1000000, d_loss: -310.80352783203125,  g_loss: -133.97463989257812\n",
            "Training epoch 4515/1000000, d_loss: -26.597002029418945,  g_loss: 56.64155960083008\n",
            "Training epoch 4516/1000000, d_loss: -114.55064392089844,  g_loss: 249.2454833984375\n",
            "Training epoch 4517/1000000, d_loss: -118.51017761230469,  g_loss: -79.48296356201172\n",
            "Training epoch 4518/1000000, d_loss: -146.2632293701172,  g_loss: 42.33518600463867\n",
            "Training epoch 4519/1000000, d_loss: -263.1343078613281,  g_loss: -218.5553741455078\n",
            "Training epoch 4520/1000000, d_loss: 73.00292205810547,  g_loss: -6.415867805480957\n",
            "Training epoch 4521/1000000, d_loss: -328.8993225097656,  g_loss: -113.6925048828125\n",
            "Training epoch 4522/1000000, d_loss: -111.6592788696289,  g_loss: 27.979034423828125\n",
            "Training epoch 4523/1000000, d_loss: -33.14593505859375,  g_loss: 153.12994384765625\n",
            "Training epoch 4524/1000000, d_loss: -70.24018859863281,  g_loss: 81.49549102783203\n",
            "Training epoch 4525/1000000, d_loss: -140.01547241210938,  g_loss: 119.64578247070312\n",
            "Training epoch 4526/1000000, d_loss: -68.66864776611328,  g_loss: 55.76255798339844\n",
            "Training epoch 4527/1000000, d_loss: -240.35206604003906,  g_loss: -40.55046081542969\n",
            "Training epoch 4528/1000000, d_loss: -137.06735229492188,  g_loss: 137.16343688964844\n",
            "Training epoch 4529/1000000, d_loss: -146.0586700439453,  g_loss: 128.75685119628906\n",
            "Training epoch 4530/1000000, d_loss: -298.7596435546875,  g_loss: -39.829437255859375\n",
            "Training epoch 4531/1000000, d_loss: -129.9276123046875,  g_loss: 64.8616943359375\n",
            "Training epoch 4532/1000000, d_loss: -76.64947509765625,  g_loss: 115.15133666992188\n",
            "Training epoch 4533/1000000, d_loss: -59.216487884521484,  g_loss: 65.55441284179688\n",
            "Training epoch 4534/1000000, d_loss: -60.0435676574707,  g_loss: 3.3517141342163086\n",
            "Training epoch 4535/1000000, d_loss: -112.28656005859375,  g_loss: 76.21502685546875\n",
            "Training epoch 4536/1000000, d_loss: -277.0038757324219,  g_loss: -37.956077575683594\n",
            "Training epoch 4537/1000000, d_loss: -30.768245697021484,  g_loss: 38.01963806152344\n",
            "Training epoch 4538/1000000, d_loss: -434.73577880859375,  g_loss: -107.29891967773438\n",
            "Training epoch 4539/1000000, d_loss: -154.6849822998047,  g_loss: 46.18438720703125\n",
            "Training epoch 4540/1000000, d_loss: -59.561424255371094,  g_loss: 78.17169189453125\n",
            "Training epoch 4541/1000000, d_loss: -7.0289764404296875,  g_loss: 75.72640228271484\n",
            "Training epoch 4542/1000000, d_loss: -111.74162292480469,  g_loss: 58.694000244140625\n",
            "Training epoch 4543/1000000, d_loss: -111.96759796142578,  g_loss: 20.45416259765625\n",
            "Training epoch 4544/1000000, d_loss: -158.00625610351562,  g_loss: 19.0396728515625\n",
            "Training epoch 4545/1000000, d_loss: 8.708900451660156,  g_loss: -11.115470886230469\n",
            "Training epoch 4546/1000000, d_loss: -160.1422119140625,  g_loss: -36.17926025390625\n",
            "Training epoch 4547/1000000, d_loss: -128.7760467529297,  g_loss: 36.04140853881836\n",
            "Training epoch 4548/1000000, d_loss: -159.60916137695312,  g_loss: -56.505889892578125\n",
            "Training epoch 4549/1000000, d_loss: -80.94474029541016,  g_loss: -67.90510559082031\n",
            "Training epoch 4550/1000000, d_loss: -112.78733825683594,  g_loss: 56.938621520996094\n",
            "Training epoch 4551/1000000, d_loss: -129.4783477783203,  g_loss: -34.56843948364258\n",
            "Training epoch 4552/1000000, d_loss: 2327.943603515625,  g_loss: -183.11270141601562\n",
            "Training epoch 4553/1000000, d_loss: -185.69293212890625,  g_loss: -267.8144836425781\n",
            "Training epoch 4554/1000000, d_loss: 149.9539794921875,  g_loss: -190.90615844726562\n",
            "Training epoch 4555/1000000, d_loss: -108.58556365966797,  g_loss: -266.81500244140625\n",
            "Training epoch 4556/1000000, d_loss: -141.95962524414062,  g_loss: -121.50843811035156\n",
            "Training epoch 4557/1000000, d_loss: 67.62033081054688,  g_loss: 53.291831970214844\n",
            "Training epoch 4558/1000000, d_loss: -70.27922058105469,  g_loss: 64.01167297363281\n",
            "Training epoch 4559/1000000, d_loss: -236.40386962890625,  g_loss: 69.43426513671875\n",
            "Training epoch 4560/1000000, d_loss: -27.615501403808594,  g_loss: -183.1948699951172\n",
            "Training epoch 4561/1000000, d_loss: -137.251220703125,  g_loss: -82.94664001464844\n",
            "Training epoch 4562/1000000, d_loss: 12.293006896972656,  g_loss: -53.990413665771484\n",
            "Training epoch 4563/1000000, d_loss: -6.6095428466796875,  g_loss: 32.04813766479492\n",
            "Training epoch 4564/1000000, d_loss: -220.19400024414062,  g_loss: -13.3622407913208\n",
            "Training epoch 4565/1000000, d_loss: -96.07022094726562,  g_loss: -14.826435089111328\n",
            "Training epoch 4566/1000000, d_loss: -62.85375213623047,  g_loss: 22.28894805908203\n",
            "Training epoch 4567/1000000, d_loss: -17.067794799804688,  g_loss: -11.433588027954102\n",
            "Training epoch 4568/1000000, d_loss: -112.6913833618164,  g_loss: 56.912841796875\n",
            "Training epoch 4569/1000000, d_loss: -69.78035736083984,  g_loss: 18.344804763793945\n",
            "Training epoch 4570/1000000, d_loss: -611.9708251953125,  g_loss: -144.5784149169922\n",
            "Training epoch 4571/1000000, d_loss: -43.84599304199219,  g_loss: -42.84653854370117\n",
            "Training epoch 4572/1000000, d_loss: -10.268143653869629,  g_loss: -99.03217315673828\n",
            "Training epoch 4573/1000000, d_loss: -57.117530822753906,  g_loss: -37.39862060546875\n",
            "Training epoch 4574/1000000, d_loss: -750.93212890625,  g_loss: -112.82423400878906\n",
            "Training epoch 4575/1000000, d_loss: -238.15093994140625,  g_loss: -197.36973571777344\n",
            "Training epoch 4576/1000000, d_loss: 22.427440643310547,  g_loss: -224.50827026367188\n",
            "Training epoch 4577/1000000, d_loss: 435.90045166015625,  g_loss: -144.42428588867188\n",
            "Training epoch 4578/1000000, d_loss: -514.0314331054688,  g_loss: -116.790771484375\n",
            "Training epoch 4579/1000000, d_loss: -168.56719970703125,  g_loss: -268.95867919921875\n",
            "Training epoch 4580/1000000, d_loss: 7.753108978271484,  g_loss: -30.30453872680664\n",
            "Training epoch 4581/1000000, d_loss: -132.2833251953125,  g_loss: 308.6082458496094\n",
            "Training epoch 4582/1000000, d_loss: -271.16278076171875,  g_loss: 572.8336181640625\n",
            "Training epoch 4583/1000000, d_loss: -129.52723693847656,  g_loss: 239.26947021484375\n",
            "Training epoch 4584/1000000, d_loss: -485.7373046875,  g_loss: 1081.39013671875\n",
            "Training epoch 4585/1000000, d_loss: -35.014312744140625,  g_loss: 107.35200500488281\n",
            "Training epoch 4586/1000000, d_loss: -36.48602294921875,  g_loss: -0.9300937652587891\n",
            "Training epoch 4587/1000000, d_loss: -75.03012084960938,  g_loss: 20.197566986083984\n",
            "Training epoch 4588/1000000, d_loss: -114.14523315429688,  g_loss: 87.19650268554688\n",
            "Training epoch 4589/1000000, d_loss: -117.9158706665039,  g_loss: 120.82462310791016\n",
            "Training epoch 4590/1000000, d_loss: -206.435791015625,  g_loss: 85.0035400390625\n",
            "Training epoch 4591/1000000, d_loss: -81.63912963867188,  g_loss: 48.6049690246582\n",
            "Training epoch 4592/1000000, d_loss: -383.83831787109375,  g_loss: -75.7704086303711\n",
            "Training epoch 4593/1000000, d_loss: 30.317344665527344,  g_loss: 83.88001251220703\n",
            "Training epoch 4594/1000000, d_loss: -140.91064453125,  g_loss: 193.02700805664062\n",
            "Training epoch 4595/1000000, d_loss: -94.53402709960938,  g_loss: 261.21142578125\n",
            "Training epoch 4596/1000000, d_loss: -34.54779052734375,  g_loss: 75.09019470214844\n",
            "Training epoch 4597/1000000, d_loss: -102.31085205078125,  g_loss: 246.7352294921875\n",
            "Training epoch 4598/1000000, d_loss: -569.3133544921875,  g_loss: -62.60601806640625\n",
            "Training epoch 4599/1000000, d_loss: -65.00564575195312,  g_loss: -168.06044006347656\n",
            "Training epoch 4600/1000000, d_loss: 20.1185302734375,  g_loss: -30.03799819946289\n",
            "Training epoch 4601/1000000, d_loss: -194.11402893066406,  g_loss: 75.76586151123047\n",
            "Training epoch 4602/1000000, d_loss: -52.466835021972656,  g_loss: -11.842626571655273\n",
            "Training epoch 4603/1000000, d_loss: -224.14141845703125,  g_loss: -46.34498596191406\n",
            "Training epoch 4604/1000000, d_loss: -145.92417907714844,  g_loss: 32.820838928222656\n",
            "Training epoch 4605/1000000, d_loss: -65.71377563476562,  g_loss: 34.843788146972656\n",
            "Training epoch 4606/1000000, d_loss: -44.06816864013672,  g_loss: -24.947975158691406\n",
            "Training epoch 4607/1000000, d_loss: -71.65916442871094,  g_loss: 39.1683349609375\n",
            "Training epoch 4608/1000000, d_loss: -129.9826202392578,  g_loss: 9.04673957824707\n",
            "Training epoch 4609/1000000, d_loss: -106.51455688476562,  g_loss: -103.35039520263672\n",
            "Training epoch 4610/1000000, d_loss: -72.10943603515625,  g_loss: -87.466064453125\n",
            "Training epoch 4611/1000000, d_loss: -134.85650634765625,  g_loss: -129.229248046875\n",
            "Training epoch 4612/1000000, d_loss: -15.098041534423828,  g_loss: -159.50173950195312\n",
            "Training epoch 4613/1000000, d_loss: -119.09832763671875,  g_loss: -201.1655731201172\n",
            "Training epoch 4614/1000000, d_loss: -113.36714172363281,  g_loss: -159.37049865722656\n",
            "Training epoch 4615/1000000, d_loss: -790.503662109375,  g_loss: -710.81640625\n",
            "Training epoch 4616/1000000, d_loss: 211.33383178710938,  g_loss: -121.85649108886719\n",
            "Training epoch 4617/1000000, d_loss: 16.542316436767578,  g_loss: -117.79741668701172\n",
            "Training epoch 4618/1000000, d_loss: -77.93878936767578,  g_loss: -50.30767822265625\n",
            "Training epoch 4619/1000000, d_loss: -46.305419921875,  g_loss: -76.01206970214844\n",
            "Training epoch 4620/1000000, d_loss: -24.871726989746094,  g_loss: -81.30624389648438\n",
            "Training epoch 4621/1000000, d_loss: -86.52690887451172,  g_loss: -74.30149841308594\n",
            "Training epoch 4622/1000000, d_loss: -29.712936401367188,  g_loss: -52.305816650390625\n",
            "Training epoch 4623/1000000, d_loss: -138.8460235595703,  g_loss: 16.73365020751953\n",
            "Training epoch 4624/1000000, d_loss: -51.49980163574219,  g_loss: 0.7712157964706421\n",
            "Training epoch 4625/1000000, d_loss: 7.0149078369140625,  g_loss: -35.14518737792969\n",
            "Training epoch 4626/1000000, d_loss: -144.46511840820312,  g_loss: -10.259403228759766\n",
            "Training epoch 4627/1000000, d_loss: -415.75640869140625,  g_loss: 0.28173160552978516\n",
            "Training epoch 4628/1000000, d_loss: -114.50120544433594,  g_loss: -78.3668212890625\n",
            "Training epoch 4629/1000000, d_loss: -109.24996948242188,  g_loss: -67.6100082397461\n",
            "Training epoch 4630/1000000, d_loss: -212.85569763183594,  g_loss: 577.1796875\n",
            "Training epoch 4631/1000000, d_loss: -28.307533264160156,  g_loss: 56.76685333251953\n",
            "Training epoch 4632/1000000, d_loss: -25.19076156616211,  g_loss: -72.36587524414062\n",
            "Training epoch 4633/1000000, d_loss: -33.430335998535156,  g_loss: 35.811363220214844\n",
            "Training epoch 4634/1000000, d_loss: -66.09131622314453,  g_loss: -7.9429931640625\n",
            "Training epoch 4635/1000000, d_loss: -122.14991760253906,  g_loss: -1.716531753540039\n",
            "Training epoch 4636/1000000, d_loss: -112.39530944824219,  g_loss: 65.16422271728516\n",
            "Training epoch 4637/1000000, d_loss: -130.60067749023438,  g_loss: -7.360166549682617\n",
            "Training epoch 4638/1000000, d_loss: -29.24099349975586,  g_loss: 51.84564971923828\n",
            "Training epoch 4639/1000000, d_loss: -170.23013305664062,  g_loss: 308.8752746582031\n",
            "Training epoch 4640/1000000, d_loss: 20.47748565673828,  g_loss: 29.7503662109375\n",
            "Training epoch 4641/1000000, d_loss: -144.4906768798828,  g_loss: 26.791240692138672\n",
            "Training epoch 4642/1000000, d_loss: -88.14134216308594,  g_loss: 70.73604583740234\n",
            "Training epoch 4643/1000000, d_loss: -587.68310546875,  g_loss: -139.26307678222656\n",
            "Training epoch 4644/1000000, d_loss: -87.9767837524414,  g_loss: -99.59954833984375\n",
            "Training epoch 4645/1000000, d_loss: -82.9317626953125,  g_loss: 73.14177703857422\n",
            "Training epoch 4646/1000000, d_loss: -316.2740478515625,  g_loss: 228.0792999267578\n",
            "Training epoch 4647/1000000, d_loss: -44.449005126953125,  g_loss: 67.58966827392578\n",
            "Training epoch 4648/1000000, d_loss: -52.4945068359375,  g_loss: -10.300613403320312\n",
            "Training epoch 4649/1000000, d_loss: -70.27859497070312,  g_loss: 37.52090835571289\n",
            "Training epoch 4650/1000000, d_loss: -58.54560089111328,  g_loss: -16.595600128173828\n",
            "Training epoch 4651/1000000, d_loss: -246.63429260253906,  g_loss: -90.98521423339844\n",
            "Training epoch 4652/1000000, d_loss: -67.66130065917969,  g_loss: 1.579817771911621\n",
            "Training epoch 4653/1000000, d_loss: -437.08917236328125,  g_loss: -3.7872467041015625\n",
            "Training epoch 4654/1000000, d_loss: 37.381629943847656,  g_loss: -40.664154052734375\n",
            "Training epoch 4655/1000000, d_loss: -172.95724487304688,  g_loss: -52.65995788574219\n",
            "Training epoch 4656/1000000, d_loss: -53.89781951904297,  g_loss: 6.871973037719727\n",
            "Training epoch 4657/1000000, d_loss: -128.09539794921875,  g_loss: 18.6151123046875\n",
            "Training epoch 4658/1000000, d_loss: -111.89226531982422,  g_loss: -20.82204818725586\n",
            "Training epoch 4659/1000000, d_loss: -177.72654724121094,  g_loss: -201.62852478027344\n",
            "Training epoch 4660/1000000, d_loss: 51.556724548339844,  g_loss: 26.0355281829834\n",
            "Training epoch 4661/1000000, d_loss: -13.76749038696289,  g_loss: 50.27275085449219\n",
            "Training epoch 4662/1000000, d_loss: -118.34957885742188,  g_loss: 101.32728576660156\n",
            "Training epoch 4663/1000000, d_loss: -2.653839111328125,  g_loss: 118.07963562011719\n",
            "Training epoch 4664/1000000, d_loss: -241.13592529296875,  g_loss: -51.46134948730469\n",
            "Training epoch 4665/1000000, d_loss: -52.8990478515625,  g_loss: -4.257777214050293\n",
            "Training epoch 4666/1000000, d_loss: -124.26365661621094,  g_loss: 82.44383239746094\n",
            "Training epoch 4667/1000000, d_loss: -51.47105407714844,  g_loss: 74.39892578125\n",
            "Training epoch 4668/1000000, d_loss: -106.57454681396484,  g_loss: 35.28032684326172\n",
            "Training epoch 4669/1000000, d_loss: -434.0033264160156,  g_loss: -77.19062805175781\n",
            "Training epoch 4670/1000000, d_loss: -702.4129028320312,  g_loss: -104.25880432128906\n",
            "Training epoch 4671/1000000, d_loss: 116.19650268554688,  g_loss: 88.5137939453125\n",
            "Training epoch 4672/1000000, d_loss: 8.431747436523438,  g_loss: 113.15544128417969\n",
            "Training epoch 4673/1000000, d_loss: -192.767822265625,  g_loss: 76.3062744140625\n",
            "Training epoch 4674/1000000, d_loss: -16.069812774658203,  g_loss: 143.21829223632812\n",
            "Training epoch 4675/1000000, d_loss: -31.474742889404297,  g_loss: 80.1436538696289\n",
            "Training epoch 4676/1000000, d_loss: -64.62895202636719,  g_loss: 72.36032104492188\n",
            "Training epoch 4677/1000000, d_loss: -35.84908676147461,  g_loss: 88.20784759521484\n",
            "Training epoch 4678/1000000, d_loss: -47.792091369628906,  g_loss: 66.8350830078125\n",
            "Training epoch 4679/1000000, d_loss: -113.52135467529297,  g_loss: -59.552337646484375\n",
            "Training epoch 4680/1000000, d_loss: -105.78828430175781,  g_loss: 63.888824462890625\n",
            "Training epoch 4681/1000000, d_loss: -88.63574981689453,  g_loss: 87.480712890625\n",
            "Training epoch 4682/1000000, d_loss: -12.809700012207031,  g_loss: 28.32306671142578\n",
            "Training epoch 4683/1000000, d_loss: -94.5281982421875,  g_loss: 27.26799774169922\n",
            "Training epoch 4684/1000000, d_loss: -82.8831787109375,  g_loss: 20.596153259277344\n",
            "Training epoch 4685/1000000, d_loss: -44.827056884765625,  g_loss: 24.609390258789062\n",
            "Training epoch 4686/1000000, d_loss: -40.343589782714844,  g_loss: 70.1093521118164\n",
            "Training epoch 4687/1000000, d_loss: -77.12596130371094,  g_loss: 191.75775146484375\n",
            "Training epoch 4688/1000000, d_loss: -134.8833465576172,  g_loss: 37.534698486328125\n",
            "Training epoch 4689/1000000, d_loss: 24.284393310546875,  g_loss: 17.198564529418945\n",
            "Training epoch 4690/1000000, d_loss: -236.8595733642578,  g_loss: -115.28396606445312\n",
            "Training epoch 4691/1000000, d_loss: -71.19812774658203,  g_loss: 88.9248046875\n",
            "Training epoch 4692/1000000, d_loss: -92.68888854980469,  g_loss: 133.81524658203125\n",
            "Training epoch 4693/1000000, d_loss: 11.567554473876953,  g_loss: 115.66849517822266\n",
            "Training epoch 4694/1000000, d_loss: -83.95352172851562,  g_loss: 117.95494842529297\n",
            "Training epoch 4695/1000000, d_loss: -99.27101135253906,  g_loss: 48.19845199584961\n",
            "Training epoch 4696/1000000, d_loss: -53.96342468261719,  g_loss: 68.78317260742188\n",
            "Training epoch 4697/1000000, d_loss: -161.41473388671875,  g_loss: -5.4280805587768555\n",
            "Training epoch 4698/1000000, d_loss: -134.5384063720703,  g_loss: 26.55225372314453\n",
            "Training epoch 4699/1000000, d_loss: -114.88514709472656,  g_loss: 197.9132537841797\n",
            "Training epoch 4700/1000000, d_loss: -350.4457702636719,  g_loss: 82.74678039550781\n",
            "Training epoch 4701/1000000, d_loss: -546.5465087890625,  g_loss: -192.89772033691406\n",
            "Training epoch 4702/1000000, d_loss: -14.805540084838867,  g_loss: -53.193782806396484\n",
            "Training epoch 4703/1000000, d_loss: -103.76416015625,  g_loss: 105.4827651977539\n",
            "Training epoch 4704/1000000, d_loss: -17.823314666748047,  g_loss: 47.25260925292969\n",
            "Training epoch 4705/1000000, d_loss: -46.96748352050781,  g_loss: 1.5549941062927246\n",
            "Training epoch 4706/1000000, d_loss: -52.14056396484375,  g_loss: 23.40081024169922\n",
            "Training epoch 4707/1000000, d_loss: -89.8103256225586,  g_loss: 85.14021301269531\n",
            "Training epoch 4708/1000000, d_loss: 60.37126159667969,  g_loss: 37.848758697509766\n",
            "Training epoch 4709/1000000, d_loss: -19.956289291381836,  g_loss: 25.985910415649414\n",
            "Training epoch 4710/1000000, d_loss: -48.455223083496094,  g_loss: 91.93626403808594\n",
            "Training epoch 4711/1000000, d_loss: -9.761592864990234,  g_loss: 61.920841217041016\n",
            "Training epoch 4712/1000000, d_loss: -193.78842163085938,  g_loss: 68.34451293945312\n",
            "Training epoch 4713/1000000, d_loss: -94.50405883789062,  g_loss: 46.70524978637695\n",
            "Training epoch 4714/1000000, d_loss: 3.8921470642089844,  g_loss: 60.995391845703125\n",
            "Training epoch 4715/1000000, d_loss: -258.96697998046875,  g_loss: 24.10901641845703\n",
            "Training epoch 4716/1000000, d_loss: -86.77660369873047,  g_loss: -18.03937530517578\n",
            "Training epoch 4717/1000000, d_loss: 3.9193954467773438,  g_loss: 22.777990341186523\n",
            "Training epoch 4718/1000000, d_loss: -99.62960052490234,  g_loss: -4.959442138671875\n",
            "Training epoch 4719/1000000, d_loss: -91.19819641113281,  g_loss: 45.66145324707031\n",
            "Training epoch 4720/1000000, d_loss: -302.7320556640625,  g_loss: -203.6538848876953\n",
            "Training epoch 4721/1000000, d_loss: -325.9437255859375,  g_loss: -230.3665008544922\n",
            "Training epoch 4722/1000000, d_loss: -413.06689453125,  g_loss: -326.7560119628906\n",
            "Training epoch 4723/1000000, d_loss: -50.939048767089844,  g_loss: 8.402395248413086\n",
            "Training epoch 4724/1000000, d_loss: -141.3564910888672,  g_loss: 85.6113510131836\n",
            "Training epoch 4725/1000000, d_loss: -72.68374633789062,  g_loss: 73.25308227539062\n",
            "Training epoch 4726/1000000, d_loss: -97.85228729248047,  g_loss: 125.30260467529297\n",
            "Training epoch 4727/1000000, d_loss: -155.50233459472656,  g_loss: 143.29617309570312\n",
            "Training epoch 4728/1000000, d_loss: -113.87977600097656,  g_loss: 74.47250366210938\n",
            "Training epoch 4729/1000000, d_loss: 4.914546966552734,  g_loss: 21.766847610473633\n",
            "Training epoch 4730/1000000, d_loss: -15.062255859375,  g_loss: 39.892616271972656\n",
            "Training epoch 4731/1000000, d_loss: -149.1593780517578,  g_loss: 107.68006134033203\n",
            "Training epoch 4732/1000000, d_loss: -128.2963409423828,  g_loss: 197.68899536132812\n",
            "Training epoch 4733/1000000, d_loss: -155.46072387695312,  g_loss: 199.96539306640625\n",
            "Training epoch 4734/1000000, d_loss: -240.58370971679688,  g_loss: 269.560546875\n",
            "Training epoch 4735/1000000, d_loss: -103.44874572753906,  g_loss: 198.8770751953125\n",
            "Training epoch 4736/1000000, d_loss: -64.80274963378906,  g_loss: 37.44563293457031\n",
            "Training epoch 4737/1000000, d_loss: -864.23583984375,  g_loss: -134.1744842529297\n",
            "Training epoch 4738/1000000, d_loss: -545.2933959960938,  g_loss: -252.70452880859375\n",
            "Training epoch 4739/1000000, d_loss: -218.74546813964844,  g_loss: -296.3572998046875\n",
            "Training epoch 4740/1000000, d_loss: -104.08831787109375,  g_loss: -145.7763214111328\n",
            "Training epoch 4741/1000000, d_loss: 90.61944580078125,  g_loss: -44.11181640625\n",
            "Training epoch 4742/1000000, d_loss: -92.80517578125,  g_loss: -44.772918701171875\n",
            "Training epoch 4743/1000000, d_loss: 20.42535400390625,  g_loss: -31.828960418701172\n",
            "Training epoch 4744/1000000, d_loss: -56.89369201660156,  g_loss: -61.769309997558594\n",
            "Training epoch 4745/1000000, d_loss: 14.66302490234375,  g_loss: -7.0372467041015625\n",
            "Training epoch 4746/1000000, d_loss: -3.8456974029541016,  g_loss: 33.84851837158203\n",
            "Training epoch 4747/1000000, d_loss: -75.04312133789062,  g_loss: -17.537277221679688\n",
            "Training epoch 4748/1000000, d_loss: -196.78506469726562,  g_loss: -89.0177001953125\n",
            "Training epoch 4749/1000000, d_loss: -1367.0501708984375,  g_loss: -217.4205780029297\n",
            "Training epoch 4750/1000000, d_loss: 5.513374328613281,  g_loss: -46.300899505615234\n",
            "Training epoch 4751/1000000, d_loss: -56.98646545410156,  g_loss: -29.82463264465332\n",
            "Training epoch 4752/1000000, d_loss: -24.82990264892578,  g_loss: 34.21794128417969\n",
            "Training epoch 4753/1000000, d_loss: -2.6420860290527344,  g_loss: 71.44395446777344\n",
            "Training epoch 4754/1000000, d_loss: 8.018217086791992,  g_loss: 37.05112838745117\n",
            "Training epoch 4755/1000000, d_loss: -67.55030822753906,  g_loss: 31.02153778076172\n",
            "Training epoch 4756/1000000, d_loss: 6.607215881347656,  g_loss: 22.048263549804688\n",
            "Training epoch 4757/1000000, d_loss: -132.94960021972656,  g_loss: 11.382195472717285\n",
            "Training epoch 4758/1000000, d_loss: -2.9157981872558594,  g_loss: 13.768243789672852\n",
            "Training epoch 4759/1000000, d_loss: -113.63472747802734,  g_loss: 8.5552396774292\n",
            "Training epoch 4760/1000000, d_loss: -108.917236328125,  g_loss: -4.9419450759887695\n",
            "Training epoch 4761/1000000, d_loss: -123.4145736694336,  g_loss: -7.283712387084961\n",
            "Training epoch 4762/1000000, d_loss: -111.51199340820312,  g_loss: 106.9867172241211\n",
            "Training epoch 4763/1000000, d_loss: -718.6622924804688,  g_loss: -78.77140808105469\n",
            "Training epoch 4764/1000000, d_loss: -20.11684799194336,  g_loss: 23.075273513793945\n",
            "Training epoch 4765/1000000, d_loss: -979.651611328125,  g_loss: -308.0389709472656\n",
            "Training epoch 4766/1000000, d_loss: 433.502685546875,  g_loss: -131.77247619628906\n",
            "Training epoch 4767/1000000, d_loss: -25.914390563964844,  g_loss: -13.081424713134766\n",
            "Training epoch 4768/1000000, d_loss: -55.00044631958008,  g_loss: 22.60340118408203\n",
            "Training epoch 4769/1000000, d_loss: -143.76055908203125,  g_loss: 38.417171478271484\n",
            "Training epoch 4770/1000000, d_loss: 22.080875396728516,  g_loss: 18.839492797851562\n",
            "Training epoch 4771/1000000, d_loss: 49.94568634033203,  g_loss: 53.74043273925781\n",
            "Training epoch 4772/1000000, d_loss: -105.67161560058594,  g_loss: 66.62324523925781\n",
            "Training epoch 4773/1000000, d_loss: 31.62965965270996,  g_loss: -11.84254264831543\n",
            "Training epoch 4774/1000000, d_loss: -30.258949279785156,  g_loss: 18.727689743041992\n",
            "Training epoch 4775/1000000, d_loss: -84.11835479736328,  g_loss: -8.92178726196289\n",
            "Training epoch 4776/1000000, d_loss: -56.82707214355469,  g_loss: 26.90906524658203\n",
            "Training epoch 4777/1000000, d_loss: -87.54953002929688,  g_loss: 65.27333068847656\n",
            "Training epoch 4778/1000000, d_loss: -39.85786437988281,  g_loss: 58.79517364501953\n",
            "Training epoch 4779/1000000, d_loss: -139.412353515625,  g_loss: -25.065153121948242\n",
            "Training epoch 4780/1000000, d_loss: -31.121599197387695,  g_loss: 1.1921377182006836\n",
            "Training epoch 4781/1000000, d_loss: -330.95062255859375,  g_loss: -38.50558090209961\n",
            "Training epoch 4782/1000000, d_loss: -156.78912353515625,  g_loss: 21.0793514251709\n",
            "Training epoch 4783/1000000, d_loss: -265.6652526855469,  g_loss: -204.76834106445312\n",
            "Training epoch 4784/1000000, d_loss: -51.94249725341797,  g_loss: 121.69288635253906\n",
            "Training epoch 4785/1000000, d_loss: -316.9411315917969,  g_loss: 298.8839111328125\n",
            "Training epoch 4786/1000000, d_loss: -464.27716064453125,  g_loss: 743.9988403320312\n",
            "Training epoch 4787/1000000, d_loss: -63.18772506713867,  g_loss: 57.270606994628906\n",
            "Training epoch 4788/1000000, d_loss: -281.1583557128906,  g_loss: 333.300537109375\n",
            "Training epoch 4789/1000000, d_loss: -109.3340072631836,  g_loss: 99.77436828613281\n",
            "Training epoch 4790/1000000, d_loss: -98.36589813232422,  g_loss: 233.2824249267578\n",
            "Training epoch 4791/1000000, d_loss: -57.50745391845703,  g_loss: 62.981082916259766\n",
            "Training epoch 4792/1000000, d_loss: -121.51557159423828,  g_loss: 56.70672607421875\n",
            "Training epoch 4793/1000000, d_loss: -85.18875122070312,  g_loss: -22.026458740234375\n",
            "Training epoch 4794/1000000, d_loss: -7.3517303466796875,  g_loss: -5.727535247802734\n",
            "Training epoch 4795/1000000, d_loss: -92.4355697631836,  g_loss: -33.20646667480469\n",
            "Training epoch 4796/1000000, d_loss: -997.654296875,  g_loss: -101.72148132324219\n",
            "Training epoch 4797/1000000, d_loss: -32.95418930053711,  g_loss: -97.08372497558594\n",
            "Training epoch 4798/1000000, d_loss: -437.563232421875,  g_loss: -156.00347900390625\n",
            "Training epoch 4799/1000000, d_loss: -44.144737243652344,  g_loss: -8.53264045715332\n",
            "Training epoch 4800/1000000, d_loss: -20.73809051513672,  g_loss: 29.8681640625\n",
            "Training epoch 4801/1000000, d_loss: -111.26611328125,  g_loss: 69.1608657836914\n",
            "Training epoch 4802/1000000, d_loss: -18.814125061035156,  g_loss: 32.53999710083008\n",
            "Training epoch 4803/1000000, d_loss: -262.3059997558594,  g_loss: -12.063056945800781\n",
            "Training epoch 4804/1000000, d_loss: -22.808277130126953,  g_loss: 58.69090270996094\n",
            "Training epoch 4805/1000000, d_loss: -185.22653198242188,  g_loss: -44.783470153808594\n",
            "Training epoch 4806/1000000, d_loss: -139.30503845214844,  g_loss: 67.48782348632812\n",
            "Training epoch 4807/1000000, d_loss: -520.1892700195312,  g_loss: -42.14708709716797\n",
            "Training epoch 4808/1000000, d_loss: -63.58122634887695,  g_loss: -63.57023239135742\n",
            "Training epoch 4809/1000000, d_loss: -89.96553039550781,  g_loss: 43.987060546875\n",
            "Training epoch 4810/1000000, d_loss: -157.28651428222656,  g_loss: -5.342889785766602\n",
            "Training epoch 4811/1000000, d_loss: -100.98697662353516,  g_loss: 78.80670928955078\n",
            "Training epoch 4812/1000000, d_loss: 10.179855346679688,  g_loss: 72.0451431274414\n",
            "Training epoch 4813/1000000, d_loss: -71.97994995117188,  g_loss: 132.59471130371094\n",
            "Training epoch 4814/1000000, d_loss: -114.15837097167969,  g_loss: 115.98641204833984\n",
            "Training epoch 4815/1000000, d_loss: -115.98551940917969,  g_loss: 170.87692260742188\n",
            "Training epoch 4816/1000000, d_loss: -115.34504699707031,  g_loss: 153.96180725097656\n",
            "Training epoch 4817/1000000, d_loss: -371.6207580566406,  g_loss: 5.483066558837891\n",
            "Training epoch 4818/1000000, d_loss: -8.256027221679688,  g_loss: 93.42459106445312\n",
            "Training epoch 4819/1000000, d_loss: -27.798995971679688,  g_loss: 25.970739364624023\n",
            "Training epoch 4820/1000000, d_loss: -260.8330078125,  g_loss: 96.90608215332031\n",
            "Training epoch 4821/1000000, d_loss: -203.55393981933594,  g_loss: 5.744535446166992\n",
            "Training epoch 4822/1000000, d_loss: -49.51422882080078,  g_loss: 52.12199401855469\n",
            "Training epoch 4823/1000000, d_loss: -8.41672134399414,  g_loss: 94.85440826416016\n",
            "Training epoch 4824/1000000, d_loss: -167.44212341308594,  g_loss: 127.32269287109375\n",
            "Training epoch 4825/1000000, d_loss: -220.84031677246094,  g_loss: 233.8105010986328\n",
            "Training epoch 4826/1000000, d_loss: -268.91845703125,  g_loss: -75.53215026855469\n",
            "Training epoch 4827/1000000, d_loss: -81.89326477050781,  g_loss: 9.560150146484375\n",
            "Training epoch 4828/1000000, d_loss: -68.69554138183594,  g_loss: 80.45578002929688\n",
            "Training epoch 4829/1000000, d_loss: -402.2952880859375,  g_loss: -186.50927734375\n",
            "Training epoch 4830/1000000, d_loss: -13.720821380615234,  g_loss: 77.50275421142578\n",
            "Training epoch 4831/1000000, d_loss: -18.875364303588867,  g_loss: 46.651222229003906\n",
            "Training epoch 4832/1000000, d_loss: -216.91812133789062,  g_loss: 36.63121032714844\n",
            "Training epoch 4833/1000000, d_loss: -223.4152374267578,  g_loss: 218.59555053710938\n",
            "Training epoch 4834/1000000, d_loss: -260.9144287109375,  g_loss: 255.92169189453125\n",
            "Training epoch 4835/1000000, d_loss: -8.695159912109375,  g_loss: 252.48435974121094\n",
            "Training epoch 4836/1000000, d_loss: -79.81128692626953,  g_loss: 119.01879119873047\n",
            "Training epoch 4837/1000000, d_loss: -81.01663208007812,  g_loss: 124.958984375\n",
            "Training epoch 4838/1000000, d_loss: -156.911865234375,  g_loss: 141.72283935546875\n",
            "Training epoch 4839/1000000, d_loss: -408.891845703125,  g_loss: -120.57818603515625\n",
            "Training epoch 4840/1000000, d_loss: -9.846792221069336,  g_loss: 229.05215454101562\n",
            "Training epoch 4841/1000000, d_loss: -128.20828247070312,  g_loss: 313.56622314453125\n",
            "Training epoch 4842/1000000, d_loss: -67.28346252441406,  g_loss: 258.14630126953125\n",
            "Training epoch 4843/1000000, d_loss: -247.27188110351562,  g_loss: 125.80870056152344\n",
            "Training epoch 4844/1000000, d_loss: 11.460777282714844,  g_loss: 163.12750244140625\n",
            "Training epoch 4845/1000000, d_loss: -15.430185317993164,  g_loss: 194.51194763183594\n",
            "Training epoch 4846/1000000, d_loss: -34.65156555175781,  g_loss: 144.22402954101562\n",
            "Training epoch 4847/1000000, d_loss: -131.0765380859375,  g_loss: 130.46151733398438\n",
            "Training epoch 4848/1000000, d_loss: -23.24396324157715,  g_loss: 103.07870483398438\n",
            "Training epoch 4849/1000000, d_loss: -192.04684448242188,  g_loss: 154.2157440185547\n",
            "Training epoch 4850/1000000, d_loss: -78.52882385253906,  g_loss: 152.9943389892578\n",
            "Training epoch 4851/1000000, d_loss: -120.01806640625,  g_loss: 174.32334899902344\n",
            "Training epoch 4852/1000000, d_loss: -112.80232238769531,  g_loss: 62.22884750366211\n",
            "Training epoch 4853/1000000, d_loss: -24.164594650268555,  g_loss: 259.294189453125\n",
            "Training epoch 4854/1000000, d_loss: -205.62416076660156,  g_loss: 65.40725708007812\n",
            "Training epoch 4855/1000000, d_loss: -132.43072509765625,  g_loss: 212.9516143798828\n",
            "Training epoch 4856/1000000, d_loss: -41.93552780151367,  g_loss: 259.73492431640625\n",
            "Training epoch 4857/1000000, d_loss: -119.15168762207031,  g_loss: 219.77337646484375\n",
            "Training epoch 4858/1000000, d_loss: -94.48723602294922,  g_loss: 194.05091857910156\n",
            "Training epoch 4859/1000000, d_loss: -26.996240615844727,  g_loss: 112.31580352783203\n",
            "Training epoch 4860/1000000, d_loss: -92.1585693359375,  g_loss: 121.15736389160156\n",
            "Training epoch 4861/1000000, d_loss: -69.08491516113281,  g_loss: 191.99118041992188\n",
            "Training epoch 4862/1000000, d_loss: -173.0633544921875,  g_loss: 71.50916290283203\n",
            "Training epoch 4863/1000000, d_loss: -358.4863586425781,  g_loss: 41.990577697753906\n",
            "Training epoch 4864/1000000, d_loss: -32.61296081542969,  g_loss: 272.32470703125\n",
            "Training epoch 4865/1000000, d_loss: -299.3232421875,  g_loss: 68.76702117919922\n",
            "Training epoch 4866/1000000, d_loss: -695.1829833984375,  g_loss: -13.911457061767578\n",
            "Training epoch 4867/1000000, d_loss: -16.142932891845703,  g_loss: -37.25810241699219\n",
            "Training epoch 4868/1000000, d_loss: -102.61228942871094,  g_loss: -138.2170867919922\n",
            "Training epoch 4869/1000000, d_loss: -23.73788833618164,  g_loss: 11.604549407958984\n",
            "Training epoch 4870/1000000, d_loss: -111.27183532714844,  g_loss: 153.03060913085938\n",
            "Training epoch 4871/1000000, d_loss: -139.28500366210938,  g_loss: 9.542400360107422\n",
            "Training epoch 4872/1000000, d_loss: -87.55963134765625,  g_loss: 37.91718292236328\n",
            "Training epoch 4873/1000000, d_loss: -141.26060485839844,  g_loss: 82.16046142578125\n",
            "Training epoch 4874/1000000, d_loss: -84.91307067871094,  g_loss: 133.94973754882812\n",
            "Training epoch 4875/1000000, d_loss: -295.64434814453125,  g_loss: 2.646055221557617\n",
            "Training epoch 4876/1000000, d_loss: -133.56005859375,  g_loss: 115.12284851074219\n",
            "Training epoch 4877/1000000, d_loss: -45.8170051574707,  g_loss: 220.552978515625\n",
            "Training epoch 4878/1000000, d_loss: -95.39521789550781,  g_loss: 244.02996826171875\n",
            "Training epoch 4879/1000000, d_loss: -75.54447937011719,  g_loss: 100.1004638671875\n",
            "Training epoch 4880/1000000, d_loss: -44.324241638183594,  g_loss: 96.16261291503906\n",
            "Training epoch 4881/1000000, d_loss: -109.30534362792969,  g_loss: 139.508544921875\n",
            "Training epoch 4882/1000000, d_loss: -10.446765899658203,  g_loss: 184.50521850585938\n",
            "Training epoch 4883/1000000, d_loss: -16.295259475708008,  g_loss: 150.64935302734375\n",
            "Training epoch 4884/1000000, d_loss: -48.98006820678711,  g_loss: 254.35028076171875\n",
            "Training epoch 4885/1000000, d_loss: -594.4993896484375,  g_loss: -121.76319885253906\n",
            "Training epoch 4886/1000000, d_loss: -12.057929992675781,  g_loss: -79.9409408569336\n",
            "Training epoch 4887/1000000, d_loss: -151.49002075195312,  g_loss: -88.14921569824219\n",
            "Training epoch 4888/1000000, d_loss: -94.3387451171875,  g_loss: -89.97563171386719\n",
            "Training epoch 4889/1000000, d_loss: -67.03604125976562,  g_loss: 24.064472198486328\n",
            "Training epoch 4890/1000000, d_loss: -233.62672424316406,  g_loss: 252.6848602294922\n",
            "Training epoch 4891/1000000, d_loss: 255.3143768310547,  g_loss: 20.591176986694336\n",
            "Training epoch 4892/1000000, d_loss: -129.85736083984375,  g_loss: 90.35220336914062\n",
            "Training epoch 4893/1000000, d_loss: -154.0825653076172,  g_loss: -35.090919494628906\n",
            "Training epoch 4894/1000000, d_loss: -86.75431823730469,  g_loss: -22.22272491455078\n",
            "Training epoch 4895/1000000, d_loss: -44.63759994506836,  g_loss: 24.50360679626465\n",
            "Training epoch 4896/1000000, d_loss: -37.14347839355469,  g_loss: 30.580551147460938\n",
            "Training epoch 4897/1000000, d_loss: -92.44660186767578,  g_loss: 72.5238037109375\n",
            "Training epoch 4898/1000000, d_loss: -194.59658813476562,  g_loss: 10.889361381530762\n",
            "Training epoch 4899/1000000, d_loss: -379.24285888671875,  g_loss: -227.4277801513672\n",
            "Training epoch 4900/1000000, d_loss: -133.03953552246094,  g_loss: -87.45121765136719\n",
            "Training epoch 4901/1000000, d_loss: -424.7408447265625,  g_loss: -411.07720947265625\n",
            "Training epoch 4902/1000000, d_loss: 66.17583465576172,  g_loss: 168.80665588378906\n",
            "Training epoch 4903/1000000, d_loss: -41.45130920410156,  g_loss: 181.12838745117188\n",
            "Training epoch 4904/1000000, d_loss: -219.251953125,  g_loss: 191.49893188476562\n",
            "Training epoch 4905/1000000, d_loss: -186.72183227539062,  g_loss: 796.5060424804688\n",
            "Training epoch 4906/1000000, d_loss: -88.83751678466797,  g_loss: 30.147226333618164\n",
            "Training epoch 4907/1000000, d_loss: -105.25347137451172,  g_loss: 67.42797088623047\n",
            "Training epoch 4908/1000000, d_loss: 3.7117538452148438,  g_loss: 20.057891845703125\n",
            "Training epoch 4909/1000000, d_loss: -113.85089111328125,  g_loss: 34.29496765136719\n",
            "Training epoch 4910/1000000, d_loss: -184.9367218017578,  g_loss: -96.20683288574219\n",
            "Training epoch 4911/1000000, d_loss: -31.0053653717041,  g_loss: 33.62066650390625\n",
            "Training epoch 4912/1000000, d_loss: -105.39824676513672,  g_loss: -0.06029987335205078\n",
            "Training epoch 4913/1000000, d_loss: -345.2001647949219,  g_loss: -118.91578674316406\n",
            "Training epoch 4914/1000000, d_loss: -105.90011596679688,  g_loss: 87.96368408203125\n",
            "Training epoch 4915/1000000, d_loss: -65.86544799804688,  g_loss: 81.07011413574219\n",
            "Training epoch 4916/1000000, d_loss: 12.877521514892578,  g_loss: 54.613609313964844\n",
            "Training epoch 4917/1000000, d_loss: -119.244873046875,  g_loss: 76.27967834472656\n",
            "Training epoch 4918/1000000, d_loss: -123.12435913085938,  g_loss: 156.55984497070312\n",
            "Training epoch 4919/1000000, d_loss: -152.00010681152344,  g_loss: 98.55484008789062\n",
            "Training epoch 4920/1000000, d_loss: -156.9915313720703,  g_loss: 114.58229064941406\n",
            "Training epoch 4921/1000000, d_loss: -115.21672821044922,  g_loss: -108.28413391113281\n",
            "Training epoch 4922/1000000, d_loss: -144.48373413085938,  g_loss: 176.41522216796875\n",
            "Training epoch 4923/1000000, d_loss: -22.906234741210938,  g_loss: 85.21846771240234\n",
            "Training epoch 4924/1000000, d_loss: -3.7067298889160156,  g_loss: 53.93720245361328\n",
            "Training epoch 4925/1000000, d_loss: -160.07278442382812,  g_loss: 249.6016845703125\n",
            "Training epoch 4926/1000000, d_loss: -197.04495239257812,  g_loss: 214.54847717285156\n",
            "Training epoch 4927/1000000, d_loss: -9.688743591308594,  g_loss: 130.45230102539062\n",
            "Training epoch 4928/1000000, d_loss: -125.83685302734375,  g_loss: 53.94554138183594\n",
            "Training epoch 4929/1000000, d_loss: -64.3146743774414,  g_loss: 53.16204833984375\n",
            "Training epoch 4930/1000000, d_loss: -118.93544006347656,  g_loss: -12.572525024414062\n",
            "Training epoch 4931/1000000, d_loss: -69.73236083984375,  g_loss: 200.78253173828125\n",
            "Training epoch 4932/1000000, d_loss: -546.5834350585938,  g_loss: -159.94937133789062\n",
            "Training epoch 4933/1000000, d_loss: -14.586261749267578,  g_loss: -60.23897933959961\n",
            "Training epoch 4934/1000000, d_loss: -74.22186279296875,  g_loss: 30.41741371154785\n",
            "Training epoch 4935/1000000, d_loss: 4.440008163452148,  g_loss: 46.204200744628906\n",
            "Training epoch 4936/1000000, d_loss: -74.87959289550781,  g_loss: -11.823046684265137\n",
            "Training epoch 4937/1000000, d_loss: -93.57760620117188,  g_loss: -4.614997863769531\n",
            "Training epoch 4938/1000000, d_loss: -39.05848693847656,  g_loss: 25.562055587768555\n",
            "Training epoch 4939/1000000, d_loss: -30.083023071289062,  g_loss: 64.12368774414062\n",
            "Training epoch 4940/1000000, d_loss: -46.03306579589844,  g_loss: 29.669971466064453\n",
            "Training epoch 4941/1000000, d_loss: -90.83639526367188,  g_loss: 44.79579162597656\n",
            "Training epoch 4942/1000000, d_loss: -89.0832290649414,  g_loss: 32.629173278808594\n",
            "Training epoch 4943/1000000, d_loss: -17.169404983520508,  g_loss: -16.954429626464844\n",
            "Training epoch 4944/1000000, d_loss: -72.84678649902344,  g_loss: -0.7364730834960938\n",
            "Training epoch 4945/1000000, d_loss: -28.51824951171875,  g_loss: -23.461091995239258\n",
            "Training epoch 4946/1000000, d_loss: -51.345054626464844,  g_loss: -12.066543579101562\n",
            "Training epoch 4947/1000000, d_loss: -54.573829650878906,  g_loss: 58.91264343261719\n",
            "Training epoch 4948/1000000, d_loss: -84.01969909667969,  g_loss: 30.736921310424805\n",
            "Training epoch 4949/1000000, d_loss: -6.303041458129883,  g_loss: 43.145835876464844\n",
            "Training epoch 4950/1000000, d_loss: -150.44577026367188,  g_loss: 12.309904098510742\n",
            "Training epoch 4951/1000000, d_loss: -31.040681838989258,  g_loss: 12.174403190612793\n",
            "Training epoch 4952/1000000, d_loss: -85.71338653564453,  g_loss: 9.3538818359375\n",
            "Training epoch 4953/1000000, d_loss: -40.290470123291016,  g_loss: -39.90888214111328\n",
            "Training epoch 4954/1000000, d_loss: -86.83106994628906,  g_loss: -42.93561553955078\n",
            "Training epoch 4955/1000000, d_loss: -220.47833251953125,  g_loss: -23.893796920776367\n",
            "Training epoch 4956/1000000, d_loss: -345.8941650390625,  g_loss: -222.67466735839844\n",
            "Training epoch 4957/1000000, d_loss: -152.54867553710938,  g_loss: 352.0989990234375\n",
            "Training epoch 4958/1000000, d_loss: -2186.399169921875,  g_loss: -774.9625854492188\n",
            "Training epoch 4959/1000000, d_loss: 141.2534942626953,  g_loss: -342.0648498535156\n",
            "Training epoch 4960/1000000, d_loss: -69.4931640625,  g_loss: 25.16607666015625\n",
            "Training epoch 4961/1000000, d_loss: -123.41830444335938,  g_loss: -40.546714782714844\n",
            "Training epoch 4962/1000000, d_loss: -33.12843322753906,  g_loss: 24.83320426940918\n",
            "Training epoch 4963/1000000, d_loss: 39.99127197265625,  g_loss: 30.950231552124023\n",
            "Training epoch 4964/1000000, d_loss: -76.86817169189453,  g_loss: 47.69989013671875\n",
            "Training epoch 4965/1000000, d_loss: -22.066593170166016,  g_loss: 0.2800006866455078\n",
            "Training epoch 4966/1000000, d_loss: 7.759529113769531,  g_loss: -75.80066680908203\n",
            "Training epoch 4967/1000000, d_loss: -34.13728332519531,  g_loss: -41.854957580566406\n",
            "Training epoch 4968/1000000, d_loss: -124.00291442871094,  g_loss: -55.66097640991211\n",
            "Training epoch 4969/1000000, d_loss: -66.76492309570312,  g_loss: 13.111688613891602\n",
            "Training epoch 4970/1000000, d_loss: -27.999244689941406,  g_loss: -50.1262321472168\n",
            "Training epoch 4971/1000000, d_loss: -65.1700439453125,  g_loss: 128.61672973632812\n",
            "Training epoch 4972/1000000, d_loss: -49.78413391113281,  g_loss: -26.245807647705078\n",
            "Training epoch 4973/1000000, d_loss: -82.87970733642578,  g_loss: 32.43333435058594\n",
            "Training epoch 4974/1000000, d_loss: -233.61895751953125,  g_loss: -100.18624877929688\n",
            "Training epoch 4975/1000000, d_loss: -185.34524536132812,  g_loss: -76.55355072021484\n",
            "Training epoch 4976/1000000, d_loss: -108.24160766601562,  g_loss: -141.2642364501953\n",
            "Training epoch 4977/1000000, d_loss: -228.91319274902344,  g_loss: 427.07275390625\n",
            "Training epoch 4978/1000000, d_loss: -67.44853210449219,  g_loss: 64.21196746826172\n",
            "Training epoch 4979/1000000, d_loss: -204.11642456054688,  g_loss: 36.0863151550293\n",
            "Training epoch 4980/1000000, d_loss: 14.103521347045898,  g_loss: 14.950557708740234\n",
            "Training epoch 4981/1000000, d_loss: -70.13986206054688,  g_loss: 68.4800033569336\n",
            "Training epoch 4982/1000000, d_loss: -112.68099212646484,  g_loss: 106.43101501464844\n",
            "Training epoch 4983/1000000, d_loss: -133.2749481201172,  g_loss: 81.7147216796875\n",
            "Training epoch 4984/1000000, d_loss: 55.57476043701172,  g_loss: 97.69144439697266\n",
            "Training epoch 4985/1000000, d_loss: -80.77302551269531,  g_loss: 100.02748107910156\n",
            "Training epoch 4986/1000000, d_loss: -135.07403564453125,  g_loss: 108.64603424072266\n",
            "Training epoch 4987/1000000, d_loss: -300.5281982421875,  g_loss: 17.282363891601562\n",
            "Training epoch 4988/1000000, d_loss: -109.53253936767578,  g_loss: 33.798919677734375\n",
            "Training epoch 4989/1000000, d_loss: -256.1415710449219,  g_loss: 0.12747764587402344\n",
            "Training epoch 4990/1000000, d_loss: -84.51387023925781,  g_loss: 117.93932342529297\n",
            "Training epoch 4991/1000000, d_loss: -71.07752227783203,  g_loss: 138.40843200683594\n",
            "Training epoch 4992/1000000, d_loss: -178.71168518066406,  g_loss: 64.12933349609375\n",
            "Training epoch 4993/1000000, d_loss: -8.846824645996094,  g_loss: 132.21270751953125\n",
            "Training epoch 4994/1000000, d_loss: -241.3028564453125,  g_loss: 64.96633911132812\n",
            "Training epoch 4995/1000000, d_loss: -80.46913146972656,  g_loss: 54.53369903564453\n",
            "Training epoch 4996/1000000, d_loss: -506.4530944824219,  g_loss: -44.40483856201172\n",
            "Training epoch 4997/1000000, d_loss: -316.61700439453125,  g_loss: -127.57981872558594\n",
            "Training epoch 4998/1000000, d_loss: 98.95667266845703,  g_loss: -12.404600143432617\n",
            "Training epoch 4999/1000000, d_loss: -101.78197479248047,  g_loss: -89.5653076171875\n",
            "Training epoch 5000/1000000, d_loss: -23.182769775390625,  g_loss: -52.84748840332031\n",
            "Training epoch 5001/1000000, d_loss: -448.7030944824219,  g_loss: -178.1355438232422\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 62/62 [00:00<00:00, 136.96it/s]\n",
            "Meshing: 100%|██████████| 25960/25960 [00:09<00:00, 2607.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_5001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_5001/assets\n",
            "Training epoch 5002/1000000, d_loss: 783.697021484375,  g_loss: -101.28712463378906\n",
            "Training epoch 5003/1000000, d_loss: 34.21227264404297,  g_loss: -120.12933349609375\n",
            "Training epoch 5004/1000000, d_loss: -704.630859375,  g_loss: -375.1179504394531\n",
            "Training epoch 5005/1000000, d_loss: -139.32647705078125,  g_loss: -213.15843200683594\n",
            "Training epoch 5006/1000000, d_loss: -69.27006530761719,  g_loss: -85.95721435546875\n",
            "Training epoch 5007/1000000, d_loss: -10.868440628051758,  g_loss: -138.64564514160156\n",
            "Training epoch 5008/1000000, d_loss: -160.2296142578125,  g_loss: 153.68971252441406\n",
            "Training epoch 5009/1000000, d_loss: -179.5712127685547,  g_loss: 222.026611328125\n",
            "Training epoch 5010/1000000, d_loss: -258.32952880859375,  g_loss: 257.16339111328125\n",
            "Training epoch 5011/1000000, d_loss: -11.21270751953125,  g_loss: 215.13958740234375\n",
            "Training epoch 5012/1000000, d_loss: -63.41438293457031,  g_loss: 99.333251953125\n",
            "Training epoch 5013/1000000, d_loss: -195.79483032226562,  g_loss: 262.0365905761719\n",
            "Training epoch 5014/1000000, d_loss: -44.87220001220703,  g_loss: 46.48754119873047\n",
            "Training epoch 5015/1000000, d_loss: -168.58763122558594,  g_loss: 212.25128173828125\n",
            "Training epoch 5016/1000000, d_loss: -35.44667053222656,  g_loss: 39.80830383300781\n",
            "Training epoch 5017/1000000, d_loss: -115.4107666015625,  g_loss: 143.33331298828125\n",
            "Training epoch 5018/1000000, d_loss: -170.5076446533203,  g_loss: 55.12323760986328\n",
            "Training epoch 5019/1000000, d_loss: -88.45901489257812,  g_loss: 229.01467895507812\n",
            "Training epoch 5020/1000000, d_loss: -230.08843994140625,  g_loss: 367.09246826171875\n",
            "Training epoch 5021/1000000, d_loss: -30.246261596679688,  g_loss: -19.699792861938477\n",
            "Training epoch 5022/1000000, d_loss: -96.15129089355469,  g_loss: 20.403467178344727\n",
            "Training epoch 5023/1000000, d_loss: -53.14323425292969,  g_loss: 47.64828872680664\n",
            "Training epoch 5024/1000000, d_loss: -60.481956481933594,  g_loss: 68.66226196289062\n",
            "Training epoch 5025/1000000, d_loss: -97.64382934570312,  g_loss: 19.835357666015625\n",
            "Training epoch 5026/1000000, d_loss: -120.68351745605469,  g_loss: 144.35853576660156\n",
            "Training epoch 5027/1000000, d_loss: -20.370988845825195,  g_loss: 163.07247924804688\n",
            "Training epoch 5028/1000000, d_loss: -75.17469024658203,  g_loss: 147.2235107421875\n",
            "Training epoch 5029/1000000, d_loss: -30.87700653076172,  g_loss: 16.94135093688965\n",
            "Training epoch 5030/1000000, d_loss: 20.01354217529297,  g_loss: 29.712677001953125\n",
            "Training epoch 5031/1000000, d_loss: -199.62220764160156,  g_loss: -133.030029296875\n",
            "Training epoch 5032/1000000, d_loss: -33.882225036621094,  g_loss: -54.127227783203125\n",
            "Training epoch 5033/1000000, d_loss: -1201.14111328125,  g_loss: -372.6397705078125\n",
            "Training epoch 5034/1000000, d_loss: -322.9976501464844,  g_loss: -50.993682861328125\n",
            "Training epoch 5035/1000000, d_loss: -194.66830444335938,  g_loss: -34.33277130126953\n",
            "Training epoch 5036/1000000, d_loss: -179.02349853515625,  g_loss: -13.646623611450195\n",
            "Training epoch 5037/1000000, d_loss: -165.37847900390625,  g_loss: -6.418203353881836\n",
            "Training epoch 5038/1000000, d_loss: -266.5531921386719,  g_loss: -31.19196891784668\n",
            "Training epoch 5039/1000000, d_loss: -154.01580810546875,  g_loss: -26.30005645751953\n",
            "Training epoch 5040/1000000, d_loss: -50.67119216918945,  g_loss: 135.53724670410156\n",
            "Training epoch 5041/1000000, d_loss: -74.28570556640625,  g_loss: 88.59806060791016\n",
            "Training epoch 5042/1000000, d_loss: -72.87413024902344,  g_loss: 41.40381622314453\n",
            "Training epoch 5043/1000000, d_loss: -118.96902465820312,  g_loss: -25.972457885742188\n",
            "Training epoch 5044/1000000, d_loss: -6.431343078613281,  g_loss: 61.70101547241211\n",
            "Training epoch 5045/1000000, d_loss: -72.19291687011719,  g_loss: 41.37699890136719\n",
            "Training epoch 5046/1000000, d_loss: -181.46958923339844,  g_loss: 20.73285675048828\n",
            "Training epoch 5047/1000000, d_loss: -130.63616943359375,  g_loss: 3.8496322631835938\n",
            "Training epoch 5048/1000000, d_loss: -53.86334228515625,  g_loss: 8.878620147705078\n",
            "Training epoch 5049/1000000, d_loss: -852.850341796875,  g_loss: -209.38858032226562\n",
            "Training epoch 5050/1000000, d_loss: 46.192291259765625,  g_loss: -80.9679183959961\n",
            "Training epoch 5051/1000000, d_loss: -134.53895568847656,  g_loss: -86.22073364257812\n",
            "Training epoch 5052/1000000, d_loss: -71.00450134277344,  g_loss: -50.937889099121094\n",
            "Training epoch 5053/1000000, d_loss: -89.72982788085938,  g_loss: 27.785720825195312\n",
            "Training epoch 5054/1000000, d_loss: -61.21543884277344,  g_loss: -22.817785263061523\n",
            "Training epoch 5055/1000000, d_loss: -35.97709274291992,  g_loss: -10.912250518798828\n",
            "Training epoch 5056/1000000, d_loss: -136.96263122558594,  g_loss: -78.15755462646484\n",
            "Training epoch 5057/1000000, d_loss: -69.98805236816406,  g_loss: -1.9619731903076172\n",
            "Training epoch 5058/1000000, d_loss: -44.29953384399414,  g_loss: -67.18804931640625\n",
            "Training epoch 5059/1000000, d_loss: -240.2285614013672,  g_loss: -148.7667236328125\n",
            "Training epoch 5060/1000000, d_loss: -18.576885223388672,  g_loss: -71.07180786132812\n",
            "Training epoch 5061/1000000, d_loss: -36.83612823486328,  g_loss: 19.613916397094727\n",
            "Training epoch 5062/1000000, d_loss: -67.49617767333984,  g_loss: 18.979076385498047\n",
            "Training epoch 5063/1000000, d_loss: -99.45826721191406,  g_loss: 34.537940979003906\n",
            "Training epoch 5064/1000000, d_loss: -138.24661254882812,  g_loss: 123.88780212402344\n",
            "Training epoch 5065/1000000, d_loss: -59.147056579589844,  g_loss: -48.31555938720703\n",
            "Training epoch 5066/1000000, d_loss: -538.03173828125,  g_loss: -157.17794799804688\n",
            "Training epoch 5067/1000000, d_loss: -58.37212371826172,  g_loss: -17.14130973815918\n",
            "Training epoch 5068/1000000, d_loss: -55.63340759277344,  g_loss: 60.84611892700195\n",
            "Training epoch 5069/1000000, d_loss: -45.39614486694336,  g_loss: 16.1937198638916\n",
            "Training epoch 5070/1000000, d_loss: -19.92626953125,  g_loss: 56.33153533935547\n",
            "Training epoch 5071/1000000, d_loss: -323.46514892578125,  g_loss: -50.81053161621094\n",
            "Training epoch 5072/1000000, d_loss: -1328.549560546875,  g_loss: -446.71832275390625\n",
            "Training epoch 5073/1000000, d_loss: 399.4937744140625,  g_loss: -75.29368591308594\n",
            "Training epoch 5074/1000000, d_loss: 70.782470703125,  g_loss: 67.77302551269531\n",
            "Training epoch 5075/1000000, d_loss: 515.405029296875,  g_loss: 295.492431640625\n",
            "Training epoch 5076/1000000, d_loss: -231.82821655273438,  g_loss: 234.92681884765625\n",
            "Training epoch 5077/1000000, d_loss: -850.696533203125,  g_loss: 149.715576171875\n",
            "Training epoch 5078/1000000, d_loss: 345.5560607910156,  g_loss: 159.61431884765625\n",
            "Training epoch 5079/1000000, d_loss: -821.365478515625,  g_loss: 1490.3271484375\n",
            "Training epoch 5080/1000000, d_loss: -153.37435913085938,  g_loss: 684.0323486328125\n",
            "Training epoch 5081/1000000, d_loss: -101.42546081542969,  g_loss: 265.25555419921875\n",
            "Training epoch 5082/1000000, d_loss: -48.87257385253906,  g_loss: 97.42090606689453\n",
            "Training epoch 5083/1000000, d_loss: -209.62684631347656,  g_loss: -23.07356071472168\n",
            "Training epoch 5084/1000000, d_loss: -70.87850952148438,  g_loss: -25.75889778137207\n",
            "Training epoch 5085/1000000, d_loss: 40.49909210205078,  g_loss: 88.57951354980469\n",
            "Training epoch 5086/1000000, d_loss: -162.5908203125,  g_loss: -110.22895812988281\n",
            "Training epoch 5087/1000000, d_loss: -93.79705810546875,  g_loss: -9.009819030761719\n",
            "Training epoch 5088/1000000, d_loss: -58.560302734375,  g_loss: 120.92036437988281\n",
            "Training epoch 5089/1000000, d_loss: 36.80604553222656,  g_loss: 35.59965515136719\n",
            "Training epoch 5090/1000000, d_loss: -81.42791748046875,  g_loss: 91.245849609375\n",
            "Training epoch 5091/1000000, d_loss: -9.046722412109375,  g_loss: 142.1776123046875\n",
            "Training epoch 5092/1000000, d_loss: -70.5023193359375,  g_loss: 262.19122314453125\n",
            "Training epoch 5093/1000000, d_loss: 3.2351112365722656,  g_loss: 91.40840148925781\n",
            "Training epoch 5094/1000000, d_loss: -118.88645935058594,  g_loss: 106.92488861083984\n",
            "Training epoch 5095/1000000, d_loss: 54.73388671875,  g_loss: 91.09547424316406\n",
            "Training epoch 5096/1000000, d_loss: -125.17984771728516,  g_loss: 77.73025512695312\n",
            "Training epoch 5097/1000000, d_loss: -100.5540771484375,  g_loss: 42.362586975097656\n",
            "Training epoch 5098/1000000, d_loss: -307.0639343261719,  g_loss: -92.54165649414062\n",
            "Training epoch 5099/1000000, d_loss: -983.8580322265625,  g_loss: -399.9190368652344\n",
            "Training epoch 5100/1000000, d_loss: -132.52906799316406,  g_loss: -556.2260131835938\n",
            "Training epoch 5101/1000000, d_loss: -2.0698471069335938,  g_loss: -231.91696166992188\n",
            "Training epoch 5102/1000000, d_loss: 120.46940612792969,  g_loss: -3.803755760192871\n",
            "Training epoch 5103/1000000, d_loss: -31.64312744140625,  g_loss: 2.408641815185547\n",
            "Training epoch 5104/1000000, d_loss: -136.9864959716797,  g_loss: 100.85360717773438\n",
            "Training epoch 5105/1000000, d_loss: -152.60049438476562,  g_loss: 374.63018798828125\n",
            "Training epoch 5106/1000000, d_loss: -135.779541015625,  g_loss: 136.0961456298828\n",
            "Training epoch 5107/1000000, d_loss: -324.2804870605469,  g_loss: -52.03650665283203\n",
            "Training epoch 5108/1000000, d_loss: -45.274452209472656,  g_loss: 42.89092254638672\n",
            "Training epoch 5109/1000000, d_loss: 31.045501708984375,  g_loss: -2.8602442741394043\n",
            "Training epoch 5110/1000000, d_loss: -24.89486312866211,  g_loss: -33.969581604003906\n",
            "Training epoch 5111/1000000, d_loss: -195.46231079101562,  g_loss: 22.512004852294922\n",
            "Training epoch 5112/1000000, d_loss: -40.22895431518555,  g_loss: 136.82125854492188\n",
            "Training epoch 5113/1000000, d_loss: -287.3026123046875,  g_loss: 12.596429824829102\n",
            "Training epoch 5114/1000000, d_loss: -62.364784240722656,  g_loss: 117.50498962402344\n",
            "Training epoch 5115/1000000, d_loss: -198.8584747314453,  g_loss: 131.99046325683594\n",
            "Training epoch 5116/1000000, d_loss: -111.44961547851562,  g_loss: 213.90362548828125\n",
            "Training epoch 5117/1000000, d_loss: -121.76165008544922,  g_loss: 254.74789428710938\n",
            "Training epoch 5118/1000000, d_loss: -58.13311004638672,  g_loss: 256.44927978515625\n",
            "Training epoch 5119/1000000, d_loss: -175.1473388671875,  g_loss: 178.19610595703125\n",
            "Training epoch 5120/1000000, d_loss: -948.259521484375,  g_loss: -225.26722717285156\n",
            "Training epoch 5121/1000000, d_loss: -142.0652313232422,  g_loss: 34.74399948120117\n",
            "Training epoch 5122/1000000, d_loss: -202.3238983154297,  g_loss: 33.2124137878418\n",
            "Training epoch 5123/1000000, d_loss: -447.94183349609375,  g_loss: -344.493408203125\n",
            "Training epoch 5124/1000000, d_loss: 22.641342163085938,  g_loss: 53.191741943359375\n",
            "Training epoch 5125/1000000, d_loss: 12.976753234863281,  g_loss: 11.434880256652832\n",
            "Training epoch 5126/1000000, d_loss: -63.21927261352539,  g_loss: 76.38802337646484\n",
            "Training epoch 5127/1000000, d_loss: -95.40563201904297,  g_loss: 125.53892517089844\n",
            "Training epoch 5128/1000000, d_loss: -185.30091857910156,  g_loss: 114.6307144165039\n",
            "Training epoch 5129/1000000, d_loss: -152.44259643554688,  g_loss: 107.989501953125\n",
            "Training epoch 5130/1000000, d_loss: -59.989295959472656,  g_loss: 1.3184986114501953\n",
            "Training epoch 5131/1000000, d_loss: -92.6141586303711,  g_loss: 221.66485595703125\n",
            "Training epoch 5132/1000000, d_loss: -53.25703430175781,  g_loss: 92.94039916992188\n",
            "Training epoch 5133/1000000, d_loss: -96.3233413696289,  g_loss: 129.42189025878906\n",
            "Training epoch 5134/1000000, d_loss: -110.3612289428711,  g_loss: 103.93191528320312\n",
            "Training epoch 5135/1000000, d_loss: -68.05510711669922,  g_loss: 63.65326690673828\n",
            "Training epoch 5136/1000000, d_loss: -64.80859375,  g_loss: 48.87023162841797\n",
            "Training epoch 5137/1000000, d_loss: -227.56939697265625,  g_loss: 3.6645889282226562\n",
            "Training epoch 5138/1000000, d_loss: -63.03512954711914,  g_loss: 59.50092315673828\n",
            "Training epoch 5139/1000000, d_loss: -53.76238250732422,  g_loss: 98.71548461914062\n",
            "Training epoch 5140/1000000, d_loss: -32.62217330932617,  g_loss: 156.96202087402344\n",
            "Training epoch 5141/1000000, d_loss: -608.3482666015625,  g_loss: -32.658226013183594\n",
            "Training epoch 5142/1000000, d_loss: -36.73770523071289,  g_loss: 133.59518432617188\n",
            "Training epoch 5143/1000000, d_loss: -76.66678619384766,  g_loss: -16.009475708007812\n",
            "Training epoch 5144/1000000, d_loss: -175.08067321777344,  g_loss: -33.3581428527832\n",
            "Training epoch 5145/1000000, d_loss: -1462.707763671875,  g_loss: -309.0081787109375\n",
            "Training epoch 5146/1000000, d_loss: -444.3322448730469,  g_loss: -137.4207000732422\n",
            "Training epoch 5147/1000000, d_loss: 362.9693603515625,  g_loss: -428.3714599609375\n",
            "Training epoch 5148/1000000, d_loss: 69.84950256347656,  g_loss: -0.7573328018188477\n",
            "Training epoch 5149/1000000, d_loss: -154.44583129882812,  g_loss: 81.65522766113281\n",
            "Training epoch 5150/1000000, d_loss: -366.8536376953125,  g_loss: -129.0189208984375\n",
            "Training epoch 5151/1000000, d_loss: 8.771520614624023,  g_loss: 34.559242248535156\n",
            "Training epoch 5152/1000000, d_loss: -184.38726806640625,  g_loss: 396.12646484375\n",
            "Training epoch 5153/1000000, d_loss: 1052.9521484375,  g_loss: 51.79042053222656\n",
            "Training epoch 5154/1000000, d_loss: -119.04043579101562,  g_loss: 246.22438049316406\n",
            "Training epoch 5155/1000000, d_loss: -166.27438354492188,  g_loss: 269.536376953125\n",
            "Training epoch 5156/1000000, d_loss: -42.27583312988281,  g_loss: 72.8021240234375\n",
            "Training epoch 5157/1000000, d_loss: -247.55331420898438,  g_loss: 253.5662841796875\n",
            "Training epoch 5158/1000000, d_loss: 8.701248168945312,  g_loss: 120.58421325683594\n",
            "Training epoch 5159/1000000, d_loss: -121.67120361328125,  g_loss: 308.2598571777344\n",
            "Training epoch 5160/1000000, d_loss: -220.28073120117188,  g_loss: 200.5260009765625\n",
            "Training epoch 5161/1000000, d_loss: -163.43350219726562,  g_loss: 387.2341003417969\n",
            "Training epoch 5162/1000000, d_loss: -61.650360107421875,  g_loss: 121.87034606933594\n",
            "Training epoch 5163/1000000, d_loss: -32.661285400390625,  g_loss: 56.71815490722656\n",
            "Training epoch 5164/1000000, d_loss: -129.13589477539062,  g_loss: 51.76972198486328\n",
            "Training epoch 5165/1000000, d_loss: 6.671858787536621,  g_loss: 59.76886749267578\n",
            "Training epoch 5166/1000000, d_loss: -139.13824462890625,  g_loss: 104.17790222167969\n",
            "Training epoch 5167/1000000, d_loss: -30.187641143798828,  g_loss: 153.12484741210938\n",
            "Training epoch 5168/1000000, d_loss: -331.9396667480469,  g_loss: 55.434505462646484\n",
            "Training epoch 5169/1000000, d_loss: -186.50132751464844,  g_loss: 9.4197359085083\n",
            "Training epoch 5170/1000000, d_loss: -223.45877075195312,  g_loss: 73.09780883789062\n",
            "Training epoch 5171/1000000, d_loss: 13.45477294921875,  g_loss: 148.24896240234375\n",
            "Training epoch 5172/1000000, d_loss: -85.73252868652344,  g_loss: 222.31533813476562\n",
            "Training epoch 5173/1000000, d_loss: -45.94407653808594,  g_loss: 195.2354736328125\n",
            "Training epoch 5174/1000000, d_loss: -140.24005126953125,  g_loss: 127.88701629638672\n",
            "Training epoch 5175/1000000, d_loss: -470.1168518066406,  g_loss: -18.65902328491211\n",
            "Training epoch 5176/1000000, d_loss: -113.77100372314453,  g_loss: -46.923484802246094\n",
            "Training epoch 5177/1000000, d_loss: 22.81781768798828,  g_loss: -65.78689575195312\n",
            "Training epoch 5178/1000000, d_loss: -94.43785858154297,  g_loss: 18.625408172607422\n",
            "Training epoch 5179/1000000, d_loss: -94.09893798828125,  g_loss: 60.3350944519043\n",
            "Training epoch 5180/1000000, d_loss: -30.674041748046875,  g_loss: 27.808147430419922\n",
            "Training epoch 5181/1000000, d_loss: -18.5498046875,  g_loss: 104.91609191894531\n",
            "Training epoch 5182/1000000, d_loss: -41.57267379760742,  g_loss: 69.90376281738281\n",
            "Training epoch 5183/1000000, d_loss: -85.09429931640625,  g_loss: 266.0131530761719\n",
            "Training epoch 5184/1000000, d_loss: -3.3894805908203125,  g_loss: 66.01011657714844\n",
            "Training epoch 5185/1000000, d_loss: -82.0303955078125,  g_loss: 122.83096313476562\n",
            "Training epoch 5186/1000000, d_loss: -65.45738220214844,  g_loss: 90.27790069580078\n",
            "Training epoch 5187/1000000, d_loss: -58.442466735839844,  g_loss: 82.31731414794922\n",
            "Training epoch 5188/1000000, d_loss: -20.297161102294922,  g_loss: 40.66962814331055\n",
            "Training epoch 5189/1000000, d_loss: 20.173442840576172,  g_loss: 17.78797149658203\n",
            "Training epoch 5190/1000000, d_loss: -154.89471435546875,  g_loss: 158.5615234375\n",
            "Training epoch 5191/1000000, d_loss: -44.408592224121094,  g_loss: 50.75840377807617\n",
            "Training epoch 5192/1000000, d_loss: 25.980674743652344,  g_loss: 168.06307983398438\n",
            "Training epoch 5193/1000000, d_loss: -84.8224868774414,  g_loss: 174.99658203125\n",
            "Training epoch 5194/1000000, d_loss: -49.901554107666016,  g_loss: 25.008148193359375\n",
            "Training epoch 5195/1000000, d_loss: -29.890636444091797,  g_loss: 15.168050765991211\n",
            "Training epoch 5196/1000000, d_loss: -28.06715202331543,  g_loss: 40.469085693359375\n",
            "Training epoch 5197/1000000, d_loss: -46.2578239440918,  g_loss: 68.02359008789062\n",
            "Training epoch 5198/1000000, d_loss: -25.636619567871094,  g_loss: 48.48326110839844\n",
            "Training epoch 5199/1000000, d_loss: -592.069091796875,  g_loss: -77.54476928710938\n",
            "Training epoch 5200/1000000, d_loss: -150.23672485351562,  g_loss: -55.87767791748047\n",
            "Training epoch 5201/1000000, d_loss: -78.18697357177734,  g_loss: 47.21772003173828\n",
            "Training epoch 5202/1000000, d_loss: -63.790897369384766,  g_loss: 4.5357513427734375\n",
            "Training epoch 5203/1000000, d_loss: -168.71487426757812,  g_loss: -133.88748168945312\n",
            "Training epoch 5204/1000000, d_loss: -66.68107604980469,  g_loss: -108.29679870605469\n",
            "Training epoch 5205/1000000, d_loss: -133.679443359375,  g_loss: -56.85316848754883\n",
            "Training epoch 5206/1000000, d_loss: -272.6358642578125,  g_loss: -123.31500244140625\n",
            "Training epoch 5207/1000000, d_loss: -64.78872680664062,  g_loss: -27.735017776489258\n",
            "Training epoch 5208/1000000, d_loss: -805.7906494140625,  g_loss: -242.6046142578125\n",
            "Training epoch 5209/1000000, d_loss: -329.02008056640625,  g_loss: -28.631898880004883\n",
            "Training epoch 5210/1000000, d_loss: -41.294525146484375,  g_loss: 47.38188934326172\n",
            "Training epoch 5211/1000000, d_loss: -33.918373107910156,  g_loss: 4.5257720947265625\n",
            "Training epoch 5212/1000000, d_loss: -95.05345153808594,  g_loss: 101.06034088134766\n",
            "Training epoch 5213/1000000, d_loss: -48.062339782714844,  g_loss: -24.500946044921875\n",
            "Training epoch 5214/1000000, d_loss: -35.74321746826172,  g_loss: -54.311279296875\n",
            "Training epoch 5215/1000000, d_loss: -63.977142333984375,  g_loss: -7.829050064086914\n",
            "Training epoch 5216/1000000, d_loss: -83.02911376953125,  g_loss: -60.716766357421875\n",
            "Training epoch 5217/1000000, d_loss: -89.31549072265625,  g_loss: -46.82064437866211\n",
            "Training epoch 5218/1000000, d_loss: -57.45346450805664,  g_loss: -15.911857604980469\n",
            "Training epoch 5219/1000000, d_loss: -62.49211120605469,  g_loss: 20.253849029541016\n",
            "Training epoch 5220/1000000, d_loss: -416.1684265136719,  g_loss: -128.44497680664062\n",
            "Training epoch 5221/1000000, d_loss: -17.750770568847656,  g_loss: -10.853029251098633\n",
            "Training epoch 5222/1000000, d_loss: -115.96021270751953,  g_loss: 144.599853515625\n",
            "Training epoch 5223/1000000, d_loss: -144.55950927734375,  g_loss: -16.670074462890625\n",
            "Training epoch 5224/1000000, d_loss: -435.84942626953125,  g_loss: -186.28985595703125\n",
            "Training epoch 5225/1000000, d_loss: -328.66998291015625,  g_loss: -84.38908386230469\n",
            "Training epoch 5226/1000000, d_loss: -45.972129821777344,  g_loss: 186.65093994140625\n",
            "Training epoch 5227/1000000, d_loss: -75.85810852050781,  g_loss: 136.01715087890625\n",
            "Training epoch 5228/1000000, d_loss: -219.4682159423828,  g_loss: 116.78054809570312\n",
            "Training epoch 5229/1000000, d_loss: -142.89288330078125,  g_loss: 329.2818908691406\n",
            "Training epoch 5230/1000000, d_loss: 43.39965057373047,  g_loss: 283.4815979003906\n",
            "Training epoch 5231/1000000, d_loss: -2.306926727294922,  g_loss: 177.62892150878906\n",
            "Training epoch 5232/1000000, d_loss: -109.9304428100586,  g_loss: 169.2780303955078\n",
            "Training epoch 5233/1000000, d_loss: -193.4003448486328,  g_loss: 67.26673126220703\n",
            "Training epoch 5234/1000000, d_loss: -114.11099243164062,  g_loss: 58.64912796020508\n",
            "Training epoch 5235/1000000, d_loss: -100.34413146972656,  g_loss: 143.5064239501953\n",
            "Training epoch 5236/1000000, d_loss: -102.82255554199219,  g_loss: 97.90496063232422\n",
            "Training epoch 5237/1000000, d_loss: -152.98797607421875,  g_loss: 84.16561889648438\n",
            "Training epoch 5238/1000000, d_loss: -3.780010223388672,  g_loss: 85.86117553710938\n",
            "Training epoch 5239/1000000, d_loss: -132.9683074951172,  g_loss: 114.11288452148438\n",
            "Training epoch 5240/1000000, d_loss: -108.23977661132812,  g_loss: 106.57613372802734\n",
            "Training epoch 5241/1000000, d_loss: -102.16387939453125,  g_loss: 90.78726196289062\n",
            "Training epoch 5242/1000000, d_loss: -98.5946044921875,  g_loss: 99.82717895507812\n",
            "Training epoch 5243/1000000, d_loss: -109.03695678710938,  g_loss: 197.08338928222656\n",
            "Training epoch 5244/1000000, d_loss: -32.753684997558594,  g_loss: 103.54719543457031\n",
            "Training epoch 5245/1000000, d_loss: -476.277099609375,  g_loss: 7.129581451416016\n",
            "Training epoch 5246/1000000, d_loss: 45.746063232421875,  g_loss: 139.89051818847656\n",
            "Training epoch 5247/1000000, d_loss: -10.462991714477539,  g_loss: 148.8186798095703\n",
            "Training epoch 5248/1000000, d_loss: -120.41146850585938,  g_loss: 167.3548126220703\n",
            "Training epoch 5249/1000000, d_loss: -59.32228088378906,  g_loss: 185.33853149414062\n",
            "Training epoch 5250/1000000, d_loss: -115.42616271972656,  g_loss: 226.59854125976562\n",
            "Training epoch 5251/1000000, d_loss: -107.29386901855469,  g_loss: 200.70550537109375\n",
            "Training epoch 5252/1000000, d_loss: -135.7614288330078,  g_loss: 109.43279266357422\n",
            "Training epoch 5253/1000000, d_loss: -180.80038452148438,  g_loss: 64.01295471191406\n",
            "Training epoch 5254/1000000, d_loss: 1.1243667602539062,  g_loss: 83.31732177734375\n",
            "Training epoch 5255/1000000, d_loss: -93.54910278320312,  g_loss: 64.66283416748047\n",
            "Training epoch 5256/1000000, d_loss: -79.83377075195312,  g_loss: 237.92599487304688\n",
            "Training epoch 5257/1000000, d_loss: -163.8926239013672,  g_loss: 212.2041778564453\n",
            "Training epoch 5258/1000000, d_loss: -87.73204803466797,  g_loss: 124.14104461669922\n",
            "Training epoch 5259/1000000, d_loss: -280.9294128417969,  g_loss: 146.03765869140625\n",
            "Training epoch 5260/1000000, d_loss: -351.95458984375,  g_loss: -55.7677001953125\n",
            "Training epoch 5261/1000000, d_loss: -327.0382385253906,  g_loss: -187.56668090820312\n",
            "Training epoch 5262/1000000, d_loss: 49.06745910644531,  g_loss: -23.472034454345703\n",
            "Training epoch 5263/1000000, d_loss: -73.12977600097656,  g_loss: 25.871227264404297\n",
            "Training epoch 5264/1000000, d_loss: 33.4293212890625,  g_loss: 11.24481201171875\n",
            "Training epoch 5265/1000000, d_loss: -434.4819641113281,  g_loss: -99.00013732910156\n",
            "Training epoch 5266/1000000, d_loss: -110.5730209350586,  g_loss: -49.98615264892578\n",
            "Training epoch 5267/1000000, d_loss: -6.493526458740234,  g_loss: 22.825937271118164\n",
            "Training epoch 5268/1000000, d_loss: -109.35800170898438,  g_loss: 24.739774703979492\n",
            "Training epoch 5269/1000000, d_loss: -120.69625854492188,  g_loss: -25.720596313476562\n",
            "Training epoch 5270/1000000, d_loss: -201.81365966796875,  g_loss: -51.576393127441406\n",
            "Training epoch 5271/1000000, d_loss: -14.059654235839844,  g_loss: -16.775630950927734\n",
            "Training epoch 5272/1000000, d_loss: 11.36334228515625,  g_loss: 27.228973388671875\n",
            "Training epoch 5273/1000000, d_loss: -39.25095748901367,  g_loss: 43.49250030517578\n",
            "Training epoch 5274/1000000, d_loss: -101.05931854248047,  g_loss: 31.121692657470703\n",
            "Training epoch 5275/1000000, d_loss: -275.06207275390625,  g_loss: -45.56891632080078\n",
            "Training epoch 5276/1000000, d_loss: 48.39788818359375,  g_loss: -188.5579833984375\n",
            "Training epoch 5277/1000000, d_loss: 123.38569641113281,  g_loss: -66.93848419189453\n",
            "Training epoch 5278/1000000, d_loss: -59.556739807128906,  g_loss: -5.264777183532715\n",
            "Training epoch 5279/1000000, d_loss: -146.93844604492188,  g_loss: -5.967723846435547\n",
            "Training epoch 5280/1000000, d_loss: -36.228946685791016,  g_loss: -33.86717224121094\n",
            "Training epoch 5281/1000000, d_loss: -73.84144592285156,  g_loss: -63.27458953857422\n",
            "Training epoch 5282/1000000, d_loss: -272.6374206542969,  g_loss: -113.81819915771484\n",
            "Training epoch 5283/1000000, d_loss: -108.44889831542969,  g_loss: 82.97074127197266\n",
            "Training epoch 5284/1000000, d_loss: -142.4756317138672,  g_loss: 117.94363403320312\n",
            "Training epoch 5285/1000000, d_loss: -445.0862121582031,  g_loss: 125.91558074951172\n",
            "Training epoch 5286/1000000, d_loss: -84.00071716308594,  g_loss: 50.37608337402344\n",
            "Training epoch 5287/1000000, d_loss: 5.611927032470703,  g_loss: 110.45530700683594\n",
            "Training epoch 5288/1000000, d_loss: -108.35894775390625,  g_loss: 65.5320816040039\n",
            "Training epoch 5289/1000000, d_loss: -14.183334350585938,  g_loss: 60.88269805908203\n",
            "Training epoch 5290/1000000, d_loss: -316.50823974609375,  g_loss: -113.09439086914062\n",
            "Training epoch 5291/1000000, d_loss: -156.64389038085938,  g_loss: 2.432605743408203\n",
            "Training epoch 5292/1000000, d_loss: -177.82022094726562,  g_loss: -13.67568302154541\n",
            "Training epoch 5293/1000000, d_loss: -132.8690185546875,  g_loss: -187.470947265625\n",
            "Training epoch 5294/1000000, d_loss: -420.44012451171875,  g_loss: -197.8350830078125\n",
            "Training epoch 5295/1000000, d_loss: -1291.7135009765625,  g_loss: -649.9033203125\n",
            "Training epoch 5296/1000000, d_loss: -48.396339416503906,  g_loss: 35.554012298583984\n",
            "Training epoch 5297/1000000, d_loss: -188.4102783203125,  g_loss: 187.09109497070312\n",
            "Training epoch 5298/1000000, d_loss: -210.6471405029297,  g_loss: 420.5457458496094\n",
            "Training epoch 5299/1000000, d_loss: -513.1048583984375,  g_loss: 491.09808349609375\n",
            "Training epoch 5300/1000000, d_loss: -376.2869873046875,  g_loss: 701.2037353515625\n",
            "Training epoch 5301/1000000, d_loss: -122.55611419677734,  g_loss: 472.54180908203125\n",
            "Training epoch 5302/1000000, d_loss: 29.600555419921875,  g_loss: 43.875022888183594\n",
            "Training epoch 5303/1000000, d_loss: 1.7062530517578125,  g_loss: 149.44180297851562\n",
            "Training epoch 5304/1000000, d_loss: -82.12437438964844,  g_loss: 139.26910400390625\n",
            "Training epoch 5305/1000000, d_loss: -302.9913330078125,  g_loss: -68.01494598388672\n",
            "Training epoch 5306/1000000, d_loss: -91.13240051269531,  g_loss: -60.153350830078125\n",
            "Training epoch 5307/1000000, d_loss: -125.20060729980469,  g_loss: 103.5145034790039\n",
            "Training epoch 5308/1000000, d_loss: -25.447444915771484,  g_loss: 38.38671112060547\n",
            "Training epoch 5309/1000000, d_loss: -173.7384033203125,  g_loss: 82.48175048828125\n",
            "Training epoch 5310/1000000, d_loss: -77.86227416992188,  g_loss: 64.52822875976562\n",
            "Training epoch 5311/1000000, d_loss: -143.1663818359375,  g_loss: 60.89508819580078\n",
            "Training epoch 5312/1000000, d_loss: -76.81895446777344,  g_loss: 140.74794006347656\n",
            "Training epoch 5313/1000000, d_loss: -13.288166046142578,  g_loss: 120.31785583496094\n",
            "Training epoch 5314/1000000, d_loss: -157.0305633544922,  g_loss: -39.464111328125\n",
            "Training epoch 5315/1000000, d_loss: -148.17544555664062,  g_loss: 147.95599365234375\n",
            "Training epoch 5316/1000000, d_loss: -185.91064453125,  g_loss: 33.47565460205078\n",
            "Training epoch 5317/1000000, d_loss: -63.7369384765625,  g_loss: 9.621391296386719\n",
            "Training epoch 5318/1000000, d_loss: 22.59756088256836,  g_loss: 225.21157836914062\n",
            "Training epoch 5319/1000000, d_loss: -177.8556671142578,  g_loss: 210.12562561035156\n",
            "Training epoch 5320/1000000, d_loss: -207.51617431640625,  g_loss: 18.6735782623291\n",
            "Training epoch 5321/1000000, d_loss: -108.9043197631836,  g_loss: 129.27267456054688\n",
            "Training epoch 5322/1000000, d_loss: -81.5919418334961,  g_loss: 80.9332504272461\n",
            "Training epoch 5323/1000000, d_loss: -29.57861328125,  g_loss: 87.07968139648438\n",
            "Training epoch 5324/1000000, d_loss: -67.75654602050781,  g_loss: 124.86140441894531\n",
            "Training epoch 5325/1000000, d_loss: -83.70081329345703,  g_loss: 132.56793212890625\n",
            "Training epoch 5326/1000000, d_loss: -45.404510498046875,  g_loss: 195.76138305664062\n",
            "Training epoch 5327/1000000, d_loss: -59.79743194580078,  g_loss: 85.578369140625\n",
            "Training epoch 5328/1000000, d_loss: -224.39967346191406,  g_loss: 121.15960693359375\n",
            "Training epoch 5329/1000000, d_loss: -64.06716918945312,  g_loss: 188.2686004638672\n",
            "Training epoch 5330/1000000, d_loss: -46.48031997680664,  g_loss: 159.65838623046875\n",
            "Training epoch 5331/1000000, d_loss: -58.0374641418457,  g_loss: 142.6585235595703\n",
            "Training epoch 5332/1000000, d_loss: -451.1874084472656,  g_loss: 125.81864166259766\n",
            "Training epoch 5333/1000000, d_loss: -474.3060607910156,  g_loss: -288.3912048339844\n",
            "Training epoch 5334/1000000, d_loss: -823.9835205078125,  g_loss: -186.34129333496094\n",
            "Training epoch 5335/1000000, d_loss: 2.9783287048339844,  g_loss: -201.4696502685547\n",
            "Training epoch 5336/1000000, d_loss: 56.242984771728516,  g_loss: -76.09457397460938\n",
            "Training epoch 5337/1000000, d_loss: -323.13885498046875,  g_loss: -190.20721435546875\n",
            "Training epoch 5338/1000000, d_loss: -24.371498107910156,  g_loss: 49.26774978637695\n",
            "Training epoch 5339/1000000, d_loss: -113.39474487304688,  g_loss: 187.35731506347656\n",
            "Training epoch 5340/1000000, d_loss: 89.29122924804688,  g_loss: 125.94038391113281\n",
            "Training epoch 5341/1000000, d_loss: -186.2405548095703,  g_loss: 306.54205322265625\n",
            "Training epoch 5342/1000000, d_loss: -128.83738708496094,  g_loss: 317.02032470703125\n",
            "Training epoch 5343/1000000, d_loss: -347.4640197753906,  g_loss: 301.21502685546875\n",
            "Training epoch 5344/1000000, d_loss: -30.912109375,  g_loss: 5.0750732421875\n",
            "Training epoch 5345/1000000, d_loss: 1.5829544067382812,  g_loss: 39.4726448059082\n",
            "Training epoch 5346/1000000, d_loss: 7.665153503417969,  g_loss: -7.303481101989746\n",
            "Training epoch 5347/1000000, d_loss: 12.870098114013672,  g_loss: -7.855476379394531\n",
            "Training epoch 5348/1000000, d_loss: -215.6302032470703,  g_loss: -0.3825721740722656\n",
            "Training epoch 5349/1000000, d_loss: -83.29249572753906,  g_loss: -27.045150756835938\n",
            "Training epoch 5350/1000000, d_loss: -169.31344604492188,  g_loss: -79.38236999511719\n",
            "Training epoch 5351/1000000, d_loss: -162.50762939453125,  g_loss: -36.933326721191406\n",
            "Training epoch 5352/1000000, d_loss: -83.82598876953125,  g_loss: -53.3506965637207\n",
            "Training epoch 5353/1000000, d_loss: -33.0592041015625,  g_loss: 41.692832946777344\n",
            "Training epoch 5354/1000000, d_loss: -79.75184631347656,  g_loss: 222.5030975341797\n",
            "Training epoch 5355/1000000, d_loss: -271.711669921875,  g_loss: 109.15620422363281\n",
            "Training epoch 5356/1000000, d_loss: -117.31082916259766,  g_loss: 36.83393096923828\n",
            "Training epoch 5357/1000000, d_loss: -84.2599105834961,  g_loss: -2.5888214111328125\n",
            "Training epoch 5358/1000000, d_loss: -23.540359497070312,  g_loss: 201.12692260742188\n",
            "Training epoch 5359/1000000, d_loss: -104.13007354736328,  g_loss: 241.71896362304688\n",
            "Training epoch 5360/1000000, d_loss: -135.4993133544922,  g_loss: 187.4922637939453\n",
            "Training epoch 5361/1000000, d_loss: -83.65415954589844,  g_loss: 29.07294464111328\n",
            "Training epoch 5362/1000000, d_loss: -132.8653106689453,  g_loss: 285.89764404296875\n",
            "Training epoch 5363/1000000, d_loss: -84.85798645019531,  g_loss: 43.40745162963867\n",
            "Training epoch 5364/1000000, d_loss: -52.389892578125,  g_loss: 179.39523315429688\n",
            "Training epoch 5365/1000000, d_loss: -29.90191650390625,  g_loss: 114.31037902832031\n",
            "Training epoch 5366/1000000, d_loss: -142.4365692138672,  g_loss: 58.450225830078125\n",
            "Training epoch 5367/1000000, d_loss: -247.17819213867188,  g_loss: 16.04482078552246\n",
            "Training epoch 5368/1000000, d_loss: 17.008880615234375,  g_loss: 41.603477478027344\n",
            "Training epoch 5369/1000000, d_loss: -41.20627212524414,  g_loss: 115.27053833007812\n",
            "Training epoch 5370/1000000, d_loss: -72.21910858154297,  g_loss: 72.0096664428711\n",
            "Training epoch 5371/1000000, d_loss: -36.24374008178711,  g_loss: 104.54817199707031\n",
            "Training epoch 5372/1000000, d_loss: -71.0028076171875,  g_loss: 38.84623718261719\n",
            "Training epoch 5373/1000000, d_loss: -192.2396697998047,  g_loss: -18.07325553894043\n",
            "Training epoch 5374/1000000, d_loss: -130.36337280273438,  g_loss: 4.2883405685424805\n",
            "Training epoch 5375/1000000, d_loss: -403.3624267578125,  g_loss: -59.19580078125\n",
            "Training epoch 5376/1000000, d_loss: -81.37242126464844,  g_loss: 112.24665832519531\n",
            "Training epoch 5377/1000000, d_loss: -186.36865234375,  g_loss: 291.4714660644531\n",
            "Training epoch 5378/1000000, d_loss: -116.34237670898438,  g_loss: 91.28672790527344\n",
            "Training epoch 5379/1000000, d_loss: -79.73747253417969,  g_loss: 34.03680419921875\n",
            "Training epoch 5380/1000000, d_loss: -82.65402221679688,  g_loss: 43.46738815307617\n",
            "Training epoch 5381/1000000, d_loss: -199.40072631835938,  g_loss: -64.35762786865234\n",
            "Training epoch 5382/1000000, d_loss: -115.17471313476562,  g_loss: 114.74859619140625\n",
            "Training epoch 5383/1000000, d_loss: -36.013877868652344,  g_loss: 49.06752014160156\n",
            "Training epoch 5384/1000000, d_loss: -359.7095642089844,  g_loss: -62.66314697265625\n",
            "Training epoch 5385/1000000, d_loss: -69.09706115722656,  g_loss: 12.677837371826172\n",
            "Training epoch 5386/1000000, d_loss: -130.90728759765625,  g_loss: 75.57026672363281\n",
            "Training epoch 5387/1000000, d_loss: -108.0573501586914,  g_loss: 74.6471176147461\n",
            "Training epoch 5388/1000000, d_loss: -63.37260437011719,  g_loss: 123.15547180175781\n",
            "Training epoch 5389/1000000, d_loss: -59.436119079589844,  g_loss: 90.75389099121094\n",
            "Training epoch 5390/1000000, d_loss: -115.52610778808594,  g_loss: 68.93165588378906\n",
            "Training epoch 5391/1000000, d_loss: -100.89550018310547,  g_loss: 116.23064422607422\n",
            "Training epoch 5392/1000000, d_loss: -60.17194747924805,  g_loss: 86.53250122070312\n",
            "Training epoch 5393/1000000, d_loss: -455.6776123046875,  g_loss: -135.92494201660156\n",
            "Training epoch 5394/1000000, d_loss: -162.38330078125,  g_loss: -102.18798828125\n",
            "Training epoch 5395/1000000, d_loss: -52.64116287231445,  g_loss: -68.16106414794922\n",
            "Training epoch 5396/1000000, d_loss: 0.4364662170410156,  g_loss: -33.194149017333984\n",
            "Training epoch 5397/1000000, d_loss: -105.19263458251953,  g_loss: -45.54804229736328\n",
            "Training epoch 5398/1000000, d_loss: -69.57463836669922,  g_loss: -10.593825340270996\n",
            "Training epoch 5399/1000000, d_loss: -108.67359924316406,  g_loss: -27.089635848999023\n",
            "Training epoch 5400/1000000, d_loss: -5.644428253173828,  g_loss: 1.3647165298461914\n",
            "Training epoch 5401/1000000, d_loss: -186.2513427734375,  g_loss: 37.75007629394531\n",
            "Training epoch 5402/1000000, d_loss: -155.4600372314453,  g_loss: -27.7563419342041\n",
            "Training epoch 5403/1000000, d_loss: -184.34730529785156,  g_loss: -68.68008422851562\n",
            "Training epoch 5404/1000000, d_loss: -15.984779357910156,  g_loss: 69.67993927001953\n",
            "Training epoch 5405/1000000, d_loss: -52.358638763427734,  g_loss: 51.372779846191406\n",
            "Training epoch 5406/1000000, d_loss: -80.6447525024414,  g_loss: 38.30384826660156\n",
            "Training epoch 5407/1000000, d_loss: -174.46893310546875,  g_loss: 63.90188217163086\n",
            "Training epoch 5408/1000000, d_loss: -134.611328125,  g_loss: -14.796638488769531\n",
            "Training epoch 5409/1000000, d_loss: -738.7781982421875,  g_loss: -143.55172729492188\n",
            "Training epoch 5410/1000000, d_loss: -73.42401885986328,  g_loss: 18.56072425842285\n",
            "Training epoch 5411/1000000, d_loss: -6.005537033081055,  g_loss: 41.319950103759766\n",
            "Training epoch 5412/1000000, d_loss: -49.763450622558594,  g_loss: 37.08652877807617\n",
            "Training epoch 5413/1000000, d_loss: -153.28488159179688,  g_loss: 63.53025817871094\n",
            "Training epoch 5414/1000000, d_loss: -54.87702560424805,  g_loss: 66.0216293334961\n",
            "Training epoch 5415/1000000, d_loss: -136.10601806640625,  g_loss: 22.876567840576172\n",
            "Training epoch 5416/1000000, d_loss: 9.938179016113281,  g_loss: 62.40342330932617\n",
            "Training epoch 5417/1000000, d_loss: -197.97662353515625,  g_loss: -20.163448333740234\n",
            "Training epoch 5418/1000000, d_loss: -554.0355224609375,  g_loss: -204.01893615722656\n",
            "Training epoch 5419/1000000, d_loss: -61.86090087890625,  g_loss: 69.1312255859375\n",
            "Training epoch 5420/1000000, d_loss: -15.255603790283203,  g_loss: 27.85433578491211\n",
            "Training epoch 5421/1000000, d_loss: 17.751502990722656,  g_loss: 45.27667236328125\n",
            "Training epoch 5422/1000000, d_loss: -158.6733856201172,  g_loss: 24.31836700439453\n",
            "Training epoch 5423/1000000, d_loss: -372.0881652832031,  g_loss: 7.841596603393555\n",
            "Training epoch 5424/1000000, d_loss: -27.478513717651367,  g_loss: 112.48295593261719\n",
            "Training epoch 5425/1000000, d_loss: -46.13316345214844,  g_loss: 160.25473022460938\n",
            "Training epoch 5426/1000000, d_loss: -177.53956604003906,  g_loss: 215.46121215820312\n",
            "Training epoch 5427/1000000, d_loss: -94.72001647949219,  g_loss: 201.89581298828125\n",
            "Training epoch 5428/1000000, d_loss: -179.13780212402344,  g_loss: 90.079833984375\n",
            "Training epoch 5429/1000000, d_loss: -1273.168212890625,  g_loss: -242.33155822753906\n",
            "Training epoch 5430/1000000, d_loss: 154.41281127929688,  g_loss: -44.6316032409668\n",
            "Training epoch 5431/1000000, d_loss: -65.26029968261719,  g_loss: 2.35745906829834\n",
            "Training epoch 5432/1000000, d_loss: -618.2807006835938,  g_loss: -249.17425537109375\n",
            "Training epoch 5433/1000000, d_loss: 56.48655700683594,  g_loss: -21.49660301208496\n",
            "Training epoch 5434/1000000, d_loss: -71.549072265625,  g_loss: 77.38398742675781\n",
            "Training epoch 5435/1000000, d_loss: -90.66482543945312,  g_loss: 83.80699157714844\n",
            "Training epoch 5436/1000000, d_loss: -255.36068725585938,  g_loss: -40.310672760009766\n",
            "Training epoch 5437/1000000, d_loss: -210.65467834472656,  g_loss: 100.18704223632812\n",
            "Training epoch 5438/1000000, d_loss: -174.9331512451172,  g_loss: -70.92774963378906\n",
            "Training epoch 5439/1000000, d_loss: 19.757144927978516,  g_loss: 9.848331451416016\n",
            "Training epoch 5440/1000000, d_loss: -40.22370910644531,  g_loss: 70.40234375\n",
            "Training epoch 5441/1000000, d_loss: -176.89669799804688,  g_loss: 69.31297302246094\n",
            "Training epoch 5442/1000000, d_loss: -221.61062622070312,  g_loss: 259.1613464355469\n",
            "Training epoch 5443/1000000, d_loss: -266.2088317871094,  g_loss: 51.419795989990234\n",
            "Training epoch 5444/1000000, d_loss: -50.27861022949219,  g_loss: 123.48316955566406\n",
            "Training epoch 5445/1000000, d_loss: -61.457889556884766,  g_loss: 143.9217529296875\n",
            "Training epoch 5446/1000000, d_loss: -142.10166931152344,  g_loss: 86.23001861572266\n",
            "Training epoch 5447/1000000, d_loss: -160.12429809570312,  g_loss: 72.45683288574219\n",
            "Training epoch 5448/1000000, d_loss: -131.23822021484375,  g_loss: 99.73595428466797\n",
            "Training epoch 5449/1000000, d_loss: -142.92169189453125,  g_loss: 106.33456420898438\n",
            "Training epoch 5450/1000000, d_loss: -270.67364501953125,  g_loss: -50.61262512207031\n",
            "Training epoch 5451/1000000, d_loss: -62.65198516845703,  g_loss: -52.90635299682617\n",
            "Training epoch 5452/1000000, d_loss: -6.722930908203125,  g_loss: -33.62481689453125\n",
            "Training epoch 5453/1000000, d_loss: -74.16233825683594,  g_loss: -23.40665054321289\n",
            "Training epoch 5454/1000000, d_loss: -243.17234802246094,  g_loss: -96.53614807128906\n",
            "Training epoch 5455/1000000, d_loss: -63.71467590332031,  g_loss: 39.83221435546875\n",
            "Training epoch 5456/1000000, d_loss: -124.17794799804688,  g_loss: 12.540393829345703\n",
            "Training epoch 5457/1000000, d_loss: -249.4688720703125,  g_loss: -223.79161071777344\n",
            "Training epoch 5458/1000000, d_loss: -45.90122985839844,  g_loss: 18.443140029907227\n",
            "Training epoch 5459/1000000, d_loss: 15.425949096679688,  g_loss: 129.84201049804688\n",
            "Training epoch 5460/1000000, d_loss: -103.1737289428711,  g_loss: 93.55450439453125\n",
            "Training epoch 5461/1000000, d_loss: -126.80780029296875,  g_loss: -10.465875625610352\n",
            "Training epoch 5462/1000000, d_loss: -111.9328384399414,  g_loss: 92.5611801147461\n",
            "Training epoch 5463/1000000, d_loss: -537.892822265625,  g_loss: -30.17749786376953\n",
            "Training epoch 5464/1000000, d_loss: -290.9532470703125,  g_loss: -260.67510986328125\n",
            "Training epoch 5465/1000000, d_loss: -376.53936767578125,  g_loss: -85.6806869506836\n",
            "Training epoch 5466/1000000, d_loss: -70.34480285644531,  g_loss: 25.005022048950195\n",
            "Training epoch 5467/1000000, d_loss: 51.94667053222656,  g_loss: 55.61742401123047\n",
            "Training epoch 5468/1000000, d_loss: -185.443115234375,  g_loss: 86.66022491455078\n",
            "Training epoch 5469/1000000, d_loss: -48.266090393066406,  g_loss: 115.016357421875\n",
            "Training epoch 5470/1000000, d_loss: -259.0872497558594,  g_loss: 26.40129852294922\n",
            "Training epoch 5471/1000000, d_loss: -23.382780075073242,  g_loss: 125.97235870361328\n",
            "Training epoch 5472/1000000, d_loss: -238.1428680419922,  g_loss: 74.9069595336914\n",
            "Training epoch 5473/1000000, d_loss: -118.01087951660156,  g_loss: 48.815826416015625\n",
            "Training epoch 5474/1000000, d_loss: -354.5986328125,  g_loss: -139.53817749023438\n",
            "Training epoch 5475/1000000, d_loss: 101.10783386230469,  g_loss: 65.52700805664062\n",
            "Training epoch 5476/1000000, d_loss: -69.7285385131836,  g_loss: 36.839725494384766\n",
            "Training epoch 5477/1000000, d_loss: -110.11002349853516,  g_loss: 263.3990478515625\n",
            "Training epoch 5478/1000000, d_loss: -97.581298828125,  g_loss: 173.58474731445312\n",
            "Training epoch 5479/1000000, d_loss: -128.61038208007812,  g_loss: 249.67442321777344\n",
            "Training epoch 5480/1000000, d_loss: -222.0612335205078,  g_loss: 316.21148681640625\n",
            "Training epoch 5481/1000000, d_loss: -37.67451477050781,  g_loss: 154.4552459716797\n",
            "Training epoch 5482/1000000, d_loss: -274.4808044433594,  g_loss: 37.275794982910156\n",
            "Training epoch 5483/1000000, d_loss: -93.52592468261719,  g_loss: 63.352134704589844\n",
            "Training epoch 5484/1000000, d_loss: -27.689983367919922,  g_loss: 90.16044616699219\n",
            "Training epoch 5485/1000000, d_loss: -75.94400787353516,  g_loss: 175.8309326171875\n",
            "Training epoch 5486/1000000, d_loss: -107.20550537109375,  g_loss: 358.5107116699219\n",
            "Training epoch 5487/1000000, d_loss: 47.88191223144531,  g_loss: 70.14116668701172\n",
            "Training epoch 5488/1000000, d_loss: -82.348876953125,  g_loss: 148.27792358398438\n",
            "Training epoch 5489/1000000, d_loss: 0.480926513671875,  g_loss: 194.86138916015625\n",
            "Training epoch 5490/1000000, d_loss: -320.88983154296875,  g_loss: 56.55986022949219\n",
            "Training epoch 5491/1000000, d_loss: -20.060516357421875,  g_loss: 49.82080078125\n",
            "Training epoch 5492/1000000, d_loss: -136.98391723632812,  g_loss: 92.13780212402344\n",
            "Training epoch 5493/1000000, d_loss: -53.78297424316406,  g_loss: 64.93130493164062\n",
            "Training epoch 5494/1000000, d_loss: -112.0037841796875,  g_loss: 122.89334106445312\n",
            "Training epoch 5495/1000000, d_loss: -254.89208984375,  g_loss: 62.215606689453125\n",
            "Training epoch 5496/1000000, d_loss: -143.76995849609375,  g_loss: 106.67134857177734\n",
            "Training epoch 5497/1000000, d_loss: -1096.76220703125,  g_loss: -139.207763671875\n",
            "Training epoch 5498/1000000, d_loss: -77.82444763183594,  g_loss: -17.655986785888672\n",
            "Training epoch 5499/1000000, d_loss: 263.79693603515625,  g_loss: -40.0217170715332\n",
            "Training epoch 5500/1000000, d_loss: -245.7683868408203,  g_loss: 374.8058776855469\n",
            "Training epoch 5501/1000000, d_loss: -47.86423873901367,  g_loss: -13.495616912841797\n",
            "Training epoch 5502/1000000, d_loss: -240.36050415039062,  g_loss: -90.02622985839844\n",
            "Training epoch 5503/1000000, d_loss: -155.25445556640625,  g_loss: -145.1039581298828\n",
            "Training epoch 5504/1000000, d_loss: 431.05194091796875,  g_loss: 57.75889587402344\n",
            "Training epoch 5505/1000000, d_loss: -145.96328735351562,  g_loss: 167.51498413085938\n",
            "Training epoch 5506/1000000, d_loss: -702.396728515625,  g_loss: -111.41291046142578\n",
            "Training epoch 5507/1000000, d_loss: 58.571685791015625,  g_loss: 23.440277099609375\n",
            "Training epoch 5508/1000000, d_loss: -38.30364227294922,  g_loss: 39.164093017578125\n",
            "Training epoch 5509/1000000, d_loss: -199.5319366455078,  g_loss: 212.11941528320312\n",
            "Training epoch 5510/1000000, d_loss: -231.69703674316406,  g_loss: 185.54339599609375\n",
            "Training epoch 5511/1000000, d_loss: -141.56027221679688,  g_loss: 88.80816650390625\n",
            "Training epoch 5512/1000000, d_loss: -200.8799591064453,  g_loss: 20.635967254638672\n",
            "Training epoch 5513/1000000, d_loss: -142.2981414794922,  g_loss: 64.79032897949219\n",
            "Training epoch 5514/1000000, d_loss: -274.57318115234375,  g_loss: -52.54973220825195\n",
            "Training epoch 5515/1000000, d_loss: -414.2579650878906,  g_loss: -276.8717346191406\n",
            "Training epoch 5516/1000000, d_loss: -14.353059768676758,  g_loss: 154.11383056640625\n",
            "Training epoch 5517/1000000, d_loss: -141.62197875976562,  g_loss: -5.635950088500977\n",
            "Training epoch 5518/1000000, d_loss: -76.25804901123047,  g_loss: 133.00367736816406\n",
            "Training epoch 5519/1000000, d_loss: -172.0580291748047,  g_loss: 225.59393310546875\n",
            "Training epoch 5520/1000000, d_loss: -72.0363998413086,  g_loss: 130.56085205078125\n",
            "Training epoch 5521/1000000, d_loss: -219.68063354492188,  g_loss: 19.141399383544922\n",
            "Training epoch 5522/1000000, d_loss: -82.53659057617188,  g_loss: 50.19048309326172\n",
            "Training epoch 5523/1000000, d_loss: 37.48455810546875,  g_loss: 82.92367553710938\n",
            "Training epoch 5524/1000000, d_loss: -173.9939727783203,  g_loss: 22.766075134277344\n",
            "Training epoch 5525/1000000, d_loss: -316.1240234375,  g_loss: -11.634286880493164\n",
            "Training epoch 5526/1000000, d_loss: -158.04283142089844,  g_loss: 225.9990234375\n",
            "Training epoch 5527/1000000, d_loss: -409.26177978515625,  g_loss: -116.40911102294922\n",
            "Training epoch 5528/1000000, d_loss: -78.17918395996094,  g_loss: 19.600296020507812\n",
            "Training epoch 5529/1000000, d_loss: -38.06550979614258,  g_loss: 6.411611557006836\n",
            "Training epoch 5530/1000000, d_loss: -232.1629638671875,  g_loss: -28.63495445251465\n",
            "Training epoch 5531/1000000, d_loss: -136.1553955078125,  g_loss: -50.01935958862305\n",
            "Training epoch 5532/1000000, d_loss: -81.92666625976562,  g_loss: 19.56112289428711\n",
            "Training epoch 5533/1000000, d_loss: -60.2889404296875,  g_loss: 19.484542846679688\n",
            "Training epoch 5534/1000000, d_loss: -156.69479370117188,  g_loss: -57.63914489746094\n",
            "Training epoch 5535/1000000, d_loss: -254.2522430419922,  g_loss: -169.12742614746094\n",
            "Training epoch 5536/1000000, d_loss: 22.946327209472656,  g_loss: 71.93905639648438\n",
            "Training epoch 5537/1000000, d_loss: -94.40357971191406,  g_loss: 157.76950073242188\n",
            "Training epoch 5538/1000000, d_loss: -146.44224548339844,  g_loss: 98.65132141113281\n",
            "Training epoch 5539/1000000, d_loss: -18.853343963623047,  g_loss: 132.7813262939453\n",
            "Training epoch 5540/1000000, d_loss: -95.10736083984375,  g_loss: 20.752784729003906\n",
            "Training epoch 5541/1000000, d_loss: -111.2730941772461,  g_loss: 47.568050384521484\n",
            "Training epoch 5542/1000000, d_loss: -174.95509338378906,  g_loss: -84.16340637207031\n",
            "Training epoch 5543/1000000, d_loss: -36.90692138671875,  g_loss: 49.55126953125\n",
            "Training epoch 5544/1000000, d_loss: -330.6210021972656,  g_loss: 252.9352569580078\n",
            "Training epoch 5545/1000000, d_loss: -390.1176452636719,  g_loss: -51.363365173339844\n",
            "Training epoch 5546/1000000, d_loss: -64.77076721191406,  g_loss: 67.28645324707031\n",
            "Training epoch 5547/1000000, d_loss: -24.22162628173828,  g_loss: 34.16844940185547\n",
            "Training epoch 5548/1000000, d_loss: -19.246158599853516,  g_loss: 37.261085510253906\n",
            "Training epoch 5549/1000000, d_loss: 1.0858678817749023,  g_loss: 3.110767364501953\n",
            "Training epoch 5550/1000000, d_loss: -4.012731552124023,  g_loss: 39.951377868652344\n",
            "Training epoch 5551/1000000, d_loss: 19.85240936279297,  g_loss: 108.67019653320312\n",
            "Training epoch 5552/1000000, d_loss: -28.362228393554688,  g_loss: 166.39328002929688\n",
            "Training epoch 5553/1000000, d_loss: -53.55599594116211,  g_loss: 158.99122619628906\n",
            "Training epoch 5554/1000000, d_loss: -67.05484008789062,  g_loss: 186.33779907226562\n",
            "Training epoch 5555/1000000, d_loss: -47.41053009033203,  g_loss: 152.94677734375\n",
            "Training epoch 5556/1000000, d_loss: -104.61103820800781,  g_loss: 158.62339782714844\n",
            "Training epoch 5557/1000000, d_loss: -96.83395385742188,  g_loss: 175.4669647216797\n",
            "Training epoch 5558/1000000, d_loss: -232.9217529296875,  g_loss: 109.896728515625\n",
            "Training epoch 5559/1000000, d_loss: -60.26451873779297,  g_loss: 122.22539520263672\n",
            "Training epoch 5560/1000000, d_loss: -85.35295104980469,  g_loss: 142.9949951171875\n",
            "Training epoch 5561/1000000, d_loss: -191.84793090820312,  g_loss: 140.453857421875\n",
            "Training epoch 5562/1000000, d_loss: -68.4007568359375,  g_loss: 235.66354370117188\n",
            "Training epoch 5563/1000000, d_loss: -336.66839599609375,  g_loss: -8.245674133300781\n",
            "Training epoch 5564/1000000, d_loss: -42.32329559326172,  g_loss: 153.0380859375\n",
            "Training epoch 5565/1000000, d_loss: -71.71852111816406,  g_loss: 193.0391845703125\n",
            "Training epoch 5566/1000000, d_loss: -162.3790740966797,  g_loss: 58.015777587890625\n",
            "Training epoch 5567/1000000, d_loss: -26.48126220703125,  g_loss: 124.62733459472656\n",
            "Training epoch 5568/1000000, d_loss: -53.08330154418945,  g_loss: 168.50648498535156\n",
            "Training epoch 5569/1000000, d_loss: -237.88475036621094,  g_loss: 57.714012145996094\n",
            "Training epoch 5570/1000000, d_loss: -85.13813781738281,  g_loss: 47.32120895385742\n",
            "Training epoch 5571/1000000, d_loss: -144.3696746826172,  g_loss: 132.5590057373047\n",
            "Training epoch 5572/1000000, d_loss: -148.91001892089844,  g_loss: 99.82318115234375\n",
            "Training epoch 5573/1000000, d_loss: -167.7423095703125,  g_loss: 302.81744384765625\n",
            "Training epoch 5574/1000000, d_loss: -37.29246520996094,  g_loss: 186.45718383789062\n",
            "Training epoch 5575/1000000, d_loss: -37.68296432495117,  g_loss: 114.03842163085938\n",
            "Training epoch 5576/1000000, d_loss: -592.2685546875,  g_loss: -294.8138427734375\n",
            "Training epoch 5577/1000000, d_loss: 1183.55908203125,  g_loss: -163.2998809814453\n",
            "Training epoch 5578/1000000, d_loss: -93.99678039550781,  g_loss: -97.02272033691406\n",
            "Training epoch 5579/1000000, d_loss: -124.77616882324219,  g_loss: -11.68328857421875\n",
            "Training epoch 5580/1000000, d_loss: 4.200660705566406,  g_loss: 23.73333740234375\n",
            "Training epoch 5581/1000000, d_loss: -87.4637451171875,  g_loss: 61.79529571533203\n",
            "Training epoch 5582/1000000, d_loss: -100.22309875488281,  g_loss: -44.975860595703125\n",
            "Training epoch 5583/1000000, d_loss: -973.2975463867188,  g_loss: -787.09619140625\n",
            "Training epoch 5584/1000000, d_loss: -2.9883251190185547,  g_loss: 106.34942626953125\n",
            "Training epoch 5585/1000000, d_loss: -172.65628051757812,  g_loss: 437.8822021484375\n",
            "Training epoch 5586/1000000, d_loss: -463.38226318359375,  g_loss: 723.8494873046875\n",
            "Training epoch 5587/1000000, d_loss: 21.092784881591797,  g_loss: -16.20557403564453\n",
            "Training epoch 5588/1000000, d_loss: -146.5633544921875,  g_loss: 189.2288360595703\n",
            "Training epoch 5589/1000000, d_loss: -500.836181640625,  g_loss: -47.23429870605469\n",
            "Training epoch 5590/1000000, d_loss: -12.763473510742188,  g_loss: 58.736576080322266\n",
            "Training epoch 5591/1000000, d_loss: -85.95347595214844,  g_loss: 99.00729370117188\n",
            "Training epoch 5592/1000000, d_loss: -152.9232177734375,  g_loss: 208.45147705078125\n",
            "Training epoch 5593/1000000, d_loss: -98.93085479736328,  g_loss: 101.36326599121094\n",
            "Training epoch 5594/1000000, d_loss: -72.45441436767578,  g_loss: 83.3962173461914\n",
            "Training epoch 5595/1000000, d_loss: -190.15380859375,  g_loss: -14.61001968383789\n",
            "Training epoch 5596/1000000, d_loss: -644.6104125976562,  g_loss: -125.99810791015625\n",
            "Training epoch 5597/1000000, d_loss: -63.873416900634766,  g_loss: -11.176088333129883\n",
            "Training epoch 5598/1000000, d_loss: -50.27704620361328,  g_loss: 116.81954956054688\n",
            "Training epoch 5599/1000000, d_loss: -30.048843383789062,  g_loss: 214.6803741455078\n",
            "Training epoch 5600/1000000, d_loss: -166.8135528564453,  g_loss: 228.8680877685547\n",
            "Training epoch 5601/1000000, d_loss: -101.13737487792969,  g_loss: 79.83053588867188\n",
            "Training epoch 5602/1000000, d_loss: -113.66683959960938,  g_loss: 224.75271606445312\n",
            "Training epoch 5603/1000000, d_loss: -84.88900756835938,  g_loss: 145.5152587890625\n",
            "Training epoch 5604/1000000, d_loss: -6.945056915283203,  g_loss: 77.38677978515625\n",
            "Training epoch 5605/1000000, d_loss: -254.52987670898438,  g_loss: 77.50167083740234\n",
            "Training epoch 5606/1000000, d_loss: -241.59457397460938,  g_loss: 7.0200958251953125\n",
            "Training epoch 5607/1000000, d_loss: -7.637443542480469,  g_loss: 144.55711364746094\n",
            "Training epoch 5608/1000000, d_loss: -194.71739196777344,  g_loss: 317.2931213378906\n",
            "Training epoch 5609/1000000, d_loss: -271.107666015625,  g_loss: 628.1363525390625\n",
            "Training epoch 5610/1000000, d_loss: -103.87446594238281,  g_loss: 141.8660888671875\n",
            "Training epoch 5611/1000000, d_loss: -179.86488342285156,  g_loss: 55.8453369140625\n",
            "Training epoch 5612/1000000, d_loss: -70.09339141845703,  g_loss: 81.57539367675781\n",
            "Training epoch 5613/1000000, d_loss: -75.12015533447266,  g_loss: 90.2873764038086\n",
            "Training epoch 5614/1000000, d_loss: -33.383541107177734,  g_loss: 62.53071594238281\n",
            "Training epoch 5615/1000000, d_loss: -122.82634735107422,  g_loss: 202.62301635742188\n",
            "Training epoch 5616/1000000, d_loss: -369.2420349121094,  g_loss: 6.382035255432129\n",
            "Training epoch 5617/1000000, d_loss: -321.43115234375,  g_loss: -68.548828125\n",
            "Training epoch 5618/1000000, d_loss: -46.761131286621094,  g_loss: 16.474987030029297\n",
            "Training epoch 5619/1000000, d_loss: -386.8858947753906,  g_loss: -80.35079956054688\n",
            "Training epoch 5620/1000000, d_loss: -136.59130859375,  g_loss: -42.89935302734375\n",
            "Training epoch 5621/1000000, d_loss: -19.060007095336914,  g_loss: 95.16622161865234\n",
            "Training epoch 5622/1000000, d_loss: -64.34669494628906,  g_loss: 48.05701446533203\n",
            "Training epoch 5623/1000000, d_loss: -217.83682250976562,  g_loss: 8.333077430725098\n",
            "Training epoch 5624/1000000, d_loss: -378.0541687011719,  g_loss: -80.15565490722656\n",
            "Training epoch 5625/1000000, d_loss: -78.38851928710938,  g_loss: -52.40509796142578\n",
            "Training epoch 5626/1000000, d_loss: 72.11585235595703,  g_loss: 145.721435546875\n",
            "Training epoch 5627/1000000, d_loss: -108.97369384765625,  g_loss: 140.03167724609375\n",
            "Training epoch 5628/1000000, d_loss: 87.43879699707031,  g_loss: 80.72230529785156\n",
            "Training epoch 5629/1000000, d_loss: -133.21592712402344,  g_loss: 102.9939956665039\n",
            "Training epoch 5630/1000000, d_loss: -121.73414611816406,  g_loss: 131.98861694335938\n",
            "Training epoch 5631/1000000, d_loss: -122.93870544433594,  g_loss: 132.6322479248047\n",
            "Training epoch 5632/1000000, d_loss: -19.12386703491211,  g_loss: 73.51606750488281\n",
            "Training epoch 5633/1000000, d_loss: -10.297470092773438,  g_loss: 30.28207015991211\n",
            "Training epoch 5634/1000000, d_loss: -26.648509979248047,  g_loss: 45.93089294433594\n",
            "Training epoch 5635/1000000, d_loss: -315.73211669921875,  g_loss: 90.29586791992188\n",
            "Training epoch 5636/1000000, d_loss: -169.73843383789062,  g_loss: -25.490575790405273\n",
            "Training epoch 5637/1000000, d_loss: -53.55767059326172,  g_loss: 123.8824462890625\n",
            "Training epoch 5638/1000000, d_loss: -101.8836669921875,  g_loss: 245.1791534423828\n",
            "Training epoch 5639/1000000, d_loss: -183.3270263671875,  g_loss: 70.67047882080078\n",
            "Training epoch 5640/1000000, d_loss: -142.0546417236328,  g_loss: 32.65302276611328\n",
            "Training epoch 5641/1000000, d_loss: -90.26768493652344,  g_loss: 67.55403137207031\n",
            "Training epoch 5642/1000000, d_loss: -43.951026916503906,  g_loss: 160.25640869140625\n",
            "Training epoch 5643/1000000, d_loss: -515.58642578125,  g_loss: 98.89106750488281\n",
            "Training epoch 5644/1000000, d_loss: -188.6097412109375,  g_loss: -9.9765625\n",
            "Training epoch 5645/1000000, d_loss: -69.75627136230469,  g_loss: 98.26284790039062\n",
            "Training epoch 5646/1000000, d_loss: -78.64622497558594,  g_loss: 202.769287109375\n",
            "Training epoch 5647/1000000, d_loss: -65.65203857421875,  g_loss: 89.19305419921875\n",
            "Training epoch 5648/1000000, d_loss: -39.50469970703125,  g_loss: 68.74275207519531\n",
            "Training epoch 5649/1000000, d_loss: 37.86054229736328,  g_loss: 77.36505126953125\n",
            "Training epoch 5650/1000000, d_loss: -39.46444320678711,  g_loss: 64.15147399902344\n",
            "Training epoch 5651/1000000, d_loss: -82.42344665527344,  g_loss: 95.54476928710938\n",
            "Training epoch 5652/1000000, d_loss: -74.09959411621094,  g_loss: 68.47026062011719\n",
            "Training epoch 5653/1000000, d_loss: -209.93179321289062,  g_loss: 5.4008378982543945\n",
            "Training epoch 5654/1000000, d_loss: -139.39743041992188,  g_loss: 47.79093933105469\n",
            "Training epoch 5655/1000000, d_loss: -131.44581604003906,  g_loss: 20.25354766845703\n",
            "Training epoch 5656/1000000, d_loss: -144.40823364257812,  g_loss: -4.184366226196289\n",
            "Training epoch 5657/1000000, d_loss: -85.14346313476562,  g_loss: 101.07368469238281\n",
            "Training epoch 5658/1000000, d_loss: -344.80755615234375,  g_loss: -42.70597839355469\n",
            "Training epoch 5659/1000000, d_loss: -710.7685546875,  g_loss: -185.96295166015625\n",
            "Training epoch 5660/1000000, d_loss: -811.22998046875,  g_loss: -155.154296875\n",
            "Training epoch 5661/1000000, d_loss: 59.89430236816406,  g_loss: -56.82867431640625\n",
            "Training epoch 5662/1000000, d_loss: -17.820171356201172,  g_loss: -42.406341552734375\n",
            "Training epoch 5663/1000000, d_loss: -297.58203125,  g_loss: -87.4667739868164\n",
            "Training epoch 5664/1000000, d_loss: -13.055946350097656,  g_loss: -27.676727294921875\n",
            "Training epoch 5665/1000000, d_loss: 76.14028930664062,  g_loss: -67.68937683105469\n",
            "Training epoch 5666/1000000, d_loss: -216.3824920654297,  g_loss: -275.2496032714844\n",
            "Training epoch 5667/1000000, d_loss: -92.06179809570312,  g_loss: 110.46536254882812\n",
            "Training epoch 5668/1000000, d_loss: -61.999900817871094,  g_loss: 31.927724838256836\n",
            "Training epoch 5669/1000000, d_loss: -191.3121337890625,  g_loss: 130.77841186523438\n",
            "Training epoch 5670/1000000, d_loss: -407.0137939453125,  g_loss: 363.9903259277344\n",
            "Training epoch 5671/1000000, d_loss: -18.111083984375,  g_loss: 200.83230590820312\n",
            "Training epoch 5672/1000000, d_loss: -70.71846771240234,  g_loss: 244.18016052246094\n",
            "Training epoch 5673/1000000, d_loss: -147.37640380859375,  g_loss: 146.17762756347656\n",
            "Training epoch 5674/1000000, d_loss: -95.56034851074219,  g_loss: 132.67271423339844\n",
            "Training epoch 5675/1000000, d_loss: -7.259845733642578,  g_loss: 97.93605041503906\n",
            "Training epoch 5676/1000000, d_loss: -99.0475845336914,  g_loss: 91.07171630859375\n",
            "Training epoch 5677/1000000, d_loss: -139.31805419921875,  g_loss: 156.2291259765625\n",
            "Training epoch 5678/1000000, d_loss: -43.94915771484375,  g_loss: 177.71539306640625\n",
            "Training epoch 5679/1000000, d_loss: -149.4896697998047,  g_loss: 240.23280334472656\n",
            "Training epoch 5680/1000000, d_loss: -42.680877685546875,  g_loss: 106.85585021972656\n",
            "Training epoch 5681/1000000, d_loss: -24.527050018310547,  g_loss: 132.69583129882812\n",
            "Training epoch 5682/1000000, d_loss: -141.34747314453125,  g_loss: 34.842010498046875\n",
            "Training epoch 5683/1000000, d_loss: -41.428672790527344,  g_loss: 87.46883392333984\n",
            "Training epoch 5684/1000000, d_loss: -76.20318603515625,  g_loss: 157.038330078125\n",
            "Training epoch 5685/1000000, d_loss: -85.65968322753906,  g_loss: 131.35671997070312\n",
            "Training epoch 5686/1000000, d_loss: -62.104042053222656,  g_loss: 104.74934387207031\n",
            "Training epoch 5687/1000000, d_loss: -31.011890411376953,  g_loss: 56.24241638183594\n",
            "Training epoch 5688/1000000, d_loss: -78.92752075195312,  g_loss: 50.45787811279297\n",
            "Training epoch 5689/1000000, d_loss: -15.8951416015625,  g_loss: 75.02118682861328\n",
            "Training epoch 5690/1000000, d_loss: -29.625272750854492,  g_loss: 82.19589233398438\n",
            "Training epoch 5691/1000000, d_loss: -387.7353210449219,  g_loss: -95.43331146240234\n",
            "Training epoch 5692/1000000, d_loss: -24.144380569458008,  g_loss: 45.13531494140625\n",
            "Training epoch 5693/1000000, d_loss: -119.89579772949219,  g_loss: 63.152225494384766\n",
            "Training epoch 5694/1000000, d_loss: -194.77711486816406,  g_loss: 144.54452514648438\n",
            "Training epoch 5695/1000000, d_loss: 46.30725860595703,  g_loss: 127.63570404052734\n",
            "Training epoch 5696/1000000, d_loss: -421.4417419433594,  g_loss: -71.53243255615234\n",
            "Training epoch 5697/1000000, d_loss: -18.374866485595703,  g_loss: 67.40574645996094\n",
            "Training epoch 5698/1000000, d_loss: -19.73494529724121,  g_loss: 42.048763275146484\n",
            "Training epoch 5699/1000000, d_loss: -79.95506286621094,  g_loss: 86.30654907226562\n",
            "Training epoch 5700/1000000, d_loss: -250.50228881835938,  g_loss: 3.3252639770507812\n",
            "Training epoch 5701/1000000, d_loss: 13.160469055175781,  g_loss: -30.84136962890625\n",
            "Training epoch 5702/1000000, d_loss: -42.024871826171875,  g_loss: 122.12109375\n",
            "Training epoch 5703/1000000, d_loss: -108.73051452636719,  g_loss: 161.82894897460938\n",
            "Training epoch 5704/1000000, d_loss: -34.65652084350586,  g_loss: 118.14757537841797\n",
            "Training epoch 5705/1000000, d_loss: -165.38156127929688,  g_loss: 111.59078979492188\n",
            "Training epoch 5706/1000000, d_loss: -56.94050979614258,  g_loss: 132.26397705078125\n",
            "Training epoch 5707/1000000, d_loss: -146.31134033203125,  g_loss: 167.10372924804688\n",
            "Training epoch 5708/1000000, d_loss: -100.17752838134766,  g_loss: 117.91780090332031\n",
            "Training epoch 5709/1000000, d_loss: -54.84553146362305,  g_loss: 160.6634063720703\n",
            "Training epoch 5710/1000000, d_loss: -159.42251586914062,  g_loss: 126.93359375\n",
            "Training epoch 5711/1000000, d_loss: 46.374114990234375,  g_loss: 144.71923828125\n",
            "Training epoch 5712/1000000, d_loss: -65.00570678710938,  g_loss: 164.15402221679688\n",
            "Training epoch 5713/1000000, d_loss: -197.32887268066406,  g_loss: 203.73574829101562\n",
            "Training epoch 5714/1000000, d_loss: -107.4439697265625,  g_loss: 194.98501586914062\n",
            "Training epoch 5715/1000000, d_loss: -132.58944702148438,  g_loss: 210.54898071289062\n",
            "Training epoch 5716/1000000, d_loss: -141.30908203125,  g_loss: 221.1626434326172\n",
            "Training epoch 5717/1000000, d_loss: 17.46584701538086,  g_loss: 135.44363403320312\n",
            "Training epoch 5718/1000000, d_loss: -81.28121948242188,  g_loss: 136.980224609375\n",
            "Training epoch 5719/1000000, d_loss: -158.9365234375,  g_loss: 107.8414535522461\n",
            "Training epoch 5720/1000000, d_loss: -709.6751708984375,  g_loss: -265.89447021484375\n",
            "Training epoch 5721/1000000, d_loss: 17.99648094177246,  g_loss: -24.52145767211914\n",
            "Training epoch 5722/1000000, d_loss: 21.491111755371094,  g_loss: 20.197834014892578\n",
            "Training epoch 5723/1000000, d_loss: -70.11210632324219,  g_loss: -18.097240447998047\n",
            "Training epoch 5724/1000000, d_loss: 80.53028869628906,  g_loss: 224.26625061035156\n",
            "Training epoch 5725/1000000, d_loss: -63.88146209716797,  g_loss: 157.79898071289062\n",
            "Training epoch 5726/1000000, d_loss: -136.6531982421875,  g_loss: 152.94256591796875\n",
            "Training epoch 5727/1000000, d_loss: -149.81056213378906,  g_loss: 307.932861328125\n",
            "Training epoch 5728/1000000, d_loss: -107.29510498046875,  g_loss: 210.6381378173828\n",
            "Training epoch 5729/1000000, d_loss: -14.709564208984375,  g_loss: 116.58830261230469\n",
            "Training epoch 5730/1000000, d_loss: -30.47320556640625,  g_loss: 103.3410873413086\n",
            "Training epoch 5731/1000000, d_loss: -37.099220275878906,  g_loss: 38.73455810546875\n",
            "Training epoch 5732/1000000, d_loss: -42.282894134521484,  g_loss: 43.9674072265625\n",
            "Training epoch 5733/1000000, d_loss: -46.98985290527344,  g_loss: 65.03531646728516\n",
            "Training epoch 5734/1000000, d_loss: -413.7367858886719,  g_loss: -8.177204132080078\n",
            "Training epoch 5735/1000000, d_loss: -6177.5966796875,  g_loss: -888.658203125\n",
            "Training epoch 5736/1000000, d_loss: 3867.39453125,  g_loss: -838.7904663085938\n",
            "Training epoch 5737/1000000, d_loss: 1995.4656982421875,  g_loss: -1038.0931396484375\n",
            "Training epoch 5738/1000000, d_loss: 13837.955078125,  g_loss: -798.286376953125\n",
            "Training epoch 5739/1000000, d_loss: 1095.112060546875,  g_loss: -1064.6864013671875\n",
            "Training epoch 5740/1000000, d_loss: 1094.38720703125,  g_loss: -485.49267578125\n",
            "Training epoch 5741/1000000, d_loss: 8.423484802246094,  g_loss: -11.576568603515625\n",
            "Training epoch 5742/1000000, d_loss: 50.550567626953125,  g_loss: -327.6553955078125\n",
            "Training epoch 5743/1000000, d_loss: -52.566864013671875,  g_loss: -301.760009765625\n",
            "Training epoch 5744/1000000, d_loss: -255.06655883789062,  g_loss: 555.8004760742188\n",
            "Training epoch 5745/1000000, d_loss: -370.6297912597656,  g_loss: 465.433349609375\n",
            "Training epoch 5746/1000000, d_loss: -326.88427734375,  g_loss: 848.99560546875\n",
            "Training epoch 5747/1000000, d_loss: -299.56060791015625,  g_loss: 636.458740234375\n",
            "Training epoch 5748/1000000, d_loss: 66.468017578125,  g_loss: 212.68910217285156\n",
            "Training epoch 5749/1000000, d_loss: -88.90907287597656,  g_loss: -25.540952682495117\n",
            "Training epoch 5750/1000000, d_loss: -130.94305419921875,  g_loss: 147.6516876220703\n",
            "Training epoch 5751/1000000, d_loss: 37.1280517578125,  g_loss: 45.154197692871094\n",
            "Training epoch 5752/1000000, d_loss: -21.00927734375,  g_loss: -180.94325256347656\n",
            "Training epoch 5753/1000000, d_loss: -145.63632202148438,  g_loss: -140.65478515625\n",
            "Training epoch 5754/1000000, d_loss: -69.70939636230469,  g_loss: -37.03654479980469\n",
            "Training epoch 5755/1000000, d_loss: -213.29920959472656,  g_loss: 236.295654296875\n",
            "Training epoch 5756/1000000, d_loss: -82.2896499633789,  g_loss: 515.02001953125\n",
            "Training epoch 5757/1000000, d_loss: -9.18792724609375,  g_loss: 33.93031311035156\n",
            "Training epoch 5758/1000000, d_loss: 43.64546203613281,  g_loss: 151.90196228027344\n",
            "Training epoch 5759/1000000, d_loss: -276.8348388671875,  g_loss: 309.8660583496094\n",
            "Training epoch 5760/1000000, d_loss: -329.2754211425781,  g_loss: 454.42059326171875\n",
            "Training epoch 5761/1000000, d_loss: -146.47071838378906,  g_loss: 318.285888671875\n",
            "Training epoch 5762/1000000, d_loss: -276.5301513671875,  g_loss: 557.7115478515625\n",
            "Training epoch 5763/1000000, d_loss: -24.715269088745117,  g_loss: 107.77751922607422\n",
            "Training epoch 5764/1000000, d_loss: -119.4945297241211,  g_loss: 64.7017822265625\n",
            "Training epoch 5765/1000000, d_loss: -87.09893035888672,  g_loss: 41.82748794555664\n",
            "Training epoch 5766/1000000, d_loss: -313.0386047363281,  g_loss: -44.428558349609375\n",
            "Training epoch 5767/1000000, d_loss: -93.63752746582031,  g_loss: -50.88582992553711\n",
            "Training epoch 5768/1000000, d_loss: -49.51227951049805,  g_loss: 41.420814514160156\n",
            "Training epoch 5769/1000000, d_loss: -184.23846435546875,  g_loss: -43.839927673339844\n",
            "Training epoch 5770/1000000, d_loss: 29.960006713867188,  g_loss: 113.87611389160156\n",
            "Training epoch 5771/1000000, d_loss: -177.5696563720703,  g_loss: 99.75338745117188\n",
            "Training epoch 5772/1000000, d_loss: -42.87093734741211,  g_loss: 103.35032653808594\n",
            "Training epoch 5773/1000000, d_loss: -127.24757385253906,  g_loss: 125.21574401855469\n",
            "Training epoch 5774/1000000, d_loss: -102.02527618408203,  g_loss: 158.1304168701172\n",
            "Training epoch 5775/1000000, d_loss: -207.75961303710938,  g_loss: -54.34718322753906\n",
            "Training epoch 5776/1000000, d_loss: 3.4310035705566406,  g_loss: 44.80921936035156\n",
            "Training epoch 5777/1000000, d_loss: -24.872974395751953,  g_loss: 126.25312805175781\n",
            "Training epoch 5778/1000000, d_loss: -107.54957580566406,  g_loss: 273.55133056640625\n",
            "Training epoch 5779/1000000, d_loss: -115.04121398925781,  g_loss: -58.32465362548828\n",
            "Training epoch 5780/1000000, d_loss: -59.60569381713867,  g_loss: 42.87348175048828\n",
            "Training epoch 5781/1000000, d_loss: -31.562856674194336,  g_loss: 52.54449462890625\n",
            "Training epoch 5782/1000000, d_loss: -180.36679077148438,  g_loss: 22.47991180419922\n",
            "Training epoch 5783/1000000, d_loss: -158.91278076171875,  g_loss: -22.5703182220459\n",
            "Training epoch 5784/1000000, d_loss: 26.97992706298828,  g_loss: 49.51612091064453\n",
            "Training epoch 5785/1000000, d_loss: -183.51156616210938,  g_loss: 83.27793884277344\n",
            "Training epoch 5786/1000000, d_loss: -1.686356544494629,  g_loss: 144.3732452392578\n",
            "Training epoch 5787/1000000, d_loss: -48.4349365234375,  g_loss: 199.8548583984375\n",
            "Training epoch 5788/1000000, d_loss: -89.15802001953125,  g_loss: 148.67755126953125\n",
            "Training epoch 5789/1000000, d_loss: -38.267173767089844,  g_loss: 47.582366943359375\n",
            "Training epoch 5790/1000000, d_loss: -199.07579040527344,  g_loss: 67.13568115234375\n",
            "Training epoch 5791/1000000, d_loss: 10.033182144165039,  g_loss: 64.49805450439453\n",
            "Training epoch 5792/1000000, d_loss: -63.977149963378906,  g_loss: 93.7143325805664\n",
            "Training epoch 5793/1000000, d_loss: -258.1105651855469,  g_loss: 76.32908630371094\n",
            "Training epoch 5794/1000000, d_loss: -120.83368682861328,  g_loss: -56.44141387939453\n",
            "Training epoch 5795/1000000, d_loss: -137.8289031982422,  g_loss: -11.743374824523926\n",
            "Training epoch 5796/1000000, d_loss: -62.14784240722656,  g_loss: 121.16841125488281\n",
            "Training epoch 5797/1000000, d_loss: -91.84307861328125,  g_loss: 114.09170532226562\n",
            "Training epoch 5798/1000000, d_loss: -35.02693176269531,  g_loss: 55.356903076171875\n",
            "Training epoch 5799/1000000, d_loss: -76.55291748046875,  g_loss: 74.83981323242188\n",
            "Training epoch 5800/1000000, d_loss: -100.84851837158203,  g_loss: 97.70477294921875\n",
            "Training epoch 5801/1000000, d_loss: -167.99636840820312,  g_loss: 63.56471252441406\n",
            "Training epoch 5802/1000000, d_loss: -412.9809265136719,  g_loss: -79.81930541992188\n",
            "Training epoch 5803/1000000, d_loss: -105.78923034667969,  g_loss: 175.1744384765625\n",
            "Training epoch 5804/1000000, d_loss: -380.07843017578125,  g_loss: -30.90035629272461\n",
            "Training epoch 5805/1000000, d_loss: -61.063995361328125,  g_loss: 7.138202667236328\n",
            "Training epoch 5806/1000000, d_loss: -82.9137954711914,  g_loss: 8.67164421081543\n",
            "Training epoch 5807/1000000, d_loss: -165.53900146484375,  g_loss: 68.6040267944336\n",
            "Training epoch 5808/1000000, d_loss: -126.03746032714844,  g_loss: -9.195674896240234\n",
            "Training epoch 5809/1000000, d_loss: -223.61373901367188,  g_loss: 60.606407165527344\n",
            "Training epoch 5810/1000000, d_loss: -275.2727355957031,  g_loss: -31.345073699951172\n",
            "Training epoch 5811/1000000, d_loss: -809.8199462890625,  g_loss: -171.20111083984375\n",
            "Training epoch 5812/1000000, d_loss: -0.42462921142578125,  g_loss: 3.2087173461914062\n",
            "Training epoch 5813/1000000, d_loss: -7.279690742492676,  g_loss: 18.7368106842041\n",
            "Training epoch 5814/1000000, d_loss: -34.3741569519043,  g_loss: 146.33291625976562\n",
            "Training epoch 5815/1000000, d_loss: -194.38504028320312,  g_loss: 116.25454711914062\n",
            "Training epoch 5816/1000000, d_loss: -35.91004943847656,  g_loss: 235.4033203125\n",
            "Training epoch 5817/1000000, d_loss: -129.80331420898438,  g_loss: 64.4339370727539\n",
            "Training epoch 5818/1000000, d_loss: 91.84626770019531,  g_loss: 22.94985008239746\n",
            "Training epoch 5819/1000000, d_loss: -224.52865600585938,  g_loss: -14.67837905883789\n",
            "Training epoch 5820/1000000, d_loss: -50.412906646728516,  g_loss: 14.415019989013672\n",
            "Training epoch 5821/1000000, d_loss: -594.5806884765625,  g_loss: -113.93336486816406\n",
            "Training epoch 5822/1000000, d_loss: 431.1483154296875,  g_loss: 81.47978973388672\n",
            "Training epoch 5823/1000000, d_loss: -32.012542724609375,  g_loss: 33.54154968261719\n",
            "Training epoch 5824/1000000, d_loss: -108.31037139892578,  g_loss: 52.532501220703125\n",
            "Training epoch 5825/1000000, d_loss: -63.66594696044922,  g_loss: 86.46501922607422\n",
            "Training epoch 5826/1000000, d_loss: -278.3970947265625,  g_loss: 147.91624450683594\n",
            "Training epoch 5827/1000000, d_loss: -411.0735168457031,  g_loss: 100.938720703125\n",
            "Training epoch 5828/1000000, d_loss: -487.0988464355469,  g_loss: 73.53632354736328\n",
            "Training epoch 5829/1000000, d_loss: -29.691116333007812,  g_loss: 152.24615478515625\n",
            "Training epoch 5830/1000000, d_loss: -138.54933166503906,  g_loss: 229.45314025878906\n",
            "Training epoch 5831/1000000, d_loss: -51.369384765625,  g_loss: 169.241455078125\n",
            "Training epoch 5832/1000000, d_loss: -95.22575378417969,  g_loss: 188.223388671875\n",
            "Training epoch 5833/1000000, d_loss: -92.13646697998047,  g_loss: 174.96929931640625\n",
            "Training epoch 5834/1000000, d_loss: -44.214866638183594,  g_loss: 166.36146545410156\n",
            "Training epoch 5835/1000000, d_loss: -164.9742431640625,  g_loss: 91.77808380126953\n",
            "Training epoch 5836/1000000, d_loss: -62.189788818359375,  g_loss: 261.959228515625\n",
            "Training epoch 5837/1000000, d_loss: -124.43244934082031,  g_loss: 139.59884643554688\n",
            "Training epoch 5838/1000000, d_loss: -143.2579803466797,  g_loss: 109.32191467285156\n",
            "Training epoch 5839/1000000, d_loss: -95.77302551269531,  g_loss: 116.98480224609375\n",
            "Training epoch 5840/1000000, d_loss: -39.545597076416016,  g_loss: 105.42339324951172\n",
            "Training epoch 5841/1000000, d_loss: 41.98509979248047,  g_loss: 131.74490356445312\n",
            "Training epoch 5842/1000000, d_loss: -76.91812896728516,  g_loss: 95.01467895507812\n",
            "Training epoch 5843/1000000, d_loss: -210.1776580810547,  g_loss: 105.55780029296875\n",
            "Training epoch 5844/1000000, d_loss: -22.624805450439453,  g_loss: 137.05471801757812\n",
            "Training epoch 5845/1000000, d_loss: -244.45211791992188,  g_loss: 44.595462799072266\n",
            "Training epoch 5846/1000000, d_loss: -371.83905029296875,  g_loss: 7.482538223266602\n",
            "Training epoch 5847/1000000, d_loss: -13.761917114257812,  g_loss: 88.87117004394531\n",
            "Training epoch 5848/1000000, d_loss: 1.7742958068847656,  g_loss: 34.420345306396484\n",
            "Training epoch 5849/1000000, d_loss: -90.0301284790039,  g_loss: 46.23944091796875\n",
            "Training epoch 5850/1000000, d_loss: -100.64418029785156,  g_loss: 88.87307739257812\n",
            "Training epoch 5851/1000000, d_loss: -139.56045532226562,  g_loss: 18.748275756835938\n",
            "Training epoch 5852/1000000, d_loss: -79.90471649169922,  g_loss: 210.62997436523438\n",
            "Training epoch 5853/1000000, d_loss: -302.7401123046875,  g_loss: -54.08643341064453\n",
            "Training epoch 5854/1000000, d_loss: -12.457841873168945,  g_loss: 98.77163696289062\n",
            "Training epoch 5855/1000000, d_loss: -167.76773071289062,  g_loss: 233.85867309570312\n",
            "Training epoch 5856/1000000, d_loss: -73.90443420410156,  g_loss: 166.7600555419922\n",
            "Training epoch 5857/1000000, d_loss: -62.51588821411133,  g_loss: 220.06146240234375\n",
            "Training epoch 5858/1000000, d_loss: -23.750904083251953,  g_loss: 102.97088623046875\n",
            "Training epoch 5859/1000000, d_loss: -56.46703338623047,  g_loss: 21.9080810546875\n",
            "Training epoch 5860/1000000, d_loss: -846.9751586914062,  g_loss: -240.08212280273438\n",
            "Training epoch 5861/1000000, d_loss: -128.93075561523438,  g_loss: -54.3552131652832\n",
            "Training epoch 5862/1000000, d_loss: -324.810546875,  g_loss: -81.70173645019531\n",
            "Training epoch 5863/1000000, d_loss: 75.07884216308594,  g_loss: 55.20147705078125\n",
            "Training epoch 5864/1000000, d_loss: -209.83811950683594,  g_loss: 182.7849578857422\n",
            "Training epoch 5865/1000000, d_loss: -196.15264892578125,  g_loss: 97.76990509033203\n",
            "Training epoch 5866/1000000, d_loss: -236.3202667236328,  g_loss: 242.0462646484375\n",
            "Training epoch 5867/1000000, d_loss: -52.01509094238281,  g_loss: -56.60639572143555\n",
            "Training epoch 5868/1000000, d_loss: -121.98541259765625,  g_loss: 73.26457977294922\n",
            "Training epoch 5869/1000000, d_loss: 10.703712463378906,  g_loss: 177.86962890625\n",
            "Training epoch 5870/1000000, d_loss: -110.6707763671875,  g_loss: 112.18862915039062\n",
            "Training epoch 5871/1000000, d_loss: -128.10134887695312,  g_loss: 199.90872192382812\n",
            "Training epoch 5872/1000000, d_loss: 35.93983459472656,  g_loss: 47.035369873046875\n",
            "Training epoch 5873/1000000, d_loss: -100.73487854003906,  g_loss: 64.62155151367188\n",
            "Training epoch 5874/1000000, d_loss: -10.738241195678711,  g_loss: 43.827232360839844\n",
            "Training epoch 5875/1000000, d_loss: -105.8323745727539,  g_loss: 61.68331527709961\n",
            "Training epoch 5876/1000000, d_loss: -126.6287841796875,  g_loss: 53.12393569946289\n",
            "Training epoch 5877/1000000, d_loss: 13.74755859375,  g_loss: 114.1238021850586\n",
            "Training epoch 5878/1000000, d_loss: -136.619384765625,  g_loss: 61.9423828125\n",
            "Training epoch 5879/1000000, d_loss: -112.48674774169922,  g_loss: 10.155097961425781\n",
            "Training epoch 5880/1000000, d_loss: -106.00602722167969,  g_loss: 54.123924255371094\n",
            "Training epoch 5881/1000000, d_loss: -161.30429077148438,  g_loss: 38.310882568359375\n",
            "Training epoch 5882/1000000, d_loss: -58.749637603759766,  g_loss: 213.1300811767578\n",
            "Training epoch 5883/1000000, d_loss: -53.72795104980469,  g_loss: 93.61468505859375\n",
            "Training epoch 5884/1000000, d_loss: -793.8534545898438,  g_loss: -213.11544799804688\n",
            "Training epoch 5885/1000000, d_loss: 29.511680603027344,  g_loss: 13.665069580078125\n",
            "Training epoch 5886/1000000, d_loss: -14.613809585571289,  g_loss: -110.89337158203125\n",
            "Training epoch 5887/1000000, d_loss: -366.6300964355469,  g_loss: -129.53675842285156\n",
            "Training epoch 5888/1000000, d_loss: -214.02601623535156,  g_loss: -11.148188591003418\n",
            "Training epoch 5889/1000000, d_loss: -16.993144989013672,  g_loss: 29.471935272216797\n",
            "Training epoch 5890/1000000, d_loss: -117.70960235595703,  g_loss: 47.83910369873047\n",
            "Training epoch 5891/1000000, d_loss: -151.44195556640625,  g_loss: -17.874839782714844\n",
            "Training epoch 5892/1000000, d_loss: -159.79100036621094,  g_loss: -14.566004753112793\n",
            "Training epoch 5893/1000000, d_loss: -485.81591796875,  g_loss: -158.79421997070312\n",
            "Training epoch 5894/1000000, d_loss: -67.24626159667969,  g_loss: 82.34178924560547\n",
            "Training epoch 5895/1000000, d_loss: -155.80064392089844,  g_loss: 56.748207092285156\n",
            "Training epoch 5896/1000000, d_loss: -239.7170867919922,  g_loss: 64.13545227050781\n",
            "Training epoch 5897/1000000, d_loss: -248.07069396972656,  g_loss: 26.491352081298828\n",
            "Training epoch 5898/1000000, d_loss: -310.96014404296875,  g_loss: -26.129526138305664\n",
            "Training epoch 5899/1000000, d_loss: -182.18521118164062,  g_loss: 233.1513214111328\n",
            "Training epoch 5900/1000000, d_loss: -401.9266357421875,  g_loss: 863.2144775390625\n",
            "Training epoch 5901/1000000, d_loss: 132.68869018554688,  g_loss: 305.84332275390625\n",
            "Training epoch 5902/1000000, d_loss: -83.80709838867188,  g_loss: 188.5990447998047\n",
            "Training epoch 5903/1000000, d_loss: -50.967891693115234,  g_loss: 218.35513305664062\n",
            "Training epoch 5904/1000000, d_loss: -314.0896301269531,  g_loss: 74.1042709350586\n",
            "Training epoch 5905/1000000, d_loss: -291.4595642089844,  g_loss: 190.66307067871094\n",
            "Training epoch 5906/1000000, d_loss: -228.08384704589844,  g_loss: 33.24694061279297\n",
            "Training epoch 5907/1000000, d_loss: -338.3905334472656,  g_loss: 18.589580535888672\n",
            "Training epoch 5908/1000000, d_loss: -304.9532775878906,  g_loss: 592.3978271484375\n",
            "Training epoch 5909/1000000, d_loss: -297.40997314453125,  g_loss: 56.74352264404297\n",
            "Training epoch 5910/1000000, d_loss: 18.32672882080078,  g_loss: 132.85800170898438\n",
            "Training epoch 5911/1000000, d_loss: -289.4297790527344,  g_loss: 35.431365966796875\n",
            "Training epoch 5912/1000000, d_loss: -37.30292892456055,  g_loss: 99.22145080566406\n",
            "Training epoch 5913/1000000, d_loss: -226.6494598388672,  g_loss: 294.1689758300781\n",
            "Training epoch 5914/1000000, d_loss: -77.93318176269531,  g_loss: 371.0630187988281\n",
            "Training epoch 5915/1000000, d_loss: -100.83843994140625,  g_loss: 220.23269653320312\n",
            "Training epoch 5916/1000000, d_loss: -169.7943115234375,  g_loss: 268.583984375\n",
            "Training epoch 5917/1000000, d_loss: -477.7857666015625,  g_loss: -8.02518081665039\n",
            "Training epoch 5918/1000000, d_loss: 429.6797180175781,  g_loss: 52.568946838378906\n",
            "Training epoch 5919/1000000, d_loss: -92.33114624023438,  g_loss: 49.7874755859375\n",
            "Training epoch 5920/1000000, d_loss: -121.6417236328125,  g_loss: 171.24771118164062\n",
            "Training epoch 5921/1000000, d_loss: -153.7071533203125,  g_loss: 82.9613265991211\n",
            "Training epoch 5922/1000000, d_loss: -278.36810302734375,  g_loss: 33.857730865478516\n",
            "Training epoch 5923/1000000, d_loss: 297.9110107421875,  g_loss: 144.74044799804688\n",
            "Training epoch 5924/1000000, d_loss: -67.47566223144531,  g_loss: 117.11226654052734\n",
            "Training epoch 5925/1000000, d_loss: -78.18467712402344,  g_loss: 190.9609375\n",
            "Training epoch 5926/1000000, d_loss: -4.306545257568359,  g_loss: 115.93155670166016\n",
            "Training epoch 5927/1000000, d_loss: -27.301727294921875,  g_loss: 95.25936889648438\n",
            "Training epoch 5928/1000000, d_loss: -271.2666931152344,  g_loss: 266.9647521972656\n",
            "Training epoch 5929/1000000, d_loss: -74.92803955078125,  g_loss: 165.54824829101562\n",
            "Training epoch 5930/1000000, d_loss: -286.00921630859375,  g_loss: 91.06083679199219\n",
            "Training epoch 5931/1000000, d_loss: -58.24611282348633,  g_loss: 130.00157165527344\n",
            "Training epoch 5932/1000000, d_loss: -58.73097229003906,  g_loss: 117.71492004394531\n",
            "Training epoch 5933/1000000, d_loss: -134.16822814941406,  g_loss: 216.0828094482422\n",
            "Training epoch 5934/1000000, d_loss: -3.109783172607422,  g_loss: 168.38987731933594\n",
            "Training epoch 5935/1000000, d_loss: -49.28458786010742,  g_loss: 89.76995849609375\n",
            "Training epoch 5936/1000000, d_loss: -53.06156921386719,  g_loss: 50.648860931396484\n",
            "Training epoch 5937/1000000, d_loss: -33.75864028930664,  g_loss: 83.12611389160156\n",
            "Training epoch 5938/1000000, d_loss: -215.97274780273438,  g_loss: -17.33418846130371\n",
            "Training epoch 5939/1000000, d_loss: -41.150123596191406,  g_loss: 103.45993041992188\n",
            "Training epoch 5940/1000000, d_loss: -110.74041748046875,  g_loss: 80.98336029052734\n",
            "Training epoch 5941/1000000, d_loss: -99.11840057373047,  g_loss: 57.0684814453125\n",
            "Training epoch 5942/1000000, d_loss: -42.35868453979492,  g_loss: 56.790863037109375\n",
            "Training epoch 5943/1000000, d_loss: -38.341182708740234,  g_loss: 44.7198486328125\n",
            "Training epoch 5944/1000000, d_loss: -303.9586181640625,  g_loss: -33.967124938964844\n",
            "Training epoch 5945/1000000, d_loss: -186.79490661621094,  g_loss: -117.10365295410156\n",
            "Training epoch 5946/1000000, d_loss: -806.2086181640625,  g_loss: -615.733642578125\n",
            "Training epoch 5947/1000000, d_loss: 258.7313537597656,  g_loss: -81.22727966308594\n",
            "Training epoch 5948/1000000, d_loss: -113.62045288085938,  g_loss: 21.493680953979492\n",
            "Training epoch 5949/1000000, d_loss: 120.70991516113281,  g_loss: 196.44337463378906\n",
            "Training epoch 5950/1000000, d_loss: 86.00711059570312,  g_loss: 173.37416076660156\n",
            "Training epoch 5951/1000000, d_loss: -78.31966400146484,  g_loss: 144.33477783203125\n",
            "Training epoch 5952/1000000, d_loss: -69.02793884277344,  g_loss: 192.12461853027344\n",
            "Training epoch 5953/1000000, d_loss: -83.51820373535156,  g_loss: 255.13577270507812\n",
            "Training epoch 5954/1000000, d_loss: -533.5717163085938,  g_loss: 22.99489974975586\n",
            "Training epoch 5955/1000000, d_loss: -72.97991180419922,  g_loss: 17.174890518188477\n",
            "Training epoch 5956/1000000, d_loss: -238.1280517578125,  g_loss: 158.78924560546875\n",
            "Training epoch 5957/1000000, d_loss: -798.7079467773438,  g_loss: -102.94699096679688\n",
            "Training epoch 5958/1000000, d_loss: -152.09481811523438,  g_loss: 21.473674774169922\n",
            "Training epoch 5959/1000000, d_loss: -108.00701904296875,  g_loss: 133.52365112304688\n",
            "Training epoch 5960/1000000, d_loss: -65.98347473144531,  g_loss: 92.5137710571289\n",
            "Training epoch 5961/1000000, d_loss: -58.616241455078125,  g_loss: 66.24687957763672\n",
            "Training epoch 5962/1000000, d_loss: -68.71875,  g_loss: 162.0483856201172\n",
            "Training epoch 5963/1000000, d_loss: -38.404579162597656,  g_loss: 145.1527099609375\n",
            "Training epoch 5964/1000000, d_loss: -165.8944549560547,  g_loss: 89.42068481445312\n",
            "Training epoch 5965/1000000, d_loss: -168.17176818847656,  g_loss: 94.82267761230469\n",
            "Training epoch 5966/1000000, d_loss: -103.23524475097656,  g_loss: 101.37478637695312\n",
            "Training epoch 5967/1000000, d_loss: -167.11325073242188,  g_loss: 32.01293182373047\n",
            "Training epoch 5968/1000000, d_loss: -13.765174865722656,  g_loss: 181.06356811523438\n",
            "Training epoch 5969/1000000, d_loss: 110.5177001953125,  g_loss: 19.125099182128906\n",
            "Training epoch 5970/1000000, d_loss: -28.928878784179688,  g_loss: 63.1447868347168\n",
            "Training epoch 5971/1000000, d_loss: -268.0753479003906,  g_loss: 197.6804656982422\n",
            "Training epoch 5972/1000000, d_loss: -434.8656921386719,  g_loss: -158.3994140625\n",
            "Training epoch 5973/1000000, d_loss: -117.219970703125,  g_loss: 92.33893585205078\n",
            "Training epoch 5974/1000000, d_loss: -240.40078735351562,  g_loss: 59.76308822631836\n",
            "Training epoch 5975/1000000, d_loss: -67.50762176513672,  g_loss: 241.09173583984375\n",
            "Training epoch 5976/1000000, d_loss: -86.6260986328125,  g_loss: 141.15634155273438\n",
            "Training epoch 5977/1000000, d_loss: -182.37486267089844,  g_loss: 71.18882751464844\n",
            "Training epoch 5978/1000000, d_loss: -272.98760986328125,  g_loss: -18.89924430847168\n",
            "Training epoch 5979/1000000, d_loss: -25.65667724609375,  g_loss: 88.33586883544922\n",
            "Training epoch 5980/1000000, d_loss: -160.14871215820312,  g_loss: 34.16355895996094\n",
            "Training epoch 5981/1000000, d_loss: -12.545204162597656,  g_loss: 126.12345886230469\n",
            "Training epoch 5982/1000000, d_loss: -465.53436279296875,  g_loss: 58.77326202392578\n",
            "Training epoch 5983/1000000, d_loss: -134.41439819335938,  g_loss: 143.35885620117188\n",
            "Training epoch 5984/1000000, d_loss: -103.46265411376953,  g_loss: 266.53485107421875\n",
            "Training epoch 5985/1000000, d_loss: -1307.6142578125,  g_loss: -56.478370666503906\n",
            "Training epoch 5986/1000000, d_loss: -63.84479904174805,  g_loss: 89.78742218017578\n",
            "Training epoch 5987/1000000, d_loss: -89.64983367919922,  g_loss: 208.38967895507812\n",
            "Training epoch 5988/1000000, d_loss: -103.84230041503906,  g_loss: 329.8437194824219\n",
            "Training epoch 5989/1000000, d_loss: -48.869422912597656,  g_loss: 265.1319580078125\n",
            "Training epoch 5990/1000000, d_loss: -127.43659973144531,  g_loss: 305.02685546875\n",
            "Training epoch 5991/1000000, d_loss: -178.93235778808594,  g_loss: 386.983154296875\n",
            "Training epoch 5992/1000000, d_loss: -274.0548095703125,  g_loss: 848.2770385742188\n",
            "Training epoch 5993/1000000, d_loss: 12.319671630859375,  g_loss: 100.4853286743164\n",
            "Training epoch 5994/1000000, d_loss: -156.11105346679688,  g_loss: 67.87461853027344\n",
            "Training epoch 5995/1000000, d_loss: -178.85308837890625,  g_loss: 102.33889770507812\n",
            "Training epoch 5996/1000000, d_loss: -167.37066650390625,  g_loss: 72.43797302246094\n",
            "Training epoch 5997/1000000, d_loss: -51.854225158691406,  g_loss: 141.39801025390625\n",
            "Training epoch 5998/1000000, d_loss: -555.8165283203125,  g_loss: -136.51333618164062\n",
            "Training epoch 5999/1000000, d_loss: -295.11029052734375,  g_loss: -167.49578857421875\n",
            "Training epoch 6000/1000000, d_loss: -5.45672607421875,  g_loss: -152.75039672851562\n",
            "Training epoch 6001/1000000, d_loss: -174.67697143554688,  g_loss: -139.26104736328125\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 21/21 [00:00<00:00, 369.05it/s]\n",
            "Meshing: 100%|██████████| 5373/5373 [00:01<00:00, 5244.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_6001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_6001/assets\n",
            "Training epoch 6002/1000000, d_loss: -290.5440368652344,  g_loss: -122.25306701660156\n",
            "Training epoch 6003/1000000, d_loss: -63.785011291503906,  g_loss: -71.53028869628906\n",
            "Training epoch 6004/1000000, d_loss: -190.5327911376953,  g_loss: 17.18065071105957\n",
            "Training epoch 6005/1000000, d_loss: -131.29940795898438,  g_loss: -10.92978286743164\n",
            "Training epoch 6006/1000000, d_loss: 15.995552062988281,  g_loss: 0.17679595947265625\n",
            "Training epoch 6007/1000000, d_loss: -216.80184936523438,  g_loss: 179.51763916015625\n",
            "Training epoch 6008/1000000, d_loss: -82.10777282714844,  g_loss: 7.839812278747559\n",
            "Training epoch 6009/1000000, d_loss: -77.5556640625,  g_loss: -14.521629333496094\n",
            "Training epoch 6010/1000000, d_loss: -503.95880126953125,  g_loss: -32.66858673095703\n",
            "Training epoch 6011/1000000, d_loss: 555.0804443359375,  g_loss: -50.974708557128906\n",
            "Training epoch 6012/1000000, d_loss: 15.891944885253906,  g_loss: -66.1021957397461\n",
            "Training epoch 6013/1000000, d_loss: -18.85699462890625,  g_loss: -102.53939819335938\n",
            "Training epoch 6014/1000000, d_loss: -113.5527114868164,  g_loss: -197.06954956054688\n",
            "Training epoch 6015/1000000, d_loss: 0.8712749481201172,  g_loss: -173.89759826660156\n",
            "Training epoch 6016/1000000, d_loss: -179.42791748046875,  g_loss: -63.14958953857422\n",
            "Training epoch 6017/1000000, d_loss: -9.154319763183594,  g_loss: 19.92327308654785\n",
            "Training epoch 6018/1000000, d_loss: -77.93010711669922,  g_loss: 44.23847961425781\n",
            "Training epoch 6019/1000000, d_loss: -125.18460083007812,  g_loss: 135.89979553222656\n",
            "Training epoch 6020/1000000, d_loss: -76.44339752197266,  g_loss: 25.002323150634766\n",
            "Training epoch 6021/1000000, d_loss: -118.26744079589844,  g_loss: 21.71863555908203\n",
            "Training epoch 6022/1000000, d_loss: -193.6057586669922,  g_loss: -73.80574035644531\n",
            "Training epoch 6023/1000000, d_loss: -1169.378662109375,  g_loss: -419.461181640625\n",
            "Training epoch 6024/1000000, d_loss: -40.14262390136719,  g_loss: 18.006061553955078\n",
            "Training epoch 6025/1000000, d_loss: -1318.4171142578125,  g_loss: -145.6092987060547\n",
            "Training epoch 6026/1000000, d_loss: -2067.803466796875,  g_loss: -671.076171875\n",
            "Training epoch 6027/1000000, d_loss: 256.22186279296875,  g_loss: -295.3279113769531\n",
            "Training epoch 6028/1000000, d_loss: 91.77050018310547,  g_loss: 96.56379699707031\n",
            "Training epoch 6029/1000000, d_loss: -162.42530822753906,  g_loss: 165.3971710205078\n",
            "Training epoch 6030/1000000, d_loss: -217.1424560546875,  g_loss: 209.31394958496094\n",
            "Training epoch 6031/1000000, d_loss: -347.4999694824219,  g_loss: 287.9868469238281\n",
            "Training epoch 6032/1000000, d_loss: 6.982078552246094,  g_loss: -92.44525146484375\n",
            "Training epoch 6033/1000000, d_loss: -43.62982940673828,  g_loss: -56.14237976074219\n",
            "Training epoch 6034/1000000, d_loss: -172.73687744140625,  g_loss: -80.13893127441406\n",
            "Training epoch 6035/1000000, d_loss: -79.8418960571289,  g_loss: 132.82337951660156\n",
            "Training epoch 6036/1000000, d_loss: 222.85316467285156,  g_loss: -5.715320587158203\n",
            "Training epoch 6037/1000000, d_loss: -210.36477661132812,  g_loss: 60.24407958984375\n",
            "Training epoch 6038/1000000, d_loss: -65.40664672851562,  g_loss: -52.36863708496094\n",
            "Training epoch 6039/1000000, d_loss: -153.3101806640625,  g_loss: -95.02398681640625\n",
            "Training epoch 6040/1000000, d_loss: -146.046630859375,  g_loss: 55.11161422729492\n",
            "Training epoch 6041/1000000, d_loss: 221.876953125,  g_loss: -6.529170989990234\n",
            "Training epoch 6042/1000000, d_loss: 2049.18310546875,  g_loss: 83.47850036621094\n",
            "Training epoch 6043/1000000, d_loss: 31.387096405029297,  g_loss: 5.841395378112793\n",
            "Training epoch 6044/1000000, d_loss: -109.17205047607422,  g_loss: 13.451854705810547\n",
            "Training epoch 6045/1000000, d_loss: -193.2251739501953,  g_loss: 28.531097412109375\n",
            "Training epoch 6046/1000000, d_loss: 66.32349395751953,  g_loss: 134.77285766601562\n",
            "Training epoch 6047/1000000, d_loss: -9.462332725524902,  g_loss: 99.99275207519531\n",
            "Training epoch 6048/1000000, d_loss: -9.157276153564453,  g_loss: 77.70130157470703\n",
            "Training epoch 6049/1000000, d_loss: -412.17535400390625,  g_loss: 97.7046127319336\n",
            "Training epoch 6050/1000000, d_loss: -850.9778442382812,  g_loss: -358.3811950683594\n",
            "Training epoch 6051/1000000, d_loss: 29.741744995117188,  g_loss: -4.541360855102539\n",
            "Training epoch 6052/1000000, d_loss: -246.09820556640625,  g_loss: -137.27578735351562\n",
            "Training epoch 6053/1000000, d_loss: -12.613967895507812,  g_loss: 47.21565246582031\n",
            "Training epoch 6054/1000000, d_loss: -65.38560485839844,  g_loss: 26.325136184692383\n",
            "Training epoch 6055/1000000, d_loss: -27.90802001953125,  g_loss: 162.01998901367188\n",
            "Training epoch 6056/1000000, d_loss: -90.59391021728516,  g_loss: 200.17796325683594\n",
            "Training epoch 6057/1000000, d_loss: -110.15066528320312,  g_loss: 133.73464965820312\n",
            "Training epoch 6058/1000000, d_loss: -91.27854919433594,  g_loss: 161.291748046875\n",
            "Training epoch 6059/1000000, d_loss: -172.04945373535156,  g_loss: -1.282480239868164\n",
            "Training epoch 6060/1000000, d_loss: -153.63600158691406,  g_loss: 91.74547576904297\n",
            "Training epoch 6061/1000000, d_loss: -200.50111389160156,  g_loss: -30.75516128540039\n",
            "Training epoch 6062/1000000, d_loss: -61.552955627441406,  g_loss: 55.55213928222656\n",
            "Training epoch 6063/1000000, d_loss: -517.079833984375,  g_loss: -63.60393524169922\n",
            "Training epoch 6064/1000000, d_loss: -137.45001220703125,  g_loss: 27.063297271728516\n",
            "Training epoch 6065/1000000, d_loss: -77.39433288574219,  g_loss: 107.73245239257812\n",
            "Training epoch 6066/1000000, d_loss: -135.923583984375,  g_loss: 46.88037109375\n",
            "Training epoch 6067/1000000, d_loss: -4.223480224609375,  g_loss: 46.8518180847168\n",
            "Training epoch 6068/1000000, d_loss: 30.105262756347656,  g_loss: 17.955020904541016\n",
            "Training epoch 6069/1000000, d_loss: -180.4209442138672,  g_loss: 93.0054931640625\n",
            "Training epoch 6070/1000000, d_loss: 28.995569229125977,  g_loss: 160.14581298828125\n",
            "Training epoch 6071/1000000, d_loss: -130.1107635498047,  g_loss: 78.39453125\n",
            "Training epoch 6072/1000000, d_loss: -56.35761260986328,  g_loss: 150.82577514648438\n",
            "Training epoch 6073/1000000, d_loss: -111.82994842529297,  g_loss: 89.09246063232422\n",
            "Training epoch 6074/1000000, d_loss: -211.50299072265625,  g_loss: 17.27924919128418\n",
            "Training epoch 6075/1000000, d_loss: 357.71905517578125,  g_loss: 54.41690444946289\n",
            "Training epoch 6076/1000000, d_loss: -95.76322174072266,  g_loss: 11.047445297241211\n",
            "Training epoch 6077/1000000, d_loss: -14.403694152832031,  g_loss: -20.873258590698242\n",
            "Training epoch 6078/1000000, d_loss: -145.8540802001953,  g_loss: 21.623422622680664\n",
            "Training epoch 6079/1000000, d_loss: -16.343305587768555,  g_loss: 40.51817321777344\n",
            "Training epoch 6080/1000000, d_loss: -111.56117248535156,  g_loss: 86.13072967529297\n",
            "Training epoch 6081/1000000, d_loss: -99.317138671875,  g_loss: -3.976766586303711\n",
            "Training epoch 6082/1000000, d_loss: 6.646611213684082,  g_loss: -22.067062377929688\n",
            "Training epoch 6083/1000000, d_loss: -485.2276306152344,  g_loss: -111.77335357666016\n",
            "Training epoch 6084/1000000, d_loss: -543.4976806640625,  g_loss: -189.9917449951172\n",
            "Training epoch 6085/1000000, d_loss: 12.783966064453125,  g_loss: 9.445680618286133\n",
            "Training epoch 6086/1000000, d_loss: -32.104270935058594,  g_loss: 2.5672874450683594\n",
            "Training epoch 6087/1000000, d_loss: -124.08552551269531,  g_loss: -60.70347213745117\n",
            "Training epoch 6088/1000000, d_loss: -1.6404151916503906,  g_loss: 36.57905197143555\n",
            "Training epoch 6089/1000000, d_loss: -156.96673583984375,  g_loss: 76.11509704589844\n",
            "Training epoch 6090/1000000, d_loss: -30.39978790283203,  g_loss: 176.26220703125\n",
            "Training epoch 6091/1000000, d_loss: -161.88064575195312,  g_loss: 133.67861938476562\n",
            "Training epoch 6092/1000000, d_loss: -65.50123596191406,  g_loss: -52.552772521972656\n",
            "Training epoch 6093/1000000, d_loss: -153.5590362548828,  g_loss: -86.28663635253906\n",
            "Training epoch 6094/1000000, d_loss: -54.59032440185547,  g_loss: -100.89177703857422\n",
            "Training epoch 6095/1000000, d_loss: -109.89295959472656,  g_loss: 21.051095962524414\n",
            "Training epoch 6096/1000000, d_loss: -95.95761108398438,  g_loss: 78.75120544433594\n",
            "Training epoch 6097/1000000, d_loss: -413.14544677734375,  g_loss: -100.08124542236328\n",
            "Training epoch 6098/1000000, d_loss: -31.50838279724121,  g_loss: 24.84413719177246\n",
            "Training epoch 6099/1000000, d_loss: -63.394866943359375,  g_loss: 47.12158966064453\n",
            "Training epoch 6100/1000000, d_loss: -246.52992248535156,  g_loss: 30.217002868652344\n",
            "Training epoch 6101/1000000, d_loss: -118.82002258300781,  g_loss: -145.55044555664062\n",
            "Training epoch 6102/1000000, d_loss: -39.5344352722168,  g_loss: 37.263694763183594\n",
            "Training epoch 6103/1000000, d_loss: -108.55362701416016,  g_loss: 63.569122314453125\n",
            "Training epoch 6104/1000000, d_loss: -32.02799987792969,  g_loss: 82.16539764404297\n",
            "Training epoch 6105/1000000, d_loss: -124.40962982177734,  g_loss: 90.31135559082031\n",
            "Training epoch 6106/1000000, d_loss: -86.84044647216797,  g_loss: 0.0042400360107421875\n",
            "Training epoch 6107/1000000, d_loss: -372.23370361328125,  g_loss: -123.15499877929688\n",
            "Training epoch 6108/1000000, d_loss: -43.4521598815918,  g_loss: -33.202884674072266\n",
            "Training epoch 6109/1000000, d_loss: -28.74561309814453,  g_loss: 38.10064697265625\n",
            "Training epoch 6110/1000000, d_loss: -146.78465270996094,  g_loss: -40.58020782470703\n",
            "Training epoch 6111/1000000, d_loss: -93.50674438476562,  g_loss: -60.14445877075195\n",
            "Training epoch 6112/1000000, d_loss: -80.43815612792969,  g_loss: -20.546205520629883\n",
            "Training epoch 6113/1000000, d_loss: 2797.413330078125,  g_loss: -1.1025936603546143\n",
            "Training epoch 6114/1000000, d_loss: -32.15088653564453,  g_loss: 30.99139976501465\n",
            "Training epoch 6115/1000000, d_loss: -75.16081237792969,  g_loss: -57.93999099731445\n",
            "Training epoch 6116/1000000, d_loss: -82.40998840332031,  g_loss: -20.07221221923828\n",
            "Training epoch 6117/1000000, d_loss: -216.67678833007812,  g_loss: -7.524115085601807\n",
            "Training epoch 6118/1000000, d_loss: -98.597900390625,  g_loss: -15.343620300292969\n",
            "Training epoch 6119/1000000, d_loss: -60.405853271484375,  g_loss: -3.8506546020507812\n",
            "Training epoch 6120/1000000, d_loss: -113.77693176269531,  g_loss: -19.151416778564453\n",
            "Training epoch 6121/1000000, d_loss: -80.92618560791016,  g_loss: 78.04772186279297\n",
            "Training epoch 6122/1000000, d_loss: -43.35199737548828,  g_loss: 20.56297492980957\n",
            "Training epoch 6123/1000000, d_loss: -107.0675277709961,  g_loss: 88.96166229248047\n",
            "Training epoch 6124/1000000, d_loss: -601.2627563476562,  g_loss: -80.86267852783203\n",
            "Training epoch 6125/1000000, d_loss: -149.04025268554688,  g_loss: -42.91466522216797\n",
            "Training epoch 6126/1000000, d_loss: -143.00808715820312,  g_loss: -25.928768157958984\n",
            "Training epoch 6127/1000000, d_loss: -143.83462524414062,  g_loss: 24.16744613647461\n",
            "Training epoch 6128/1000000, d_loss: -199.2993621826172,  g_loss: -41.41257858276367\n",
            "Training epoch 6129/1000000, d_loss: -132.39849853515625,  g_loss: 149.11399841308594\n",
            "Training epoch 6130/1000000, d_loss: -84.09501647949219,  g_loss: -43.685428619384766\n",
            "Training epoch 6131/1000000, d_loss: -75.83302307128906,  g_loss: -36.24673080444336\n",
            "Training epoch 6132/1000000, d_loss: -97.52214050292969,  g_loss: -12.835078239440918\n",
            "Training epoch 6133/1000000, d_loss: -290.7039489746094,  g_loss: -146.84727478027344\n",
            "Training epoch 6134/1000000, d_loss: -161.27557373046875,  g_loss: -19.916343688964844\n",
            "Training epoch 6135/1000000, d_loss: 2.3361072540283203,  g_loss: -15.566312789916992\n",
            "Training epoch 6136/1000000, d_loss: -172.0692901611328,  g_loss: -52.4696159362793\n",
            "Training epoch 6137/1000000, d_loss: -26.5397891998291,  g_loss: 28.35601234436035\n",
            "Training epoch 6138/1000000, d_loss: -197.19573974609375,  g_loss: -54.99347686767578\n",
            "Training epoch 6139/1000000, d_loss: -15.46782112121582,  g_loss: 15.547285079956055\n",
            "Training epoch 6140/1000000, d_loss: 34.55734634399414,  g_loss: 2.5248889923095703\n",
            "Training epoch 6141/1000000, d_loss: -236.10455322265625,  g_loss: -16.419307708740234\n",
            "Training epoch 6142/1000000, d_loss: -153.69882202148438,  g_loss: -87.78459167480469\n",
            "Training epoch 6143/1000000, d_loss: -63.62674331665039,  g_loss: -33.65814971923828\n",
            "Training epoch 6144/1000000, d_loss: -177.29635620117188,  g_loss: 24.92173194885254\n",
            "Training epoch 6145/1000000, d_loss: -56.82734680175781,  g_loss: 13.811697959899902\n",
            "Training epoch 6146/1000000, d_loss: -89.47721862792969,  g_loss: 79.19944763183594\n",
            "Training epoch 6147/1000000, d_loss: -129.7625732421875,  g_loss: 25.753427505493164\n",
            "Training epoch 6148/1000000, d_loss: -77.32012176513672,  g_loss: -36.86566162109375\n",
            "Training epoch 6149/1000000, d_loss: -98.00634765625,  g_loss: -16.957944869995117\n",
            "Training epoch 6150/1000000, d_loss: -144.59022521972656,  g_loss: 62.72565460205078\n",
            "Training epoch 6151/1000000, d_loss: -335.9528503417969,  g_loss: 40.82341766357422\n",
            "Training epoch 6152/1000000, d_loss: -87.3903579711914,  g_loss: -4.2135467529296875\n",
            "Training epoch 6153/1000000, d_loss: -65.70377349853516,  g_loss: 19.786516189575195\n",
            "Training epoch 6154/1000000, d_loss: -402.6340026855469,  g_loss: -53.47798538208008\n",
            "Training epoch 6155/1000000, d_loss: 9.225475311279297,  g_loss: -5.594154357910156\n",
            "Training epoch 6156/1000000, d_loss: -44.97044372558594,  g_loss: 34.193511962890625\n",
            "Training epoch 6157/1000000, d_loss: -107.41738891601562,  g_loss: 49.83495330810547\n",
            "Training epoch 6158/1000000, d_loss: -89.1082763671875,  g_loss: 21.231687545776367\n",
            "Training epoch 6159/1000000, d_loss: -104.63298034667969,  g_loss: -25.243574142456055\n",
            "Training epoch 6160/1000000, d_loss: -86.55939483642578,  g_loss: 29.446012496948242\n",
            "Training epoch 6161/1000000, d_loss: -71.61668395996094,  g_loss: 36.8278694152832\n",
            "Training epoch 6162/1000000, d_loss: -346.87347412109375,  g_loss: 27.831279754638672\n",
            "Training epoch 6163/1000000, d_loss: -134.58895874023438,  g_loss: 61.46923828125\n",
            "Training epoch 6164/1000000, d_loss: -117.69860076904297,  g_loss: 61.268131256103516\n",
            "Training epoch 6165/1000000, d_loss: -74.09309387207031,  g_loss: 30.21044158935547\n",
            "Training epoch 6166/1000000, d_loss: -251.68856811523438,  g_loss: 68.32074737548828\n",
            "Training epoch 6167/1000000, d_loss: -126.59498596191406,  g_loss: 55.69718933105469\n",
            "Training epoch 6168/1000000, d_loss: 18.30945587158203,  g_loss: 57.611427307128906\n",
            "Training epoch 6169/1000000, d_loss: -192.50326538085938,  g_loss: 77.38645935058594\n",
            "Training epoch 6170/1000000, d_loss: -142.9877471923828,  g_loss: 235.70697021484375\n",
            "Training epoch 6171/1000000, d_loss: -79.9029312133789,  g_loss: 197.60763549804688\n",
            "Training epoch 6172/1000000, d_loss: -0.3424530029296875,  g_loss: 78.67829132080078\n",
            "Training epoch 6173/1000000, d_loss: -113.73651885986328,  g_loss: 67.45536804199219\n",
            "Training epoch 6174/1000000, d_loss: -120.99828338623047,  g_loss: 21.694717407226562\n",
            "Training epoch 6175/1000000, d_loss: -156.84458923339844,  g_loss: -7.424861907958984\n",
            "Training epoch 6176/1000000, d_loss: -45.11448669433594,  g_loss: -57.8820915222168\n",
            "Training epoch 6177/1000000, d_loss: -31.77826499938965,  g_loss: 80.12728881835938\n",
            "Training epoch 6178/1000000, d_loss: -43.41901397705078,  g_loss: -62.109222412109375\n",
            "Training epoch 6179/1000000, d_loss: 810.8756103515625,  g_loss: -80.89067077636719\n",
            "Training epoch 6180/1000000, d_loss: 62.097198486328125,  g_loss: -8.626543045043945\n",
            "Training epoch 6181/1000000, d_loss: -118.57657623291016,  g_loss: 36.82395553588867\n",
            "Training epoch 6182/1000000, d_loss: -102.83345031738281,  g_loss: -70.19955444335938\n",
            "Training epoch 6183/1000000, d_loss: -202.40184020996094,  g_loss: -77.88574981689453\n",
            "Training epoch 6184/1000000, d_loss: -113.2499771118164,  g_loss: -62.6701774597168\n",
            "Training epoch 6185/1000000, d_loss: 4.158817291259766,  g_loss: -117.72969818115234\n",
            "Training epoch 6186/1000000, d_loss: -338.7322692871094,  g_loss: 34.23692321777344\n",
            "Training epoch 6187/1000000, d_loss: -90.57455444335938,  g_loss: 29.775466918945312\n",
            "Training epoch 6188/1000000, d_loss: -153.126708984375,  g_loss: 182.32154846191406\n",
            "Training epoch 6189/1000000, d_loss: -174.49681091308594,  g_loss: 60.546546936035156\n",
            "Training epoch 6190/1000000, d_loss: -149.95233154296875,  g_loss: 68.64995574951172\n",
            "Training epoch 6191/1000000, d_loss: -121.57049560546875,  g_loss: -0.43265819549560547\n",
            "Training epoch 6192/1000000, d_loss: -169.9790802001953,  g_loss: 220.38819885253906\n",
            "Training epoch 6193/1000000, d_loss: -147.275390625,  g_loss: 9.641336441040039\n",
            "Training epoch 6194/1000000, d_loss: -64.72277069091797,  g_loss: -0.016513347625732422\n",
            "Training epoch 6195/1000000, d_loss: 24.2164306640625,  g_loss: -14.962264060974121\n",
            "Training epoch 6196/1000000, d_loss: -57.35066604614258,  g_loss: 59.968482971191406\n",
            "Training epoch 6197/1000000, d_loss: -290.9412841796875,  g_loss: 24.57410430908203\n",
            "Training epoch 6198/1000000, d_loss: -154.53378295898438,  g_loss: 78.78797912597656\n",
            "Training epoch 6199/1000000, d_loss: 4.379804611206055,  g_loss: 43.55310821533203\n",
            "Training epoch 6200/1000000, d_loss: -362.33026123046875,  g_loss: -0.8306064605712891\n",
            "Training epoch 6201/1000000, d_loss: -65.11994934082031,  g_loss: 10.568946838378906\n",
            "Training epoch 6202/1000000, d_loss: -7.435695648193359,  g_loss: -16.030323028564453\n",
            "Training epoch 6203/1000000, d_loss: -40.44749450683594,  g_loss: -0.21971750259399414\n",
            "Training epoch 6204/1000000, d_loss: -82.95188903808594,  g_loss: 9.4359712600708\n",
            "Training epoch 6205/1000000, d_loss: -209.9419708251953,  g_loss: 1.9268035888671875\n",
            "Training epoch 6206/1000000, d_loss: -127.66437530517578,  g_loss: -35.986976623535156\n",
            "Training epoch 6207/1000000, d_loss: -42.10327911376953,  g_loss: 61.5272216796875\n",
            "Training epoch 6208/1000000, d_loss: -84.98696899414062,  g_loss: 4.182833194732666\n",
            "Training epoch 6209/1000000, d_loss: -118.68962097167969,  g_loss: -7.934804439544678\n",
            "Training epoch 6210/1000000, d_loss: -142.3277587890625,  g_loss: -88.75504302978516\n",
            "Training epoch 6211/1000000, d_loss: -52.271305084228516,  g_loss: 5.704290390014648\n",
            "Training epoch 6212/1000000, d_loss: -86.11930847167969,  g_loss: 3.558096170425415\n",
            "Training epoch 6213/1000000, d_loss: -93.59376525878906,  g_loss: 55.605384826660156\n",
            "Training epoch 6214/1000000, d_loss: -113.71318054199219,  g_loss: 202.00982666015625\n",
            "Training epoch 6215/1000000, d_loss: -49.510257720947266,  g_loss: 43.37013244628906\n",
            "Training epoch 6216/1000000, d_loss: -109.09519958496094,  g_loss: 73.77093505859375\n",
            "Training epoch 6217/1000000, d_loss: -41.307373046875,  g_loss: 107.45710754394531\n",
            "Training epoch 6218/1000000, d_loss: -362.37432861328125,  g_loss: -53.631526947021484\n",
            "Training epoch 6219/1000000, d_loss: 7.262912750244141,  g_loss: -42.439292907714844\n",
            "Training epoch 6220/1000000, d_loss: -257.97259521484375,  g_loss: 9.83150577545166\n",
            "Training epoch 6221/1000000, d_loss: -40.49374008178711,  g_loss: -9.030866622924805\n",
            "Training epoch 6222/1000000, d_loss: -77.1328125,  g_loss: 4.273299217224121\n",
            "Training epoch 6223/1000000, d_loss: -233.32789611816406,  g_loss: 0.30316686630249023\n",
            "Training epoch 6224/1000000, d_loss: -71.60968780517578,  g_loss: 53.58067321777344\n",
            "Training epoch 6225/1000000, d_loss: -60.816551208496094,  g_loss: -24.58456802368164\n",
            "Training epoch 6226/1000000, d_loss: -555.7935180664062,  g_loss: -112.35118103027344\n",
            "Training epoch 6227/1000000, d_loss: 119.68657684326172,  g_loss: 43.85102844238281\n",
            "Training epoch 6228/1000000, d_loss: -128.2779083251953,  g_loss: 49.978302001953125\n",
            "Training epoch 6229/1000000, d_loss: -230.1740264892578,  g_loss: 308.6822814941406\n",
            "Training epoch 6230/1000000, d_loss: -115.045166015625,  g_loss: 265.6065673828125\n",
            "Training epoch 6231/1000000, d_loss: -851.0208740234375,  g_loss: 21.28551483154297\n",
            "Training epoch 6232/1000000, d_loss: -512.18994140625,  g_loss: -23.080280303955078\n",
            "Training epoch 6233/1000000, d_loss: 146.6961669921875,  g_loss: -137.56838989257812\n",
            "Training epoch 6234/1000000, d_loss: -2.7724151611328125,  g_loss: 25.323455810546875\n",
            "Training epoch 6235/1000000, d_loss: 56.61382293701172,  g_loss: -19.039119720458984\n",
            "Training epoch 6236/1000000, d_loss: -115.7285385131836,  g_loss: 37.723899841308594\n",
            "Training epoch 6237/1000000, d_loss: -96.08387756347656,  g_loss: 83.51451873779297\n",
            "Training epoch 6238/1000000, d_loss: -186.9422149658203,  g_loss: -44.88905334472656\n",
            "Training epoch 6239/1000000, d_loss: -82.17286682128906,  g_loss: 220.95872497558594\n",
            "Training epoch 6240/1000000, d_loss: -147.48358154296875,  g_loss: 126.28266143798828\n",
            "Training epoch 6241/1000000, d_loss: -47.91114807128906,  g_loss: 84.72279357910156\n",
            "Training epoch 6242/1000000, d_loss: -10.425086975097656,  g_loss: 99.67637634277344\n",
            "Training epoch 6243/1000000, d_loss: -38.191871643066406,  g_loss: 94.37774658203125\n",
            "Training epoch 6244/1000000, d_loss: -87.32476043701172,  g_loss: 124.87596893310547\n",
            "Training epoch 6245/1000000, d_loss: -132.35069274902344,  g_loss: -12.258419036865234\n",
            "Training epoch 6246/1000000, d_loss: -160.69056701660156,  g_loss: 74.05978393554688\n",
            "Training epoch 6247/1000000, d_loss: -68.8340072631836,  g_loss: 77.04438781738281\n",
            "Training epoch 6248/1000000, d_loss: -94.25458526611328,  g_loss: 17.028240203857422\n",
            "Training epoch 6249/1000000, d_loss: -180.48788452148438,  g_loss: 54.128021240234375\n",
            "Training epoch 6250/1000000, d_loss: -365.902587890625,  g_loss: -56.49964904785156\n",
            "Training epoch 6251/1000000, d_loss: -145.2099151611328,  g_loss: 38.037689208984375\n",
            "Training epoch 6252/1000000, d_loss: -156.47401428222656,  g_loss: 18.817214965820312\n",
            "Training epoch 6253/1000000, d_loss: -97.64073181152344,  g_loss: 40.270652770996094\n",
            "Training epoch 6254/1000000, d_loss: -72.026123046875,  g_loss: 137.72946166992188\n",
            "Training epoch 6255/1000000, d_loss: -143.56529235839844,  g_loss: 162.31976318359375\n",
            "Training epoch 6256/1000000, d_loss: -338.8497009277344,  g_loss: -29.581218719482422\n",
            "Training epoch 6257/1000000, d_loss: -47.76155090332031,  g_loss: 37.79912567138672\n",
            "Training epoch 6258/1000000, d_loss: -1398.904296875,  g_loss: -499.39849853515625\n",
            "Training epoch 6259/1000000, d_loss: 263.61029052734375,  g_loss: -80.53123474121094\n",
            "Training epoch 6260/1000000, d_loss: 7.371696472167969,  g_loss: 273.1037292480469\n",
            "Training epoch 6261/1000000, d_loss: -278.0380859375,  g_loss: 529.0089721679688\n",
            "Training epoch 6262/1000000, d_loss: -215.12277221679688,  g_loss: 651.391357421875\n",
            "Training epoch 6263/1000000, d_loss: -75.69693756103516,  g_loss: 88.28752136230469\n",
            "Training epoch 6264/1000000, d_loss: -166.6429443359375,  g_loss: 110.8040542602539\n",
            "Training epoch 6265/1000000, d_loss: -130.67294311523438,  g_loss: 380.6479187011719\n",
            "Training epoch 6266/1000000, d_loss: -158.73927307128906,  g_loss: 298.101806640625\n",
            "Training epoch 6267/1000000, d_loss: -37.88543701171875,  g_loss: 167.37014770507812\n",
            "Training epoch 6268/1000000, d_loss: -123.18409729003906,  g_loss: 285.08612060546875\n",
            "Training epoch 6269/1000000, d_loss: -104.14823913574219,  g_loss: 297.6148986816406\n",
            "Training epoch 6270/1000000, d_loss: -288.82904052734375,  g_loss: 625.306884765625\n",
            "Training epoch 6271/1000000, d_loss: -134.75265502929688,  g_loss: 24.250938415527344\n",
            "Training epoch 6272/1000000, d_loss: 54.2047233581543,  g_loss: 61.396697998046875\n",
            "Training epoch 6273/1000000, d_loss: -131.79852294921875,  g_loss: 109.50739288330078\n",
            "Training epoch 6274/1000000, d_loss: -20.961944580078125,  g_loss: 136.69325256347656\n",
            "Training epoch 6275/1000000, d_loss: -200.09426879882812,  g_loss: 49.525962829589844\n",
            "Training epoch 6276/1000000, d_loss: -95.00469207763672,  g_loss: 36.57533264160156\n",
            "Training epoch 6277/1000000, d_loss: -129.40869140625,  g_loss: 138.10650634765625\n",
            "Training epoch 6278/1000000, d_loss: -25.919883728027344,  g_loss: 126.65108489990234\n",
            "Training epoch 6279/1000000, d_loss: -54.1754035949707,  g_loss: 201.83883666992188\n",
            "Training epoch 6280/1000000, d_loss: -462.83636474609375,  g_loss: 74.83554077148438\n",
            "Training epoch 6281/1000000, d_loss: -109.22300720214844,  g_loss: 176.21202087402344\n",
            "Training epoch 6282/1000000, d_loss: -93.40705871582031,  g_loss: 164.0586395263672\n",
            "Training epoch 6283/1000000, d_loss: -301.11663818359375,  g_loss: 84.949951171875\n",
            "Training epoch 6284/1000000, d_loss: -841.6635131835938,  g_loss: -91.4205322265625\n",
            "Training epoch 6285/1000000, d_loss: 72.27583312988281,  g_loss: 3.853374481201172\n",
            "Training epoch 6286/1000000, d_loss: 2.353862762451172,  g_loss: 58.206363677978516\n",
            "Training epoch 6287/1000000, d_loss: -428.2386474609375,  g_loss: -61.403533935546875\n",
            "Training epoch 6288/1000000, d_loss: -69.07599639892578,  g_loss: 9.440200805664062\n",
            "Training epoch 6289/1000000, d_loss: -57.9798698425293,  g_loss: -122.37522888183594\n",
            "Training epoch 6290/1000000, d_loss: 67.50613403320312,  g_loss: 11.682472229003906\n",
            "Training epoch 6291/1000000, d_loss: -226.52891540527344,  g_loss: -8.74222183227539\n",
            "Training epoch 6292/1000000, d_loss: -238.1290740966797,  g_loss: -163.504150390625\n",
            "Training epoch 6293/1000000, d_loss: -211.68869018554688,  g_loss: -303.39422607421875\n",
            "Training epoch 6294/1000000, d_loss: -75.06930541992188,  g_loss: 49.48733901977539\n",
            "Training epoch 6295/1000000, d_loss: -158.57046508789062,  g_loss: 41.46475601196289\n",
            "Training epoch 6296/1000000, d_loss: -236.45468139648438,  g_loss: 178.3023223876953\n",
            "Training epoch 6297/1000000, d_loss: -188.57269287109375,  g_loss: 137.65525817871094\n",
            "Training epoch 6298/1000000, d_loss: 11.452625274658203,  g_loss: 90.97151184082031\n",
            "Training epoch 6299/1000000, d_loss: -77.91375732421875,  g_loss: 179.673828125\n",
            "Training epoch 6300/1000000, d_loss: -240.71755981445312,  g_loss: 163.8652801513672\n",
            "Training epoch 6301/1000000, d_loss: -134.9962158203125,  g_loss: 197.163330078125\n",
            "Training epoch 6302/1000000, d_loss: -143.61846923828125,  g_loss: 297.898681640625\n",
            "Training epoch 6303/1000000, d_loss: -23.002723693847656,  g_loss: 145.87176513671875\n",
            "Training epoch 6304/1000000, d_loss: -5.273223876953125,  g_loss: 165.21701049804688\n",
            "Training epoch 6305/1000000, d_loss: -70.18544006347656,  g_loss: 170.0446319580078\n",
            "Training epoch 6306/1000000, d_loss: -44.98115921020508,  g_loss: 140.60818481445312\n",
            "Training epoch 6307/1000000, d_loss: -81.07877349853516,  g_loss: 103.23439025878906\n",
            "Training epoch 6308/1000000, d_loss: -106.76580810546875,  g_loss: 51.27204895019531\n",
            "Training epoch 6309/1000000, d_loss: -122.67528533935547,  g_loss: 129.23330688476562\n",
            "Training epoch 6310/1000000, d_loss: -248.62054443359375,  g_loss: -143.81878662109375\n",
            "Training epoch 6311/1000000, d_loss: -169.60450744628906,  g_loss: -60.459495544433594\n",
            "Training epoch 6312/1000000, d_loss: -58.526058197021484,  g_loss: 17.791473388671875\n",
            "Training epoch 6313/1000000, d_loss: -68.0767822265625,  g_loss: 118.36487579345703\n",
            "Training epoch 6314/1000000, d_loss: -46.8415641784668,  g_loss: 173.51612854003906\n",
            "Training epoch 6315/1000000, d_loss: -100.30674743652344,  g_loss: 353.6270751953125\n",
            "Training epoch 6316/1000000, d_loss: -26.908842086791992,  g_loss: 154.82286071777344\n",
            "Training epoch 6317/1000000, d_loss: -117.82495880126953,  g_loss: 156.29827880859375\n",
            "Training epoch 6318/1000000, d_loss: -62.264747619628906,  g_loss: 191.03273010253906\n",
            "Training epoch 6319/1000000, d_loss: -102.19264221191406,  g_loss: 127.95614624023438\n",
            "Training epoch 6320/1000000, d_loss: -318.1640319824219,  g_loss: 60.38857650756836\n",
            "Training epoch 6321/1000000, d_loss: -352.0909423828125,  g_loss: 27.03164291381836\n",
            "Training epoch 6322/1000000, d_loss: -30.696035385131836,  g_loss: 48.161659240722656\n",
            "Training epoch 6323/1000000, d_loss: -1.2564697265625,  g_loss: -5.211414337158203\n",
            "Training epoch 6324/1000000, d_loss: -696.701171875,  g_loss: -504.1127624511719\n",
            "Training epoch 6325/1000000, d_loss: -29.22079086303711,  g_loss: -34.87322235107422\n",
            "Training epoch 6326/1000000, d_loss: 6.8157196044921875,  g_loss: 15.311050415039062\n",
            "Training epoch 6327/1000000, d_loss: -82.36278533935547,  g_loss: 103.61325073242188\n",
            "Training epoch 6328/1000000, d_loss: -65.9103012084961,  g_loss: 36.94049072265625\n",
            "Training epoch 6329/1000000, d_loss: -100.82902526855469,  g_loss: 121.26997375488281\n",
            "Training epoch 6330/1000000, d_loss: -304.53875732421875,  g_loss: -2.6648473739624023\n",
            "Training epoch 6331/1000000, d_loss: -230.974609375,  g_loss: 334.0596008300781\n",
            "Training epoch 6332/1000000, d_loss: -100.07205200195312,  g_loss: 340.33148193359375\n",
            "Training epoch 6333/1000000, d_loss: -40.068634033203125,  g_loss: 79.1165542602539\n",
            "Training epoch 6334/1000000, d_loss: -65.22282409667969,  g_loss: 40.855892181396484\n",
            "Training epoch 6335/1000000, d_loss: -195.07054138183594,  g_loss: 53.8056755065918\n",
            "Training epoch 6336/1000000, d_loss: -298.5436096191406,  g_loss: -11.666440963745117\n",
            "Training epoch 6337/1000000, d_loss: 62.65106964111328,  g_loss: -2.0239171981811523\n",
            "Training epoch 6338/1000000, d_loss: -1314.462890625,  g_loss: -150.12704467773438\n",
            "Training epoch 6339/1000000, d_loss: -99.258056640625,  g_loss: -105.88136291503906\n",
            "Training epoch 6340/1000000, d_loss: 36.51806640625,  g_loss: 39.63248825073242\n",
            "Training epoch 6341/1000000, d_loss: -29.462425231933594,  g_loss: 47.633033752441406\n",
            "Training epoch 6342/1000000, d_loss: 39.88026428222656,  g_loss: -9.19108772277832\n",
            "Training epoch 6343/1000000, d_loss: -14.053428649902344,  g_loss: -26.618194580078125\n",
            "Training epoch 6344/1000000, d_loss: -18.670833587646484,  g_loss: 43.33286666870117\n",
            "Training epoch 6345/1000000, d_loss: -111.87637329101562,  g_loss: -9.173212051391602\n",
            "Training epoch 6346/1000000, d_loss: -31.91240882873535,  g_loss: 38.01643371582031\n",
            "Training epoch 6347/1000000, d_loss: -132.4264678955078,  g_loss: 20.574846267700195\n",
            "Training epoch 6348/1000000, d_loss: -110.17111206054688,  g_loss: -37.42530059814453\n",
            "Training epoch 6349/1000000, d_loss: -170.84483337402344,  g_loss: -19.70757293701172\n",
            "Training epoch 6350/1000000, d_loss: -69.63215637207031,  g_loss: 6.240292072296143\n",
            "Training epoch 6351/1000000, d_loss: -332.2697448730469,  g_loss: -10.176900863647461\n",
            "Training epoch 6352/1000000, d_loss: -106.98112487792969,  g_loss: 21.948375701904297\n",
            "Training epoch 6353/1000000, d_loss: -7.344081878662109,  g_loss: 15.396620750427246\n",
            "Training epoch 6354/1000000, d_loss: -87.116455078125,  g_loss: -7.868481636047363\n",
            "Training epoch 6355/1000000, d_loss: -84.36282348632812,  g_loss: 40.22043991088867\n",
            "Training epoch 6356/1000000, d_loss: -64.0303955078125,  g_loss: -10.82772159576416\n",
            "Training epoch 6357/1000000, d_loss: -682.5667724609375,  g_loss: -171.4122772216797\n",
            "Training epoch 6358/1000000, d_loss: -59.624752044677734,  g_loss: 73.11746215820312\n",
            "Training epoch 6359/1000000, d_loss: -181.52728271484375,  g_loss: 66.57904815673828\n",
            "Training epoch 6360/1000000, d_loss: -36.35419845581055,  g_loss: 102.73153686523438\n",
            "Training epoch 6361/1000000, d_loss: -117.99015808105469,  g_loss: 67.99158477783203\n",
            "Training epoch 6362/1000000, d_loss: -85.6527099609375,  g_loss: 64.85596466064453\n",
            "Training epoch 6363/1000000, d_loss: -65.85747528076172,  g_loss: 6.398589134216309\n",
            "Training epoch 6364/1000000, d_loss: -157.0365447998047,  g_loss: -22.354881286621094\n",
            "Training epoch 6365/1000000, d_loss: -162.075439453125,  g_loss: 35.5318717956543\n",
            "Training epoch 6366/1000000, d_loss: -75.30052185058594,  g_loss: 20.976726531982422\n",
            "Training epoch 6367/1000000, d_loss: -322.90924072265625,  g_loss: -78.12712860107422\n",
            "Training epoch 6368/1000000, d_loss: -22.778364181518555,  g_loss: -25.042930603027344\n",
            "Training epoch 6369/1000000, d_loss: 78.41751098632812,  g_loss: 62.0949821472168\n",
            "Training epoch 6370/1000000, d_loss: -1052.6024169921875,  g_loss: -229.99195861816406\n",
            "Training epoch 6371/1000000, d_loss: -430.8783874511719,  g_loss: -585.8216552734375\n",
            "Training epoch 6372/1000000, d_loss: -47.470619201660156,  g_loss: 85.91203308105469\n",
            "Training epoch 6373/1000000, d_loss: -95.876708984375,  g_loss: 128.99862670898438\n",
            "Training epoch 6374/1000000, d_loss: 0.6119384765625,  g_loss: 63.12744140625\n",
            "Training epoch 6375/1000000, d_loss: -176.55039978027344,  g_loss: 100.65823364257812\n",
            "Training epoch 6376/1000000, d_loss: -78.4629898071289,  g_loss: 105.43958282470703\n",
            "Training epoch 6377/1000000, d_loss: -87.66936492919922,  g_loss: 91.3984375\n",
            "Training epoch 6378/1000000, d_loss: -198.9824981689453,  g_loss: 391.46710205078125\n",
            "Training epoch 6379/1000000, d_loss: -297.8836669921875,  g_loss: 478.22149658203125\n",
            "Training epoch 6380/1000000, d_loss: -245.069580078125,  g_loss: 491.1049499511719\n",
            "Training epoch 6381/1000000, d_loss: -388.54779052734375,  g_loss: 617.0478515625\n",
            "Training epoch 6382/1000000, d_loss: -81.5693359375,  g_loss: -5.385997772216797\n",
            "Training epoch 6383/1000000, d_loss: -34.146305084228516,  g_loss: 37.47089767456055\n",
            "Training epoch 6384/1000000, d_loss: -164.11622619628906,  g_loss: 126.30091857910156\n",
            "Training epoch 6385/1000000, d_loss: -115.75752258300781,  g_loss: 126.56219482421875\n",
            "Training epoch 6386/1000000, d_loss: -126.0800552368164,  g_loss: 43.19748306274414\n",
            "Training epoch 6387/1000000, d_loss: -453.99530029296875,  g_loss: -141.4507598876953\n",
            "Training epoch 6388/1000000, d_loss: -113.98787689208984,  g_loss: -22.627532958984375\n",
            "Training epoch 6389/1000000, d_loss: 109.55279541015625,  g_loss: -62.26891326904297\n",
            "Training epoch 6390/1000000, d_loss: -22.373149871826172,  g_loss: 1.3089218139648438\n",
            "Training epoch 6391/1000000, d_loss: 44.01951599121094,  g_loss: 116.80343627929688\n",
            "Training epoch 6392/1000000, d_loss: -87.61428833007812,  g_loss: 157.07713317871094\n",
            "Training epoch 6393/1000000, d_loss: -43.91972351074219,  g_loss: 260.624267578125\n",
            "Training epoch 6394/1000000, d_loss: -124.6004638671875,  g_loss: 136.88961791992188\n",
            "Training epoch 6395/1000000, d_loss: -63.04057312011719,  g_loss: 170.64373779296875\n",
            "Training epoch 6396/1000000, d_loss: -163.1305694580078,  g_loss: 351.1976318359375\n",
            "Training epoch 6397/1000000, d_loss: -104.31664276123047,  g_loss: 84.85649871826172\n",
            "Training epoch 6398/1000000, d_loss: -119.82250213623047,  g_loss: 24.505556106567383\n",
            "Training epoch 6399/1000000, d_loss: -386.7604675292969,  g_loss: -255.13897705078125\n",
            "Training epoch 6400/1000000, d_loss: -113.61363983154297,  g_loss: -167.6863250732422\n",
            "Training epoch 6401/1000000, d_loss: 44.654624938964844,  g_loss: -70.09857177734375\n",
            "Training epoch 6402/1000000, d_loss: -15.265083312988281,  g_loss: -32.24256134033203\n",
            "Training epoch 6403/1000000, d_loss: -30.605331420898438,  g_loss: 9.288618087768555\n",
            "Training epoch 6404/1000000, d_loss: -195.46002197265625,  g_loss: 23.507883071899414\n",
            "Training epoch 6405/1000000, d_loss: -328.7139892578125,  g_loss: -192.00796508789062\n",
            "Training epoch 6406/1000000, d_loss: -115.51763916015625,  g_loss: 117.4662094116211\n",
            "Training epoch 6407/1000000, d_loss: 16.60965347290039,  g_loss: 163.03305053710938\n",
            "Training epoch 6408/1000000, d_loss: -74.87200927734375,  g_loss: 66.57611846923828\n",
            "Training epoch 6409/1000000, d_loss: -128.37435913085938,  g_loss: 59.80684280395508\n",
            "Training epoch 6410/1000000, d_loss: -71.79615783691406,  g_loss: 19.355709075927734\n",
            "Training epoch 6411/1000000, d_loss: -72.3496322631836,  g_loss: 62.7402458190918\n",
            "Training epoch 6412/1000000, d_loss: -84.92517852783203,  g_loss: 70.54244995117188\n",
            "Training epoch 6413/1000000, d_loss: -52.972328186035156,  g_loss: 63.32415771484375\n",
            "Training epoch 6414/1000000, d_loss: -21.75162124633789,  g_loss: 24.779207229614258\n",
            "Training epoch 6415/1000000, d_loss: -83.40652465820312,  g_loss: 33.487754821777344\n",
            "Training epoch 6416/1000000, d_loss: -69.406005859375,  g_loss: 33.75218200683594\n",
            "Training epoch 6417/1000000, d_loss: -165.11410522460938,  g_loss: -44.58559036254883\n",
            "Training epoch 6418/1000000, d_loss: -93.52583312988281,  g_loss: 58.418827056884766\n",
            "Training epoch 6419/1000000, d_loss: -155.41505432128906,  g_loss: 19.005115509033203\n",
            "Training epoch 6420/1000000, d_loss: -148.849365234375,  g_loss: 20.870105743408203\n",
            "Training epoch 6421/1000000, d_loss: -81.76646423339844,  g_loss: 7.223785400390625\n",
            "Training epoch 6422/1000000, d_loss: -107.13943481445312,  g_loss: 47.35685348510742\n",
            "Training epoch 6423/1000000, d_loss: 392.2452392578125,  g_loss: 53.1787109375\n",
            "Training epoch 6424/1000000, d_loss: -148.61465454101562,  g_loss: 93.882080078125\n",
            "Training epoch 6425/1000000, d_loss: -110.643798828125,  g_loss: 135.3941192626953\n",
            "Training epoch 6426/1000000, d_loss: -48.950469970703125,  g_loss: 102.94402313232422\n",
            "Training epoch 6427/1000000, d_loss: -136.01776123046875,  g_loss: 107.22449493408203\n",
            "Training epoch 6428/1000000, d_loss: -68.74532318115234,  g_loss: 138.32017517089844\n",
            "Training epoch 6429/1000000, d_loss: -51.108428955078125,  g_loss: 82.29335021972656\n",
            "Training epoch 6430/1000000, d_loss: -47.001712799072266,  g_loss: 145.67333984375\n",
            "Training epoch 6431/1000000, d_loss: -16.606063842773438,  g_loss: 151.60610961914062\n",
            "Training epoch 6432/1000000, d_loss: -62.47233581542969,  g_loss: 74.20199584960938\n",
            "Training epoch 6433/1000000, d_loss: -78.09593200683594,  g_loss: 179.2768096923828\n",
            "Training epoch 6434/1000000, d_loss: -69.4185791015625,  g_loss: 155.1719207763672\n",
            "Training epoch 6435/1000000, d_loss: -32.21751403808594,  g_loss: 114.89308166503906\n",
            "Training epoch 6436/1000000, d_loss: -102.57106018066406,  g_loss: 69.6929702758789\n",
            "Training epoch 6437/1000000, d_loss: -43.8332633972168,  g_loss: 81.74172973632812\n",
            "Training epoch 6438/1000000, d_loss: -53.546287536621094,  g_loss: 115.61857604980469\n",
            "Training epoch 6439/1000000, d_loss: -284.6856994628906,  g_loss: -126.01290893554688\n",
            "Training epoch 6440/1000000, d_loss: -412.79376220703125,  g_loss: -92.70699310302734\n",
            "Training epoch 6441/1000000, d_loss: -22.556352615356445,  g_loss: 14.724151611328125\n",
            "Training epoch 6442/1000000, d_loss: -3.7113914489746094,  g_loss: 69.30723571777344\n",
            "Training epoch 6443/1000000, d_loss: -130.48838806152344,  g_loss: 24.008747100830078\n",
            "Training epoch 6444/1000000, d_loss: -1.8237953186035156,  g_loss: 59.876617431640625\n",
            "Training epoch 6445/1000000, d_loss: -64.34882354736328,  g_loss: 81.62271118164062\n",
            "Training epoch 6446/1000000, d_loss: -125.09817504882812,  g_loss: 46.831390380859375\n",
            "Training epoch 6447/1000000, d_loss: -253.23521423339844,  g_loss: -13.378612518310547\n",
            "Training epoch 6448/1000000, d_loss: -324.2587890625,  g_loss: -17.443403244018555\n",
            "Training epoch 6449/1000000, d_loss: -103.89199829101562,  g_loss: -144.4918670654297\n",
            "Training epoch 6450/1000000, d_loss: -309.4085998535156,  g_loss: 43.884910583496094\n",
            "Training epoch 6451/1000000, d_loss: -15.201522827148438,  g_loss: 94.20478057861328\n",
            "Training epoch 6452/1000000, d_loss: -204.9674835205078,  g_loss: 217.69534301757812\n",
            "Training epoch 6453/1000000, d_loss: -81.64784240722656,  g_loss: 189.39175415039062\n",
            "Training epoch 6454/1000000, d_loss: -19.115619659423828,  g_loss: 106.23876953125\n",
            "Training epoch 6455/1000000, d_loss: -56.99943161010742,  g_loss: 85.58767700195312\n",
            "Training epoch 6456/1000000, d_loss: -151.46810913085938,  g_loss: 74.30694580078125\n",
            "Training epoch 6457/1000000, d_loss: -85.1707763671875,  g_loss: -13.880091667175293\n",
            "Training epoch 6458/1000000, d_loss: -150.2603759765625,  g_loss: 6.792232513427734\n",
            "Training epoch 6459/1000000, d_loss: -145.3795928955078,  g_loss: 23.97723388671875\n",
            "Training epoch 6460/1000000, d_loss: -101.82829284667969,  g_loss: 146.91888427734375\n",
            "Training epoch 6461/1000000, d_loss: -91.10099792480469,  g_loss: 77.99964904785156\n",
            "Training epoch 6462/1000000, d_loss: -126.82186126708984,  g_loss: 65.25205993652344\n",
            "Training epoch 6463/1000000, d_loss: 1.0545768737792969,  g_loss: 115.23591613769531\n",
            "Training epoch 6464/1000000, d_loss: 18.22707748413086,  g_loss: 135.11264038085938\n",
            "Training epoch 6465/1000000, d_loss: -148.15440368652344,  g_loss: 160.86050415039062\n",
            "Training epoch 6466/1000000, d_loss: -76.58126831054688,  g_loss: 106.8050537109375\n",
            "Training epoch 6467/1000000, d_loss: -107.30741882324219,  g_loss: 68.49870300292969\n",
            "Training epoch 6468/1000000, d_loss: -90.78271484375,  g_loss: 88.50930786132812\n",
            "Training epoch 6469/1000000, d_loss: -215.26681518554688,  g_loss: 83.98478698730469\n",
            "Training epoch 6470/1000000, d_loss: -100.87356567382812,  g_loss: 198.07980346679688\n",
            "Training epoch 6471/1000000, d_loss: -112.41361236572266,  g_loss: 92.50155639648438\n",
            "Training epoch 6472/1000000, d_loss: -491.1410217285156,  g_loss: -177.632568359375\n",
            "Training epoch 6473/1000000, d_loss: -384.9580383300781,  g_loss: -99.96247863769531\n",
            "Training epoch 6474/1000000, d_loss: 81.90357971191406,  g_loss: 41.115203857421875\n",
            "Training epoch 6475/1000000, d_loss: -146.52484130859375,  g_loss: 60.710506439208984\n",
            "Training epoch 6476/1000000, d_loss: -110.4246826171875,  g_loss: 83.8382339477539\n",
            "Training epoch 6477/1000000, d_loss: -10.63226318359375,  g_loss: 17.09571075439453\n",
            "Training epoch 6478/1000000, d_loss: -133.4892578125,  g_loss: -23.76731300354004\n",
            "Training epoch 6479/1000000, d_loss: -121.53873443603516,  g_loss: 65.1607894897461\n",
            "Training epoch 6480/1000000, d_loss: -66.34576416015625,  g_loss: 87.77096557617188\n",
            "Training epoch 6481/1000000, d_loss: -400.91717529296875,  g_loss: 20.596405029296875\n",
            "Training epoch 6482/1000000, d_loss: -60.64918518066406,  g_loss: 20.181861877441406\n",
            "Training epoch 6483/1000000, d_loss: -58.161502838134766,  g_loss: 36.157711029052734\n",
            "Training epoch 6484/1000000, d_loss: -88.86428833007812,  g_loss: 58.34236145019531\n",
            "Training epoch 6485/1000000, d_loss: -148.50296020507812,  g_loss: -90.496826171875\n",
            "Training epoch 6486/1000000, d_loss: -18.40972328186035,  g_loss: 53.72829818725586\n",
            "Training epoch 6487/1000000, d_loss: -36.66246795654297,  g_loss: 114.82506561279297\n",
            "Training epoch 6488/1000000, d_loss: -213.90921020507812,  g_loss: 34.95379638671875\n",
            "Training epoch 6489/1000000, d_loss: -85.58277130126953,  g_loss: 46.19142150878906\n",
            "Training epoch 6490/1000000, d_loss: -97.57597351074219,  g_loss: 136.30538940429688\n",
            "Training epoch 6491/1000000, d_loss: -369.7541198730469,  g_loss: 60.123565673828125\n",
            "Training epoch 6492/1000000, d_loss: -70.95036315917969,  g_loss: 90.97874450683594\n",
            "Training epoch 6493/1000000, d_loss: -104.973388671875,  g_loss: 121.97473907470703\n",
            "Training epoch 6494/1000000, d_loss: -160.8516387939453,  g_loss: 44.592281341552734\n",
            "Training epoch 6495/1000000, d_loss: -421.0994567871094,  g_loss: -182.4080047607422\n",
            "Training epoch 6496/1000000, d_loss: -154.82467651367188,  g_loss: 39.33711242675781\n",
            "Training epoch 6497/1000000, d_loss: -28.48827362060547,  g_loss: 208.9529266357422\n",
            "Training epoch 6498/1000000, d_loss: -156.01547241210938,  g_loss: 189.2862548828125\n",
            "Training epoch 6499/1000000, d_loss: 35.89818572998047,  g_loss: 72.51950073242188\n",
            "Training epoch 6500/1000000, d_loss: -116.45169067382812,  g_loss: 192.11923217773438\n",
            "Training epoch 6501/1000000, d_loss: -131.95326232910156,  g_loss: 77.21133422851562\n",
            "Training epoch 6502/1000000, d_loss: 4.852333068847656,  g_loss: 108.5223159790039\n",
            "Training epoch 6503/1000000, d_loss: -39.33880615234375,  g_loss: 47.91179656982422\n",
            "Training epoch 6504/1000000, d_loss: -33.77714538574219,  g_loss: 83.46382141113281\n",
            "Training epoch 6505/1000000, d_loss: -96.85935974121094,  g_loss: 100.87063598632812\n",
            "Training epoch 6506/1000000, d_loss: -96.15926361083984,  g_loss: 105.56344604492188\n",
            "Training epoch 6507/1000000, d_loss: -76.17019653320312,  g_loss: 72.18539428710938\n",
            "Training epoch 6508/1000000, d_loss: -95.3056640625,  g_loss: -8.113759994506836\n",
            "Training epoch 6509/1000000, d_loss: -31.182376861572266,  g_loss: 94.78305053710938\n",
            "Training epoch 6510/1000000, d_loss: -98.34288024902344,  g_loss: 56.50469970703125\n",
            "Training epoch 6511/1000000, d_loss: -349.5906677246094,  g_loss: -50.05653381347656\n",
            "Training epoch 6512/1000000, d_loss: -65.59915161132812,  g_loss: 26.25185775756836\n",
            "Training epoch 6513/1000000, d_loss: -107.17340087890625,  g_loss: 14.031003952026367\n",
            "Training epoch 6514/1000000, d_loss: 16.58115005493164,  g_loss: 90.54991149902344\n",
            "Training epoch 6515/1000000, d_loss: 4.044441223144531,  g_loss: 135.87667846679688\n",
            "Training epoch 6516/1000000, d_loss: -199.19854736328125,  g_loss: 15.027175903320312\n",
            "Training epoch 6517/1000000, d_loss: -58.003883361816406,  g_loss: 112.27513122558594\n",
            "Training epoch 6518/1000000, d_loss: -33.30290985107422,  g_loss: 142.0880126953125\n",
            "Training epoch 6519/1000000, d_loss: -118.80545806884766,  g_loss: 155.0137481689453\n",
            "Training epoch 6520/1000000, d_loss: -108.22203826904297,  g_loss: 117.0155258178711\n",
            "Training epoch 6521/1000000, d_loss: -11.779136657714844,  g_loss: 191.239013671875\n",
            "Training epoch 6522/1000000, d_loss: -119.49866485595703,  g_loss: 51.04332733154297\n",
            "Training epoch 6523/1000000, d_loss: -78.75895690917969,  g_loss: 86.60574340820312\n",
            "Training epoch 6524/1000000, d_loss: -202.95762634277344,  g_loss: 27.943044662475586\n",
            "Training epoch 6525/1000000, d_loss: -205.4443817138672,  g_loss: -119.61630249023438\n",
            "Training epoch 6526/1000000, d_loss: -106.95953369140625,  g_loss: -1.8647847175598145\n",
            "Training epoch 6527/1000000, d_loss: -182.82456970214844,  g_loss: -35.935386657714844\n",
            "Training epoch 6528/1000000, d_loss: -207.105224609375,  g_loss: -110.98311614990234\n",
            "Training epoch 6529/1000000, d_loss: -112.10540008544922,  g_loss: 141.63514709472656\n",
            "Training epoch 6530/1000000, d_loss: -76.51266479492188,  g_loss: 176.49485778808594\n",
            "Training epoch 6531/1000000, d_loss: -90.39950561523438,  g_loss: 118.49967193603516\n",
            "Training epoch 6532/1000000, d_loss: -165.70343017578125,  g_loss: 18.49298667907715\n",
            "Training epoch 6533/1000000, d_loss: -103.43673706054688,  g_loss: 14.658208847045898\n",
            "Training epoch 6534/1000000, d_loss: -146.3760528564453,  g_loss: -49.11027145385742\n",
            "Training epoch 6535/1000000, d_loss: -92.30023193359375,  g_loss: -78.98094177246094\n",
            "Training epoch 6536/1000000, d_loss: -32.72708511352539,  g_loss: 27.808475494384766\n",
            "Training epoch 6537/1000000, d_loss: -237.51193237304688,  g_loss: 253.97853088378906\n",
            "Training epoch 6538/1000000, d_loss: -51.80101013183594,  g_loss: -38.76289367675781\n",
            "Training epoch 6539/1000000, d_loss: -29.46088409423828,  g_loss: -45.555328369140625\n",
            "Training epoch 6540/1000000, d_loss: -96.28202819824219,  g_loss: -1.7601566314697266\n",
            "Training epoch 6541/1000000, d_loss: -74.31881713867188,  g_loss: -7.142215728759766\n",
            "Training epoch 6542/1000000, d_loss: -122.87824249267578,  g_loss: -11.686576843261719\n",
            "Training epoch 6543/1000000, d_loss: -76.06531524658203,  g_loss: -68.15543365478516\n",
            "Training epoch 6544/1000000, d_loss: -54.37346649169922,  g_loss: -89.82598876953125\n",
            "Training epoch 6545/1000000, d_loss: -187.45578002929688,  g_loss: -62.071800231933594\n",
            "Training epoch 6546/1000000, d_loss: -103.36348724365234,  g_loss: -65.32380676269531\n",
            "Training epoch 6547/1000000, d_loss: -187.7567901611328,  g_loss: -157.05453491210938\n",
            "Training epoch 6548/1000000, d_loss: 25.87530517578125,  g_loss: 38.02997589111328\n",
            "Training epoch 6549/1000000, d_loss: -118.94182586669922,  g_loss: 43.14356994628906\n",
            "Training epoch 6550/1000000, d_loss: -101.4624252319336,  g_loss: 48.67688751220703\n",
            "Training epoch 6551/1000000, d_loss: -124.2119369506836,  g_loss: -6.727366924285889\n",
            "Training epoch 6552/1000000, d_loss: -86.62564086914062,  g_loss: -45.718299865722656\n",
            "Training epoch 6553/1000000, d_loss: -190.5854034423828,  g_loss: -65.19461059570312\n",
            "Training epoch 6554/1000000, d_loss: -172.70074462890625,  g_loss: -78.05982971191406\n",
            "Training epoch 6555/1000000, d_loss: 2295.3828125,  g_loss: -399.50421142578125\n",
            "Training epoch 6556/1000000, d_loss: 27.915390014648438,  g_loss: -165.01956176757812\n",
            "Training epoch 6557/1000000, d_loss: -0.8577766418457031,  g_loss: -219.33804321289062\n",
            "Training epoch 6558/1000000, d_loss: -195.9095001220703,  g_loss: -115.7436294555664\n",
            "Training epoch 6559/1000000, d_loss: 314.46697998046875,  g_loss: -162.83294677734375\n",
            "Training epoch 6560/1000000, d_loss: -92.18751525878906,  g_loss: -64.8695297241211\n",
            "Training epoch 6561/1000000, d_loss: -213.70066833496094,  g_loss: -82.02014923095703\n",
            "Training epoch 6562/1000000, d_loss: -100.94485473632812,  g_loss: -51.07275390625\n",
            "Training epoch 6563/1000000, d_loss: -179.2711181640625,  g_loss: -120.12226867675781\n",
            "Training epoch 6564/1000000, d_loss: -18.186798095703125,  g_loss: -104.95420837402344\n",
            "Training epoch 6565/1000000, d_loss: -161.0866241455078,  g_loss: 179.79318237304688\n",
            "Training epoch 6566/1000000, d_loss: -210.18382263183594,  g_loss: 158.28675842285156\n",
            "Training epoch 6567/1000000, d_loss: -68.6885757446289,  g_loss: -65.67622375488281\n",
            "Training epoch 6568/1000000, d_loss: 52.54154968261719,  g_loss: -131.2037353515625\n",
            "Training epoch 6569/1000000, d_loss: -76.38763427734375,  g_loss: -7.300731658935547\n",
            "Training epoch 6570/1000000, d_loss: -145.40464782714844,  g_loss: 58.3740234375\n",
            "Training epoch 6571/1000000, d_loss: -55.930450439453125,  g_loss: -6.728782653808594\n",
            "Training epoch 6572/1000000, d_loss: -146.83358764648438,  g_loss: 67.40536499023438\n",
            "Training epoch 6573/1000000, d_loss: -60.66075134277344,  g_loss: 56.003562927246094\n",
            "Training epoch 6574/1000000, d_loss: -24.662193298339844,  g_loss: -6.092784881591797\n",
            "Training epoch 6575/1000000, d_loss: -20.226179122924805,  g_loss: 13.092826843261719\n",
            "Training epoch 6576/1000000, d_loss: 0.7764511108398438,  g_loss: 9.764683723449707\n",
            "Training epoch 6577/1000000, d_loss: -81.90678405761719,  g_loss: -34.701812744140625\n",
            "Training epoch 6578/1000000, d_loss: -362.23809814453125,  g_loss: -110.8083267211914\n",
            "Training epoch 6579/1000000, d_loss: -167.59291076660156,  g_loss: -114.7491455078125\n",
            "Training epoch 6580/1000000, d_loss: -233.36485290527344,  g_loss: -37.31217956542969\n",
            "Training epoch 6581/1000000, d_loss: 896.9974365234375,  g_loss: -40.433616638183594\n",
            "Training epoch 6582/1000000, d_loss: -98.30266571044922,  g_loss: -29.64011001586914\n",
            "Training epoch 6583/1000000, d_loss: -58.62965393066406,  g_loss: -18.99874496459961\n",
            "Training epoch 6584/1000000, d_loss: -120.35757446289062,  g_loss: 17.403249740600586\n",
            "Training epoch 6585/1000000, d_loss: -21.342741012573242,  g_loss: 45.367713928222656\n",
            "Training epoch 6586/1000000, d_loss: -47.48991775512695,  g_loss: 9.607295036315918\n",
            "Training epoch 6587/1000000, d_loss: -653.59326171875,  g_loss: -64.95344543457031\n",
            "Training epoch 6588/1000000, d_loss: -75.3048095703125,  g_loss: 55.04197692871094\n",
            "Training epoch 6589/1000000, d_loss: -55.20862579345703,  g_loss: 115.41943359375\n",
            "Training epoch 6590/1000000, d_loss: -26.574270248413086,  g_loss: 58.5006103515625\n",
            "Training epoch 6591/1000000, d_loss: -152.14816284179688,  g_loss: 384.3290710449219\n",
            "Training epoch 6592/1000000, d_loss: -30.417564392089844,  g_loss: 104.89238739013672\n",
            "Training epoch 6593/1000000, d_loss: -307.28692626953125,  g_loss: -34.715267181396484\n",
            "Training epoch 6594/1000000, d_loss: -59.86420440673828,  g_loss: 9.541023254394531\n",
            "Training epoch 6595/1000000, d_loss: -147.48773193359375,  g_loss: -76.05626678466797\n",
            "Training epoch 6596/1000000, d_loss: -67.93748474121094,  g_loss: -44.42813491821289\n",
            "Training epoch 6597/1000000, d_loss: -296.2614440917969,  g_loss: -230.9692840576172\n",
            "Training epoch 6598/1000000, d_loss: -17.115880966186523,  g_loss: -39.561500549316406\n",
            "Training epoch 6599/1000000, d_loss: -57.191322326660156,  g_loss: 61.76670837402344\n",
            "Training epoch 6600/1000000, d_loss: -74.86392211914062,  g_loss: -35.7779655456543\n",
            "Training epoch 6601/1000000, d_loss: -49.20404052734375,  g_loss: -45.478233337402344\n",
            "Training epoch 6602/1000000, d_loss: -58.31496047973633,  g_loss: -28.804086685180664\n",
            "Training epoch 6603/1000000, d_loss: -29.160856246948242,  g_loss: 9.191832542419434\n",
            "Training epoch 6604/1000000, d_loss: -248.74697875976562,  g_loss: -56.94063949584961\n",
            "Training epoch 6605/1000000, d_loss: -131.80226135253906,  g_loss: 10.881742477416992\n",
            "Training epoch 6606/1000000, d_loss: -156.0384063720703,  g_loss: -124.57218933105469\n",
            "Training epoch 6607/1000000, d_loss: -133.00685119628906,  g_loss: -100.2337875366211\n",
            "Training epoch 6608/1000000, d_loss: -72.85128784179688,  g_loss: 113.91922760009766\n",
            "Training epoch 6609/1000000, d_loss: -556.3902587890625,  g_loss: -172.41412353515625\n",
            "Training epoch 6610/1000000, d_loss: 73.06082916259766,  g_loss: -78.93180847167969\n",
            "Training epoch 6611/1000000, d_loss: -133.19119262695312,  g_loss: -129.40850830078125\n",
            "Training epoch 6612/1000000, d_loss: -67.71846008300781,  g_loss: 2.8698806762695312\n",
            "Training epoch 6613/1000000, d_loss: -41.14415740966797,  g_loss: 69.65252685546875\n",
            "Training epoch 6614/1000000, d_loss: 2.457805633544922,  g_loss: 92.31016540527344\n",
            "Training epoch 6615/1000000, d_loss: -203.15960693359375,  g_loss: 42.04042053222656\n",
            "Training epoch 6616/1000000, d_loss: -178.6301727294922,  g_loss: -11.292542457580566\n",
            "Training epoch 6617/1000000, d_loss: -27.946046829223633,  g_loss: 74.17327117919922\n",
            "Training epoch 6618/1000000, d_loss: -118.75482940673828,  g_loss: 13.304930686950684\n",
            "Training epoch 6619/1000000, d_loss: -129.45401000976562,  g_loss: 39.31125259399414\n",
            "Training epoch 6620/1000000, d_loss: -178.68283081054688,  g_loss: 251.86117553710938\n",
            "Training epoch 6621/1000000, d_loss: -270.36492919921875,  g_loss: 279.2718505859375\n",
            "Training epoch 6622/1000000, d_loss: -197.3911895751953,  g_loss: 374.8773498535156\n",
            "Training epoch 6623/1000000, d_loss: 156.41305541992188,  g_loss: 55.112457275390625\n",
            "Training epoch 6624/1000000, d_loss: -189.49777221679688,  g_loss: -18.799901962280273\n",
            "Training epoch 6625/1000000, d_loss: -274.862548828125,  g_loss: -74.7362060546875\n",
            "Training epoch 6626/1000000, d_loss: -145.69468688964844,  g_loss: -42.18794631958008\n",
            "Training epoch 6627/1000000, d_loss: -258.7206115722656,  g_loss: -39.878684997558594\n",
            "Training epoch 6628/1000000, d_loss: -21.589200973510742,  g_loss: 47.2257194519043\n",
            "Training epoch 6629/1000000, d_loss: -138.3938751220703,  g_loss: 60.724143981933594\n",
            "Training epoch 6630/1000000, d_loss: 10.34512710571289,  g_loss: 14.976414680480957\n",
            "Training epoch 6631/1000000, d_loss: -87.75231170654297,  g_loss: 2.6758475303649902\n",
            "Training epoch 6632/1000000, d_loss: -418.96661376953125,  g_loss: -68.62506103515625\n",
            "Training epoch 6633/1000000, d_loss: -298.2106628417969,  g_loss: -7.522479057312012\n",
            "Training epoch 6634/1000000, d_loss: 11.86245346069336,  g_loss: -49.086448669433594\n",
            "Training epoch 6635/1000000, d_loss: -47.81364822387695,  g_loss: 32.519439697265625\n",
            "Training epoch 6636/1000000, d_loss: -153.3029022216797,  g_loss: -28.923141479492188\n",
            "Training epoch 6637/1000000, d_loss: -311.0627136230469,  g_loss: -102.42008972167969\n",
            "Training epoch 6638/1000000, d_loss: -121.73104095458984,  g_loss: -62.4492301940918\n",
            "Training epoch 6639/1000000, d_loss: -263.19464111328125,  g_loss: -371.7886657714844\n",
            "Training epoch 6640/1000000, d_loss: -24.243881225585938,  g_loss: -63.651031494140625\n",
            "Training epoch 6641/1000000, d_loss: 0.05635833740234375,  g_loss: -13.3392972946167\n",
            "Training epoch 6642/1000000, d_loss: -124.82013702392578,  g_loss: -19.888277053833008\n",
            "Training epoch 6643/1000000, d_loss: -239.74990844726562,  g_loss: 51.5443000793457\n",
            "Training epoch 6644/1000000, d_loss: -93.06277465820312,  g_loss: -99.25923156738281\n",
            "Training epoch 6645/1000000, d_loss: -565.442138671875,  g_loss: -215.10047912597656\n",
            "Training epoch 6646/1000000, d_loss: -43.922698974609375,  g_loss: -113.52700805664062\n",
            "Training epoch 6647/1000000, d_loss: -88.79353332519531,  g_loss: 129.31190490722656\n",
            "Training epoch 6648/1000000, d_loss: -126.40570068359375,  g_loss: -7.206239700317383\n",
            "Training epoch 6649/1000000, d_loss: -216.661865234375,  g_loss: 89.51832580566406\n",
            "Training epoch 6650/1000000, d_loss: -143.11666870117188,  g_loss: -29.605283737182617\n",
            "Training epoch 6651/1000000, d_loss: -85.34492492675781,  g_loss: 81.24118041992188\n",
            "Training epoch 6652/1000000, d_loss: -135.7069091796875,  g_loss: -74.1094970703125\n",
            "Training epoch 6653/1000000, d_loss: -132.82586669921875,  g_loss: 20.059776306152344\n",
            "Training epoch 6654/1000000, d_loss: -118.15103149414062,  g_loss: -58.16196060180664\n",
            "Training epoch 6655/1000000, d_loss: -192.3255615234375,  g_loss: 178.6832733154297\n",
            "Training epoch 6656/1000000, d_loss: -78.70101165771484,  g_loss: 124.06763458251953\n",
            "Training epoch 6657/1000000, d_loss: -279.1854248046875,  g_loss: 206.71661376953125\n",
            "Training epoch 6658/1000000, d_loss: -14.005577087402344,  g_loss: 142.3886260986328\n",
            "Training epoch 6659/1000000, d_loss: -130.63671875,  g_loss: 19.31169319152832\n",
            "Training epoch 6660/1000000, d_loss: -123.00715637207031,  g_loss: 7.22908878326416\n",
            "Training epoch 6661/1000000, d_loss: -228.34347534179688,  g_loss: -35.06585693359375\n",
            "Training epoch 6662/1000000, d_loss: -19.224567413330078,  g_loss: 94.50770568847656\n",
            "Training epoch 6663/1000000, d_loss: 13.966445922851562,  g_loss: 107.52561950683594\n",
            "Training epoch 6664/1000000, d_loss: -56.23386001586914,  g_loss: 170.95303344726562\n",
            "Training epoch 6665/1000000, d_loss: -133.50030517578125,  g_loss: 111.93372344970703\n",
            "Training epoch 6666/1000000, d_loss: -87.7618637084961,  g_loss: 120.71061706542969\n",
            "Training epoch 6667/1000000, d_loss: -48.798095703125,  g_loss: 163.46810913085938\n",
            "Training epoch 6668/1000000, d_loss: -75.72200775146484,  g_loss: 133.73587036132812\n",
            "Training epoch 6669/1000000, d_loss: -237.78004455566406,  g_loss: 82.97105407714844\n",
            "Training epoch 6670/1000000, d_loss: -418.3963623046875,  g_loss: -165.72047424316406\n",
            "Training epoch 6671/1000000, d_loss: -145.96307373046875,  g_loss: -47.53022766113281\n",
            "Training epoch 6672/1000000, d_loss: -118.96102905273438,  g_loss: -0.23663806915283203\n",
            "Training epoch 6673/1000000, d_loss: -57.67279052734375,  g_loss: -36.092384338378906\n",
            "Training epoch 6674/1000000, d_loss: 195.65599060058594,  g_loss: 23.133193969726562\n",
            "Training epoch 6675/1000000, d_loss: -93.09954833984375,  g_loss: 44.15501403808594\n",
            "Training epoch 6676/1000000, d_loss: -12.839075088500977,  g_loss: 14.694989204406738\n",
            "Training epoch 6677/1000000, d_loss: -37.169681549072266,  g_loss: 7.750973701477051\n",
            "Training epoch 6678/1000000, d_loss: -407.8859558105469,  g_loss: -65.27032470703125\n",
            "Training epoch 6679/1000000, d_loss: -385.9048156738281,  g_loss: -87.79484558105469\n",
            "Training epoch 6680/1000000, d_loss: 11.253662109375,  g_loss: 28.71198272705078\n",
            "Training epoch 6681/1000000, d_loss: -17.83795928955078,  g_loss: 24.35169219970703\n",
            "Training epoch 6682/1000000, d_loss: -75.60303497314453,  g_loss: 73.0030517578125\n",
            "Training epoch 6683/1000000, d_loss: -92.09479522705078,  g_loss: 142.889404296875\n",
            "Training epoch 6684/1000000, d_loss: -74.94732666015625,  g_loss: 57.26486587524414\n",
            "Training epoch 6685/1000000, d_loss: -106.15412902832031,  g_loss: 87.59686279296875\n",
            "Training epoch 6686/1000000, d_loss: -52.0583610534668,  g_loss: -5.0849809646606445\n",
            "Training epoch 6687/1000000, d_loss: -267.4874267578125,  g_loss: -19.685632705688477\n",
            "Training epoch 6688/1000000, d_loss: -192.23855590820312,  g_loss: -234.12747192382812\n",
            "Training epoch 6689/1000000, d_loss: -66.56356048583984,  g_loss: 93.6486587524414\n",
            "Training epoch 6690/1000000, d_loss: -102.84503173828125,  g_loss: 112.40805053710938\n",
            "Training epoch 6691/1000000, d_loss: -169.28829956054688,  g_loss: 201.51040649414062\n",
            "Training epoch 6692/1000000, d_loss: -10.0938720703125,  g_loss: 244.4146728515625\n",
            "Training epoch 6693/1000000, d_loss: -211.50021362304688,  g_loss: 29.69835662841797\n",
            "Training epoch 6694/1000000, d_loss: -254.85330200195312,  g_loss: 1.0336551666259766\n",
            "Training epoch 6695/1000000, d_loss: -108.74259948730469,  g_loss: 31.665546417236328\n",
            "Training epoch 6696/1000000, d_loss: -122.17778015136719,  g_loss: 149.65042114257812\n",
            "Training epoch 6697/1000000, d_loss: -101.662109375,  g_loss: 228.1651153564453\n",
            "Training epoch 6698/1000000, d_loss: -510.81280517578125,  g_loss: 1.9308624267578125\n",
            "Training epoch 6699/1000000, d_loss: -83.64299011230469,  g_loss: 53.055641174316406\n",
            "Training epoch 6700/1000000, d_loss: -12.753639221191406,  g_loss: 33.3684196472168\n",
            "Training epoch 6701/1000000, d_loss: -112.75347900390625,  g_loss: 8.238609313964844\n",
            "Training epoch 6702/1000000, d_loss: -150.6772918701172,  g_loss: 71.72525024414062\n",
            "Training epoch 6703/1000000, d_loss: -248.47930908203125,  g_loss: 18.49842071533203\n",
            "Training epoch 6704/1000000, d_loss: -8.33062744140625,  g_loss: -7.724612236022949\n",
            "Training epoch 6705/1000000, d_loss: -802.6331787109375,  g_loss: -121.51223754882812\n",
            "Training epoch 6706/1000000, d_loss: 105.71073913574219,  g_loss: -54.876434326171875\n",
            "Training epoch 6707/1000000, d_loss: -10.961601257324219,  g_loss: -28.67473030090332\n",
            "Training epoch 6708/1000000, d_loss: -96.60086059570312,  g_loss: -57.12726974487305\n",
            "Training epoch 6709/1000000, d_loss: -132.90716552734375,  g_loss: -26.47208023071289\n",
            "Training epoch 6710/1000000, d_loss: -24.973400115966797,  g_loss: 30.5626163482666\n",
            "Training epoch 6711/1000000, d_loss: -135.91342163085938,  g_loss: -64.74854278564453\n",
            "Training epoch 6712/1000000, d_loss: -82.75684356689453,  g_loss: 0.8898983001708984\n",
            "Training epoch 6713/1000000, d_loss: -133.74710083007812,  g_loss: 18.814186096191406\n",
            "Training epoch 6714/1000000, d_loss: -62.40287780761719,  g_loss: 119.8084716796875\n",
            "Training epoch 6715/1000000, d_loss: -90.8765869140625,  g_loss: 130.71572875976562\n",
            "Training epoch 6716/1000000, d_loss: -259.9757385253906,  g_loss: 274.3852844238281\n",
            "Training epoch 6717/1000000, d_loss: -146.63800048828125,  g_loss: 211.7337646484375\n",
            "Training epoch 6718/1000000, d_loss: -22.28924560546875,  g_loss: 102.2135238647461\n",
            "Training epoch 6719/1000000, d_loss: -109.10108947753906,  g_loss: 27.604713439941406\n",
            "Training epoch 6720/1000000, d_loss: -73.34861755371094,  g_loss: 123.50119018554688\n",
            "Training epoch 6721/1000000, d_loss: 12.750717163085938,  g_loss: 183.21231079101562\n",
            "Training epoch 6722/1000000, d_loss: -53.11931610107422,  g_loss: 95.0685043334961\n",
            "Training epoch 6723/1000000, d_loss: -113.41671752929688,  g_loss: 119.74053192138672\n",
            "Training epoch 6724/1000000, d_loss: -168.7009735107422,  g_loss: 47.6756591796875\n",
            "Training epoch 6725/1000000, d_loss: -88.49488830566406,  g_loss: 124.14313507080078\n",
            "Training epoch 6726/1000000, d_loss: -274.04180908203125,  g_loss: -3.7961387634277344\n",
            "Training epoch 6727/1000000, d_loss: -132.7283477783203,  g_loss: 223.56500244140625\n",
            "Training epoch 6728/1000000, d_loss: -6.516227722167969,  g_loss: 143.91262817382812\n",
            "Training epoch 6729/1000000, d_loss: -694.8464965820312,  g_loss: -18.173721313476562\n",
            "Training epoch 6730/1000000, d_loss: 61.4990234375,  g_loss: -176.785888671875\n",
            "Training epoch 6731/1000000, d_loss: -1087.073974609375,  g_loss: -398.4736022949219\n",
            "Training epoch 6732/1000000, d_loss: 116.71080780029297,  g_loss: 38.876468658447266\n",
            "Training epoch 6733/1000000, d_loss: -412.4455261230469,  g_loss: -11.414093971252441\n",
            "Training epoch 6734/1000000, d_loss: -69.46209716796875,  g_loss: 38.48255157470703\n",
            "Training epoch 6735/1000000, d_loss: -301.1830749511719,  g_loss: 318.9985046386719\n",
            "Training epoch 6736/1000000, d_loss: -269.8568420410156,  g_loss: 64.78131103515625\n",
            "Training epoch 6737/1000000, d_loss: -36.542335510253906,  g_loss: 144.60968017578125\n",
            "Training epoch 6738/1000000, d_loss: -216.51046752929688,  g_loss: 401.8173522949219\n",
            "Training epoch 6739/1000000, d_loss: -8.998519897460938,  g_loss: 28.043502807617188\n",
            "Training epoch 6740/1000000, d_loss: 101.8483657836914,  g_loss: 79.25914001464844\n",
            "Training epoch 6741/1000000, d_loss: -85.68836975097656,  g_loss: 132.742431640625\n",
            "Training epoch 6742/1000000, d_loss: -34.714759826660156,  g_loss: 81.77289581298828\n",
            "Training epoch 6743/1000000, d_loss: -70.71614074707031,  g_loss: 7.741543769836426\n",
            "Training epoch 6744/1000000, d_loss: -248.23532104492188,  g_loss: -107.23625183105469\n",
            "Training epoch 6745/1000000, d_loss: -57.8841438293457,  g_loss: 6.045490264892578\n",
            "Training epoch 6746/1000000, d_loss: -0.3995933532714844,  g_loss: 71.58856201171875\n",
            "Training epoch 6747/1000000, d_loss: -84.75465393066406,  g_loss: 138.34487915039062\n",
            "Training epoch 6748/1000000, d_loss: -217.2264404296875,  g_loss: 223.56686401367188\n",
            "Training epoch 6749/1000000, d_loss: -149.58700561523438,  g_loss: 198.17041015625\n",
            "Training epoch 6750/1000000, d_loss: 35.25236511230469,  g_loss: 171.8651885986328\n",
            "Training epoch 6751/1000000, d_loss: -97.12468719482422,  g_loss: 207.83584594726562\n",
            "Training epoch 6752/1000000, d_loss: -109.26734924316406,  g_loss: 309.6896667480469\n",
            "Training epoch 6753/1000000, d_loss: -53.02217483520508,  g_loss: 151.53160095214844\n",
            "Training epoch 6754/1000000, d_loss: -47.41896057128906,  g_loss: 91.51486206054688\n",
            "Training epoch 6755/1000000, d_loss: -157.30264282226562,  g_loss: 246.18063354492188\n",
            "Training epoch 6756/1000000, d_loss: -19.98342514038086,  g_loss: 57.791160583496094\n",
            "Training epoch 6757/1000000, d_loss: -168.9635772705078,  g_loss: -64.6126708984375\n",
            "Training epoch 6758/1000000, d_loss: -136.07618713378906,  g_loss: -46.2608757019043\n",
            "Training epoch 6759/1000000, d_loss: 12.417472839355469,  g_loss: 60.50716018676758\n",
            "Training epoch 6760/1000000, d_loss: -149.3601531982422,  g_loss: 57.41089630126953\n",
            "Training epoch 6761/1000000, d_loss: -180.52403259277344,  g_loss: 75.07028198242188\n",
            "Training epoch 6762/1000000, d_loss: -94.71357727050781,  g_loss: 80.85374450683594\n",
            "Training epoch 6763/1000000, d_loss: 10.69070816040039,  g_loss: 100.11427307128906\n",
            "Training epoch 6764/1000000, d_loss: -222.24130249023438,  g_loss: 22.77332878112793\n",
            "Training epoch 6765/1000000, d_loss: -35.2313232421875,  g_loss: -11.279863357543945\n",
            "Training epoch 6766/1000000, d_loss: -65.31253814697266,  g_loss: 30.233604431152344\n",
            "Training epoch 6767/1000000, d_loss: -25.363723754882812,  g_loss: 77.60091400146484\n",
            "Training epoch 6768/1000000, d_loss: -254.98854064941406,  g_loss: 134.8497314453125\n",
            "Training epoch 6769/1000000, d_loss: -441.74298095703125,  g_loss: -41.5767822265625\n",
            "Training epoch 6770/1000000, d_loss: -87.65068054199219,  g_loss: -7.023723602294922\n",
            "Training epoch 6771/1000000, d_loss: -218.3883056640625,  g_loss: -166.40895080566406\n",
            "Training epoch 6772/1000000, d_loss: 398.2254638671875,  g_loss: -76.41452026367188\n",
            "Training epoch 6773/1000000, d_loss: 41.0625,  g_loss: -108.798828125\n",
            "Training epoch 6774/1000000, d_loss: 13.250293731689453,  g_loss: -57.33052062988281\n",
            "Training epoch 6775/1000000, d_loss: -899.2007446289062,  g_loss: -190.10812377929688\n",
            "Training epoch 6776/1000000, d_loss: 169.7716827392578,  g_loss: -187.1438751220703\n",
            "Training epoch 6777/1000000, d_loss: -46.46355438232422,  g_loss: -53.625282287597656\n",
            "Training epoch 6778/1000000, d_loss: 122.24796295166016,  g_loss: -68.00856018066406\n",
            "Training epoch 6779/1000000, d_loss: -141.443603515625,  g_loss: -17.386093139648438\n",
            "Training epoch 6780/1000000, d_loss: -305.58551025390625,  g_loss: -3.861295700073242\n",
            "Training epoch 6781/1000000, d_loss: -132.409912109375,  g_loss: -47.982086181640625\n",
            "Training epoch 6782/1000000, d_loss: -35.136470794677734,  g_loss: -8.080896377563477\n",
            "Training epoch 6783/1000000, d_loss: -109.04962158203125,  g_loss: -162.4171600341797\n",
            "Training epoch 6784/1000000, d_loss: -175.2600860595703,  g_loss: -263.31781005859375\n",
            "Training epoch 6785/1000000, d_loss: 5205.9404296875,  g_loss: 86.44123077392578\n",
            "Training epoch 6786/1000000, d_loss: -43.403656005859375,  g_loss: 25.883563995361328\n",
            "Training epoch 6787/1000000, d_loss: 20.723796844482422,  g_loss: 60.364967346191406\n",
            "Training epoch 6788/1000000, d_loss: -99.28788757324219,  g_loss: 25.193153381347656\n",
            "Training epoch 6789/1000000, d_loss: 221.53089904785156,  g_loss: 129.32037353515625\n",
            "Training epoch 6790/1000000, d_loss: -82.00993347167969,  g_loss: 88.74024963378906\n",
            "Training epoch 6791/1000000, d_loss: -248.45008850097656,  g_loss: 249.6714324951172\n",
            "Training epoch 6792/1000000, d_loss: -81.05459594726562,  g_loss: 159.45724487304688\n",
            "Training epoch 6793/1000000, d_loss: -188.83309936523438,  g_loss: 173.94354248046875\n",
            "Training epoch 6794/1000000, d_loss: -177.0958709716797,  g_loss: 251.7347412109375\n",
            "Training epoch 6795/1000000, d_loss: -229.6558380126953,  g_loss: 198.02931213378906\n",
            "Training epoch 6796/1000000, d_loss: -14.699041366577148,  g_loss: -46.3994140625\n",
            "Training epoch 6797/1000000, d_loss: -199.06065368652344,  g_loss: 41.8333854675293\n",
            "Training epoch 6798/1000000, d_loss: -109.36405944824219,  g_loss: 62.29547882080078\n",
            "Training epoch 6799/1000000, d_loss: -97.11456298828125,  g_loss: 140.31056213378906\n",
            "Training epoch 6800/1000000, d_loss: -94.04176330566406,  g_loss: 126.41993713378906\n",
            "Training epoch 6801/1000000, d_loss: -124.48161315917969,  g_loss: 85.95185852050781\n",
            "Training epoch 6802/1000000, d_loss: 1.7278633117675781,  g_loss: -26.035648345947266\n",
            "Training epoch 6803/1000000, d_loss: -128.02090454101562,  g_loss: 177.547119140625\n",
            "Training epoch 6804/1000000, d_loss: -83.4508285522461,  g_loss: 103.25143432617188\n",
            "Training epoch 6805/1000000, d_loss: 15.060552597045898,  g_loss: 139.43789672851562\n",
            "Training epoch 6806/1000000, d_loss: -103.88798522949219,  g_loss: 93.89613342285156\n",
            "Training epoch 6807/1000000, d_loss: -121.53874969482422,  g_loss: 82.34788513183594\n",
            "Training epoch 6808/1000000, d_loss: -225.07872009277344,  g_loss: 39.48134231567383\n",
            "Training epoch 6809/1000000, d_loss: -161.58644104003906,  g_loss: 0.20643901824951172\n",
            "Training epoch 6810/1000000, d_loss: -243.82545471191406,  g_loss: 90.03398895263672\n",
            "Training epoch 6811/1000000, d_loss: -98.97540283203125,  g_loss: -47.60816192626953\n",
            "Training epoch 6812/1000000, d_loss: -976.8673095703125,  g_loss: -397.46856689453125\n",
            "Training epoch 6813/1000000, d_loss: 101.36451721191406,  g_loss: 43.711517333984375\n",
            "Training epoch 6814/1000000, d_loss: 60.2226676940918,  g_loss: 17.926990509033203\n",
            "Training epoch 6815/1000000, d_loss: -57.050804138183594,  g_loss: 27.25551986694336\n",
            "Training epoch 6816/1000000, d_loss: -59.96709060668945,  g_loss: 174.19027709960938\n",
            "Training epoch 6817/1000000, d_loss: -27.061767578125,  g_loss: 180.71192932128906\n",
            "Training epoch 6818/1000000, d_loss: -133.81690979003906,  g_loss: 150.75245666503906\n",
            "Training epoch 6819/1000000, d_loss: -653.9667358398438,  g_loss: -133.11734008789062\n",
            "Training epoch 6820/1000000, d_loss: 63.06256866455078,  g_loss: 59.74350357055664\n",
            "Training epoch 6821/1000000, d_loss: 31.317638397216797,  g_loss: 63.014076232910156\n",
            "Training epoch 6822/1000000, d_loss: -154.1945343017578,  g_loss: 7.229842185974121\n",
            "Training epoch 6823/1000000, d_loss: -241.32388305664062,  g_loss: 12.9920654296875\n",
            "Training epoch 6824/1000000, d_loss: -160.88381958007812,  g_loss: 187.92367553710938\n",
            "Training epoch 6825/1000000, d_loss: -103.25785827636719,  g_loss: 121.9814224243164\n",
            "Training epoch 6826/1000000, d_loss: -189.12924194335938,  g_loss: 446.0612487792969\n",
            "Training epoch 6827/1000000, d_loss: 148.75601196289062,  g_loss: 41.91283416748047\n",
            "Training epoch 6828/1000000, d_loss: -74.99998474121094,  g_loss: 14.807363510131836\n",
            "Training epoch 6829/1000000, d_loss: -192.7664794921875,  g_loss: 127.3060073852539\n",
            "Training epoch 6830/1000000, d_loss: -99.59082794189453,  g_loss: -46.89822769165039\n",
            "Training epoch 6831/1000000, d_loss: -64.55247497558594,  g_loss: 40.58329772949219\n",
            "Training epoch 6832/1000000, d_loss: -162.36831665039062,  g_loss: 4.262615203857422\n",
            "Training epoch 6833/1000000, d_loss: -258.04632568359375,  g_loss: 113.20533752441406\n",
            "Training epoch 6834/1000000, d_loss: -128.90359497070312,  g_loss: 87.775634765625\n",
            "Training epoch 6835/1000000, d_loss: -107.3563232421875,  g_loss: 74.93380737304688\n",
            "Training epoch 6836/1000000, d_loss: -47.069610595703125,  g_loss: 62.13930130004883\n",
            "Training epoch 6837/1000000, d_loss: -230.33314514160156,  g_loss: -54.25157928466797\n",
            "Training epoch 6838/1000000, d_loss: -1209.1083984375,  g_loss: -337.6497497558594\n",
            "Training epoch 6839/1000000, d_loss: -700.2601928710938,  g_loss: -412.4306945800781\n",
            "Training epoch 6840/1000000, d_loss: 26.905385971069336,  g_loss: -48.447994232177734\n",
            "Training epoch 6841/1000000, d_loss: -48.886653900146484,  g_loss: 109.74824523925781\n",
            "Training epoch 6842/1000000, d_loss: -18.95721435546875,  g_loss: 22.69122314453125\n",
            "Training epoch 6843/1000000, d_loss: -168.0703582763672,  g_loss: 44.75004577636719\n",
            "Training epoch 6844/1000000, d_loss: -99.71509552001953,  g_loss: -7.243389129638672\n",
            "Training epoch 6845/1000000, d_loss: -157.35548400878906,  g_loss: 75.7120361328125\n",
            "Training epoch 6846/1000000, d_loss: -243.32461547851562,  g_loss: 237.72015380859375\n",
            "Training epoch 6847/1000000, d_loss: -134.08404541015625,  g_loss: -35.63669967651367\n",
            "Training epoch 6848/1000000, d_loss: -1.2450294494628906,  g_loss: 9.0476713180542\n",
            "Training epoch 6849/1000000, d_loss: -174.3853759765625,  g_loss: -63.76734161376953\n",
            "Training epoch 6850/1000000, d_loss: -64.83822631835938,  g_loss: 133.70379638671875\n",
            "Training epoch 6851/1000000, d_loss: -223.66891479492188,  g_loss: 98.13467407226562\n",
            "Training epoch 6852/1000000, d_loss: -120.93521118164062,  g_loss: 100.34638214111328\n",
            "Training epoch 6853/1000000, d_loss: -398.2459716796875,  g_loss: -97.37958526611328\n",
            "Training epoch 6854/1000000, d_loss: -1316.60986328125,  g_loss: -829.5880126953125\n",
            "Training epoch 6855/1000000, d_loss: -61.90803527832031,  g_loss: 95.25233459472656\n",
            "Training epoch 6856/1000000, d_loss: -304.043212890625,  g_loss: -6.943103790283203\n",
            "Training epoch 6857/1000000, d_loss: -349.45111083984375,  g_loss: -55.53083801269531\n",
            "Training epoch 6858/1000000, d_loss: 220.17608642578125,  g_loss: -119.2177734375\n",
            "Training epoch 6859/1000000, d_loss: -25.776931762695312,  g_loss: 17.416269302368164\n",
            "Training epoch 6860/1000000, d_loss: 191.57064819335938,  g_loss: 74.51362609863281\n",
            "Training epoch 6861/1000000, d_loss: -46.4168701171875,  g_loss: 186.0652313232422\n",
            "Training epoch 6862/1000000, d_loss: -108.0872802734375,  g_loss: 269.4071044921875\n",
            "Training epoch 6863/1000000, d_loss: -78.65962219238281,  g_loss: 269.7107849121094\n",
            "Training epoch 6864/1000000, d_loss: -113.60844421386719,  g_loss: 186.05001831054688\n",
            "Training epoch 6865/1000000, d_loss: -11.561237335205078,  g_loss: 141.69259643554688\n",
            "Training epoch 6866/1000000, d_loss: -179.03921508789062,  g_loss: 274.7204284667969\n",
            "Training epoch 6867/1000000, d_loss: 17.073410034179688,  g_loss: -11.934904098510742\n",
            "Training epoch 6868/1000000, d_loss: 59.362369537353516,  g_loss: 17.426109313964844\n",
            "Training epoch 6869/1000000, d_loss: -69.18988037109375,  g_loss: 49.587337493896484\n",
            "Training epoch 6870/1000000, d_loss: 43.012794494628906,  g_loss: 45.17333221435547\n",
            "Training epoch 6871/1000000, d_loss: -235.24758911132812,  g_loss: 161.85336303710938\n",
            "Training epoch 6872/1000000, d_loss: -103.26004791259766,  g_loss: 62.12303924560547\n",
            "Training epoch 6873/1000000, d_loss: -143.05274963378906,  g_loss: -9.580795288085938\n",
            "Training epoch 6874/1000000, d_loss: -94.9603271484375,  g_loss: -1.4940719604492188\n",
            "Training epoch 6875/1000000, d_loss: -61.59349822998047,  g_loss: 104.52664184570312\n",
            "Training epoch 6876/1000000, d_loss: -201.12841796875,  g_loss: -8.857593536376953\n",
            "Training epoch 6877/1000000, d_loss: -487.91156005859375,  g_loss: -267.0509338378906\n",
            "Training epoch 6878/1000000, d_loss: 100.31937408447266,  g_loss: 72.36302185058594\n",
            "Training epoch 6879/1000000, d_loss: -71.68846130371094,  g_loss: 132.50885009765625\n",
            "Training epoch 6880/1000000, d_loss: -125.10265350341797,  g_loss: 60.77212905883789\n",
            "Training epoch 6881/1000000, d_loss: -206.48439025878906,  g_loss: 113.03874206542969\n",
            "Training epoch 6882/1000000, d_loss: -227.56649780273438,  g_loss: 70.87077331542969\n",
            "Training epoch 6883/1000000, d_loss: -279.40478515625,  g_loss: 55.51003646850586\n",
            "Training epoch 6884/1000000, d_loss: -307.1762390136719,  g_loss: -204.33993530273438\n",
            "Training epoch 6885/1000000, d_loss: -142.36338806152344,  g_loss: 138.99420166015625\n",
            "Training epoch 6886/1000000, d_loss: -187.28970336914062,  g_loss: 276.8809814453125\n",
            "Training epoch 6887/1000000, d_loss: -144.04376220703125,  g_loss: 107.74452209472656\n",
            "Training epoch 6888/1000000, d_loss: -150.36270141601562,  g_loss: -76.55648803710938\n",
            "Training epoch 6889/1000000, d_loss: -152.7874755859375,  g_loss: 130.7007598876953\n",
            "Training epoch 6890/1000000, d_loss: -99.85481262207031,  g_loss: 58.68498229980469\n",
            "Training epoch 6891/1000000, d_loss: -107.49046325683594,  g_loss: 206.92709350585938\n",
            "Training epoch 6892/1000000, d_loss: -85.141357421875,  g_loss: 161.7309112548828\n",
            "Training epoch 6893/1000000, d_loss: 4489.439453125,  g_loss: 101.2314453125\n",
            "Training epoch 6894/1000000, d_loss: -98.491943359375,  g_loss: 54.903778076171875\n",
            "Training epoch 6895/1000000, d_loss: -47.20913314819336,  g_loss: 140.35708618164062\n",
            "Training epoch 6896/1000000, d_loss: 21.730268478393555,  g_loss: 82.47225189208984\n",
            "Training epoch 6897/1000000, d_loss: -83.9522476196289,  g_loss: 159.27804565429688\n",
            "Training epoch 6898/1000000, d_loss: -341.71697998046875,  g_loss: 116.27980041503906\n",
            "Training epoch 6899/1000000, d_loss: -52.11780548095703,  g_loss: 121.53240966796875\n",
            "Training epoch 6900/1000000, d_loss: -389.4773254394531,  g_loss: 6.398597717285156\n",
            "Training epoch 6901/1000000, d_loss: -47.283042907714844,  g_loss: 11.875133514404297\n",
            "Training epoch 6902/1000000, d_loss: 13.341968536376953,  g_loss: 23.716100692749023\n",
            "Training epoch 6903/1000000, d_loss: -105.0356674194336,  g_loss: 79.58715057373047\n",
            "Training epoch 6904/1000000, d_loss: 119.218505859375,  g_loss: -46.409950256347656\n",
            "Training epoch 6905/1000000, d_loss: -317.46197509765625,  g_loss: -5.889190673828125\n",
            "Training epoch 6906/1000000, d_loss: -70.0212173461914,  g_loss: 38.45951461791992\n",
            "Training epoch 6907/1000000, d_loss: -227.95481872558594,  g_loss: 203.9402313232422\n",
            "Training epoch 6908/1000000, d_loss: -45.21269226074219,  g_loss: 120.88722229003906\n",
            "Training epoch 6909/1000000, d_loss: -171.8492889404297,  g_loss: 102.90534210205078\n",
            "Training epoch 6910/1000000, d_loss: -244.57101440429688,  g_loss: -3.141124725341797\n",
            "Training epoch 6911/1000000, d_loss: -47.34027862548828,  g_loss: -114.52875518798828\n",
            "Training epoch 6912/1000000, d_loss: -90.9696044921875,  g_loss: -111.09417724609375\n",
            "Training epoch 6913/1000000, d_loss: -43.35430145263672,  g_loss: -38.39683151245117\n",
            "Training epoch 6914/1000000, d_loss: 38.63694763183594,  g_loss: -8.283878326416016\n",
            "Training epoch 6915/1000000, d_loss: -78.32400512695312,  g_loss: 46.84140396118164\n",
            "Training epoch 6916/1000000, d_loss: -230.84591674804688,  g_loss: 38.287967681884766\n",
            "Training epoch 6917/1000000, d_loss: -263.4518737792969,  g_loss: -62.916709899902344\n",
            "Training epoch 6918/1000000, d_loss: -201.09906005859375,  g_loss: 82.81999969482422\n",
            "Training epoch 6919/1000000, d_loss: -122.30406951904297,  g_loss: 223.76612854003906\n",
            "Training epoch 6920/1000000, d_loss: -117.86114501953125,  g_loss: 9.165546417236328\n",
            "Training epoch 6921/1000000, d_loss: -943.0675659179688,  g_loss: -62.161338806152344\n",
            "Training epoch 6922/1000000, d_loss: 303.0321044921875,  g_loss: -212.65821838378906\n",
            "Training epoch 6923/1000000, d_loss: -80.82015991210938,  g_loss: 143.3046875\n",
            "Training epoch 6924/1000000, d_loss: -51.44624328613281,  g_loss: -93.2840805053711\n",
            "Training epoch 6925/1000000, d_loss: 73.735107421875,  g_loss: 210.282958984375\n",
            "Training epoch 6926/1000000, d_loss: -137.20875549316406,  g_loss: 127.02189636230469\n",
            "Training epoch 6927/1000000, d_loss: -72.80281829833984,  g_loss: -106.05536651611328\n",
            "Training epoch 6928/1000000, d_loss: -67.49336242675781,  g_loss: -298.1092529296875\n",
            "Training epoch 6929/1000000, d_loss: -85.91748809814453,  g_loss: -164.15261840820312\n",
            "Training epoch 6930/1000000, d_loss: 87.45657348632812,  g_loss: -59.079978942871094\n",
            "Training epoch 6931/1000000, d_loss: 3.5113697052001953,  g_loss: -127.21368408203125\n",
            "Training epoch 6932/1000000, d_loss: -183.30474853515625,  g_loss: -19.233848571777344\n",
            "Training epoch 6933/1000000, d_loss: -485.3383483886719,  g_loss: -165.1353759765625\n",
            "Training epoch 6934/1000000, d_loss: -135.10614013671875,  g_loss: -206.76473999023438\n",
            "Training epoch 6935/1000000, d_loss: -181.91705322265625,  g_loss: -62.0852165222168\n",
            "Training epoch 6936/1000000, d_loss: -154.4552001953125,  g_loss: 2.038778305053711\n",
            "Training epoch 6937/1000000, d_loss: -175.78402709960938,  g_loss: -26.03855323791504\n",
            "Training epoch 6938/1000000, d_loss: -7.927845001220703,  g_loss: -81.1868896484375\n",
            "Training epoch 6939/1000000, d_loss: -85.22019958496094,  g_loss: 21.211978912353516\n",
            "Training epoch 6940/1000000, d_loss: -713.91162109375,  g_loss: -27.352304458618164\n",
            "Training epoch 6941/1000000, d_loss: -78.31903076171875,  g_loss: -150.70309448242188\n",
            "Training epoch 6942/1000000, d_loss: 6.895641326904297,  g_loss: 77.68263244628906\n",
            "Training epoch 6943/1000000, d_loss: -231.9485321044922,  g_loss: 139.4488525390625\n",
            "Training epoch 6944/1000000, d_loss: -301.0194396972656,  g_loss: 777.0468139648438\n",
            "Training epoch 6945/1000000, d_loss: 22.343711853027344,  g_loss: 58.31233596801758\n",
            "Training epoch 6946/1000000, d_loss: -94.57888793945312,  g_loss: -34.2865104675293\n",
            "Training epoch 6947/1000000, d_loss: -70.02876281738281,  g_loss: -131.87136840820312\n",
            "Training epoch 6948/1000000, d_loss: -44.28925704956055,  g_loss: -67.20280456542969\n",
            "Training epoch 6949/1000000, d_loss: -183.17404174804688,  g_loss: -131.346435546875\n",
            "Training epoch 6950/1000000, d_loss: -146.99391174316406,  g_loss: -35.882415771484375\n",
            "Training epoch 6951/1000000, d_loss: -118.3199462890625,  g_loss: -167.6944580078125\n",
            "Training epoch 6952/1000000, d_loss: -259.80914306640625,  g_loss: -89.34574127197266\n",
            "Training epoch 6953/1000000, d_loss: -291.8435974121094,  g_loss: -54.140647888183594\n",
            "Training epoch 6954/1000000, d_loss: 188.31809997558594,  g_loss: -50.20051574707031\n",
            "Training epoch 6955/1000000, d_loss: 502.25311279296875,  g_loss: -6.68934440612793\n",
            "Training epoch 6956/1000000, d_loss: 6.10906982421875,  g_loss: 32.829200744628906\n",
            "Training epoch 6957/1000000, d_loss: -86.09017181396484,  g_loss: -129.68478393554688\n",
            "Training epoch 6958/1000000, d_loss: -44.05533981323242,  g_loss: 30.42911148071289\n",
            "Training epoch 6959/1000000, d_loss: -68.77336120605469,  g_loss: -88.88926696777344\n",
            "Training epoch 6960/1000000, d_loss: -150.8748779296875,  g_loss: -86.59788513183594\n",
            "Training epoch 6961/1000000, d_loss: -191.70245361328125,  g_loss: -7.399234771728516\n",
            "Training epoch 6962/1000000, d_loss: -75.11024475097656,  g_loss: 104.87809753417969\n",
            "Training epoch 6963/1000000, d_loss: -19.151470184326172,  g_loss: 89.43745422363281\n",
            "Training epoch 6964/1000000, d_loss: -55.57402038574219,  g_loss: 124.79154205322266\n",
            "Training epoch 6965/1000000, d_loss: -150.7703399658203,  g_loss: 51.851470947265625\n",
            "Training epoch 6966/1000000, d_loss: -162.07318115234375,  g_loss: -34.103492736816406\n",
            "Training epoch 6967/1000000, d_loss: -23.874919891357422,  g_loss: 107.95169830322266\n",
            "Training epoch 6968/1000000, d_loss: 25.486400604248047,  g_loss: 75.19314575195312\n",
            "Training epoch 6969/1000000, d_loss: -44.10121154785156,  g_loss: 65.90180206298828\n",
            "Training epoch 6970/1000000, d_loss: -99.57017517089844,  g_loss: 1.9160795211791992\n",
            "Training epoch 6971/1000000, d_loss: -108.87257385253906,  g_loss: -10.42884635925293\n",
            "Training epoch 6972/1000000, d_loss: -107.2425537109375,  g_loss: -9.738506317138672\n",
            "Training epoch 6973/1000000, d_loss: -191.00393676757812,  g_loss: -53.559410095214844\n",
            "Training epoch 6974/1000000, d_loss: 46.82071304321289,  g_loss: 42.063575744628906\n",
            "Training epoch 6975/1000000, d_loss: -411.154052734375,  g_loss: -134.8712615966797\n",
            "Training epoch 6976/1000000, d_loss: 11.590568542480469,  g_loss: -8.955257415771484\n",
            "Training epoch 6977/1000000, d_loss: -86.07759094238281,  g_loss: 36.31517028808594\n",
            "Training epoch 6978/1000000, d_loss: -58.92292785644531,  g_loss: 8.92668342590332\n",
            "Training epoch 6979/1000000, d_loss: 94.67875671386719,  g_loss: -24.934186935424805\n",
            "Training epoch 6980/1000000, d_loss: -61.883338928222656,  g_loss: 135.919677734375\n",
            "Training epoch 6981/1000000, d_loss: -82.84391784667969,  g_loss: 195.2351531982422\n",
            "Training epoch 6982/1000000, d_loss: -143.61102294921875,  g_loss: 35.99675369262695\n",
            "Training epoch 6983/1000000, d_loss: -198.76577758789062,  g_loss: 33.621524810791016\n",
            "Training epoch 6984/1000000, d_loss: -488.2465515136719,  g_loss: -215.4242706298828\n",
            "Training epoch 6985/1000000, d_loss: -14.532608032226562,  g_loss: 10.254814147949219\n",
            "Training epoch 6986/1000000, d_loss: -103.45770263671875,  g_loss: 214.56666564941406\n",
            "Training epoch 6987/1000000, d_loss: -216.52220153808594,  g_loss: 248.56707763671875\n",
            "Training epoch 6988/1000000, d_loss: -278.3312072753906,  g_loss: 492.34136962890625\n",
            "Training epoch 6989/1000000, d_loss: -98.70610046386719,  g_loss: 122.42500305175781\n",
            "Training epoch 6990/1000000, d_loss: -360.81097412109375,  g_loss: 246.00552368164062\n",
            "Training epoch 6991/1000000, d_loss: -190.8218231201172,  g_loss: 57.20643615722656\n",
            "Training epoch 6992/1000000, d_loss: -181.25106811523438,  g_loss: 38.462547302246094\n",
            "Training epoch 6993/1000000, d_loss: -839.123779296875,  g_loss: -255.30381774902344\n",
            "Training epoch 6994/1000000, d_loss: -65.69338989257812,  g_loss: -86.1482925415039\n",
            "Training epoch 6995/1000000, d_loss: -63.64976501464844,  g_loss: -42.43495178222656\n",
            "Training epoch 6996/1000000, d_loss: -190.3570556640625,  g_loss: -41.29423141479492\n",
            "Training epoch 6997/1000000, d_loss: -94.67977905273438,  g_loss: -41.575927734375\n",
            "Training epoch 6998/1000000, d_loss: -142.06793212890625,  g_loss: -19.72055435180664\n",
            "Training epoch 6999/1000000, d_loss: -32.15742111206055,  g_loss: -35.370262145996094\n",
            "Training epoch 7000/1000000, d_loss: -25.100387573242188,  g_loss: -15.8870267868042\n",
            "Training epoch 7001/1000000, d_loss: -876.781982421875,  g_loss: -43.66995620727539\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 20/20 [00:00<00:00, 196.91it/s]\n",
            "Meshing: 100%|██████████| 7536/7536 [00:01<00:00, 5159.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_7001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_7001/assets\n",
            "Training epoch 7002/1000000, d_loss: 236.11264038085938,  g_loss: -27.31896209716797\n",
            "Training epoch 7003/1000000, d_loss: -104.55765533447266,  g_loss: 45.422569274902344\n",
            "Training epoch 7004/1000000, d_loss: -78.06538391113281,  g_loss: -105.829345703125\n",
            "Training epoch 7005/1000000, d_loss: -50.26995086669922,  g_loss: -7.256004333496094\n",
            "Training epoch 7006/1000000, d_loss: -98.78263854980469,  g_loss: -102.93551635742188\n",
            "Training epoch 7007/1000000, d_loss: -156.16004943847656,  g_loss: -2.0679898262023926\n",
            "Training epoch 7008/1000000, d_loss: -2.0642051696777344,  g_loss: -59.44964599609375\n",
            "Training epoch 7009/1000000, d_loss: 25.31494140625,  g_loss: 3.4076833724975586\n",
            "Training epoch 7010/1000000, d_loss: -114.9218978881836,  g_loss: 22.34984016418457\n",
            "Training epoch 7011/1000000, d_loss: -126.68657684326172,  g_loss: -24.94625473022461\n",
            "Training epoch 7012/1000000, d_loss: -37.02665710449219,  g_loss: -27.330289840698242\n",
            "Training epoch 7013/1000000, d_loss: -96.74581909179688,  g_loss: -0.46579551696777344\n",
            "Training epoch 7014/1000000, d_loss: -33.439430236816406,  g_loss: -42.63020706176758\n",
            "Training epoch 7015/1000000, d_loss: -48.99638366699219,  g_loss: -6.060647964477539\n",
            "Training epoch 7016/1000000, d_loss: -71.5239028930664,  g_loss: -15.881778717041016\n",
            "Training epoch 7017/1000000, d_loss: -361.04132080078125,  g_loss: -25.248558044433594\n",
            "Training epoch 7018/1000000, d_loss: 19.628644943237305,  g_loss: 124.220458984375\n",
            "Training epoch 7019/1000000, d_loss: 35.14219284057617,  g_loss: -28.395763397216797\n",
            "Training epoch 7020/1000000, d_loss: -93.73947143554688,  g_loss: -26.417551040649414\n",
            "Training epoch 7021/1000000, d_loss: -92.67615509033203,  g_loss: 1.9007282257080078\n",
            "Training epoch 7022/1000000, d_loss: -92.11543273925781,  g_loss: -17.12740135192871\n",
            "Training epoch 7023/1000000, d_loss: -44.36534118652344,  g_loss: -10.368982315063477\n",
            "Training epoch 7024/1000000, d_loss: -46.85641098022461,  g_loss: -24.060955047607422\n",
            "Training epoch 7025/1000000, d_loss: -11.766298294067383,  g_loss: -25.0295467376709\n",
            "Training epoch 7026/1000000, d_loss: -262.58477783203125,  g_loss: -39.35127639770508\n",
            "Training epoch 7027/1000000, d_loss: -87.07950592041016,  g_loss: -29.81517791748047\n",
            "Training epoch 7028/1000000, d_loss: -60.4415397644043,  g_loss: -12.480010986328125\n",
            "Training epoch 7029/1000000, d_loss: -166.7911376953125,  g_loss: 8.475549697875977\n",
            "Training epoch 7030/1000000, d_loss: 18.007137298583984,  g_loss: 67.45361328125\n",
            "Training epoch 7031/1000000, d_loss: 48.449729919433594,  g_loss: 52.72014617919922\n",
            "Training epoch 7032/1000000, d_loss: -140.74249267578125,  g_loss: 33.404090881347656\n",
            "Training epoch 7033/1000000, d_loss: -223.11549377441406,  g_loss: 9.72227668762207\n",
            "Training epoch 7034/1000000, d_loss: -34.48274230957031,  g_loss: 32.75816345214844\n",
            "Training epoch 7035/1000000, d_loss: -827.7743530273438,  g_loss: -134.891357421875\n",
            "Training epoch 7036/1000000, d_loss: -472.81939697265625,  g_loss: -78.17020416259766\n",
            "Training epoch 7037/1000000, d_loss: -8.124061584472656,  g_loss: -98.24848937988281\n",
            "Training epoch 7038/1000000, d_loss: -250.371826171875,  g_loss: -186.88885498046875\n",
            "Training epoch 7039/1000000, d_loss: -81.48973083496094,  g_loss: -259.4189147949219\n",
            "Training epoch 7040/1000000, d_loss: -66.72979736328125,  g_loss: 19.342426300048828\n",
            "Training epoch 7041/1000000, d_loss: -193.08387756347656,  g_loss: 184.94630432128906\n",
            "Training epoch 7042/1000000, d_loss: -525.416748046875,  g_loss: 557.2786865234375\n",
            "Training epoch 7043/1000000, d_loss: 42.98329162597656,  g_loss: 42.226844787597656\n",
            "Training epoch 7044/1000000, d_loss: -31.811267852783203,  g_loss: 49.760520935058594\n",
            "Training epoch 7045/1000000, d_loss: -202.2030487060547,  g_loss: 96.58992767333984\n",
            "Training epoch 7046/1000000, d_loss: -21.481224060058594,  g_loss: -24.346820831298828\n",
            "Training epoch 7047/1000000, d_loss: -129.08734130859375,  g_loss: 1.6846847534179688\n",
            "Training epoch 7048/1000000, d_loss: -61.399658203125,  g_loss: -17.681758880615234\n",
            "Training epoch 7049/1000000, d_loss: -131.73483276367188,  g_loss: -11.954964637756348\n",
            "Training epoch 7050/1000000, d_loss: -28.984249114990234,  g_loss: 41.147666931152344\n",
            "Training epoch 7051/1000000, d_loss: -138.93934631347656,  g_loss: -26.26761817932129\n",
            "Training epoch 7052/1000000, d_loss: -30.033470153808594,  g_loss: 63.693580627441406\n",
            "Training epoch 7053/1000000, d_loss: -84.16844177246094,  g_loss: 9.62166690826416\n",
            "Training epoch 7054/1000000, d_loss: -22.509803771972656,  g_loss: 48.073299407958984\n",
            "Training epoch 7055/1000000, d_loss: -63.22312927246094,  g_loss: 49.4012565612793\n",
            "Training epoch 7056/1000000, d_loss: -125.1363525390625,  g_loss: 45.227874755859375\n",
            "Training epoch 7057/1000000, d_loss: -191.24850463867188,  g_loss: 66.38591003417969\n",
            "Training epoch 7058/1000000, d_loss: -104.21992492675781,  g_loss: 7.395848274230957\n",
            "Training epoch 7059/1000000, d_loss: -112.67658996582031,  g_loss: 4.237017631530762\n",
            "Training epoch 7060/1000000, d_loss: -79.71300506591797,  g_loss: -74.85897827148438\n",
            "Training epoch 7061/1000000, d_loss: -112.48213958740234,  g_loss: 68.70323181152344\n",
            "Training epoch 7062/1000000, d_loss: -72.03766632080078,  g_loss: 138.71151733398438\n",
            "Training epoch 7063/1000000, d_loss: -255.95254516601562,  g_loss: -52.839332580566406\n",
            "Training epoch 7064/1000000, d_loss: -17.417064666748047,  g_loss: 90.82349395751953\n",
            "Training epoch 7065/1000000, d_loss: -101.44915771484375,  g_loss: 40.684814453125\n",
            "Training epoch 7066/1000000, d_loss: -70.52015686035156,  g_loss: 63.92889404296875\n",
            "Training epoch 7067/1000000, d_loss: 18.77356719970703,  g_loss: 100.1270751953125\n",
            "Training epoch 7068/1000000, d_loss: -59.92261505126953,  g_loss: 149.99896240234375\n",
            "Training epoch 7069/1000000, d_loss: -418.033447265625,  g_loss: 90.02650451660156\n",
            "Training epoch 7070/1000000, d_loss: -64.30078887939453,  g_loss: 113.10292053222656\n",
            "Training epoch 7071/1000000, d_loss: -121.65032196044922,  g_loss: 51.76292419433594\n",
            "Training epoch 7072/1000000, d_loss: -386.8384094238281,  g_loss: 3.399322986602783\n",
            "Training epoch 7073/1000000, d_loss: -57.29815673828125,  g_loss: 32.667720794677734\n",
            "Training epoch 7074/1000000, d_loss: -420.6015319824219,  g_loss: -45.022159576416016\n",
            "Training epoch 7075/1000000, d_loss: 25.674091339111328,  g_loss: -123.1330337524414\n",
            "Training epoch 7076/1000000, d_loss: -101.37014770507812,  g_loss: -15.219292640686035\n",
            "Training epoch 7077/1000000, d_loss: -112.77494812011719,  g_loss: 147.32864379882812\n",
            "Training epoch 7078/1000000, d_loss: -247.6960906982422,  g_loss: 217.89825439453125\n",
            "Training epoch 7079/1000000, d_loss: -115.12026977539062,  g_loss: -86.84443664550781\n",
            "Training epoch 7080/1000000, d_loss: -77.84974670410156,  g_loss: -101.06266021728516\n",
            "Training epoch 7081/1000000, d_loss: -239.6865692138672,  g_loss: -124.54496002197266\n",
            "Training epoch 7082/1000000, d_loss: 20.64916229248047,  g_loss: 40.22212600708008\n",
            "Training epoch 7083/1000000, d_loss: -34.445556640625,  g_loss: 77.5416488647461\n",
            "Training epoch 7084/1000000, d_loss: -201.3343505859375,  g_loss: -18.353591918945312\n",
            "Training epoch 7085/1000000, d_loss: -159.62841796875,  g_loss: -45.33592224121094\n",
            "Training epoch 7086/1000000, d_loss: -74.95384979248047,  g_loss: 11.123000144958496\n",
            "Training epoch 7087/1000000, d_loss: -40.6422004699707,  g_loss: 12.432515144348145\n",
            "Training epoch 7088/1000000, d_loss: -55.71574783325195,  g_loss: 28.861312866210938\n",
            "Training epoch 7089/1000000, d_loss: -57.21820068359375,  g_loss: -25.85364532470703\n",
            "Training epoch 7090/1000000, d_loss: 22.678512573242188,  g_loss: 16.375925064086914\n",
            "Training epoch 7091/1000000, d_loss: -26.996646881103516,  g_loss: -23.688426971435547\n",
            "Training epoch 7092/1000000, d_loss: -98.55816650390625,  g_loss: 1.4701347351074219\n",
            "Training epoch 7093/1000000, d_loss: 8.914485931396484,  g_loss: -44.095333099365234\n",
            "Training epoch 7094/1000000, d_loss: -46.453277587890625,  g_loss: 144.36865234375\n",
            "Training epoch 7095/1000000, d_loss: -79.86961364746094,  g_loss: 93.39248657226562\n",
            "Training epoch 7096/1000000, d_loss: -341.5818176269531,  g_loss: -123.0997085571289\n",
            "Training epoch 7097/1000000, d_loss: -6.4940185546875,  g_loss: -143.17904663085938\n",
            "Training epoch 7098/1000000, d_loss: -94.76708221435547,  g_loss: -46.2656364440918\n",
            "Training epoch 7099/1000000, d_loss: -186.6370391845703,  g_loss: -85.33721160888672\n",
            "Training epoch 7100/1000000, d_loss: 25.80416488647461,  g_loss: 14.692652702331543\n",
            "Training epoch 7101/1000000, d_loss: -51.01074981689453,  g_loss: -25.787065505981445\n",
            "Training epoch 7102/1000000, d_loss: -50.64921188354492,  g_loss: -43.1726188659668\n",
            "Training epoch 7103/1000000, d_loss: -251.07852172851562,  g_loss: -97.02853393554688\n",
            "Training epoch 7104/1000000, d_loss: -972.28857421875,  g_loss: -195.18487548828125\n",
            "Training epoch 7105/1000000, d_loss: -29.37633514404297,  g_loss: -43.8523063659668\n",
            "Training epoch 7106/1000000, d_loss: -150.90695190429688,  g_loss: -29.23260498046875\n",
            "Training epoch 7107/1000000, d_loss: -27.527830123901367,  g_loss: -79.939208984375\n",
            "Training epoch 7108/1000000, d_loss: -22.306203842163086,  g_loss: -65.49425506591797\n",
            "Training epoch 7109/1000000, d_loss: -18.678733825683594,  g_loss: 11.221173286437988\n",
            "Training epoch 7110/1000000, d_loss: -142.65904235839844,  g_loss: -18.18252182006836\n",
            "Training epoch 7111/1000000, d_loss: -233.85537719726562,  g_loss: -14.723268508911133\n",
            "Training epoch 7112/1000000, d_loss: -48.77580642700195,  g_loss: -110.49427795410156\n",
            "Training epoch 7113/1000000, d_loss: -126.58802032470703,  g_loss: -26.50090217590332\n",
            "Training epoch 7114/1000000, d_loss: -82.12762451171875,  g_loss: 8.30491828918457\n",
            "Training epoch 7115/1000000, d_loss: -144.64932250976562,  g_loss: -36.014190673828125\n",
            "Training epoch 7116/1000000, d_loss: -142.81552124023438,  g_loss: 132.10049438476562\n",
            "Training epoch 7117/1000000, d_loss: -20.46045684814453,  g_loss: -37.79564666748047\n",
            "Training epoch 7118/1000000, d_loss: -92.02867126464844,  g_loss: -25.86880874633789\n",
            "Training epoch 7119/1000000, d_loss: -51.201297760009766,  g_loss: 75.33999633789062\n",
            "Training epoch 7120/1000000, d_loss: -99.65655517578125,  g_loss: 50.232032775878906\n",
            "Training epoch 7121/1000000, d_loss: 0.5006866455078125,  g_loss: 7.129831314086914\n",
            "Training epoch 7122/1000000, d_loss: -152.26699829101562,  g_loss: -52.8815803527832\n",
            "Training epoch 7123/1000000, d_loss: -185.38156127929688,  g_loss: -156.78927612304688\n",
            "Training epoch 7124/1000000, d_loss: -414.93829345703125,  g_loss: -185.9962921142578\n",
            "Training epoch 7125/1000000, d_loss: -374.1282958984375,  g_loss: -262.18524169921875\n",
            "Training epoch 7126/1000000, d_loss: -10.66778564453125,  g_loss: -182.5142059326172\n",
            "Training epoch 7127/1000000, d_loss: -280.5459289550781,  g_loss: -103.83818054199219\n",
            "Training epoch 7128/1000000, d_loss: -217.07044982910156,  g_loss: 192.36669921875\n",
            "Training epoch 7129/1000000, d_loss: 22.052352905273438,  g_loss: 137.77301025390625\n",
            "Training epoch 7130/1000000, d_loss: -215.50001525878906,  g_loss: -46.64659881591797\n",
            "Training epoch 7131/1000000, d_loss: -174.93780517578125,  g_loss: 7.447553634643555\n",
            "Training epoch 7132/1000000, d_loss: -55.97206115722656,  g_loss: -77.76179504394531\n",
            "Training epoch 7133/1000000, d_loss: -22.098861694335938,  g_loss: -15.566307067871094\n",
            "Training epoch 7134/1000000, d_loss: -109.62779235839844,  g_loss: 88.70179748535156\n",
            "Training epoch 7135/1000000, d_loss: -265.2911682128906,  g_loss: 195.67910766601562\n",
            "Training epoch 7136/1000000, d_loss: -169.46189880371094,  g_loss: -154.5209197998047\n",
            "Training epoch 7137/1000000, d_loss: -148.86068725585938,  g_loss: -76.21470642089844\n",
            "Training epoch 7138/1000000, d_loss: -19.678180694580078,  g_loss: 103.27618408203125\n",
            "Training epoch 7139/1000000, d_loss: -57.47894287109375,  g_loss: -31.170063018798828\n",
            "Training epoch 7140/1000000, d_loss: -114.95777130126953,  g_loss: 23.305316925048828\n",
            "Training epoch 7141/1000000, d_loss: -660.89208984375,  g_loss: -14.437402725219727\n",
            "Training epoch 7142/1000000, d_loss: -136.79205322265625,  g_loss: -27.17394256591797\n",
            "Training epoch 7143/1000000, d_loss: 30.497188568115234,  g_loss: 32.9910888671875\n",
            "Training epoch 7144/1000000, d_loss: -105.53871154785156,  g_loss: -37.60905075073242\n",
            "Training epoch 7145/1000000, d_loss: -24.351364135742188,  g_loss: -31.33934783935547\n",
            "Training epoch 7146/1000000, d_loss: -63.727821350097656,  g_loss: 8.99224853515625\n",
            "Training epoch 7147/1000000, d_loss: -117.31534576416016,  g_loss: 35.135284423828125\n",
            "Training epoch 7148/1000000, d_loss: -106.73574829101562,  g_loss: 51.546226501464844\n",
            "Training epoch 7149/1000000, d_loss: -410.5562744140625,  g_loss: -101.62506103515625\n",
            "Training epoch 7150/1000000, d_loss: -59.4035758972168,  g_loss: 7.26149845123291\n",
            "Training epoch 7151/1000000, d_loss: -74.79466247558594,  g_loss: -97.7798080444336\n",
            "Training epoch 7152/1000000, d_loss: -116.97566223144531,  g_loss: 67.42535400390625\n",
            "Training epoch 7153/1000000, d_loss: -155.7630157470703,  g_loss: 74.0201416015625\n",
            "Training epoch 7154/1000000, d_loss: -12.620466232299805,  g_loss: 76.920654296875\n",
            "Training epoch 7155/1000000, d_loss: -21.033344268798828,  g_loss: 157.38775634765625\n",
            "Training epoch 7156/1000000, d_loss: -113.50086975097656,  g_loss: 148.61949157714844\n",
            "Training epoch 7157/1000000, d_loss: -342.6976013183594,  g_loss: 18.405485153198242\n",
            "Training epoch 7158/1000000, d_loss: -59.29324722290039,  g_loss: 52.37367248535156\n",
            "Training epoch 7159/1000000, d_loss: -162.2967529296875,  g_loss: 26.596088409423828\n",
            "Training epoch 7160/1000000, d_loss: -122.80601501464844,  g_loss: 35.30772018432617\n",
            "Training epoch 7161/1000000, d_loss: -41.67494583129883,  g_loss: 121.89277648925781\n",
            "Training epoch 7162/1000000, d_loss: -35.213077545166016,  g_loss: 94.31607818603516\n",
            "Training epoch 7163/1000000, d_loss: -90.1175537109375,  g_loss: 123.64131164550781\n",
            "Training epoch 7164/1000000, d_loss: -60.25115203857422,  g_loss: 92.61167907714844\n",
            "Training epoch 7165/1000000, d_loss: -35.2686882019043,  g_loss: 80.19635009765625\n",
            "Training epoch 7166/1000000, d_loss: -112.01219177246094,  g_loss: 215.260009765625\n",
            "Training epoch 7167/1000000, d_loss: -59.253501892089844,  g_loss: 66.24766540527344\n",
            "Training epoch 7168/1000000, d_loss: -126.91793823242188,  g_loss: 92.2437973022461\n",
            "Training epoch 7169/1000000, d_loss: -167.84616088867188,  g_loss: 72.88165283203125\n",
            "Training epoch 7170/1000000, d_loss: -100.91154479980469,  g_loss: 33.8320198059082\n",
            "Training epoch 7171/1000000, d_loss: -21.521575927734375,  g_loss: 122.2036361694336\n",
            "Training epoch 7172/1000000, d_loss: -7.017021179199219,  g_loss: 66.2511215209961\n",
            "Training epoch 7173/1000000, d_loss: -117.78399658203125,  g_loss: 19.81609344482422\n",
            "Training epoch 7174/1000000, d_loss: -43.70917510986328,  g_loss: 14.313724517822266\n",
            "Training epoch 7175/1000000, d_loss: -85.95249938964844,  g_loss: 45.77992248535156\n",
            "Training epoch 7176/1000000, d_loss: -220.9303436279297,  g_loss: -58.791481018066406\n",
            "Training epoch 7177/1000000, d_loss: -5.127496719360352,  g_loss: 42.280094146728516\n",
            "Training epoch 7178/1000000, d_loss: -416.0286865234375,  g_loss: -97.61286926269531\n",
            "Training epoch 7179/1000000, d_loss: -133.97012329101562,  g_loss: 45.65123748779297\n",
            "Training epoch 7180/1000000, d_loss: -211.48802185058594,  g_loss: 6.284461975097656\n",
            "Training epoch 7181/1000000, d_loss: -138.86007690429688,  g_loss: 24.939516067504883\n",
            "Training epoch 7182/1000000, d_loss: -109.0311279296875,  g_loss: -46.306373596191406\n",
            "Training epoch 7183/1000000, d_loss: -400.3917236328125,  g_loss: -141.1258087158203\n",
            "Training epoch 7184/1000000, d_loss: 17.43596649169922,  g_loss: 72.8936767578125\n",
            "Training epoch 7185/1000000, d_loss: -131.7564239501953,  g_loss: 38.67486572265625\n",
            "Training epoch 7186/1000000, d_loss: -28.017196655273438,  g_loss: -11.557388305664062\n",
            "Training epoch 7187/1000000, d_loss: -479.14544677734375,  g_loss: 49.75178909301758\n",
            "Training epoch 7188/1000000, d_loss: -47.645999908447266,  g_loss: 23.587778091430664\n",
            "Training epoch 7189/1000000, d_loss: -99.56476593017578,  g_loss: -33.827579498291016\n",
            "Training epoch 7190/1000000, d_loss: -1145.0830078125,  g_loss: -267.3979797363281\n",
            "Training epoch 7191/1000000, d_loss: -176.29568481445312,  g_loss: -168.89552307128906\n",
            "Training epoch 7192/1000000, d_loss: -206.80715942382812,  g_loss: -190.0896453857422\n",
            "Training epoch 7193/1000000, d_loss: 382.6021728515625,  g_loss: -119.86969757080078\n",
            "Training epoch 7194/1000000, d_loss: -103.6496810913086,  g_loss: -115.18644714355469\n",
            "Training epoch 7195/1000000, d_loss: 16.622028350830078,  g_loss: -174.97091674804688\n",
            "Training epoch 7196/1000000, d_loss: 44.9614372253418,  g_loss: -120.81123352050781\n",
            "Training epoch 7197/1000000, d_loss: -64.51799011230469,  g_loss: -21.4459228515625\n",
            "Training epoch 7198/1000000, d_loss: -71.85051727294922,  g_loss: 2.0073013305664062\n",
            "Training epoch 7199/1000000, d_loss: 12.346755981445312,  g_loss: -35.49906539916992\n",
            "Training epoch 7200/1000000, d_loss: -33.83828353881836,  g_loss: -91.3974838256836\n",
            "Training epoch 7201/1000000, d_loss: -433.7239990234375,  g_loss: -196.47349548339844\n",
            "Training epoch 7202/1000000, d_loss: -65.58809661865234,  g_loss: -97.94720458984375\n",
            "Training epoch 7203/1000000, d_loss: -137.9781494140625,  g_loss: -310.42523193359375\n",
            "Training epoch 7204/1000000, d_loss: 235.3790283203125,  g_loss: -38.033504486083984\n",
            "Training epoch 7205/1000000, d_loss: -162.47482299804688,  g_loss: 86.86567687988281\n",
            "Training epoch 7206/1000000, d_loss: -337.53857421875,  g_loss: 457.66448974609375\n",
            "Training epoch 7207/1000000, d_loss: -305.73394775390625,  g_loss: 207.9950408935547\n",
            "Training epoch 7208/1000000, d_loss: -8.492485046386719,  g_loss: 68.93701934814453\n",
            "Training epoch 7209/1000000, d_loss: -98.22903442382812,  g_loss: 115.28376770019531\n",
            "Training epoch 7210/1000000, d_loss: -294.2352294921875,  g_loss: -99.33501434326172\n",
            "Training epoch 7211/1000000, d_loss: -61.80339050292969,  g_loss: 83.76897430419922\n",
            "Training epoch 7212/1000000, d_loss: -250.02737426757812,  g_loss: -74.1739501953125\n",
            "Training epoch 7213/1000000, d_loss: 28.28974151611328,  g_loss: 34.12323760986328\n",
            "Training epoch 7214/1000000, d_loss: -25.185028076171875,  g_loss: 70.50173950195312\n",
            "Training epoch 7215/1000000, d_loss: -271.8468017578125,  g_loss: 393.82635498046875\n",
            "Training epoch 7216/1000000, d_loss: -59.604305267333984,  g_loss: 57.170658111572266\n",
            "Training epoch 7217/1000000, d_loss: -1015.866943359375,  g_loss: -41.61115264892578\n",
            "Training epoch 7218/1000000, d_loss: -22.891427993774414,  g_loss: -4.484235763549805\n",
            "Training epoch 7219/1000000, d_loss: -82.05879211425781,  g_loss: 44.386878967285156\n",
            "Training epoch 7220/1000000, d_loss: -98.38621520996094,  g_loss: 70.23896789550781\n",
            "Training epoch 7221/1000000, d_loss: -27.649694442749023,  g_loss: 71.05157470703125\n",
            "Training epoch 7222/1000000, d_loss: -68.44622039794922,  g_loss: 51.56095886230469\n",
            "Training epoch 7223/1000000, d_loss: -528.0296020507812,  g_loss: -114.57858276367188\n",
            "Training epoch 7224/1000000, d_loss: 32.842559814453125,  g_loss: 79.65853881835938\n",
            "Training epoch 7225/1000000, d_loss: 1.0338821411132812,  g_loss: 95.53562927246094\n",
            "Training epoch 7226/1000000, d_loss: 1.7151107788085938,  g_loss: 58.47509765625\n",
            "Training epoch 7227/1000000, d_loss: -72.66446685791016,  g_loss: 59.96230697631836\n",
            "Training epoch 7228/1000000, d_loss: -506.1600646972656,  g_loss: -129.9148712158203\n",
            "Training epoch 7229/1000000, d_loss: -49.22983932495117,  g_loss: 90.33155822753906\n",
            "Training epoch 7230/1000000, d_loss: -112.92576599121094,  g_loss: 32.7176513671875\n",
            "Training epoch 7231/1000000, d_loss: 46.95164489746094,  g_loss: 44.769615173339844\n",
            "Training epoch 7232/1000000, d_loss: -225.81060791015625,  g_loss: 158.07090759277344\n",
            "Training epoch 7233/1000000, d_loss: -19.271724700927734,  g_loss: 11.1790771484375\n",
            "Training epoch 7234/1000000, d_loss: 2.8287878036499023,  g_loss: 24.726430892944336\n",
            "Training epoch 7235/1000000, d_loss: -158.88931274414062,  g_loss: -96.03111267089844\n",
            "Training epoch 7236/1000000, d_loss: -81.43167877197266,  g_loss: -21.950857162475586\n",
            "Training epoch 7237/1000000, d_loss: -463.1740417480469,  g_loss: -47.47245407104492\n",
            "Training epoch 7238/1000000, d_loss: -19.71448516845703,  g_loss: 125.91216278076172\n",
            "Training epoch 7239/1000000, d_loss: -156.92808532714844,  g_loss: 227.91497802734375\n",
            "Training epoch 7240/1000000, d_loss: -665.218994140625,  g_loss: -32.94102478027344\n",
            "Training epoch 7241/1000000, d_loss: -3.4893455505371094,  g_loss: -23.442615509033203\n",
            "Training epoch 7242/1000000, d_loss: -141.7024688720703,  g_loss: 51.85881805419922\n",
            "Training epoch 7243/1000000, d_loss: -72.40235137939453,  g_loss: 43.348655700683594\n",
            "Training epoch 7244/1000000, d_loss: 50.55340576171875,  g_loss: 81.98977661132812\n",
            "Training epoch 7245/1000000, d_loss: -220.16940307617188,  g_loss: 519.530029296875\n",
            "Training epoch 7246/1000000, d_loss: -99.29021453857422,  g_loss: 113.16641235351562\n",
            "Training epoch 7247/1000000, d_loss: 1.7394485473632812,  g_loss: 93.70589447021484\n",
            "Training epoch 7248/1000000, d_loss: -88.06641387939453,  g_loss: 30.27223014831543\n",
            "Training epoch 7249/1000000, d_loss: -50.29288101196289,  g_loss: 119.31159973144531\n",
            "Training epoch 7250/1000000, d_loss: -23.268665313720703,  g_loss: 133.97097778320312\n",
            "Training epoch 7251/1000000, d_loss: -121.62249755859375,  g_loss: 103.47916412353516\n",
            "Training epoch 7252/1000000, d_loss: -40.15262222290039,  g_loss: 100.24041748046875\n",
            "Training epoch 7253/1000000, d_loss: -37.95724105834961,  g_loss: 264.73956298828125\n",
            "Training epoch 7254/1000000, d_loss: -168.90365600585938,  g_loss: 13.247880935668945\n",
            "Training epoch 7255/1000000, d_loss: -4.9456634521484375,  g_loss: 37.829437255859375\n",
            "Training epoch 7256/1000000, d_loss: -60.88090515136719,  g_loss: 22.30952262878418\n",
            "Training epoch 7257/1000000, d_loss: -58.85891342163086,  g_loss: 23.656417846679688\n",
            "Training epoch 7258/1000000, d_loss: -117.06905364990234,  g_loss: 57.03437805175781\n",
            "Training epoch 7259/1000000, d_loss: -863.518798828125,  g_loss: -130.94857788085938\n",
            "Training epoch 7260/1000000, d_loss: -46.53619384765625,  g_loss: -52.98273849487305\n",
            "Training epoch 7261/1000000, d_loss: -364.82073974609375,  g_loss: -21.880367279052734\n",
            "Training epoch 7262/1000000, d_loss: -483.55426025390625,  g_loss: -48.39878845214844\n",
            "Training epoch 7263/1000000, d_loss: 78.6523666381836,  g_loss: -11.902819633483887\n",
            "Training epoch 7264/1000000, d_loss: -67.60943603515625,  g_loss: -15.22059154510498\n",
            "Training epoch 7265/1000000, d_loss: -1282.8802490234375,  g_loss: -140.37307739257812\n",
            "Training epoch 7266/1000000, d_loss: 170.98655700683594,  g_loss: -110.55043029785156\n",
            "Training epoch 7267/1000000, d_loss: -264.8763122558594,  g_loss: 716.999755859375\n",
            "Training epoch 7268/1000000, d_loss: -158.69842529296875,  g_loss: 151.84457397460938\n",
            "Training epoch 7269/1000000, d_loss: -103.81132507324219,  g_loss: 266.4260559082031\n",
            "Training epoch 7270/1000000, d_loss: -137.6784210205078,  g_loss: 127.88499450683594\n",
            "Training epoch 7271/1000000, d_loss: -81.91869354248047,  g_loss: 39.70164489746094\n",
            "Training epoch 7272/1000000, d_loss: -213.30307006835938,  g_loss: -19.320201873779297\n",
            "Training epoch 7273/1000000, d_loss: -57.217445373535156,  g_loss: -11.52869987487793\n",
            "Training epoch 7274/1000000, d_loss: -113.55867004394531,  g_loss: -25.366134643554688\n",
            "Training epoch 7275/1000000, d_loss: -182.70254516601562,  g_loss: -70.24928283691406\n",
            "Training epoch 7276/1000000, d_loss: -442.28192138671875,  g_loss: -91.7311019897461\n",
            "Training epoch 7277/1000000, d_loss: -140.94931030273438,  g_loss: 194.40182495117188\n",
            "Training epoch 7278/1000000, d_loss: -119.53555297851562,  g_loss: 4.731708526611328\n",
            "Training epoch 7279/1000000, d_loss: -123.66106414794922,  g_loss: -52.62466812133789\n",
            "Training epoch 7280/1000000, d_loss: -144.53192138671875,  g_loss: -30.331270217895508\n",
            "Training epoch 7281/1000000, d_loss: -571.2886962890625,  g_loss: -247.3829345703125\n",
            "Training epoch 7282/1000000, d_loss: -76.9735107421875,  g_loss: 60.321746826171875\n",
            "Training epoch 7283/1000000, d_loss: -1477.100830078125,  g_loss: -103.62671661376953\n",
            "Training epoch 7284/1000000, d_loss: 93.3392333984375,  g_loss: 39.113136291503906\n",
            "Training epoch 7285/1000000, d_loss: -61.868019104003906,  g_loss: -345.6979675292969\n",
            "Training epoch 7286/1000000, d_loss: 43.427005767822266,  g_loss: 7.833127975463867\n",
            "Training epoch 7287/1000000, d_loss: -19.74728012084961,  g_loss: 105.63485717773438\n",
            "Training epoch 7288/1000000, d_loss: -106.70577239990234,  g_loss: 143.1317138671875\n",
            "Training epoch 7289/1000000, d_loss: -146.6368865966797,  g_loss: 41.31641387939453\n",
            "Training epoch 7290/1000000, d_loss: -18.302696228027344,  g_loss: -31.522079467773438\n",
            "Training epoch 7291/1000000, d_loss: -494.521240234375,  g_loss: -70.76634979248047\n",
            "Training epoch 7292/1000000, d_loss: -62.967552185058594,  g_loss: 59.031532287597656\n",
            "Training epoch 7293/1000000, d_loss: -191.46958923339844,  g_loss: 189.770751953125\n",
            "Training epoch 7294/1000000, d_loss: -92.64045715332031,  g_loss: 221.91307067871094\n",
            "Training epoch 7295/1000000, d_loss: -5.528594970703125,  g_loss: 71.17764282226562\n",
            "Training epoch 7296/1000000, d_loss: -151.5811767578125,  g_loss: 92.487548828125\n",
            "Training epoch 7297/1000000, d_loss: -125.2887954711914,  g_loss: 195.9766082763672\n",
            "Training epoch 7298/1000000, d_loss: -61.85625457763672,  g_loss: 118.52667999267578\n",
            "Training epoch 7299/1000000, d_loss: -14.051712036132812,  g_loss: 87.11787414550781\n",
            "Training epoch 7300/1000000, d_loss: -80.88526916503906,  g_loss: 67.60490417480469\n",
            "Training epoch 7301/1000000, d_loss: -565.22265625,  g_loss: -51.24138641357422\n",
            "Training epoch 7302/1000000, d_loss: -119.06259155273438,  g_loss: 55.77070617675781\n",
            "Training epoch 7303/1000000, d_loss: -603.9786376953125,  g_loss: -51.09428405761719\n",
            "Training epoch 7304/1000000, d_loss: -174.67813110351562,  g_loss: -121.1490249633789\n",
            "Training epoch 7305/1000000, d_loss: -566.38232421875,  g_loss: -78.34239196777344\n",
            "Training epoch 7306/1000000, d_loss: -183.7116241455078,  g_loss: 16.127506256103516\n",
            "Training epoch 7307/1000000, d_loss: -56.487823486328125,  g_loss: 306.86041259765625\n",
            "Training epoch 7308/1000000, d_loss: -152.95680236816406,  g_loss: 257.97772216796875\n",
            "Training epoch 7309/1000000, d_loss: -36.796791076660156,  g_loss: -102.14698791503906\n",
            "Training epoch 7310/1000000, d_loss: -71.75868225097656,  g_loss: -71.7298812866211\n",
            "Training epoch 7311/1000000, d_loss: -121.67750549316406,  g_loss: -17.651958465576172\n",
            "Training epoch 7312/1000000, d_loss: -5.802764892578125,  g_loss: 6.350025177001953\n",
            "Training epoch 7313/1000000, d_loss: -237.3715362548828,  g_loss: -1.3801231384277344\n",
            "Training epoch 7314/1000000, d_loss: -145.31056213378906,  g_loss: 25.84709930419922\n",
            "Training epoch 7315/1000000, d_loss: -368.4208984375,  g_loss: -101.9325942993164\n",
            "Training epoch 7316/1000000, d_loss: 92.3060302734375,  g_loss: 3.7358551025390625\n",
            "Training epoch 7317/1000000, d_loss: -504.861083984375,  g_loss: -92.8859634399414\n",
            "Training epoch 7318/1000000, d_loss: -63.6383056640625,  g_loss: 233.30274963378906\n",
            "Training epoch 7319/1000000, d_loss: -143.8048095703125,  g_loss: 243.81210327148438\n",
            "Training epoch 7320/1000000, d_loss: -142.8286895751953,  g_loss: 154.00770568847656\n",
            "Training epoch 7321/1000000, d_loss: -179.23336791992188,  g_loss: 142.33079528808594\n",
            "Training epoch 7322/1000000, d_loss: -212.9005126953125,  g_loss: 467.8842468261719\n",
            "Training epoch 7323/1000000, d_loss: -130.2958221435547,  g_loss: 26.803077697753906\n",
            "Training epoch 7324/1000000, d_loss: 28.579570770263672,  g_loss: 64.83684539794922\n",
            "Training epoch 7325/1000000, d_loss: -48.88323974609375,  g_loss: 65.37953186035156\n",
            "Training epoch 7326/1000000, d_loss: -449.32452392578125,  g_loss: -58.15029525756836\n",
            "Training epoch 7327/1000000, d_loss: -107.20784759521484,  g_loss: 62.88945770263672\n",
            "Training epoch 7328/1000000, d_loss: -300.3682861328125,  g_loss: -57.702781677246094\n",
            "Training epoch 7329/1000000, d_loss: -536.7578125,  g_loss: -556.3817138671875\n",
            "Training epoch 7330/1000000, d_loss: -784.1474609375,  g_loss: -292.40020751953125\n",
            "Training epoch 7331/1000000, d_loss: -76.56710815429688,  g_loss: -111.96852111816406\n",
            "Training epoch 7332/1000000, d_loss: 64.03716278076172,  g_loss: -98.62876892089844\n",
            "Training epoch 7333/1000000, d_loss: -112.82694244384766,  g_loss: 69.53266906738281\n",
            "Training epoch 7334/1000000, d_loss: -160.03785705566406,  g_loss: 135.21218872070312\n",
            "Training epoch 7335/1000000, d_loss: -277.44256591796875,  g_loss: -19.70870590209961\n",
            "Training epoch 7336/1000000, d_loss: -154.67892456054688,  g_loss: -87.88521575927734\n",
            "Training epoch 7337/1000000, d_loss: -89.85729217529297,  g_loss: -38.43132019042969\n",
            "Training epoch 7338/1000000, d_loss: -196.15155029296875,  g_loss: 335.03228759765625\n",
            "Training epoch 7339/1000000, d_loss: -39.30105209350586,  g_loss: 32.70751953125\n",
            "Training epoch 7340/1000000, d_loss: 39.11988067626953,  g_loss: -44.19477844238281\n",
            "Training epoch 7341/1000000, d_loss: -42.67008972167969,  g_loss: 20.579875946044922\n",
            "Training epoch 7342/1000000, d_loss: -55.76832962036133,  g_loss: -5.840035438537598\n",
            "Training epoch 7343/1000000, d_loss: -122.37276458740234,  g_loss: 7.84423828125\n",
            "Training epoch 7344/1000000, d_loss: -114.72721862792969,  g_loss: -49.13589859008789\n",
            "Training epoch 7345/1000000, d_loss: -105.5418472290039,  g_loss: -46.30461502075195\n",
            "Training epoch 7346/1000000, d_loss: -145.779052734375,  g_loss: 40.948734283447266\n",
            "Training epoch 7347/1000000, d_loss: -101.63117980957031,  g_loss: 146.1563720703125\n",
            "Training epoch 7348/1000000, d_loss: -27.439855575561523,  g_loss: 23.707033157348633\n",
            "Training epoch 7349/1000000, d_loss: -42.389183044433594,  g_loss: 118.46635437011719\n",
            "Training epoch 7350/1000000, d_loss: -20.799049377441406,  g_loss: 14.869158744812012\n",
            "Training epoch 7351/1000000, d_loss: -105.61222839355469,  g_loss: 44.27841567993164\n",
            "Training epoch 7352/1000000, d_loss: -657.893310546875,  g_loss: -163.74058532714844\n",
            "Training epoch 7353/1000000, d_loss: 28.15738868713379,  g_loss: -10.377181053161621\n",
            "Training epoch 7354/1000000, d_loss: -237.4332275390625,  g_loss: -5.181755065917969\n",
            "Training epoch 7355/1000000, d_loss: -175.00230407714844,  g_loss: 8.068501472473145\n",
            "Training epoch 7356/1000000, d_loss: -71.96826934814453,  g_loss: -21.764022827148438\n",
            "Training epoch 7357/1000000, d_loss: -55.00447082519531,  g_loss: -14.413704872131348\n",
            "Training epoch 7358/1000000, d_loss: -134.24319458007812,  g_loss: -48.381568908691406\n",
            "Training epoch 7359/1000000, d_loss: -20.09657859802246,  g_loss: -29.748319625854492\n",
            "Training epoch 7360/1000000, d_loss: -118.64258575439453,  g_loss: -58.0980224609375\n",
            "Training epoch 7361/1000000, d_loss: -544.09375,  g_loss: -135.12071228027344\n",
            "Training epoch 7362/1000000, d_loss: -58.271728515625,  g_loss: -86.20897674560547\n",
            "Training epoch 7363/1000000, d_loss: -54.73168182373047,  g_loss: -58.05852508544922\n",
            "Training epoch 7364/1000000, d_loss: -818.6029052734375,  g_loss: -232.82884216308594\n",
            "Training epoch 7365/1000000, d_loss: -359.9664611816406,  g_loss: -268.9091796875\n",
            "Training epoch 7366/1000000, d_loss: -368.4799499511719,  g_loss: -441.5568542480469\n",
            "Training epoch 7367/1000000, d_loss: -93.8271255493164,  g_loss: 315.39727783203125\n",
            "Training epoch 7368/1000000, d_loss: -226.1924285888672,  g_loss: 111.41120147705078\n",
            "Training epoch 7369/1000000, d_loss: -212.47283935546875,  g_loss: 428.31585693359375\n",
            "Training epoch 7370/1000000, d_loss: -579.9599609375,  g_loss: 972.6453247070312\n",
            "Training epoch 7371/1000000, d_loss: -59.201011657714844,  g_loss: -32.780616760253906\n",
            "Training epoch 7372/1000000, d_loss: -76.84642791748047,  g_loss: -44.24725341796875\n",
            "Training epoch 7373/1000000, d_loss: -317.0528564453125,  g_loss: -88.7078857421875\n",
            "Training epoch 7374/1000000, d_loss: -1432.56396484375,  g_loss: -268.8027038574219\n",
            "Training epoch 7375/1000000, d_loss: -383.67572021484375,  g_loss: -16.11511993408203\n",
            "Training epoch 7376/1000000, d_loss: 196.74697875976562,  g_loss: 45.90409851074219\n",
            "Training epoch 7377/1000000, d_loss: 141.54855346679688,  g_loss: -4.742122173309326\n",
            "Training epoch 7378/1000000, d_loss: 39.64997863769531,  g_loss: 121.51396179199219\n",
            "Training epoch 7379/1000000, d_loss: -153.08966064453125,  g_loss: 98.17295837402344\n",
            "Training epoch 7380/1000000, d_loss: -236.21026611328125,  g_loss: 122.07952880859375\n",
            "Training epoch 7381/1000000, d_loss: -137.8929901123047,  g_loss: 100.01499938964844\n",
            "Training epoch 7382/1000000, d_loss: -76.50647735595703,  g_loss: 47.04960632324219\n",
            "Training epoch 7383/1000000, d_loss: -11.201301574707031,  g_loss: 86.41691589355469\n",
            "Training epoch 7384/1000000, d_loss: -458.8838195800781,  g_loss: 69.98980712890625\n",
            "Training epoch 7385/1000000, d_loss: -171.1912841796875,  g_loss: 19.99502944946289\n",
            "Training epoch 7386/1000000, d_loss: -143.33135986328125,  g_loss: 64.38226318359375\n",
            "Training epoch 7387/1000000, d_loss: -197.48086547851562,  g_loss: 270.1605224609375\n",
            "Training epoch 7388/1000000, d_loss: -52.25438690185547,  g_loss: 144.60389709472656\n",
            "Training epoch 7389/1000000, d_loss: -239.8040008544922,  g_loss: 88.40605163574219\n",
            "Training epoch 7390/1000000, d_loss: -366.2323913574219,  g_loss: 30.54627799987793\n",
            "Training epoch 7391/1000000, d_loss: 28.07650375366211,  g_loss: 113.48103332519531\n",
            "Training epoch 7392/1000000, d_loss: -74.95430755615234,  g_loss: 85.34664154052734\n",
            "Training epoch 7393/1000000, d_loss: -530.0199584960938,  g_loss: 1297.04833984375\n",
            "Training epoch 7394/1000000, d_loss: -44.79926300048828,  g_loss: -56.94129180908203\n",
            "Training epoch 7395/1000000, d_loss: 56.101078033447266,  g_loss: 49.87782287597656\n",
            "Training epoch 7396/1000000, d_loss: -679.2903442382812,  g_loss: -49.98495864868164\n",
            "Training epoch 7397/1000000, d_loss: -33.110877990722656,  g_loss: 7.9164252281188965\n",
            "Training epoch 7398/1000000, d_loss: -1591.213134765625,  g_loss: -934.8936767578125\n",
            "Training epoch 7399/1000000, d_loss: -50.15662384033203,  g_loss: -55.465980529785156\n",
            "Training epoch 7400/1000000, d_loss: 99.9455337524414,  g_loss: -235.1383056640625\n",
            "Training epoch 7401/1000000, d_loss: -44.523681640625,  g_loss: 108.06251525878906\n",
            "Training epoch 7402/1000000, d_loss: 29.698673248291016,  g_loss: -41.334373474121094\n",
            "Training epoch 7403/1000000, d_loss: -78.18293762207031,  g_loss: -13.448657989501953\n",
            "Training epoch 7404/1000000, d_loss: 139.59494018554688,  g_loss: 89.66553497314453\n",
            "Training epoch 7405/1000000, d_loss: -127.94293975830078,  g_loss: 213.7152099609375\n",
            "Training epoch 7406/1000000, d_loss: -53.01396942138672,  g_loss: 89.15756225585938\n",
            "Training epoch 7407/1000000, d_loss: -80.7474365234375,  g_loss: 15.31365966796875\n",
            "Training epoch 7408/1000000, d_loss: -57.31037139892578,  g_loss: 12.199399948120117\n",
            "Training epoch 7409/1000000, d_loss: -503.0423889160156,  g_loss: -12.49197006225586\n",
            "Training epoch 7410/1000000, d_loss: 499.6956481933594,  g_loss: 43.29490661621094\n",
            "Training epoch 7411/1000000, d_loss: 226.5177001953125,  g_loss: 12.884613037109375\n",
            "Training epoch 7412/1000000, d_loss: -48.45541763305664,  g_loss: -36.37251281738281\n",
            "Training epoch 7413/1000000, d_loss: -88.88780975341797,  g_loss: -78.20425415039062\n",
            "Training epoch 7414/1000000, d_loss: -76.18675994873047,  g_loss: -63.11175537109375\n",
            "Training epoch 7415/1000000, d_loss: -101.6080093383789,  g_loss: -49.43229293823242\n",
            "Training epoch 7416/1000000, d_loss: -157.60787963867188,  g_loss: 251.89491271972656\n",
            "Training epoch 7417/1000000, d_loss: -127.41120147705078,  g_loss: 32.48124694824219\n",
            "Training epoch 7418/1000000, d_loss: -155.11184692382812,  g_loss: 68.4087905883789\n",
            "Training epoch 7419/1000000, d_loss: -211.96475219726562,  g_loss: -191.98324584960938\n",
            "Training epoch 7420/1000000, d_loss: -75.91934967041016,  g_loss: -19.846588134765625\n",
            "Training epoch 7421/1000000, d_loss: -223.48216247558594,  g_loss: 104.04930114746094\n",
            "Training epoch 7422/1000000, d_loss: -168.61407470703125,  g_loss: -11.364039421081543\n",
            "Training epoch 7423/1000000, d_loss: -72.50849151611328,  g_loss: -33.28985595703125\n",
            "Training epoch 7424/1000000, d_loss: -75.46951293945312,  g_loss: 139.80450439453125\n",
            "Training epoch 7425/1000000, d_loss: -117.44091796875,  g_loss: 242.79840087890625\n",
            "Training epoch 7426/1000000, d_loss: -256.61505126953125,  g_loss: -66.21591186523438\n",
            "Training epoch 7427/1000000, d_loss: -209.1360626220703,  g_loss: -118.00523376464844\n",
            "Training epoch 7428/1000000, d_loss: -112.59521484375,  g_loss: -23.322589874267578\n",
            "Training epoch 7429/1000000, d_loss: -118.70896911621094,  g_loss: 5.023585319519043\n",
            "Training epoch 7430/1000000, d_loss: -1352.9012451171875,  g_loss: -223.87266540527344\n",
            "Training epoch 7431/1000000, d_loss: -85.39265441894531,  g_loss: -37.3795280456543\n",
            "Training epoch 7432/1000000, d_loss: -86.07654571533203,  g_loss: -51.777835845947266\n",
            "Training epoch 7433/1000000, d_loss: -93.19168853759766,  g_loss: -93.07852172851562\n",
            "Training epoch 7434/1000000, d_loss: -135.62619018554688,  g_loss: -92.2485122680664\n",
            "Training epoch 7435/1000000, d_loss: -115.08888244628906,  g_loss: 67.10467529296875\n",
            "Training epoch 7436/1000000, d_loss: -169.6622772216797,  g_loss: 63.9928092956543\n",
            "Training epoch 7437/1000000, d_loss: -31.107921600341797,  g_loss: -55.883583068847656\n",
            "Training epoch 7438/1000000, d_loss: -81.62285614013672,  g_loss: -110.37542724609375\n",
            "Training epoch 7439/1000000, d_loss: -15.432628631591797,  g_loss: -57.97228240966797\n",
            "Training epoch 7440/1000000, d_loss: -311.91180419921875,  g_loss: -62.63215255737305\n",
            "Training epoch 7441/1000000, d_loss: -62.08380126953125,  g_loss: -83.62774658203125\n",
            "Training epoch 7442/1000000, d_loss: -70.288818359375,  g_loss: -58.89653015136719\n",
            "Training epoch 7443/1000000, d_loss: -163.6629180908203,  g_loss: -29.74484634399414\n",
            "Training epoch 7444/1000000, d_loss: 6.727991104125977,  g_loss: -5.166790962219238\n",
            "Training epoch 7445/1000000, d_loss: -13.141555786132812,  g_loss: 35.804752349853516\n",
            "Training epoch 7446/1000000, d_loss: -208.8408203125,  g_loss: -34.23303985595703\n",
            "Training epoch 7447/1000000, d_loss: -62.77534103393555,  g_loss: -119.74349975585938\n",
            "Training epoch 7448/1000000, d_loss: -29.458297729492188,  g_loss: 61.89203643798828\n",
            "Training epoch 7449/1000000, d_loss: -780.9960327148438,  g_loss: 2.1483287811279297\n",
            "Training epoch 7450/1000000, d_loss: 145.53176879882812,  g_loss: 124.36054992675781\n",
            "Training epoch 7451/1000000, d_loss: -225.63294982910156,  g_loss: 184.94081115722656\n",
            "Training epoch 7452/1000000, d_loss: 31.876907348632812,  g_loss: 0.30904603004455566\n",
            "Training epoch 7453/1000000, d_loss: -49.27901840209961,  g_loss: 73.75541687011719\n",
            "Training epoch 7454/1000000, d_loss: 10.28506088256836,  g_loss: 38.870018005371094\n",
            "Training epoch 7455/1000000, d_loss: -125.48880767822266,  g_loss: 14.76468276977539\n",
            "Training epoch 7456/1000000, d_loss: -109.8387680053711,  g_loss: 58.703372955322266\n",
            "Training epoch 7457/1000000, d_loss: -90.3663558959961,  g_loss: 10.607490539550781\n",
            "Training epoch 7458/1000000, d_loss: -163.6323699951172,  g_loss: -53.896446228027344\n",
            "Training epoch 7459/1000000, d_loss: -116.42484283447266,  g_loss: 59.66124725341797\n",
            "Training epoch 7460/1000000, d_loss: -135.0823974609375,  g_loss: -21.267681121826172\n",
            "Training epoch 7461/1000000, d_loss: -854.787109375,  g_loss: -43.67091369628906\n",
            "Training epoch 7462/1000000, d_loss: -33.367454528808594,  g_loss: -88.526123046875\n",
            "Training epoch 7463/1000000, d_loss: -256.6268005371094,  g_loss: -102.39502716064453\n",
            "Training epoch 7464/1000000, d_loss: -534.764404296875,  g_loss: -67.42558288574219\n",
            "Training epoch 7465/1000000, d_loss: -160.85269165039062,  g_loss: -14.388408660888672\n",
            "Training epoch 7466/1000000, d_loss: -294.0128173828125,  g_loss: -69.38943481445312\n",
            "Training epoch 7467/1000000, d_loss: 111.21087646484375,  g_loss: -173.97718811035156\n",
            "Training epoch 7468/1000000, d_loss: -75.44548034667969,  g_loss: 55.85553741455078\n",
            "Training epoch 7469/1000000, d_loss: -96.49870300292969,  g_loss: -31.81288719177246\n",
            "Training epoch 7470/1000000, d_loss: -92.87594604492188,  g_loss: -20.445568084716797\n",
            "Training epoch 7471/1000000, d_loss: -42.69093322753906,  g_loss: 105.44051361083984\n",
            "Training epoch 7472/1000000, d_loss: -151.47177124023438,  g_loss: 110.93607330322266\n",
            "Training epoch 7473/1000000, d_loss: -86.14026641845703,  g_loss: 170.4857635498047\n",
            "Training epoch 7474/1000000, d_loss: -66.00435638427734,  g_loss: 8.343589782714844\n",
            "Training epoch 7475/1000000, d_loss: -131.8391876220703,  g_loss: 25.920820236206055\n",
            "Training epoch 7476/1000000, d_loss: -99.74175262451172,  g_loss: 83.57899475097656\n",
            "Training epoch 7477/1000000, d_loss: -2230.72412109375,  g_loss: -317.0566101074219\n",
            "Training epoch 7478/1000000, d_loss: 26.577529907226562,  g_loss: -328.37371826171875\n",
            "Training epoch 7479/1000000, d_loss: -57.077945709228516,  g_loss: -99.31575012207031\n",
            "Training epoch 7480/1000000, d_loss: 84.56805419921875,  g_loss: -64.92183685302734\n",
            "Training epoch 7481/1000000, d_loss: -79.52312469482422,  g_loss: -18.42433738708496\n",
            "Training epoch 7482/1000000, d_loss: -29.83995819091797,  g_loss: 55.8648681640625\n",
            "Training epoch 7483/1000000, d_loss: -21.952117919921875,  g_loss: -57.93830108642578\n",
            "Training epoch 7484/1000000, d_loss: 31.18963050842285,  g_loss: 16.8604793548584\n",
            "Training epoch 7485/1000000, d_loss: -131.61558532714844,  g_loss: 161.60299682617188\n",
            "Training epoch 7486/1000000, d_loss: 134.63377380371094,  g_loss: 24.1052303314209\n",
            "Training epoch 7487/1000000, d_loss: 63.04350280761719,  g_loss: 43.161720275878906\n",
            "Training epoch 7488/1000000, d_loss: -19.76190757751465,  g_loss: 49.65850830078125\n",
            "Training epoch 7489/1000000, d_loss: -127.87935638427734,  g_loss: 77.32615661621094\n",
            "Training epoch 7490/1000000, d_loss: -46.9285888671875,  g_loss: 68.87782287597656\n",
            "Training epoch 7491/1000000, d_loss: -189.204833984375,  g_loss: 99.56857299804688\n",
            "Training epoch 7492/1000000, d_loss: -22.757007598876953,  g_loss: 95.94947814941406\n",
            "Training epoch 7493/1000000, d_loss: -67.42582702636719,  g_loss: 22.708417892456055\n",
            "Training epoch 7494/1000000, d_loss: -161.06381225585938,  g_loss: 129.59521484375\n",
            "Training epoch 7495/1000000, d_loss: -69.21780395507812,  g_loss: 64.10482025146484\n",
            "Training epoch 7496/1000000, d_loss: -13.324630737304688,  g_loss: 119.8266372680664\n",
            "Training epoch 7497/1000000, d_loss: -70.34017181396484,  g_loss: 170.31866455078125\n",
            "Training epoch 7498/1000000, d_loss: -61.940940856933594,  g_loss: 122.4900131225586\n",
            "Training epoch 7499/1000000, d_loss: -164.0386505126953,  g_loss: 171.3826904296875\n",
            "Training epoch 7500/1000000, d_loss: -188.16656494140625,  g_loss: 39.702632904052734\n",
            "Training epoch 7501/1000000, d_loss: 112.496337890625,  g_loss: -117.40861511230469\n",
            "Training epoch 7502/1000000, d_loss: -536.621826171875,  g_loss: -17.885414123535156\n",
            "Training epoch 7503/1000000, d_loss: -725.7342529296875,  g_loss: -54.19230651855469\n",
            "Training epoch 7504/1000000, d_loss: 17.249868392944336,  g_loss: -93.42335510253906\n",
            "Training epoch 7505/1000000, d_loss: -217.25865173339844,  g_loss: 0.5069055557250977\n",
            "Training epoch 7506/1000000, d_loss: 23.47393798828125,  g_loss: -64.02571105957031\n",
            "Training epoch 7507/1000000, d_loss: -60.07248306274414,  g_loss: -31.130481719970703\n",
            "Training epoch 7508/1000000, d_loss: -132.76478576660156,  g_loss: -26.210826873779297\n",
            "Training epoch 7509/1000000, d_loss: -164.56198120117188,  g_loss: 18.539236068725586\n",
            "Training epoch 7510/1000000, d_loss: -24.33022689819336,  g_loss: -32.66114807128906\n",
            "Training epoch 7511/1000000, d_loss: -127.50862121582031,  g_loss: -20.829360961914062\n",
            "Training epoch 7512/1000000, d_loss: -98.48652648925781,  g_loss: -8.553406715393066\n",
            "Training epoch 7513/1000000, d_loss: -114.35340881347656,  g_loss: -0.9595623016357422\n",
            "Training epoch 7514/1000000, d_loss: -130.66062927246094,  g_loss: -50.93793869018555\n",
            "Training epoch 7515/1000000, d_loss: -99.92938232421875,  g_loss: -12.461755752563477\n",
            "Training epoch 7516/1000000, d_loss: 40.741233825683594,  g_loss: 3.8405113220214844\n",
            "Training epoch 7517/1000000, d_loss: -60.336395263671875,  g_loss: 57.362220764160156\n",
            "Training epoch 7518/1000000, d_loss: -215.3607635498047,  g_loss: 103.21897888183594\n",
            "Training epoch 7519/1000000, d_loss: -89.25858306884766,  g_loss: -18.156864166259766\n",
            "Training epoch 7520/1000000, d_loss: -143.45526123046875,  g_loss: -9.699776649475098\n",
            "Training epoch 7521/1000000, d_loss: -474.74871826171875,  g_loss: -251.68966674804688\n",
            "Training epoch 7522/1000000, d_loss: -108.4424057006836,  g_loss: 129.6591033935547\n",
            "Training epoch 7523/1000000, d_loss: -70.47183227539062,  g_loss: 140.33291625976562\n",
            "Training epoch 7524/1000000, d_loss: -71.55899810791016,  g_loss: 72.23167419433594\n",
            "Training epoch 7525/1000000, d_loss: -45.148162841796875,  g_loss: 75.0772705078125\n",
            "Training epoch 7526/1000000, d_loss: -47.590904235839844,  g_loss: 18.570188522338867\n",
            "Training epoch 7527/1000000, d_loss: 21.148996353149414,  g_loss: 25.940855026245117\n",
            "Training epoch 7528/1000000, d_loss: -119.81645965576172,  g_loss: 29.299522399902344\n",
            "Training epoch 7529/1000000, d_loss: -93.19245147705078,  g_loss: 100.41693115234375\n",
            "Training epoch 7530/1000000, d_loss: -145.1332244873047,  g_loss: -32.683494567871094\n",
            "Training epoch 7531/1000000, d_loss: -271.73443603515625,  g_loss: -25.395130157470703\n",
            "Training epoch 7532/1000000, d_loss: 12.869491577148438,  g_loss: 71.59544372558594\n",
            "Training epoch 7533/1000000, d_loss: 34.11631774902344,  g_loss: 78.35568237304688\n",
            "Training epoch 7534/1000000, d_loss: -12.362964630126953,  g_loss: 100.8519287109375\n",
            "Training epoch 7535/1000000, d_loss: -35.800079345703125,  g_loss: 125.09150695800781\n",
            "Training epoch 7536/1000000, d_loss: -84.55622863769531,  g_loss: 31.3265380859375\n",
            "Training epoch 7537/1000000, d_loss: -180.085205078125,  g_loss: 38.88954162597656\n",
            "Training epoch 7538/1000000, d_loss: -541.1229858398438,  g_loss: -68.50745391845703\n",
            "Training epoch 7539/1000000, d_loss: 14899.4033203125,  g_loss: 106.6905746459961\n",
            "Training epoch 7540/1000000, d_loss: -136.93743896484375,  g_loss: 18.130613327026367\n",
            "Training epoch 7541/1000000, d_loss: 1.1001949310302734,  g_loss: 86.74034118652344\n",
            "Training epoch 7542/1000000, d_loss: -18.458106994628906,  g_loss: 55.249755859375\n",
            "Training epoch 7543/1000000, d_loss: -42.60748291015625,  g_loss: 104.02153015136719\n",
            "Training epoch 7544/1000000, d_loss: -61.36009979248047,  g_loss: 136.675048828125\n",
            "Training epoch 7545/1000000, d_loss: -90.63026428222656,  g_loss: 89.87203979492188\n",
            "Training epoch 7546/1000000, d_loss: -71.60795593261719,  g_loss: 96.45008850097656\n",
            "Training epoch 7547/1000000, d_loss: -91.06112670898438,  g_loss: 94.85417938232422\n",
            "Training epoch 7548/1000000, d_loss: -126.99254608154297,  g_loss: 214.49049377441406\n",
            "Training epoch 7549/1000000, d_loss: -24.146644592285156,  g_loss: 45.11343765258789\n",
            "Training epoch 7550/1000000, d_loss: -53.38373565673828,  g_loss: -24.534040451049805\n",
            "Training epoch 7551/1000000, d_loss: 201.41246032714844,  g_loss: 70.52471923828125\n",
            "Training epoch 7552/1000000, d_loss: -54.5338020324707,  g_loss: 119.04613494873047\n",
            "Training epoch 7553/1000000, d_loss: -88.77803039550781,  g_loss: 85.16674041748047\n",
            "Training epoch 7554/1000000, d_loss: 60.317848205566406,  g_loss: 120.96614074707031\n",
            "Training epoch 7555/1000000, d_loss: -90.06266784667969,  g_loss: 138.2877197265625\n",
            "Training epoch 7556/1000000, d_loss: -58.15495300292969,  g_loss: 61.85295867919922\n",
            "Training epoch 7557/1000000, d_loss: -1.2903289794921875,  g_loss: 127.7093505859375\n",
            "Training epoch 7558/1000000, d_loss: -37.23957061767578,  g_loss: 62.82442855834961\n",
            "Training epoch 7559/1000000, d_loss: -143.90744018554688,  g_loss: 32.33177185058594\n",
            "Training epoch 7560/1000000, d_loss: -199.9227294921875,  g_loss: 16.400968551635742\n",
            "Training epoch 7561/1000000, d_loss: -33.22309112548828,  g_loss: 60.36320495605469\n",
            "Training epoch 7562/1000000, d_loss: -19.314071655273438,  g_loss: 102.9212417602539\n",
            "Training epoch 7563/1000000, d_loss: -61.44766616821289,  g_loss: 45.574851989746094\n",
            "Training epoch 7564/1000000, d_loss: -600.9979248046875,  g_loss: -248.93064880371094\n",
            "Training epoch 7565/1000000, d_loss: -97.48384857177734,  g_loss: 34.8924560546875\n",
            "Training epoch 7566/1000000, d_loss: -123.7215576171875,  g_loss: 51.7472038269043\n",
            "Training epoch 7567/1000000, d_loss: -293.4013977050781,  g_loss: 6.8410539627075195\n",
            "Training epoch 7568/1000000, d_loss: -304.0066833496094,  g_loss: -19.64550018310547\n",
            "Training epoch 7569/1000000, d_loss: -270.24261474609375,  g_loss: -281.0641784667969\n",
            "Training epoch 7570/1000000, d_loss: -165.18714904785156,  g_loss: -0.3849906921386719\n",
            "Training epoch 7571/1000000, d_loss: -150.37075805664062,  g_loss: 77.76953125\n",
            "Training epoch 7572/1000000, d_loss: -116.35620880126953,  g_loss: 187.7387237548828\n",
            "Training epoch 7573/1000000, d_loss: -370.758056640625,  g_loss: 114.91914367675781\n",
            "Training epoch 7574/1000000, d_loss: 70.7884521484375,  g_loss: 31.776697158813477\n",
            "Training epoch 7575/1000000, d_loss: -109.06431579589844,  g_loss: 47.14363098144531\n",
            "Training epoch 7576/1000000, d_loss: -257.1033630371094,  g_loss: 120.22732543945312\n",
            "Training epoch 7577/1000000, d_loss: 51.45499038696289,  g_loss: -99.54638671875\n",
            "Training epoch 7578/1000000, d_loss: -38.70228576660156,  g_loss: -125.9632339477539\n",
            "Training epoch 7579/1000000, d_loss: -106.19119262695312,  g_loss: 120.30770874023438\n",
            "Training epoch 7580/1000000, d_loss: -133.54299926757812,  g_loss: 58.219207763671875\n",
            "Training epoch 7581/1000000, d_loss: -160.8726043701172,  g_loss: -81.21791076660156\n",
            "Training epoch 7582/1000000, d_loss: -106.76856231689453,  g_loss: -7.861072540283203\n",
            "Training epoch 7583/1000000, d_loss: -20.074737548828125,  g_loss: -16.040138244628906\n",
            "Training epoch 7584/1000000, d_loss: -111.46546936035156,  g_loss: -47.08994674682617\n",
            "Training epoch 7585/1000000, d_loss: -52.72325897216797,  g_loss: -14.604034423828125\n",
            "Training epoch 7586/1000000, d_loss: -74.40869140625,  g_loss: 29.843082427978516\n",
            "Training epoch 7587/1000000, d_loss: -138.3642120361328,  g_loss: -19.69902229309082\n",
            "Training epoch 7588/1000000, d_loss: -22.251853942871094,  g_loss: -12.168084144592285\n",
            "Training epoch 7589/1000000, d_loss: -149.62428283691406,  g_loss: 40.073638916015625\n",
            "Training epoch 7590/1000000, d_loss: -983.9381713867188,  g_loss: -181.2681884765625\n",
            "Training epoch 7591/1000000, d_loss: -608.2598876953125,  g_loss: -358.30706787109375\n",
            "Training epoch 7592/1000000, d_loss: 38.798213958740234,  g_loss: -51.540367126464844\n",
            "Training epoch 7593/1000000, d_loss: -125.17601013183594,  g_loss: -33.20696258544922\n",
            "Training epoch 7594/1000000, d_loss: -158.13833618164062,  g_loss: 63.636592864990234\n",
            "Training epoch 7595/1000000, d_loss: -163.90899658203125,  g_loss: 51.20789337158203\n",
            "Training epoch 7596/1000000, d_loss: -164.0377197265625,  g_loss: -17.8017520904541\n",
            "Training epoch 7597/1000000, d_loss: -289.8657531738281,  g_loss: 367.35223388671875\n",
            "Training epoch 7598/1000000, d_loss: -375.3783264160156,  g_loss: 425.4945068359375\n",
            "Training epoch 7599/1000000, d_loss: -221.01483154296875,  g_loss: 65.92524719238281\n",
            "Training epoch 7600/1000000, d_loss: -70.58646392822266,  g_loss: 72.67308807373047\n",
            "Training epoch 7601/1000000, d_loss: -280.31829833984375,  g_loss: 276.2505187988281\n",
            "Training epoch 7602/1000000, d_loss: 675.7247314453125,  g_loss: 110.77488708496094\n",
            "Training epoch 7603/1000000, d_loss: -109.93731689453125,  g_loss: 126.16512298583984\n",
            "Training epoch 7604/1000000, d_loss: 71.36825561523438,  g_loss: 17.716819763183594\n",
            "Training epoch 7605/1000000, d_loss: -121.10584259033203,  g_loss: 59.61092758178711\n",
            "Training epoch 7606/1000000, d_loss: -244.666259765625,  g_loss: 26.556602478027344\n",
            "Training epoch 7607/1000000, d_loss: -207.80819702148438,  g_loss: 295.57086181640625\n",
            "Training epoch 7608/1000000, d_loss: -326.0958557128906,  g_loss: 118.98233795166016\n",
            "Training epoch 7609/1000000, d_loss: -146.87847900390625,  g_loss: 80.37242126464844\n",
            "Training epoch 7610/1000000, d_loss: -981.4515380859375,  g_loss: -308.7700500488281\n",
            "Training epoch 7611/1000000, d_loss: 68.57672882080078,  g_loss: 3.33388614654541\n",
            "Training epoch 7612/1000000, d_loss: -147.1953582763672,  g_loss: -43.33903503417969\n",
            "Training epoch 7613/1000000, d_loss: -90.18553161621094,  g_loss: -73.40164947509766\n",
            "Training epoch 7614/1000000, d_loss: -142.36627197265625,  g_loss: 36.584110260009766\n",
            "Training epoch 7615/1000000, d_loss: -401.94500732421875,  g_loss: -80.60095977783203\n",
            "Training epoch 7616/1000000, d_loss: -65.04255676269531,  g_loss: 12.54823112487793\n",
            "Training epoch 7617/1000000, d_loss: -416.2107849121094,  g_loss: -214.05706787109375\n",
            "Training epoch 7618/1000000, d_loss: -83.42118072509766,  g_loss: 32.08251953125\n",
            "Training epoch 7619/1000000, d_loss: -11.996719360351562,  g_loss: 74.44253540039062\n",
            "Training epoch 7620/1000000, d_loss: -235.5160675048828,  g_loss: 260.2892150878906\n",
            "Training epoch 7621/1000000, d_loss: 21.610271453857422,  g_loss: 65.078125\n",
            "Training epoch 7622/1000000, d_loss: -131.2373504638672,  g_loss: 36.88302993774414\n",
            "Training epoch 7623/1000000, d_loss: -52.562408447265625,  g_loss: 60.14637756347656\n",
            "Training epoch 7624/1000000, d_loss: -108.99407958984375,  g_loss: 246.27783203125\n",
            "Training epoch 7625/1000000, d_loss: -165.8285675048828,  g_loss: 68.43873596191406\n",
            "Training epoch 7626/1000000, d_loss: -249.82333374023438,  g_loss: -21.919584274291992\n",
            "Training epoch 7627/1000000, d_loss: -93.36558532714844,  g_loss: 130.2566680908203\n",
            "Training epoch 7628/1000000, d_loss: 987.0814819335938,  g_loss: 155.23104858398438\n",
            "Training epoch 7629/1000000, d_loss: -70.89793395996094,  g_loss: 132.0514678955078\n",
            "Training epoch 7630/1000000, d_loss: -52.514869689941406,  g_loss: 119.29234313964844\n",
            "Training epoch 7631/1000000, d_loss: -66.96146392822266,  g_loss: 47.639774322509766\n",
            "Training epoch 7632/1000000, d_loss: -51.989768981933594,  g_loss: 32.668212890625\n",
            "Training epoch 7633/1000000, d_loss: -55.35403060913086,  g_loss: 51.94964599609375\n",
            "Training epoch 7634/1000000, d_loss: -31.19613265991211,  g_loss: 29.352859497070312\n",
            "Training epoch 7635/1000000, d_loss: -171.446044921875,  g_loss: 65.70283508300781\n",
            "Training epoch 7636/1000000, d_loss: -432.12725830078125,  g_loss: -37.558982849121094\n",
            "Training epoch 7637/1000000, d_loss: -66.97640228271484,  g_loss: 32.5948486328125\n",
            "Training epoch 7638/1000000, d_loss: -112.35616302490234,  g_loss: 3.948516845703125\n",
            "Training epoch 7639/1000000, d_loss: -62.82420349121094,  g_loss: 117.62145233154297\n",
            "Training epoch 7640/1000000, d_loss: -205.94171142578125,  g_loss: -41.58948516845703\n",
            "Training epoch 7641/1000000, d_loss: -92.5648193359375,  g_loss: 3.9379615783691406\n",
            "Training epoch 7642/1000000, d_loss: -74.851318359375,  g_loss: 192.95486450195312\n",
            "Training epoch 7643/1000000, d_loss: -38.12207794189453,  g_loss: 125.90411376953125\n",
            "Training epoch 7644/1000000, d_loss: -128.95936584472656,  g_loss: 66.2566909790039\n",
            "Training epoch 7645/1000000, d_loss: -86.90447235107422,  g_loss: 122.35572814941406\n",
            "Training epoch 7646/1000000, d_loss: -47.68605422973633,  g_loss: 98.68023681640625\n",
            "Training epoch 7647/1000000, d_loss: -7.204119682312012,  g_loss: 141.07135009765625\n",
            "Training epoch 7648/1000000, d_loss: -97.76029968261719,  g_loss: 113.17869567871094\n",
            "Training epoch 7649/1000000, d_loss: 45.103118896484375,  g_loss: 58.602088928222656\n",
            "Training epoch 7650/1000000, d_loss: -278.9381408691406,  g_loss: 30.843326568603516\n",
            "Training epoch 7651/1000000, d_loss: -77.3701171875,  g_loss: 27.20240592956543\n",
            "Training epoch 7652/1000000, d_loss: -70.10699462890625,  g_loss: 67.90565490722656\n",
            "Training epoch 7653/1000000, d_loss: -133.13299560546875,  g_loss: 185.1831512451172\n",
            "Training epoch 7654/1000000, d_loss: -244.23619079589844,  g_loss: 3.4491324424743652\n",
            "Training epoch 7655/1000000, d_loss: -590.5243530273438,  g_loss: -18.60707664489746\n",
            "Training epoch 7656/1000000, d_loss: -28.308063507080078,  g_loss: 6.014932632446289\n",
            "Training epoch 7657/1000000, d_loss: -63.87054443359375,  g_loss: 17.55580711364746\n",
            "Training epoch 7658/1000000, d_loss: -4.019502639770508,  g_loss: 29.044565200805664\n",
            "Training epoch 7659/1000000, d_loss: -78.94803619384766,  g_loss: 14.584085464477539\n",
            "Training epoch 7660/1000000, d_loss: -103.65725708007812,  g_loss: 47.780338287353516\n",
            "Training epoch 7661/1000000, d_loss: -152.27255249023438,  g_loss: -55.369014739990234\n",
            "Training epoch 7662/1000000, d_loss: -155.40036010742188,  g_loss: 72.90345764160156\n",
            "Training epoch 7663/1000000, d_loss: -62.89645004272461,  g_loss: 29.487491607666016\n",
            "Training epoch 7664/1000000, d_loss: -77.27213287353516,  g_loss: -5.178816318511963\n",
            "Training epoch 7665/1000000, d_loss: -93.07504272460938,  g_loss: 29.542171478271484\n",
            "Training epoch 7666/1000000, d_loss: -53.46874237060547,  g_loss: -6.989586353302002\n",
            "Training epoch 7667/1000000, d_loss: -14.138906478881836,  g_loss: 41.58502960205078\n",
            "Training epoch 7668/1000000, d_loss: -46.55434799194336,  g_loss: 14.661155700683594\n",
            "Training epoch 7669/1000000, d_loss: -941.5546875,  g_loss: -109.51612091064453\n",
            "Training epoch 7670/1000000, d_loss: -204.15548706054688,  g_loss: -39.43346405029297\n",
            "Training epoch 7671/1000000, d_loss: 335.25006103515625,  g_loss: 13.071056365966797\n",
            "Training epoch 7672/1000000, d_loss: -431.74951171875,  g_loss: -100.58354187011719\n",
            "Training epoch 7673/1000000, d_loss: 14.070625305175781,  g_loss: 57.666683197021484\n",
            "Training epoch 7674/1000000, d_loss: 74.48161315917969,  g_loss: 90.62248229980469\n",
            "Training epoch 7675/1000000, d_loss: -63.908363342285156,  g_loss: 106.0658950805664\n",
            "Training epoch 7676/1000000, d_loss: -458.6275329589844,  g_loss: 44.464088439941406\n",
            "Training epoch 7677/1000000, d_loss: -149.12393188476562,  g_loss: -122.41542053222656\n",
            "Training epoch 7678/1000000, d_loss: 20.415096282958984,  g_loss: 93.2647476196289\n",
            "Training epoch 7679/1000000, d_loss: -26.204753875732422,  g_loss: 103.10678100585938\n",
            "Training epoch 7680/1000000, d_loss: -119.33092498779297,  g_loss: 139.40768432617188\n",
            "Training epoch 7681/1000000, d_loss: -127.70230865478516,  g_loss: 200.74365234375\n",
            "Training epoch 7682/1000000, d_loss: -148.96597290039062,  g_loss: 124.13591766357422\n",
            "Training epoch 7683/1000000, d_loss: -80.53791046142578,  g_loss: 109.55694580078125\n",
            "Training epoch 7684/1000000, d_loss: -93.96574401855469,  g_loss: 13.307695388793945\n",
            "Training epoch 7685/1000000, d_loss: -25.8988037109375,  g_loss: 84.71421813964844\n",
            "Training epoch 7686/1000000, d_loss: -79.98780822753906,  g_loss: 34.086814880371094\n",
            "Training epoch 7687/1000000, d_loss: -170.81097412109375,  g_loss: -15.317411422729492\n",
            "Training epoch 7688/1000000, d_loss: -68.11614990234375,  g_loss: 65.78683471679688\n",
            "Training epoch 7689/1000000, d_loss: -264.0022277832031,  g_loss: -47.59696960449219\n",
            "Training epoch 7690/1000000, d_loss: 38.47657775878906,  g_loss: -55.064659118652344\n",
            "Training epoch 7691/1000000, d_loss: -16.39130401611328,  g_loss: -39.484703063964844\n",
            "Training epoch 7692/1000000, d_loss: -54.67373275756836,  g_loss: -20.715614318847656\n",
            "Training epoch 7693/1000000, d_loss: -197.07717895507812,  g_loss: -55.40488815307617\n",
            "Training epoch 7694/1000000, d_loss: -168.75592041015625,  g_loss: -119.35040283203125\n",
            "Training epoch 7695/1000000, d_loss: -190.26731872558594,  g_loss: 33.20111846923828\n",
            "Training epoch 7696/1000000, d_loss: -95.89299011230469,  g_loss: 149.17434692382812\n",
            "Training epoch 7697/1000000, d_loss: -206.05740356445312,  g_loss: -62.47758102416992\n",
            "Training epoch 7698/1000000, d_loss: -238.913330078125,  g_loss: 25.09702491760254\n",
            "Training epoch 7699/1000000, d_loss: -747.5298461914062,  g_loss: -163.32247924804688\n",
            "Training epoch 7700/1000000, d_loss: -22.844802856445312,  g_loss: -38.151405334472656\n",
            "Training epoch 7701/1000000, d_loss: 67.00837707519531,  g_loss: -0.6036033630371094\n",
            "Training epoch 7702/1000000, d_loss: -139.7730712890625,  g_loss: 91.62399291992188\n",
            "Training epoch 7703/1000000, d_loss: -65.23372650146484,  g_loss: 303.6005859375\n",
            "Training epoch 7704/1000000, d_loss: -311.2031555175781,  g_loss: 740.3958129882812\n",
            "Training epoch 7705/1000000, d_loss: -66.91285705566406,  g_loss: 2.0620763301849365\n",
            "Training epoch 7706/1000000, d_loss: -75.0123291015625,  g_loss: 50.900028228759766\n",
            "Training epoch 7707/1000000, d_loss: 54.220848083496094,  g_loss: 54.84952926635742\n",
            "Training epoch 7708/1000000, d_loss: -42.804298400878906,  g_loss: 8.54621696472168\n",
            "Training epoch 7709/1000000, d_loss: -1351.395751953125,  g_loss: -43.40441131591797\n",
            "Training epoch 7710/1000000, d_loss: -31.762222290039062,  g_loss: -2.829862594604492\n",
            "Training epoch 7711/1000000, d_loss: -152.25526428222656,  g_loss: 11.0514554977417\n",
            "Training epoch 7712/1000000, d_loss: -3.6659927368164062,  g_loss: 90.69132995605469\n",
            "Training epoch 7713/1000000, d_loss: -2.533557891845703,  g_loss: 76.55607604980469\n",
            "Training epoch 7714/1000000, d_loss: -86.81283569335938,  g_loss: 67.47480773925781\n",
            "Training epoch 7715/1000000, d_loss: -97.33540344238281,  g_loss: 42.495269775390625\n",
            "Training epoch 7716/1000000, d_loss: -85.95223999023438,  g_loss: 6.367185592651367\n",
            "Training epoch 7717/1000000, d_loss: -194.57034301757812,  g_loss: 23.20789337158203\n",
            "Training epoch 7718/1000000, d_loss: -21.887344360351562,  g_loss: 81.98615264892578\n",
            "Training epoch 7719/1000000, d_loss: -104.82347106933594,  g_loss: 123.192626953125\n",
            "Training epoch 7720/1000000, d_loss: -27.051795959472656,  g_loss: 52.80686950683594\n",
            "Training epoch 7721/1000000, d_loss: -85.43000793457031,  g_loss: 31.08099365234375\n",
            "Training epoch 7722/1000000, d_loss: -60.27077102661133,  g_loss: 42.886634826660156\n",
            "Training epoch 7723/1000000, d_loss: -170.3995819091797,  g_loss: 20.988330841064453\n",
            "Training epoch 7724/1000000, d_loss: -67.7747573852539,  g_loss: 40.75132751464844\n",
            "Training epoch 7725/1000000, d_loss: -88.50284576416016,  g_loss: 52.40968704223633\n",
            "Training epoch 7726/1000000, d_loss: -318.0654296875,  g_loss: 5.583671569824219\n",
            "Training epoch 7727/1000000, d_loss: -92.44564056396484,  g_loss: -9.574699401855469\n",
            "Training epoch 7728/1000000, d_loss: -282.9892578125,  g_loss: -181.72671508789062\n",
            "Training epoch 7729/1000000, d_loss: -143.61048889160156,  g_loss: -38.906898498535156\n",
            "Training epoch 7730/1000000, d_loss: 7.348949432373047,  g_loss: -22.281095504760742\n",
            "Training epoch 7731/1000000, d_loss: -85.77955627441406,  g_loss: 39.51600646972656\n",
            "Training epoch 7732/1000000, d_loss: -70.55262756347656,  g_loss: 98.29478454589844\n",
            "Training epoch 7733/1000000, d_loss: -98.38894653320312,  g_loss: -24.704010009765625\n",
            "Training epoch 7734/1000000, d_loss: -19.825136184692383,  g_loss: 47.53981399536133\n",
            "Training epoch 7735/1000000, d_loss: -53.80333709716797,  g_loss: 42.3273811340332\n",
            "Training epoch 7736/1000000, d_loss: -88.89500427246094,  g_loss: 23.324386596679688\n",
            "Training epoch 7737/1000000, d_loss: -251.7298583984375,  g_loss: 34.28559875488281\n",
            "Training epoch 7738/1000000, d_loss: -75.15182495117188,  g_loss: -4.914752960205078\n",
            "Training epoch 7739/1000000, d_loss: -147.0620880126953,  g_loss: -11.445625305175781\n",
            "Training epoch 7740/1000000, d_loss: -434.4339599609375,  g_loss: 69.3177490234375\n",
            "Training epoch 7741/1000000, d_loss: -597.2147827148438,  g_loss: -2.200654983520508\n",
            "Training epoch 7742/1000000, d_loss: -205.62960815429688,  g_loss: -182.6572265625\n",
            "Training epoch 7743/1000000, d_loss: -42.692291259765625,  g_loss: -169.83096313476562\n",
            "Training epoch 7744/1000000, d_loss: -83.02960205078125,  g_loss: 1.3817026615142822\n",
            "Training epoch 7745/1000000, d_loss: -29.82021713256836,  g_loss: 27.192155838012695\n",
            "Training epoch 7746/1000000, d_loss: -151.4663543701172,  g_loss: -8.53341293334961\n",
            "Training epoch 7747/1000000, d_loss: -21.388307571411133,  g_loss: 35.15648651123047\n",
            "Training epoch 7748/1000000, d_loss: -141.22674560546875,  g_loss: 127.861328125\n",
            "Training epoch 7749/1000000, d_loss: 1.24578857421875,  g_loss: 93.86572265625\n",
            "Training epoch 7750/1000000, d_loss: -107.79497528076172,  g_loss: 37.66718292236328\n",
            "Training epoch 7751/1000000, d_loss: -63.21844482421875,  g_loss: 133.78599548339844\n",
            "Training epoch 7752/1000000, d_loss: -73.54949951171875,  g_loss: 188.7886962890625\n",
            "Training epoch 7753/1000000, d_loss: -78.5918197631836,  g_loss: 155.3784942626953\n",
            "Training epoch 7754/1000000, d_loss: -69.29517364501953,  g_loss: 176.3550262451172\n",
            "Training epoch 7755/1000000, d_loss: -193.02650451660156,  g_loss: 150.00924682617188\n",
            "Training epoch 7756/1000000, d_loss: -25.226890563964844,  g_loss: 124.63922119140625\n",
            "Training epoch 7757/1000000, d_loss: -120.88894653320312,  g_loss: 44.25642013549805\n",
            "Training epoch 7758/1000000, d_loss: -73.81533813476562,  g_loss: 49.56290054321289\n",
            "Training epoch 7759/1000000, d_loss: -37.71730041503906,  g_loss: 32.25579833984375\n",
            "Training epoch 7760/1000000, d_loss: -103.6305160522461,  g_loss: -10.437952041625977\n",
            "Training epoch 7761/1000000, d_loss: -220.92529296875,  g_loss: -15.433871269226074\n",
            "Training epoch 7762/1000000, d_loss: -37.23342514038086,  g_loss: -58.297767639160156\n",
            "Training epoch 7763/1000000, d_loss: -47.41917037963867,  g_loss: -27.535232543945312\n",
            "Training epoch 7764/1000000, d_loss: -11.589160919189453,  g_loss: -27.27601432800293\n",
            "Training epoch 7765/1000000, d_loss: 79.3897705078125,  g_loss: -39.43116760253906\n",
            "Training epoch 7766/1000000, d_loss: -35.65749740600586,  g_loss: 42.523292541503906\n",
            "Training epoch 7767/1000000, d_loss: -56.06321716308594,  g_loss: -22.13715934753418\n",
            "Training epoch 7768/1000000, d_loss: -264.0760803222656,  g_loss: -58.5071907043457\n",
            "Training epoch 7769/1000000, d_loss: -204.41798400878906,  g_loss: -35.16595458984375\n",
            "Training epoch 7770/1000000, d_loss: -38.43233871459961,  g_loss: 79.8730697631836\n",
            "Training epoch 7771/1000000, d_loss: -67.92062377929688,  g_loss: 137.08824157714844\n",
            "Training epoch 7772/1000000, d_loss: -169.1515350341797,  g_loss: 262.2430725097656\n",
            "Training epoch 7773/1000000, d_loss: -26.292015075683594,  g_loss: 46.81285095214844\n",
            "Training epoch 7774/1000000, d_loss: -77.61026763916016,  g_loss: 25.89168930053711\n",
            "Training epoch 7775/1000000, d_loss: -117.51595306396484,  g_loss: 1.8050498962402344\n",
            "Training epoch 7776/1000000, d_loss: -26.694143295288086,  g_loss: 4.944318771362305\n",
            "Training epoch 7777/1000000, d_loss: -24.457504272460938,  g_loss: 63.63663864135742\n",
            "Training epoch 7778/1000000, d_loss: -137.42898559570312,  g_loss: 67.20772552490234\n",
            "Training epoch 7779/1000000, d_loss: -157.42120361328125,  g_loss: -9.365599632263184\n",
            "Training epoch 7780/1000000, d_loss: -28.524456024169922,  g_loss: 95.738525390625\n",
            "Training epoch 7781/1000000, d_loss: -89.3414306640625,  g_loss: 62.23822021484375\n",
            "Training epoch 7782/1000000, d_loss: 19.839950561523438,  g_loss: 89.25387573242188\n",
            "Training epoch 7783/1000000, d_loss: -115.08430480957031,  g_loss: 65.70086669921875\n",
            "Training epoch 7784/1000000, d_loss: -299.00189208984375,  g_loss: 43.234214782714844\n",
            "Training epoch 7785/1000000, d_loss: -73.46365356445312,  g_loss: -9.222572326660156\n",
            "Training epoch 7786/1000000, d_loss: 13.220187187194824,  g_loss: 41.30463409423828\n",
            "Training epoch 7787/1000000, d_loss: -56.2181510925293,  g_loss: 44.16038131713867\n",
            "Training epoch 7788/1000000, d_loss: -141.353759765625,  g_loss: 134.14918518066406\n",
            "Training epoch 7789/1000000, d_loss: -59.955039978027344,  g_loss: 64.80897521972656\n",
            "Training epoch 7790/1000000, d_loss: -180.49855041503906,  g_loss: 30.559532165527344\n",
            "Training epoch 7791/1000000, d_loss: -50.10523986816406,  g_loss: 40.098289489746094\n",
            "Training epoch 7792/1000000, d_loss: -18.32115936279297,  g_loss: 17.47655487060547\n",
            "Training epoch 7793/1000000, d_loss: -91.93944549560547,  g_loss: -37.71986389160156\n",
            "Training epoch 7794/1000000, d_loss: -236.82484436035156,  g_loss: 73.21533203125\n",
            "Training epoch 7795/1000000, d_loss: -6.778512954711914,  g_loss: 19.98116683959961\n",
            "Training epoch 7796/1000000, d_loss: -23.459545135498047,  g_loss: 1.098785400390625\n",
            "Training epoch 7797/1000000, d_loss: -81.98260498046875,  g_loss: 20.62483787536621\n",
            "Training epoch 7798/1000000, d_loss: -42.69764709472656,  g_loss: -16.99329376220703\n",
            "Training epoch 7799/1000000, d_loss: -61.241668701171875,  g_loss: 14.909278869628906\n",
            "Training epoch 7800/1000000, d_loss: -244.75086975097656,  g_loss: -36.56492233276367\n",
            "Training epoch 7801/1000000, d_loss: -137.98033142089844,  g_loss: -115.49114227294922\n",
            "Training epoch 7802/1000000, d_loss: -668.0235595703125,  g_loss: -145.73880004882812\n",
            "Training epoch 7803/1000000, d_loss: -42.84423828125,  g_loss: -17.522750854492188\n",
            "Training epoch 7804/1000000, d_loss: -55.474002838134766,  g_loss: 41.12018585205078\n",
            "Training epoch 7805/1000000, d_loss: -96.61675262451172,  g_loss: 30.200424194335938\n",
            "Training epoch 7806/1000000, d_loss: -385.76605224609375,  g_loss: -72.1986083984375\n",
            "Training epoch 7807/1000000, d_loss: -426.64093017578125,  g_loss: -43.23740005493164\n",
            "Training epoch 7808/1000000, d_loss: -45.975772857666016,  g_loss: -38.90782165527344\n",
            "Training epoch 7809/1000000, d_loss: -328.69390869140625,  g_loss: -10.532632827758789\n",
            "Training epoch 7810/1000000, d_loss: -554.3067016601562,  g_loss: -511.2470703125\n",
            "Training epoch 7811/1000000, d_loss: 15.432708740234375,  g_loss: 27.226200103759766\n",
            "Training epoch 7812/1000000, d_loss: -188.47103881835938,  g_loss: -32.75981140136719\n",
            "Training epoch 7813/1000000, d_loss: 55.9346923828125,  g_loss: 20.73067283630371\n",
            "Training epoch 7814/1000000, d_loss: 2.11737060546875,  g_loss: -38.8267707824707\n",
            "Training epoch 7815/1000000, d_loss: -137.50904846191406,  g_loss: -49.06576919555664\n",
            "Training epoch 7816/1000000, d_loss: -107.80557250976562,  g_loss: 17.846477508544922\n",
            "Training epoch 7817/1000000, d_loss: -210.3308563232422,  g_loss: 396.15533447265625\n",
            "Training epoch 7818/1000000, d_loss: -85.88922119140625,  g_loss: -44.43690490722656\n",
            "Training epoch 7819/1000000, d_loss: -50.25425720214844,  g_loss: -27.98370933532715\n",
            "Training epoch 7820/1000000, d_loss: -318.922607421875,  g_loss: 8.402910232543945\n",
            "Training epoch 7821/1000000, d_loss: 8.199714660644531,  g_loss: 6.30853796005249\n",
            "Training epoch 7822/1000000, d_loss: -144.30203247070312,  g_loss: -11.739114761352539\n",
            "Training epoch 7823/1000000, d_loss: -142.77304077148438,  g_loss: 14.295536041259766\n",
            "Training epoch 7824/1000000, d_loss: -196.32025146484375,  g_loss: 25.505109786987305\n",
            "Training epoch 7825/1000000, d_loss: -40.948638916015625,  g_loss: -6.210047245025635\n",
            "Training epoch 7826/1000000, d_loss: -160.85415649414062,  g_loss: 57.60850524902344\n",
            "Training epoch 7827/1000000, d_loss: -0.2816619873046875,  g_loss: 3.9721155166625977\n",
            "Training epoch 7828/1000000, d_loss: -78.31478118896484,  g_loss: 36.75794219970703\n",
            "Training epoch 7829/1000000, d_loss: -144.44631958007812,  g_loss: -64.32609558105469\n",
            "Training epoch 7830/1000000, d_loss: 26.96881103515625,  g_loss: -20.409059524536133\n",
            "Training epoch 7831/1000000, d_loss: -146.41061401367188,  g_loss: 71.29486846923828\n",
            "Training epoch 7832/1000000, d_loss: -104.62962341308594,  g_loss: 64.4613037109375\n",
            "Training epoch 7833/1000000, d_loss: -53.13356018066406,  g_loss: 8.455850601196289\n",
            "Training epoch 7834/1000000, d_loss: -142.31631469726562,  g_loss: 49.517250061035156\n",
            "Training epoch 7835/1000000, d_loss: -68.67288208007812,  g_loss: 80.04906463623047\n",
            "Training epoch 7836/1000000, d_loss: -258.95977783203125,  g_loss: 41.22203826904297\n",
            "Training epoch 7837/1000000, d_loss: -321.20635986328125,  g_loss: -39.53010559082031\n",
            "Training epoch 7838/1000000, d_loss: -108.02023315429688,  g_loss: -3.5061681270599365\n",
            "Training epoch 7839/1000000, d_loss: -160.2926788330078,  g_loss: -70.3748779296875\n",
            "Training epoch 7840/1000000, d_loss: -56.80482482910156,  g_loss: -32.14777755737305\n",
            "Training epoch 7841/1000000, d_loss: -492.02984619140625,  g_loss: -132.50660705566406\n",
            "Training epoch 7842/1000000, d_loss: 41.983741760253906,  g_loss: 92.09123992919922\n",
            "Training epoch 7843/1000000, d_loss: -231.02476501464844,  g_loss: 207.70870971679688\n",
            "Training epoch 7844/1000000, d_loss: -154.28469848632812,  g_loss: 269.2423095703125\n",
            "Training epoch 7845/1000000, d_loss: -293.1000671386719,  g_loss: 485.4393310546875\n",
            "Training epoch 7846/1000000, d_loss: -109.67630004882812,  g_loss: 196.98501586914062\n",
            "Training epoch 7847/1000000, d_loss: 40.143402099609375,  g_loss: -3.8656558990478516\n",
            "Training epoch 7848/1000000, d_loss: -38.02716064453125,  g_loss: 24.735252380371094\n",
            "Training epoch 7849/1000000, d_loss: -82.64466857910156,  g_loss: 43.244754791259766\n",
            "Training epoch 7850/1000000, d_loss: -65.27117919921875,  g_loss: 104.04518127441406\n",
            "Training epoch 7851/1000000, d_loss: -264.20037841796875,  g_loss: 21.616371154785156\n",
            "Training epoch 7852/1000000, d_loss: -90.07919311523438,  g_loss: 41.64435958862305\n",
            "Training epoch 7853/1000000, d_loss: -288.27154541015625,  g_loss: 83.59498596191406\n",
            "Training epoch 7854/1000000, d_loss: -50.52440643310547,  g_loss: -10.276518821716309\n",
            "Training epoch 7855/1000000, d_loss: -90.79863739013672,  g_loss: 70.91510772705078\n",
            "Training epoch 7856/1000000, d_loss: -22.27787208557129,  g_loss: 41.14649200439453\n",
            "Training epoch 7857/1000000, d_loss: -35.19253921508789,  g_loss: 80.53303527832031\n",
            "Training epoch 7858/1000000, d_loss: -197.3336944580078,  g_loss: -17.4508056640625\n",
            "Training epoch 7859/1000000, d_loss: -161.99774169921875,  g_loss: -12.576613426208496\n",
            "Training epoch 7860/1000000, d_loss: -0.052234649658203125,  g_loss: 53.125877380371094\n",
            "Training epoch 7861/1000000, d_loss: -32.87907791137695,  g_loss: 38.67113494873047\n",
            "Training epoch 7862/1000000, d_loss: -135.58285522460938,  g_loss: 1.661747932434082\n",
            "Training epoch 7863/1000000, d_loss: -39.23859405517578,  g_loss: 41.35185241699219\n",
            "Training epoch 7864/1000000, d_loss: -155.20126342773438,  g_loss: -11.692991256713867\n",
            "Training epoch 7865/1000000, d_loss: -646.5377197265625,  g_loss: -201.463623046875\n",
            "Training epoch 7866/1000000, d_loss: -148.70620727539062,  g_loss: -14.307425498962402\n",
            "Training epoch 7867/1000000, d_loss: -125.56590270996094,  g_loss: 66.02820587158203\n",
            "Training epoch 7868/1000000, d_loss: 259.81329345703125,  g_loss: 125.72337341308594\n",
            "Training epoch 7869/1000000, d_loss: -208.58912658691406,  g_loss: 153.4897003173828\n",
            "Training epoch 7870/1000000, d_loss: -91.2442626953125,  g_loss: 116.21845245361328\n",
            "Training epoch 7871/1000000, d_loss: -68.8721694946289,  g_loss: 93.73097229003906\n",
            "Training epoch 7872/1000000, d_loss: -112.83206176757812,  g_loss: 72.26422119140625\n",
            "Training epoch 7873/1000000, d_loss: -226.0465087890625,  g_loss: -37.24678421020508\n",
            "Training epoch 7874/1000000, d_loss: -194.23455810546875,  g_loss: -15.97351360321045\n",
            "Training epoch 7875/1000000, d_loss: -23.906044006347656,  g_loss: 14.08647346496582\n",
            "Training epoch 7876/1000000, d_loss: -64.38253021240234,  g_loss: -19.940879821777344\n",
            "Training epoch 7877/1000000, d_loss: 5.995536804199219,  g_loss: -22.436124801635742\n",
            "Training epoch 7878/1000000, d_loss: -29.142677307128906,  g_loss: 9.701754570007324\n",
            "Training epoch 7879/1000000, d_loss: -317.74285888671875,  g_loss: 5.64741849899292\n",
            "Training epoch 7880/1000000, d_loss: -25.240074157714844,  g_loss: 56.564144134521484\n",
            "Training epoch 7881/1000000, d_loss: -146.42555236816406,  g_loss: -3.712888240814209\n",
            "Training epoch 7882/1000000, d_loss: -122.02529907226562,  g_loss: 25.43134307861328\n",
            "Training epoch 7883/1000000, d_loss: -87.09541320800781,  g_loss: -16.344600677490234\n",
            "Training epoch 7884/1000000, d_loss: -105.37583923339844,  g_loss: -3.2174606323242188\n",
            "Training epoch 7885/1000000, d_loss: -53.9540901184082,  g_loss: 24.918725967407227\n",
            "Training epoch 7886/1000000, d_loss: -395.407470703125,  g_loss: -82.4776611328125\n",
            "Training epoch 7887/1000000, d_loss: -147.03915405273438,  g_loss: -58.455665588378906\n",
            "Training epoch 7888/1000000, d_loss: -219.11541748046875,  g_loss: -49.3983154296875\n",
            "Training epoch 7889/1000000, d_loss: -495.8148193359375,  g_loss: -214.7261199951172\n",
            "Training epoch 7890/1000000, d_loss: -64.20681762695312,  g_loss: -13.713171005249023\n",
            "Training epoch 7891/1000000, d_loss: -27.1298828125,  g_loss: 100.3388442993164\n",
            "Training epoch 7892/1000000, d_loss: -137.97998046875,  g_loss: 21.000558853149414\n",
            "Training epoch 7893/1000000, d_loss: -17.549606323242188,  g_loss: 74.76521301269531\n",
            "Training epoch 7894/1000000, d_loss: 756.1201782226562,  g_loss: 115.41692352294922\n",
            "Training epoch 7895/1000000, d_loss: -48.94305419921875,  g_loss: 119.90215301513672\n",
            "Training epoch 7896/1000000, d_loss: -41.084598541259766,  g_loss: 122.06857299804688\n",
            "Training epoch 7897/1000000, d_loss: -16.217283248901367,  g_loss: 93.64115142822266\n",
            "Training epoch 7898/1000000, d_loss: -97.21514129638672,  g_loss: 118.6269302368164\n",
            "Training epoch 7899/1000000, d_loss: 20.435409545898438,  g_loss: 83.99264526367188\n",
            "Training epoch 7900/1000000, d_loss: -113.43547821044922,  g_loss: 149.1649169921875\n",
            "Training epoch 7901/1000000, d_loss: -144.90380859375,  g_loss: 104.27994537353516\n",
            "Training epoch 7902/1000000, d_loss: -235.89666748046875,  g_loss: 61.49870300292969\n",
            "Training epoch 7903/1000000, d_loss: -115.85700988769531,  g_loss: 149.35787963867188\n",
            "Training epoch 7904/1000000, d_loss: -59.834964752197266,  g_loss: 124.12630462646484\n",
            "Training epoch 7905/1000000, d_loss: -33.83171081542969,  g_loss: 111.67008209228516\n",
            "Training epoch 7906/1000000, d_loss: -817.7274169921875,  g_loss: 39.227325439453125\n",
            "Training epoch 7907/1000000, d_loss: -199.18487548828125,  g_loss: -13.190841674804688\n",
            "Training epoch 7908/1000000, d_loss: -53.02174758911133,  g_loss: 158.24581909179688\n",
            "Training epoch 7909/1000000, d_loss: -130.69384765625,  g_loss: 427.96600341796875\n",
            "Training epoch 7910/1000000, d_loss: -176.84336853027344,  g_loss: 590.2055053710938\n",
            "Training epoch 7911/1000000, d_loss: -176.66156005859375,  g_loss: 317.16339111328125\n",
            "Training epoch 7912/1000000, d_loss: -131.70465087890625,  g_loss: 156.06629943847656\n",
            "Training epoch 7913/1000000, d_loss: -89.67755126953125,  g_loss: 6.704418182373047\n",
            "Training epoch 7914/1000000, d_loss: -54.77228546142578,  g_loss: 136.88104248046875\n",
            "Training epoch 7915/1000000, d_loss: -182.69302368164062,  g_loss: 163.426025390625\n",
            "Training epoch 7916/1000000, d_loss: -960.3711547851562,  g_loss: -74.03419494628906\n",
            "Training epoch 7917/1000000, d_loss: -1.7947769165039062,  g_loss: 15.068477630615234\n",
            "Training epoch 7918/1000000, d_loss: -138.47671508789062,  g_loss: 140.31597900390625\n",
            "Training epoch 7919/1000000, d_loss: -44.596805572509766,  g_loss: 34.57756042480469\n",
            "Training epoch 7920/1000000, d_loss: -139.57725524902344,  g_loss: 244.07745361328125\n",
            "Training epoch 7921/1000000, d_loss: -70.53868103027344,  g_loss: 48.02204895019531\n",
            "Training epoch 7922/1000000, d_loss: -30.44010353088379,  g_loss: 62.487632751464844\n",
            "Training epoch 7923/1000000, d_loss: -336.37139892578125,  g_loss: -19.668760299682617\n",
            "Training epoch 7924/1000000, d_loss: -3.6566390991210938,  g_loss: 81.00692749023438\n",
            "Training epoch 7925/1000000, d_loss: -47.16203308105469,  g_loss: 100.14486694335938\n",
            "Training epoch 7926/1000000, d_loss: -189.36300659179688,  g_loss: 35.613059997558594\n",
            "Training epoch 7927/1000000, d_loss: -41.38682174682617,  g_loss: 64.66923522949219\n",
            "Training epoch 7928/1000000, d_loss: -46.73307418823242,  g_loss: 52.93635559082031\n",
            "Training epoch 7929/1000000, d_loss: -108.56098175048828,  g_loss: 90.40423583984375\n",
            "Training epoch 7930/1000000, d_loss: -466.181884765625,  g_loss: -170.94451904296875\n",
            "Training epoch 7931/1000000, d_loss: 5.9340057373046875,  g_loss: 122.32230377197266\n",
            "Training epoch 7932/1000000, d_loss: -142.66859436035156,  g_loss: 210.5052947998047\n",
            "Training epoch 7933/1000000, d_loss: -125.86672973632812,  g_loss: 178.49871826171875\n",
            "Training epoch 7934/1000000, d_loss: -150.17445373535156,  g_loss: 231.5408172607422\n",
            "Training epoch 7935/1000000, d_loss: -253.61456298828125,  g_loss: 357.14251708984375\n",
            "Training epoch 7936/1000000, d_loss: -71.18637084960938,  g_loss: 45.268062591552734\n",
            "Training epoch 7937/1000000, d_loss: -147.23117065429688,  g_loss: 40.85704040527344\n",
            "Training epoch 7938/1000000, d_loss: -118.63265991210938,  g_loss: 2.716371536254883\n",
            "Training epoch 7939/1000000, d_loss: -1002.7957153320312,  g_loss: -157.96168518066406\n",
            "Training epoch 7940/1000000, d_loss: 41.9154052734375,  g_loss: -10.349842071533203\n",
            "Training epoch 7941/1000000, d_loss: -56.11541748046875,  g_loss: 71.62713623046875\n",
            "Training epoch 7942/1000000, d_loss: -830.0172729492188,  g_loss: -156.31585693359375\n",
            "Training epoch 7943/1000000, d_loss: 942.5401611328125,  g_loss: 35.436241149902344\n",
            "Training epoch 7944/1000000, d_loss: -60.02507019042969,  g_loss: 43.57436752319336\n",
            "Training epoch 7945/1000000, d_loss: -262.60552978515625,  g_loss: -1.4213547706604004\n",
            "Training epoch 7946/1000000, d_loss: -27.460674285888672,  g_loss: 9.871561050415039\n",
            "Training epoch 7947/1000000, d_loss: -196.37075805664062,  g_loss: 40.82358169555664\n",
            "Training epoch 7948/1000000, d_loss: -206.49192810058594,  g_loss: 113.57870483398438\n",
            "Training epoch 7949/1000000, d_loss: -53.62201690673828,  g_loss: 54.77266311645508\n",
            "Training epoch 7950/1000000, d_loss: -639.8196411132812,  g_loss: -217.5913848876953\n",
            "Training epoch 7951/1000000, d_loss: -75.49507904052734,  g_loss: 245.7371826171875\n",
            "Training epoch 7952/1000000, d_loss: 676.8392333984375,  g_loss: -27.46764373779297\n",
            "Training epoch 7953/1000000, d_loss: -68.48800659179688,  g_loss: -19.79749870300293\n",
            "Training epoch 7954/1000000, d_loss: -27.5533504486084,  g_loss: 61.219398498535156\n",
            "Training epoch 7955/1000000, d_loss: -79.68170166015625,  g_loss: -13.121084213256836\n",
            "Training epoch 7956/1000000, d_loss: -142.98866271972656,  g_loss: 37.77045440673828\n",
            "Training epoch 7957/1000000, d_loss: -427.43975830078125,  g_loss: -28.71483039855957\n",
            "Training epoch 7958/1000000, d_loss: -163.3595733642578,  g_loss: 27.477596282958984\n",
            "Training epoch 7959/1000000, d_loss: -226.97732543945312,  g_loss: 86.56957244873047\n",
            "Training epoch 7960/1000000, d_loss: -579.9832763671875,  g_loss: -136.76641845703125\n",
            "Training epoch 7961/1000000, d_loss: 210.22195434570312,  g_loss: 2.5598316192626953\n",
            "Training epoch 7962/1000000, d_loss: -78.07009887695312,  g_loss: 30.96547508239746\n",
            "Training epoch 7963/1000000, d_loss: 17.230941772460938,  g_loss: 35.37550354003906\n",
            "Training epoch 7964/1000000, d_loss: -86.79047393798828,  g_loss: 89.05224609375\n",
            "Training epoch 7965/1000000, d_loss: -107.45230102539062,  g_loss: 98.5624008178711\n",
            "Training epoch 7966/1000000, d_loss: -513.1475830078125,  g_loss: 15.330978393554688\n",
            "Training epoch 7967/1000000, d_loss: -207.78282165527344,  g_loss: 48.01972198486328\n",
            "Training epoch 7968/1000000, d_loss: -434.51092529296875,  g_loss: -11.513509750366211\n",
            "Training epoch 7969/1000000, d_loss: -5.68018913269043,  g_loss: 148.8416290283203\n",
            "Training epoch 7970/1000000, d_loss: -259.571533203125,  g_loss: 148.3194580078125\n",
            "Training epoch 7971/1000000, d_loss: -123.5569076538086,  g_loss: 147.5432586669922\n",
            "Training epoch 7972/1000000, d_loss: -123.85086822509766,  g_loss: 301.38848876953125\n",
            "Training epoch 7973/1000000, d_loss: -166.68359375,  g_loss: -7.041704177856445\n",
            "Training epoch 7974/1000000, d_loss: -960.6328125,  g_loss: -966.1795043945312\n",
            "Training epoch 7975/1000000, d_loss: -356.819091796875,  g_loss: -563.2247314453125\n",
            "Training epoch 7976/1000000, d_loss: 48.64888000488281,  g_loss: 82.48992156982422\n",
            "Training epoch 7977/1000000, d_loss: 90.31887817382812,  g_loss: 178.48118591308594\n",
            "Training epoch 7978/1000000, d_loss: -74.4366455078125,  g_loss: 229.09823608398438\n",
            "Training epoch 7979/1000000, d_loss: -85.03016662597656,  g_loss: 322.6854248046875\n",
            "Training epoch 7980/1000000, d_loss: -95.52799987792969,  g_loss: 97.55375671386719\n",
            "Training epoch 7981/1000000, d_loss: -167.03427124023438,  g_loss: 345.01641845703125\n",
            "Training epoch 7982/1000000, d_loss: -103.94696044921875,  g_loss: 139.19961547851562\n",
            "Training epoch 7983/1000000, d_loss: -92.8364486694336,  g_loss: 162.48794555664062\n",
            "Training epoch 7984/1000000, d_loss: -119.90733337402344,  g_loss: 85.0433349609375\n",
            "Training epoch 7985/1000000, d_loss: 152.690673828125,  g_loss: 111.94835662841797\n",
            "Training epoch 7986/1000000, d_loss: -50.65582275390625,  g_loss: 135.92233276367188\n",
            "Training epoch 7987/1000000, d_loss: -204.53726196289062,  g_loss: 213.18377685546875\n",
            "Training epoch 7988/1000000, d_loss: -57.780513763427734,  g_loss: 77.5610122680664\n",
            "Training epoch 7989/1000000, d_loss: -74.25701141357422,  g_loss: 60.02625274658203\n",
            "Training epoch 7990/1000000, d_loss: -137.32933044433594,  g_loss: 39.420162200927734\n",
            "Training epoch 7991/1000000, d_loss: -32.85513687133789,  g_loss: 12.327960968017578\n",
            "Training epoch 7992/1000000, d_loss: -283.5461730957031,  g_loss: -56.158626556396484\n",
            "Training epoch 7993/1000000, d_loss: -113.8858642578125,  g_loss: 312.85943603515625\n",
            "Training epoch 7994/1000000, d_loss: -205.74070739746094,  g_loss: 340.85601806640625\n",
            "Training epoch 7995/1000000, d_loss: -11.722179412841797,  g_loss: 107.09168243408203\n",
            "Training epoch 7996/1000000, d_loss: -52.06357192993164,  g_loss: 116.73438262939453\n",
            "Training epoch 7997/1000000, d_loss: -486.400390625,  g_loss: -78.33624267578125\n",
            "Training epoch 7998/1000000, d_loss: -70.58683013916016,  g_loss: 178.2531280517578\n",
            "Training epoch 7999/1000000, d_loss: -91.85104370117188,  g_loss: 128.1821746826172\n",
            "Training epoch 8000/1000000, d_loss: -143.0045166015625,  g_loss: 71.12004089355469\n",
            "Training epoch 8001/1000000, d_loss: -31.921104431152344,  g_loss: 48.15981674194336\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 46/46 [00:00<00:00, 738.60it/s]\n",
            "Meshing: 100%|██████████| 5445/5445 [00:00<00:00, 5517.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_8001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_8001/assets\n",
            "Training epoch 8002/1000000, d_loss: -75.44586181640625,  g_loss: 48.383934020996094\n",
            "Training epoch 8003/1000000, d_loss: -22.22894287109375,  g_loss: 50.34117889404297\n",
            "Training epoch 8004/1000000, d_loss: -127.96621704101562,  g_loss: 42.5876350402832\n",
            "Training epoch 8005/1000000, d_loss: -64.6435317993164,  g_loss: 65.51936340332031\n",
            "Training epoch 8006/1000000, d_loss: -97.25424194335938,  g_loss: 120.61441040039062\n",
            "Training epoch 8007/1000000, d_loss: -299.1024475097656,  g_loss: -10.069415092468262\n",
            "Training epoch 8008/1000000, d_loss: -189.4789276123047,  g_loss: 103.49089050292969\n",
            "Training epoch 8009/1000000, d_loss: 30.705015182495117,  g_loss: 128.39361572265625\n",
            "Training epoch 8010/1000000, d_loss: -110.25477600097656,  g_loss: 115.46298217773438\n",
            "Training epoch 8011/1000000, d_loss: -139.2535400390625,  g_loss: 130.1271514892578\n",
            "Training epoch 8012/1000000, d_loss: -133.97454833984375,  g_loss: 76.96151733398438\n",
            "Training epoch 8013/1000000, d_loss: -167.68484497070312,  g_loss: 20.359893798828125\n",
            "Training epoch 8014/1000000, d_loss: -69.17476654052734,  g_loss: 23.873716354370117\n",
            "Training epoch 8015/1000000, d_loss: -77.69477844238281,  g_loss: 123.4122543334961\n",
            "Training epoch 8016/1000000, d_loss: -78.44792175292969,  g_loss: 127.76573181152344\n",
            "Training epoch 8017/1000000, d_loss: -117.41874694824219,  g_loss: 222.78851318359375\n",
            "Training epoch 8018/1000000, d_loss: -381.67779541015625,  g_loss: -34.22154998779297\n",
            "Training epoch 8019/1000000, d_loss: -24.88836669921875,  g_loss: -49.33304214477539\n",
            "Training epoch 8020/1000000, d_loss: -93.944091796875,  g_loss: 103.86483764648438\n",
            "Training epoch 8021/1000000, d_loss: -137.04534912109375,  g_loss: 58.81757354736328\n",
            "Training epoch 8022/1000000, d_loss: -51.33990478515625,  g_loss: 27.72848892211914\n",
            "Training epoch 8023/1000000, d_loss: -63.13025665283203,  g_loss: 49.332942962646484\n",
            "Training epoch 8024/1000000, d_loss: -46.377655029296875,  g_loss: 31.420787811279297\n",
            "Training epoch 8025/1000000, d_loss: -46.403594970703125,  g_loss: 27.425832748413086\n",
            "Training epoch 8026/1000000, d_loss: -37.45441818237305,  g_loss: 36.39359664916992\n",
            "Training epoch 8027/1000000, d_loss: -94.52326202392578,  g_loss: -19.837671279907227\n",
            "Training epoch 8028/1000000, d_loss: -745.0154418945312,  g_loss: -25.48874282836914\n",
            "Training epoch 8029/1000000, d_loss: -3533.184326171875,  g_loss: -296.1590881347656\n",
            "Training epoch 8030/1000000, d_loss: 367.1882019042969,  g_loss: -321.4759521484375\n",
            "Training epoch 8031/1000000, d_loss: 172.02615356445312,  g_loss: -213.7019805908203\n",
            "Training epoch 8032/1000000, d_loss: 44.51411437988281,  g_loss: -213.01846313476562\n",
            "Training epoch 8033/1000000, d_loss: 528.1473999023438,  g_loss: -254.93505859375\n",
            "Training epoch 8034/1000000, d_loss: 158.32778930664062,  g_loss: -106.25804138183594\n",
            "Training epoch 8035/1000000, d_loss: 91.0833511352539,  g_loss: 13.457878112792969\n",
            "Training epoch 8036/1000000, d_loss: -393.6597595214844,  g_loss: 5.350865840911865\n",
            "Training epoch 8037/1000000, d_loss: -227.75534057617188,  g_loss: 457.3330383300781\n",
            "Training epoch 8038/1000000, d_loss: -150.37612915039062,  g_loss: 203.9927520751953\n",
            "Training epoch 8039/1000000, d_loss: -385.85614013671875,  g_loss: 391.9088134765625\n",
            "Training epoch 8040/1000000, d_loss: -371.7774963378906,  g_loss: 332.16876220703125\n",
            "Training epoch 8041/1000000, d_loss: -239.58285522460938,  g_loss: 191.70225524902344\n",
            "Training epoch 8042/1000000, d_loss: -119.5723876953125,  g_loss: 55.05329513549805\n",
            "Training epoch 8043/1000000, d_loss: -38.01801300048828,  g_loss: 100.60942077636719\n",
            "Training epoch 8044/1000000, d_loss: -204.036865234375,  g_loss: 69.26119232177734\n",
            "Training epoch 8045/1000000, d_loss: -180.52838134765625,  g_loss: -21.199867248535156\n",
            "Training epoch 8046/1000000, d_loss: -61.45149230957031,  g_loss: 60.878211975097656\n",
            "Training epoch 8047/1000000, d_loss: -101.42815399169922,  g_loss: 22.08611488342285\n",
            "Training epoch 8048/1000000, d_loss: -88.04841613769531,  g_loss: 103.25692749023438\n",
            "Training epoch 8049/1000000, d_loss: 43.58668518066406,  g_loss: -6.566196441650391\n",
            "Training epoch 8050/1000000, d_loss: -233.48361206054688,  g_loss: -92.85601806640625\n",
            "Training epoch 8051/1000000, d_loss: -346.69097900390625,  g_loss: -102.79976654052734\n",
            "Training epoch 8052/1000000, d_loss: 3451.550048828125,  g_loss: 102.41797637939453\n",
            "Training epoch 8053/1000000, d_loss: 23.477832794189453,  g_loss: 83.36235809326172\n",
            "Training epoch 8054/1000000, d_loss: -94.29620361328125,  g_loss: 72.58614349365234\n",
            "Training epoch 8055/1000000, d_loss: -100.93905639648438,  g_loss: 118.24325561523438\n",
            "Training epoch 8056/1000000, d_loss: 23.035552978515625,  g_loss: 83.08731842041016\n",
            "Training epoch 8057/1000000, d_loss: -99.7457046508789,  g_loss: 100.97706604003906\n",
            "Training epoch 8058/1000000, d_loss: -271.9383544921875,  g_loss: 10.332010269165039\n",
            "Training epoch 8059/1000000, d_loss: -92.37792205810547,  g_loss: 53.27033615112305\n",
            "Training epoch 8060/1000000, d_loss: 155.0675048828125,  g_loss: 107.41584777832031\n",
            "Training epoch 8061/1000000, d_loss: 4.823886871337891,  g_loss: 112.72259521484375\n",
            "Training epoch 8062/1000000, d_loss: -82.09723663330078,  g_loss: 117.34564208984375\n",
            "Training epoch 8063/1000000, d_loss: -247.1396026611328,  g_loss: 144.3002166748047\n",
            "Training epoch 8064/1000000, d_loss: -82.03312683105469,  g_loss: 45.38773727416992\n",
            "Training epoch 8065/1000000, d_loss: -20.57587432861328,  g_loss: 32.43354797363281\n",
            "Training epoch 8066/1000000, d_loss: -93.5987548828125,  g_loss: 9.241373062133789\n",
            "Training epoch 8067/1000000, d_loss: -1035.5322265625,  g_loss: -96.46477508544922\n",
            "Training epoch 8068/1000000, d_loss: 38.03692626953125,  g_loss: -10.325334548950195\n",
            "Training epoch 8069/1000000, d_loss: -12.39154052734375,  g_loss: 97.32891845703125\n",
            "Training epoch 8070/1000000, d_loss: -66.88977813720703,  g_loss: 109.98760223388672\n",
            "Training epoch 8071/1000000, d_loss: -150.78012084960938,  g_loss: 53.20762634277344\n",
            "Training epoch 8072/1000000, d_loss: -64.17123413085938,  g_loss: 145.59854125976562\n",
            "Training epoch 8073/1000000, d_loss: -96.81879425048828,  g_loss: 70.18262481689453\n",
            "Training epoch 8074/1000000, d_loss: -166.57327270507812,  g_loss: 60.15595626831055\n",
            "Training epoch 8075/1000000, d_loss: -193.642333984375,  g_loss: -21.63169288635254\n",
            "Training epoch 8076/1000000, d_loss: -259.46502685546875,  g_loss: 196.00013732910156\n",
            "Training epoch 8077/1000000, d_loss: -14.57712173461914,  g_loss: 14.106167793273926\n",
            "Training epoch 8078/1000000, d_loss: -86.46681213378906,  g_loss: 42.25774383544922\n",
            "Training epoch 8079/1000000, d_loss: -46.22300720214844,  g_loss: 140.7366943359375\n",
            "Training epoch 8080/1000000, d_loss: -69.91539001464844,  g_loss: 97.65010070800781\n",
            "Training epoch 8081/1000000, d_loss: -229.98605346679688,  g_loss: -12.087005615234375\n",
            "Training epoch 8082/1000000, d_loss: -129.27804565429688,  g_loss: 252.9708709716797\n",
            "Training epoch 8083/1000000, d_loss: -64.23160552978516,  g_loss: 230.18370056152344\n",
            "Training epoch 8084/1000000, d_loss: -226.78591918945312,  g_loss: 93.55227661132812\n",
            "Training epoch 8085/1000000, d_loss: -154.54226684570312,  g_loss: 63.6487922668457\n",
            "Training epoch 8086/1000000, d_loss: -157.01483154296875,  g_loss: -2.274181365966797\n",
            "Training epoch 8087/1000000, d_loss: -489.3067626953125,  g_loss: -66.91205596923828\n",
            "Training epoch 8088/1000000, d_loss: 1.862335205078125,  g_loss: 57.14338302612305\n",
            "Training epoch 8089/1000000, d_loss: -127.98368835449219,  g_loss: 31.39197540283203\n",
            "Training epoch 8090/1000000, d_loss: -1502.5667724609375,  g_loss: -39.28435516357422\n",
            "Training epoch 8091/1000000, d_loss: -85.9119644165039,  g_loss: -120.76753234863281\n",
            "Training epoch 8092/1000000, d_loss: -289.0794982910156,  g_loss: 6.044471740722656\n",
            "Training epoch 8093/1000000, d_loss: 75.45470428466797,  g_loss: 201.74195861816406\n",
            "Training epoch 8094/1000000, d_loss: -951.0596923828125,  g_loss: 1.3245878219604492\n",
            "Training epoch 8095/1000000, d_loss: -54.65840530395508,  g_loss: 174.16934204101562\n",
            "Training epoch 8096/1000000, d_loss: -183.3873291015625,  g_loss: 352.5870361328125\n",
            "Training epoch 8097/1000000, d_loss: -229.808349609375,  g_loss: 62.425106048583984\n",
            "Training epoch 8098/1000000, d_loss: -65.89784240722656,  g_loss: 245.29409790039062\n",
            "Training epoch 8099/1000000, d_loss: -295.8530578613281,  g_loss: 142.79942321777344\n",
            "Training epoch 8100/1000000, d_loss: -96.58494567871094,  g_loss: 168.48297119140625\n",
            "Training epoch 8101/1000000, d_loss: -165.60665893554688,  g_loss: 101.83909606933594\n",
            "Training epoch 8102/1000000, d_loss: -455.7816467285156,  g_loss: 777.345458984375\n",
            "Training epoch 8103/1000000, d_loss: -121.97956848144531,  g_loss: 139.5426025390625\n",
            "Training epoch 8104/1000000, d_loss: -167.9752960205078,  g_loss: -16.1475830078125\n",
            "Training epoch 8105/1000000, d_loss: 3.3810272216796875,  g_loss: 74.4349365234375\n",
            "Training epoch 8106/1000000, d_loss: -126.4847412109375,  g_loss: 113.96929931640625\n",
            "Training epoch 8107/1000000, d_loss: -353.522705078125,  g_loss: -104.98399353027344\n",
            "Training epoch 8108/1000000, d_loss: -65.40755462646484,  g_loss: 75.51789855957031\n",
            "Training epoch 8109/1000000, d_loss: -1.8523597717285156,  g_loss: 142.31787109375\n",
            "Training epoch 8110/1000000, d_loss: -28.881168365478516,  g_loss: 74.53662109375\n",
            "Training epoch 8111/1000000, d_loss: -955.6148681640625,  g_loss: 2.2521371841430664\n",
            "Training epoch 8112/1000000, d_loss: 29.510154724121094,  g_loss: 79.25011444091797\n",
            "Training epoch 8113/1000000, d_loss: 2.3750839233398438,  g_loss: 18.062992095947266\n",
            "Training epoch 8114/1000000, d_loss: -684.7443237304688,  g_loss: -181.2427520751953\n",
            "Training epoch 8115/1000000, d_loss: -109.09487915039062,  g_loss: -84.01099395751953\n",
            "Training epoch 8116/1000000, d_loss: -321.8775634765625,  g_loss: 637.1365966796875\n",
            "Training epoch 8117/1000000, d_loss: -315.5847473144531,  g_loss: 11.602005958557129\n",
            "Training epoch 8118/1000000, d_loss: -56.70518493652344,  g_loss: 234.9430694580078\n",
            "Training epoch 8119/1000000, d_loss: -90.99849700927734,  g_loss: 135.21127319335938\n",
            "Training epoch 8120/1000000, d_loss: -73.11802673339844,  g_loss: 154.89389038085938\n",
            "Training epoch 8121/1000000, d_loss: 41.87586212158203,  g_loss: 84.01640319824219\n",
            "Training epoch 8122/1000000, d_loss: -148.0763702392578,  g_loss: 299.96051025390625\n",
            "Training epoch 8123/1000000, d_loss: -9.977298736572266,  g_loss: 33.968360900878906\n",
            "Training epoch 8124/1000000, d_loss: -84.00373840332031,  g_loss: 49.49699401855469\n",
            "Training epoch 8125/1000000, d_loss: -59.71894836425781,  g_loss: 139.3086395263672\n",
            "Training epoch 8126/1000000, d_loss: -376.2483825683594,  g_loss: -159.0486602783203\n",
            "Training epoch 8127/1000000, d_loss: -93.2034912109375,  g_loss: 127.70836639404297\n",
            "Training epoch 8128/1000000, d_loss: -258.5814208984375,  g_loss: 37.665916442871094\n",
            "Training epoch 8129/1000000, d_loss: -34.45270538330078,  g_loss: 62.7086296081543\n",
            "Training epoch 8130/1000000, d_loss: -255.1119384765625,  g_loss: 49.87092208862305\n",
            "Training epoch 8131/1000000, d_loss: -475.9974365234375,  g_loss: -144.61727905273438\n",
            "Training epoch 8132/1000000, d_loss: -95.83412170410156,  g_loss: 25.131174087524414\n",
            "Training epoch 8133/1000000, d_loss: -78.47303009033203,  g_loss: 22.939918518066406\n",
            "Training epoch 8134/1000000, d_loss: -228.44366455078125,  g_loss: 268.447998046875\n",
            "Training epoch 8135/1000000, d_loss: -59.79779815673828,  g_loss: -13.447218894958496\n",
            "Training epoch 8136/1000000, d_loss: -89.059814453125,  g_loss: 18.938840866088867\n",
            "Training epoch 8137/1000000, d_loss: -178.82833862304688,  g_loss: -51.36705017089844\n",
            "Training epoch 8138/1000000, d_loss: -108.80555725097656,  g_loss: 78.1094741821289\n",
            "Training epoch 8139/1000000, d_loss: -1496.993896484375,  g_loss: -187.6369171142578\n",
            "Training epoch 8140/1000000, d_loss: 23.123306274414062,  g_loss: 45.90733337402344\n",
            "Training epoch 8141/1000000, d_loss: -208.18890380859375,  g_loss: 17.29352378845215\n",
            "Training epoch 8142/1000000, d_loss: -55.212974548339844,  g_loss: 43.954490661621094\n",
            "Training epoch 8143/1000000, d_loss: -145.631103515625,  g_loss: -11.394688606262207\n",
            "Training epoch 8144/1000000, d_loss: -102.53460693359375,  g_loss: 175.84295654296875\n",
            "Training epoch 8145/1000000, d_loss: -13.419307708740234,  g_loss: 68.60685729980469\n",
            "Training epoch 8146/1000000, d_loss: -88.09188842773438,  g_loss: 52.2022705078125\n",
            "Training epoch 8147/1000000, d_loss: -56.67115783691406,  g_loss: 35.891021728515625\n",
            "Training epoch 8148/1000000, d_loss: -112.45608520507812,  g_loss: 91.11679077148438\n",
            "Training epoch 8149/1000000, d_loss: -51.778324127197266,  g_loss: 29.63496971130371\n",
            "Training epoch 8150/1000000, d_loss: -47.40473937988281,  g_loss: 9.959495544433594\n",
            "Training epoch 8151/1000000, d_loss: 176.6508026123047,  g_loss: 48.26824188232422\n",
            "Training epoch 8152/1000000, d_loss: -122.99186706542969,  g_loss: -11.166805267333984\n",
            "Training epoch 8153/1000000, d_loss: -288.45819091796875,  g_loss: 211.5050048828125\n",
            "Training epoch 8154/1000000, d_loss: -109.06915283203125,  g_loss: 98.49771881103516\n",
            "Training epoch 8155/1000000, d_loss: -122.3028335571289,  g_loss: 92.38856506347656\n",
            "Training epoch 8156/1000000, d_loss: -83.98249053955078,  g_loss: 51.66863250732422\n",
            "Training epoch 8157/1000000, d_loss: -82.50244903564453,  g_loss: 81.49461364746094\n",
            "Training epoch 8158/1000000, d_loss: -71.62367248535156,  g_loss: 72.03874206542969\n",
            "Training epoch 8159/1000000, d_loss: -79.53778076171875,  g_loss: 224.23838806152344\n",
            "Training epoch 8160/1000000, d_loss: -287.6655578613281,  g_loss: -40.6492805480957\n",
            "Training epoch 8161/1000000, d_loss: -36.916709899902344,  g_loss: 103.671875\n",
            "Training epoch 8162/1000000, d_loss: -150.0567626953125,  g_loss: 84.34375\n",
            "Training epoch 8163/1000000, d_loss: -50.837059020996094,  g_loss: -3.4252209663391113\n",
            "Training epoch 8164/1000000, d_loss: 3.7231979370117188,  g_loss: 96.83207702636719\n",
            "Training epoch 8165/1000000, d_loss: -112.45470428466797,  g_loss: 118.3028564453125\n",
            "Training epoch 8166/1000000, d_loss: -105.0206069946289,  g_loss: 116.30726623535156\n",
            "Training epoch 8167/1000000, d_loss: -302.3478088378906,  g_loss: -2.3627848625183105\n",
            "Training epoch 8168/1000000, d_loss: -21.84768295288086,  g_loss: 77.11217498779297\n",
            "Training epoch 8169/1000000, d_loss: -112.3010482788086,  g_loss: 67.47419738769531\n",
            "Training epoch 8170/1000000, d_loss: -1183.8460693359375,  g_loss: -312.207275390625\n",
            "Training epoch 8171/1000000, d_loss: 51.23642349243164,  g_loss: -55.39591979980469\n",
            "Training epoch 8172/1000000, d_loss: 279.3700256347656,  g_loss: -125.31145477294922\n",
            "Training epoch 8173/1000000, d_loss: -25.065109252929688,  g_loss: -78.8862075805664\n",
            "Training epoch 8174/1000000, d_loss: 17.52936553955078,  g_loss: -58.5689582824707\n",
            "Training epoch 8175/1000000, d_loss: -325.3795471191406,  g_loss: 159.1077880859375\n",
            "Training epoch 8176/1000000, d_loss: -96.33463287353516,  g_loss: 24.21569061279297\n",
            "Training epoch 8177/1000000, d_loss: -75.72929382324219,  g_loss: 53.5002326965332\n",
            "Training epoch 8178/1000000, d_loss: -236.4805450439453,  g_loss: -70.15674591064453\n",
            "Training epoch 8179/1000000, d_loss: -183.27806091308594,  g_loss: 27.861328125\n",
            "Training epoch 8180/1000000, d_loss: -33.92737579345703,  g_loss: 104.25080871582031\n",
            "Training epoch 8181/1000000, d_loss: -73.07420349121094,  g_loss: 30.93942642211914\n",
            "Training epoch 8182/1000000, d_loss: -379.8232727050781,  g_loss: 318.4275817871094\n",
            "Training epoch 8183/1000000, d_loss: -81.98538208007812,  g_loss: -27.68183135986328\n",
            "Training epoch 8184/1000000, d_loss: -113.16785430908203,  g_loss: 127.70332336425781\n",
            "Training epoch 8185/1000000, d_loss: -137.5388641357422,  g_loss: 10.921249389648438\n",
            "Training epoch 8186/1000000, d_loss: 618.5367431640625,  g_loss: 165.86001586914062\n",
            "Training epoch 8187/1000000, d_loss: -48.18387222290039,  g_loss: 137.95639038085938\n",
            "Training epoch 8188/1000000, d_loss: -390.5147705078125,  g_loss: 69.52607727050781\n",
            "Training epoch 8189/1000000, d_loss: -40.17051315307617,  g_loss: 120.92601776123047\n",
            "Training epoch 8190/1000000, d_loss: -1115.9710693359375,  g_loss: -207.47247314453125\n",
            "Training epoch 8191/1000000, d_loss: -80.34471130371094,  g_loss: 1.3047676086425781\n",
            "Training epoch 8192/1000000, d_loss: -73.97607421875,  g_loss: 138.61334228515625\n",
            "Training epoch 8193/1000000, d_loss: -182.56613159179688,  g_loss: 41.095428466796875\n",
            "Training epoch 8194/1000000, d_loss: -200.3505859375,  g_loss: 217.04296875\n",
            "Training epoch 8195/1000000, d_loss: -78.51006317138672,  g_loss: 176.0646514892578\n",
            "Training epoch 8196/1000000, d_loss: -583.296142578125,  g_loss: 920.6287231445312\n",
            "Training epoch 8197/1000000, d_loss: -11.556449890136719,  g_loss: 120.35292053222656\n",
            "Training epoch 8198/1000000, d_loss: -69.85560607910156,  g_loss: 171.5994873046875\n",
            "Training epoch 8199/1000000, d_loss: 144.24411010742188,  g_loss: 232.34925842285156\n",
            "Training epoch 8200/1000000, d_loss: -65.3697738647461,  g_loss: 168.66622924804688\n",
            "Training epoch 8201/1000000, d_loss: -103.40130615234375,  g_loss: 279.5852355957031\n",
            "Training epoch 8202/1000000, d_loss: -1191.948974609375,  g_loss: -213.68772888183594\n",
            "Training epoch 8203/1000000, d_loss: 46.18736267089844,  g_loss: 54.910064697265625\n",
            "Training epoch 8204/1000000, d_loss: -1.955850601196289,  g_loss: 83.87680053710938\n",
            "Training epoch 8205/1000000, d_loss: -133.91244506835938,  g_loss: 144.89659118652344\n",
            "Training epoch 8206/1000000, d_loss: -157.78054809570312,  g_loss: 258.84710693359375\n",
            "Training epoch 8207/1000000, d_loss: -119.77971649169922,  g_loss: 176.6776885986328\n",
            "Training epoch 8208/1000000, d_loss: -576.20947265625,  g_loss: 329.38922119140625\n",
            "Training epoch 8209/1000000, d_loss: -89.87132263183594,  g_loss: 106.85234069824219\n",
            "Training epoch 8210/1000000, d_loss: -87.74851989746094,  g_loss: 10.575918197631836\n",
            "Training epoch 8211/1000000, d_loss: -6.56158447265625,  g_loss: 97.42932891845703\n",
            "Training epoch 8212/1000000, d_loss: -136.1248016357422,  g_loss: 85.59310150146484\n",
            "Training epoch 8213/1000000, d_loss: 17.376445770263672,  g_loss: 128.93890380859375\n",
            "Training epoch 8214/1000000, d_loss: -93.32891845703125,  g_loss: 79.70277404785156\n",
            "Training epoch 8215/1000000, d_loss: -126.45417785644531,  g_loss: 81.23812866210938\n",
            "Training epoch 8216/1000000, d_loss: -15.622631072998047,  g_loss: 44.584205627441406\n",
            "Training epoch 8217/1000000, d_loss: -112.2495346069336,  g_loss: 108.26075744628906\n",
            "Training epoch 8218/1000000, d_loss: -372.79730224609375,  g_loss: 27.320453643798828\n",
            "Training epoch 8219/1000000, d_loss: -72.53094482421875,  g_loss: 56.40141677856445\n",
            "Training epoch 8220/1000000, d_loss: -65.64396667480469,  g_loss: 103.99862670898438\n",
            "Training epoch 8221/1000000, d_loss: -58.82627868652344,  g_loss: 75.01190185546875\n",
            "Training epoch 8222/1000000, d_loss: -108.70413208007812,  g_loss: 228.76315307617188\n",
            "Training epoch 8223/1000000, d_loss: -82.20977020263672,  g_loss: 30.628589630126953\n",
            "Training epoch 8224/1000000, d_loss: -97.821044921875,  g_loss: 39.501953125\n",
            "Training epoch 8225/1000000, d_loss: -1762.43017578125,  g_loss: -142.9080047607422\n",
            "Training epoch 8226/1000000, d_loss: -1841.791259765625,  g_loss: -130.60781860351562\n",
            "Training epoch 8227/1000000, d_loss: 1047.409912109375,  g_loss: -50.49055480957031\n",
            "Training epoch 8228/1000000, d_loss: 1974.2965087890625,  g_loss: 123.06625366210938\n",
            "Training epoch 8229/1000000, d_loss: 78.31193542480469,  g_loss: 94.84872436523438\n",
            "Training epoch 8230/1000000, d_loss: -14.83133316040039,  g_loss: 100.61767578125\n",
            "Training epoch 8231/1000000, d_loss: -10.82809066772461,  g_loss: 134.6522979736328\n",
            "Training epoch 8232/1000000, d_loss: -118.54582214355469,  g_loss: 144.65618896484375\n",
            "Training epoch 8233/1000000, d_loss: -155.88626098632812,  g_loss: 159.94796752929688\n",
            "Training epoch 8234/1000000, d_loss: 86.3427963256836,  g_loss: 26.41176986694336\n",
            "Training epoch 8235/1000000, d_loss: -112.74336242675781,  g_loss: 43.2330322265625\n",
            "Training epoch 8236/1000000, d_loss: -146.3209228515625,  g_loss: 201.404541015625\n",
            "Training epoch 8237/1000000, d_loss: -6.3563079833984375,  g_loss: 79.2296371459961\n",
            "Training epoch 8238/1000000, d_loss: -100.83399200439453,  g_loss: -15.381059646606445\n",
            "Training epoch 8239/1000000, d_loss: -92.26831817626953,  g_loss: 37.090675354003906\n",
            "Training epoch 8240/1000000, d_loss: -105.3046646118164,  g_loss: 35.52989959716797\n",
            "Training epoch 8241/1000000, d_loss: -27.618236541748047,  g_loss: 58.39216232299805\n",
            "Training epoch 8242/1000000, d_loss: -60.62572479248047,  g_loss: 9.29958724975586\n",
            "Training epoch 8243/1000000, d_loss: -71.90284729003906,  g_loss: 172.56304931640625\n",
            "Training epoch 8244/1000000, d_loss: -97.47649383544922,  g_loss: 12.568485260009766\n",
            "Training epoch 8245/1000000, d_loss: -488.1839599609375,  g_loss: -203.48818969726562\n",
            "Training epoch 8246/1000000, d_loss: -24.701126098632812,  g_loss: -2.476304054260254\n",
            "Training epoch 8247/1000000, d_loss: -36.58363723754883,  g_loss: 49.04657745361328\n",
            "Training epoch 8248/1000000, d_loss: -108.1600341796875,  g_loss: 14.79038143157959\n",
            "Training epoch 8249/1000000, d_loss: -119.43354797363281,  g_loss: 9.912150382995605\n",
            "Training epoch 8250/1000000, d_loss: -236.21456909179688,  g_loss: 28.12883758544922\n",
            "Training epoch 8251/1000000, d_loss: -756.9795532226562,  g_loss: 19.716693878173828\n",
            "Training epoch 8252/1000000, d_loss: -79.14952087402344,  g_loss: 29.159454345703125\n",
            "Training epoch 8253/1000000, d_loss: -72.35411834716797,  g_loss: 10.912114143371582\n",
            "Training epoch 8254/1000000, d_loss: -119.07784271240234,  g_loss: 7.357126712799072\n",
            "Training epoch 8255/1000000, d_loss: -55.450416564941406,  g_loss: 19.35495376586914\n",
            "Training epoch 8256/1000000, d_loss: -59.45882797241211,  g_loss: 6.777254104614258\n",
            "Training epoch 8257/1000000, d_loss: -299.76568603515625,  g_loss: -87.43329620361328\n",
            "Training epoch 8258/1000000, d_loss: -28.801876068115234,  g_loss: -14.661251068115234\n",
            "Training epoch 8259/1000000, d_loss: -22.696853637695312,  g_loss: 24.522611618041992\n",
            "Training epoch 8260/1000000, d_loss: -94.18441772460938,  g_loss: 40.66663360595703\n",
            "Training epoch 8261/1000000, d_loss: -259.2821350097656,  g_loss: 39.58158874511719\n",
            "Training epoch 8262/1000000, d_loss: -80.80877685546875,  g_loss: 37.07196044921875\n",
            "Training epoch 8263/1000000, d_loss: -24.605083465576172,  g_loss: 50.350120544433594\n",
            "Training epoch 8264/1000000, d_loss: 43.419124603271484,  g_loss: 89.12576293945312\n",
            "Training epoch 8265/1000000, d_loss: -73.11080932617188,  g_loss: 45.09832000732422\n",
            "Training epoch 8266/1000000, d_loss: -63.054779052734375,  g_loss: 128.02313232421875\n",
            "Training epoch 8267/1000000, d_loss: -94.12277221679688,  g_loss: 26.272411346435547\n",
            "Training epoch 8268/1000000, d_loss: -80.91035461425781,  g_loss: 3.8887948989868164\n",
            "Training epoch 8269/1000000, d_loss: -96.71289825439453,  g_loss: 6.73612117767334\n",
            "Training epoch 8270/1000000, d_loss: -181.95948791503906,  g_loss: 258.2580261230469\n",
            "Training epoch 8271/1000000, d_loss: -85.31253814697266,  g_loss: 36.811302185058594\n",
            "Training epoch 8272/1000000, d_loss: -28.311309814453125,  g_loss: 71.06103515625\n",
            "Training epoch 8273/1000000, d_loss: -88.38664245605469,  g_loss: 32.96141052246094\n",
            "Training epoch 8274/1000000, d_loss: -65.03924560546875,  g_loss: 31.460540771484375\n",
            "Training epoch 8275/1000000, d_loss: -71.72187805175781,  g_loss: 85.6804428100586\n",
            "Training epoch 8276/1000000, d_loss: -256.72137451171875,  g_loss: -16.128562927246094\n",
            "Training epoch 8277/1000000, d_loss: -2238.3984375,  g_loss: -172.70901489257812\n",
            "Training epoch 8278/1000000, d_loss: 151.08865356445312,  g_loss: -62.427696228027344\n",
            "Training epoch 8279/1000000, d_loss: -27.391788482666016,  g_loss: -75.3593978881836\n",
            "Training epoch 8280/1000000, d_loss: -77.40554809570312,  g_loss: -57.637474060058594\n",
            "Training epoch 8281/1000000, d_loss: -60.57630920410156,  g_loss: 93.50336456298828\n",
            "Training epoch 8282/1000000, d_loss: -31.402130126953125,  g_loss: -5.0515899658203125\n",
            "Training epoch 8283/1000000, d_loss: -717.9678344726562,  g_loss: 5.561323165893555\n",
            "Training epoch 8284/1000000, d_loss: -586.0970458984375,  g_loss: -237.19029235839844\n",
            "Training epoch 8285/1000000, d_loss: 55.18917465209961,  g_loss: -50.532196044921875\n",
            "Training epoch 8286/1000000, d_loss: -235.33021545410156,  g_loss: -19.704044342041016\n",
            "Training epoch 8287/1000000, d_loss: -210.9071807861328,  g_loss: 87.39908599853516\n",
            "Training epoch 8288/1000000, d_loss: 157.68344116210938,  g_loss: -80.7852554321289\n",
            "Training epoch 8289/1000000, d_loss: -163.6795654296875,  g_loss: -9.503910064697266\n",
            "Training epoch 8290/1000000, d_loss: -200.1375732421875,  g_loss: 234.78338623046875\n",
            "Training epoch 8291/1000000, d_loss: -43.06742858886719,  g_loss: -131.669921875\n",
            "Training epoch 8292/1000000, d_loss: -178.1915740966797,  g_loss: -115.89984130859375\n",
            "Training epoch 8293/1000000, d_loss: -73.27619171142578,  g_loss: -61.62377166748047\n",
            "Training epoch 8294/1000000, d_loss: -154.83811950683594,  g_loss: 27.87893295288086\n",
            "Training epoch 8295/1000000, d_loss: -169.42343139648438,  g_loss: 20.273530960083008\n",
            "Training epoch 8296/1000000, d_loss: -129.97390747070312,  g_loss: 76.54290008544922\n",
            "Training epoch 8297/1000000, d_loss: -107.24115753173828,  g_loss: -3.46826171875\n",
            "Training epoch 8298/1000000, d_loss: -155.68751525878906,  g_loss: -21.263187408447266\n",
            "Training epoch 8299/1000000, d_loss: -157.190185546875,  g_loss: -90.72018432617188\n",
            "Training epoch 8300/1000000, d_loss: 9.579267501831055,  g_loss: -70.2369384765625\n",
            "Training epoch 8301/1000000, d_loss: 81.45335388183594,  g_loss: -41.20964050292969\n",
            "Training epoch 8302/1000000, d_loss: -31.633602142333984,  g_loss: -55.22540283203125\n",
            "Training epoch 8303/1000000, d_loss: -133.82220458984375,  g_loss: 7.569517135620117\n",
            "Training epoch 8304/1000000, d_loss: -84.79360961914062,  g_loss: -26.108129501342773\n",
            "Training epoch 8305/1000000, d_loss: -91.43929290771484,  g_loss: 2.8183116912841797\n",
            "Training epoch 8306/1000000, d_loss: -65.09283447265625,  g_loss: -26.528974533081055\n",
            "Training epoch 8307/1000000, d_loss: -114.70515441894531,  g_loss: -22.46024513244629\n",
            "Training epoch 8308/1000000, d_loss: -72.31552124023438,  g_loss: -85.64856719970703\n",
            "Training epoch 8309/1000000, d_loss: -121.36380767822266,  g_loss: -2.5550804138183594\n",
            "Training epoch 8310/1000000, d_loss: -49.87351989746094,  g_loss: 8.550661087036133\n",
            "Training epoch 8311/1000000, d_loss: -664.1036376953125,  g_loss: -143.27029418945312\n",
            "Training epoch 8312/1000000, d_loss: -216.47723388671875,  g_loss: -164.1519775390625\n",
            "Training epoch 8313/1000000, d_loss: -337.1529846191406,  g_loss: 40.38513946533203\n",
            "Training epoch 8314/1000000, d_loss: -71.66983032226562,  g_loss: 6.311674118041992\n",
            "Training epoch 8315/1000000, d_loss: -23.099023818969727,  g_loss: 69.15670776367188\n",
            "Training epoch 8316/1000000, d_loss: -86.241943359375,  g_loss: -12.28531265258789\n",
            "Training epoch 8317/1000000, d_loss: -163.69729614257812,  g_loss: -35.08479309082031\n",
            "Training epoch 8318/1000000, d_loss: -141.23516845703125,  g_loss: 62.22222137451172\n",
            "Training epoch 8319/1000000, d_loss: -328.4394226074219,  g_loss: -99.16983795166016\n",
            "Training epoch 8320/1000000, d_loss: -99.14470672607422,  g_loss: 122.17784118652344\n",
            "Training epoch 8321/1000000, d_loss: -53.71472930908203,  g_loss: 15.660604476928711\n",
            "Training epoch 8322/1000000, d_loss: -25.892303466796875,  g_loss: 53.766143798828125\n",
            "Training epoch 8323/1000000, d_loss: -96.51502990722656,  g_loss: -0.5258407592773438\n",
            "Training epoch 8324/1000000, d_loss: -146.59706115722656,  g_loss: 413.4894104003906\n",
            "Training epoch 8325/1000000, d_loss: -24.252193450927734,  g_loss: 54.957550048828125\n",
            "Training epoch 8326/1000000, d_loss: -331.0494689941406,  g_loss: -60.087928771972656\n",
            "Training epoch 8327/1000000, d_loss: -65.88652801513672,  g_loss: -5.76841926574707\n",
            "Training epoch 8328/1000000, d_loss: -54.7266845703125,  g_loss: -13.322433471679688\n",
            "Training epoch 8329/1000000, d_loss: 31.73565673828125,  g_loss: 95.46720886230469\n",
            "Training epoch 8330/1000000, d_loss: -307.4476623535156,  g_loss: -5.582256317138672\n",
            "Training epoch 8331/1000000, d_loss: -31.35763931274414,  g_loss: 62.48439407348633\n",
            "Training epoch 8332/1000000, d_loss: -143.22833251953125,  g_loss: 88.20989990234375\n",
            "Training epoch 8333/1000000, d_loss: -68.88294219970703,  g_loss: 34.91414260864258\n",
            "Training epoch 8334/1000000, d_loss: -406.751708984375,  g_loss: 13.571788787841797\n",
            "Training epoch 8335/1000000, d_loss: -6.517995834350586,  g_loss: 93.12409973144531\n",
            "Training epoch 8336/1000000, d_loss: -67.90350341796875,  g_loss: 15.847023010253906\n",
            "Training epoch 8337/1000000, d_loss: -200.2618408203125,  g_loss: 143.25991821289062\n",
            "Training epoch 8338/1000000, d_loss: -47.71538162231445,  g_loss: 114.14527893066406\n",
            "Training epoch 8339/1000000, d_loss: 12.430572509765625,  g_loss: 116.89561462402344\n",
            "Training epoch 8340/1000000, d_loss: -56.34903335571289,  g_loss: 112.48916625976562\n",
            "Training epoch 8341/1000000, d_loss: -335.3399353027344,  g_loss: 55.84282302856445\n",
            "Training epoch 8342/1000000, d_loss: -39.679107666015625,  g_loss: 40.181087493896484\n",
            "Training epoch 8343/1000000, d_loss: -63.88422393798828,  g_loss: 71.70816040039062\n",
            "Training epoch 8344/1000000, d_loss: -28.491798400878906,  g_loss: 39.175941467285156\n",
            "Training epoch 8345/1000000, d_loss: -27.840312957763672,  g_loss: 101.35332489013672\n",
            "Training epoch 8346/1000000, d_loss: -109.98062133789062,  g_loss: 96.00049591064453\n",
            "Training epoch 8347/1000000, d_loss: -150.9505615234375,  g_loss: -21.2023983001709\n",
            "Training epoch 8348/1000000, d_loss: -198.8094024658203,  g_loss: -7.5525360107421875\n",
            "Training epoch 8349/1000000, d_loss: -140.75155639648438,  g_loss: 46.922569274902344\n",
            "Training epoch 8350/1000000, d_loss: -58.75963592529297,  g_loss: 47.64243698120117\n",
            "Training epoch 8351/1000000, d_loss: -212.69139099121094,  g_loss: 78.90406799316406\n",
            "Training epoch 8352/1000000, d_loss: -108.66438293457031,  g_loss: 8.901735305786133\n",
            "Training epoch 8353/1000000, d_loss: -161.851318359375,  g_loss: -34.054359436035156\n",
            "Training epoch 8354/1000000, d_loss: -90.54039001464844,  g_loss: 89.98954772949219\n",
            "Training epoch 8355/1000000, d_loss: -135.2743377685547,  g_loss: 89.47695922851562\n",
            "Training epoch 8356/1000000, d_loss: -16.94974136352539,  g_loss: 115.26318359375\n",
            "Training epoch 8357/1000000, d_loss: -113.88016510009766,  g_loss: 130.90847778320312\n",
            "Training epoch 8358/1000000, d_loss: -54.73948287963867,  g_loss: 25.353897094726562\n",
            "Training epoch 8359/1000000, d_loss: -918.272705078125,  g_loss: 16.129592895507812\n",
            "Training epoch 8360/1000000, d_loss: -7.593890190124512,  g_loss: -4.281008720397949\n",
            "Training epoch 8361/1000000, d_loss: -99.79671478271484,  g_loss: -37.97520446777344\n",
            "Training epoch 8362/1000000, d_loss: -65.35814666748047,  g_loss: 49.626243591308594\n",
            "Training epoch 8363/1000000, d_loss: -436.58648681640625,  g_loss: -65.03097534179688\n",
            "Training epoch 8364/1000000, d_loss: 17.23455238342285,  g_loss: -12.661758422851562\n",
            "Training epoch 8365/1000000, d_loss: -102.24510192871094,  g_loss: -43.66364288330078\n",
            "Training epoch 8366/1000000, d_loss: 17.29326629638672,  g_loss: 30.884082794189453\n",
            "Training epoch 8367/1000000, d_loss: -27.942588806152344,  g_loss: 26.72694969177246\n",
            "Training epoch 8368/1000000, d_loss: -45.858055114746094,  g_loss: 102.16840362548828\n",
            "Training epoch 8369/1000000, d_loss: -90.32968139648438,  g_loss: 145.93017578125\n",
            "Training epoch 8370/1000000, d_loss: -190.8194580078125,  g_loss: 320.9353942871094\n",
            "Training epoch 8371/1000000, d_loss: -76.9048843383789,  g_loss: 46.374717712402344\n",
            "Training epoch 8372/1000000, d_loss: -142.73907470703125,  g_loss: 89.01022338867188\n",
            "Training epoch 8373/1000000, d_loss: -141.0653839111328,  g_loss: 91.88484191894531\n",
            "Training epoch 8374/1000000, d_loss: -394.97900390625,  g_loss: -71.27643585205078\n",
            "Training epoch 8375/1000000, d_loss: -259.1806640625,  g_loss: -28.120529174804688\n",
            "Training epoch 8376/1000000, d_loss: -123.62310791015625,  g_loss: 188.05043029785156\n",
            "Training epoch 8377/1000000, d_loss: -276.23016357421875,  g_loss: 333.77716064453125\n",
            "Training epoch 8378/1000000, d_loss: -1350.541015625,  g_loss: -142.13067626953125\n",
            "Training epoch 8379/1000000, d_loss: -454.57257080078125,  g_loss: 55.672271728515625\n",
            "Training epoch 8380/1000000, d_loss: 19.456645965576172,  g_loss: -112.62883758544922\n",
            "Training epoch 8381/1000000, d_loss: -13.048271179199219,  g_loss: -80.25965881347656\n",
            "Training epoch 8382/1000000, d_loss: -20.002689361572266,  g_loss: 43.452213287353516\n",
            "Training epoch 8383/1000000, d_loss: -87.89091491699219,  g_loss: -10.504889488220215\n",
            "Training epoch 8384/1000000, d_loss: -26.588897705078125,  g_loss: 41.86450958251953\n",
            "Training epoch 8385/1000000, d_loss: -84.67364501953125,  g_loss: 61.11573791503906\n",
            "Training epoch 8386/1000000, d_loss: 60.66368865966797,  g_loss: 61.89076614379883\n",
            "Training epoch 8387/1000000, d_loss: -795.888427734375,  g_loss: 11.631118774414062\n",
            "Training epoch 8388/1000000, d_loss: 71.06619262695312,  g_loss: 23.585037231445312\n",
            "Training epoch 8389/1000000, d_loss: -107.98309326171875,  g_loss: 116.26483154296875\n",
            "Training epoch 8390/1000000, d_loss: -19.220123291015625,  g_loss: 95.89002227783203\n",
            "Training epoch 8391/1000000, d_loss: -128.9456787109375,  g_loss: 112.29988098144531\n",
            "Training epoch 8392/1000000, d_loss: -191.49658203125,  g_loss: 467.337158203125\n",
            "Training epoch 8393/1000000, d_loss: -80.98194885253906,  g_loss: 222.0428009033203\n",
            "Training epoch 8394/1000000, d_loss: -208.1707763671875,  g_loss: 403.9780578613281\n",
            "Training epoch 8395/1000000, d_loss: -213.8668975830078,  g_loss: 543.0283813476562\n",
            "Training epoch 8396/1000000, d_loss: -127.98731231689453,  g_loss: 50.657875061035156\n",
            "Training epoch 8397/1000000, d_loss: -208.89334106445312,  g_loss: 63.720218658447266\n",
            "Training epoch 8398/1000000, d_loss: -1474.1854248046875,  g_loss: 13.774843215942383\n",
            "Training epoch 8399/1000000, d_loss: -144.88442993164062,  g_loss: 130.56536865234375\n",
            "Training epoch 8400/1000000, d_loss: -204.10699462890625,  g_loss: 52.595096588134766\n",
            "Training epoch 8401/1000000, d_loss: -345.8393859863281,  g_loss: -12.769004821777344\n",
            "Training epoch 8402/1000000, d_loss: -47.743873596191406,  g_loss: -24.379714965820312\n",
            "Training epoch 8403/1000000, d_loss: 128.8072967529297,  g_loss: -170.84046936035156\n",
            "Training epoch 8404/1000000, d_loss: -57.95426559448242,  g_loss: -3.6662540435791016\n",
            "Training epoch 8405/1000000, d_loss: -80.59085083007812,  g_loss: 40.69465637207031\n",
            "Training epoch 8406/1000000, d_loss: -196.3846435546875,  g_loss: 6.495388031005859\n",
            "Training epoch 8407/1000000, d_loss: -115.04528045654297,  g_loss: 12.090691566467285\n",
            "Training epoch 8408/1000000, d_loss: -85.7108383178711,  g_loss: -10.850425720214844\n",
            "Training epoch 8409/1000000, d_loss: -10.140390396118164,  g_loss: 21.668014526367188\n",
            "Training epoch 8410/1000000, d_loss: 53.009521484375,  g_loss: 101.24504089355469\n",
            "Training epoch 8411/1000000, d_loss: -289.5913391113281,  g_loss: -253.20826721191406\n",
            "Training epoch 8412/1000000, d_loss: 8.673049926757812,  g_loss: 166.51806640625\n",
            "Training epoch 8413/1000000, d_loss: -43.993812561035156,  g_loss: 147.56771850585938\n",
            "Training epoch 8414/1000000, d_loss: -61.80738830566406,  g_loss: 46.892337799072266\n",
            "Training epoch 8415/1000000, d_loss: -134.02041625976562,  g_loss: 106.41229248046875\n",
            "Training epoch 8416/1000000, d_loss: -60.21135711669922,  g_loss: 109.81333923339844\n",
            "Training epoch 8417/1000000, d_loss: -82.69694519042969,  g_loss: 139.58847045898438\n",
            "Training epoch 8418/1000000, d_loss: -120.65110778808594,  g_loss: 44.704864501953125\n",
            "Training epoch 8419/1000000, d_loss: -80.60891723632812,  g_loss: 39.7752685546875\n",
            "Training epoch 8420/1000000, d_loss: -48.51972579956055,  g_loss: 4.231554985046387\n",
            "Training epoch 8421/1000000, d_loss: -160.881591796875,  g_loss: 234.6557159423828\n",
            "Training epoch 8422/1000000, d_loss: -58.10456466674805,  g_loss: 62.990814208984375\n",
            "Training epoch 8423/1000000, d_loss: -181.21592712402344,  g_loss: 151.63104248046875\n",
            "Training epoch 8424/1000000, d_loss: -97.36475372314453,  g_loss: 44.47465515136719\n",
            "Training epoch 8425/1000000, d_loss: -82.19175720214844,  g_loss: 66.41165161132812\n",
            "Training epoch 8426/1000000, d_loss: -87.79745483398438,  g_loss: 146.86236572265625\n",
            "Training epoch 8427/1000000, d_loss: -330.00604248046875,  g_loss: 9.252763748168945\n",
            "Training epoch 8428/1000000, d_loss: -28.348129272460938,  g_loss: 23.23007583618164\n",
            "Training epoch 8429/1000000, d_loss: -161.95046997070312,  g_loss: 15.536874771118164\n",
            "Training epoch 8430/1000000, d_loss: -11.611358642578125,  g_loss: 42.29131317138672\n",
            "Training epoch 8431/1000000, d_loss: 10.661746978759766,  g_loss: 15.093533515930176\n",
            "Training epoch 8432/1000000, d_loss: -53.949119567871094,  g_loss: 24.143144607543945\n",
            "Training epoch 8433/1000000, d_loss: -1482.0086669921875,  g_loss: 25.695907592773438\n",
            "Training epoch 8434/1000000, d_loss: 892.6739501953125,  g_loss: 46.629188537597656\n",
            "Training epoch 8435/1000000, d_loss: 5.662152290344238,  g_loss: 25.177034378051758\n",
            "Training epoch 8436/1000000, d_loss: -339.606689453125,  g_loss: 9.524333953857422\n",
            "Training epoch 8437/1000000, d_loss: -33.372886657714844,  g_loss: 18.327945709228516\n",
            "Training epoch 8438/1000000, d_loss: -35.73579406738281,  g_loss: 5.0524444580078125\n",
            "Training epoch 8439/1000000, d_loss: -64.47518920898438,  g_loss: -12.880861282348633\n",
            "Training epoch 8440/1000000, d_loss: -45.753299713134766,  g_loss: -21.940500259399414\n",
            "Training epoch 8441/1000000, d_loss: -72.00338745117188,  g_loss: -64.12744140625\n",
            "Training epoch 8442/1000000, d_loss: 5.327980041503906,  g_loss: 50.00087356567383\n",
            "Training epoch 8443/1000000, d_loss: -121.17896270751953,  g_loss: 13.566789627075195\n",
            "Training epoch 8444/1000000, d_loss: -448.7754211425781,  g_loss: -18.356220245361328\n",
            "Training epoch 8445/1000000, d_loss: -131.97879028320312,  g_loss: -10.35942554473877\n",
            "Training epoch 8446/1000000, d_loss: -105.031982421875,  g_loss: 47.14146423339844\n",
            "Training epoch 8447/1000000, d_loss: -46.41274642944336,  g_loss: 27.906536102294922\n",
            "Training epoch 8448/1000000, d_loss: -127.13882446289062,  g_loss: 59.802398681640625\n",
            "Training epoch 8449/1000000, d_loss: -50.8179817199707,  g_loss: 160.84075927734375\n",
            "Training epoch 8450/1000000, d_loss: -14.090232849121094,  g_loss: 138.10952758789062\n",
            "Training epoch 8451/1000000, d_loss: -111.48334503173828,  g_loss: 241.060546875\n",
            "Training epoch 8452/1000000, d_loss: -390.96453857421875,  g_loss: -16.6076717376709\n",
            "Training epoch 8453/1000000, d_loss: -150.72146606445312,  g_loss: 23.102962493896484\n",
            "Training epoch 8454/1000000, d_loss: -153.16964721679688,  g_loss: -62.478492736816406\n",
            "Training epoch 8455/1000000, d_loss: -121.66567993164062,  g_loss: -164.396240234375\n",
            "Training epoch 8456/1000000, d_loss: -61.69048309326172,  g_loss: 138.45535278320312\n",
            "Training epoch 8457/1000000, d_loss: -146.06021118164062,  g_loss: 145.547119140625\n",
            "Training epoch 8458/1000000, d_loss: -172.14077758789062,  g_loss: 390.4578857421875\n",
            "Training epoch 8459/1000000, d_loss: -102.62860870361328,  g_loss: 167.29537963867188\n",
            "Training epoch 8460/1000000, d_loss: -26.687463760375977,  g_loss: 66.35281372070312\n",
            "Training epoch 8461/1000000, d_loss: -352.025634765625,  g_loss: 359.7861633300781\n",
            "Training epoch 8462/1000000, d_loss: 151.04867553710938,  g_loss: -15.025144577026367\n",
            "Training epoch 8463/1000000, d_loss: -263.0160217285156,  g_loss: -31.555980682373047\n",
            "Training epoch 8464/1000000, d_loss: -60.39887237548828,  g_loss: 18.091171264648438\n",
            "Training epoch 8465/1000000, d_loss: -207.4046630859375,  g_loss: -3.2112388610839844\n",
            "Training epoch 8466/1000000, d_loss: -71.72200012207031,  g_loss: 90.44514465332031\n",
            "Training epoch 8467/1000000, d_loss: -180.42837524414062,  g_loss: -35.40111541748047\n",
            "Training epoch 8468/1000000, d_loss: -170.5244140625,  g_loss: 66.40211486816406\n",
            "Training epoch 8469/1000000, d_loss: -86.09270477294922,  g_loss: 23.977916717529297\n",
            "Training epoch 8470/1000000, d_loss: -49.09342956542969,  g_loss: 5.426422119140625\n",
            "Training epoch 8471/1000000, d_loss: -82.68480682373047,  g_loss: 36.18304443359375\n",
            "Training epoch 8472/1000000, d_loss: -131.85818481445312,  g_loss: 133.7306671142578\n",
            "Training epoch 8473/1000000, d_loss: -135.6040802001953,  g_loss: 107.37834930419922\n",
            "Training epoch 8474/1000000, d_loss: -321.4104919433594,  g_loss: 735.892578125\n",
            "Training epoch 8475/1000000, d_loss: -10.12838363647461,  g_loss: 45.40196990966797\n",
            "Training epoch 8476/1000000, d_loss: -123.63526153564453,  g_loss: 32.73830795288086\n",
            "Training epoch 8477/1000000, d_loss: 13.109298706054688,  g_loss: 56.91315841674805\n",
            "Training epoch 8478/1000000, d_loss: -81.51394653320312,  g_loss: 103.27957916259766\n",
            "Training epoch 8479/1000000, d_loss: -84.04529571533203,  g_loss: 179.30645751953125\n",
            "Training epoch 8480/1000000, d_loss: -131.00418090820312,  g_loss: 13.422883987426758\n",
            "Training epoch 8481/1000000, d_loss: -223.66061401367188,  g_loss: -94.22306823730469\n",
            "Training epoch 8482/1000000, d_loss: 36.990020751953125,  g_loss: 85.83689880371094\n",
            "Training epoch 8483/1000000, d_loss: -8.940650939941406,  g_loss: 108.45938110351562\n",
            "Training epoch 8484/1000000, d_loss: -72.60637664794922,  g_loss: 53.522430419921875\n",
            "Training epoch 8485/1000000, d_loss: -21.798709869384766,  g_loss: 82.63092041015625\n",
            "Training epoch 8486/1000000, d_loss: -348.83782958984375,  g_loss: -46.22648620605469\n",
            "Training epoch 8487/1000000, d_loss: -312.38446044921875,  g_loss: 29.17221450805664\n",
            "Training epoch 8488/1000000, d_loss: -1027.5313720703125,  g_loss: -883.4518432617188\n",
            "Training epoch 8489/1000000, d_loss: -226.57168579101562,  g_loss: -13.146635055541992\n",
            "Training epoch 8490/1000000, d_loss: 123.15406799316406,  g_loss: 130.05259704589844\n",
            "Training epoch 8491/1000000, d_loss: -98.82726287841797,  g_loss: 182.57443237304688\n",
            "Training epoch 8492/1000000, d_loss: -1695.483154296875,  g_loss: 112.29063415527344\n",
            "Training epoch 8493/1000000, d_loss: -576.7208251953125,  g_loss: -318.49273681640625\n",
            "Training epoch 8494/1000000, d_loss: -38.925384521484375,  g_loss: -29.398635864257812\n",
            "Training epoch 8495/1000000, d_loss: -87.2573471069336,  g_loss: 378.4537658691406\n",
            "Training epoch 8496/1000000, d_loss: -71.53648376464844,  g_loss: 232.78515625\n",
            "Training epoch 8497/1000000, d_loss: -250.59849548339844,  g_loss: 343.21673583984375\n",
            "Training epoch 8498/1000000, d_loss: 24.983932495117188,  g_loss: 124.19822692871094\n",
            "Training epoch 8499/1000000, d_loss: -423.7181091308594,  g_loss: 525.9679565429688\n",
            "Training epoch 8500/1000000, d_loss: -7.132568359375,  g_loss: -61.407371520996094\n",
            "Training epoch 8501/1000000, d_loss: -3.9915847778320312,  g_loss: 4.6335930824279785\n",
            "Training epoch 8502/1000000, d_loss: -19.811241149902344,  g_loss: 42.916629791259766\n",
            "Training epoch 8503/1000000, d_loss: -104.06151580810547,  g_loss: 186.13177490234375\n",
            "Training epoch 8504/1000000, d_loss: -159.81163024902344,  g_loss: 257.46533203125\n",
            "Training epoch 8505/1000000, d_loss: -89.72928619384766,  g_loss: 173.9139862060547\n",
            "Training epoch 8506/1000000, d_loss: -68.882568359375,  g_loss: 134.2452392578125\n",
            "Training epoch 8507/1000000, d_loss: 36.875144958496094,  g_loss: -18.490009307861328\n",
            "Training epoch 8508/1000000, d_loss: -181.06552124023438,  g_loss: 192.6083526611328\n",
            "Training epoch 8509/1000000, d_loss: -75.75009155273438,  g_loss: 25.8997745513916\n",
            "Training epoch 8510/1000000, d_loss: -44.93317413330078,  g_loss: 53.537132263183594\n",
            "Training epoch 8511/1000000, d_loss: -422.80767822265625,  g_loss: -25.51059913635254\n",
            "Training epoch 8512/1000000, d_loss: -903.1095581054688,  g_loss: -889.9915161132812\n",
            "Training epoch 8513/1000000, d_loss: 26.061983108520508,  g_loss: 55.041229248046875\n",
            "Training epoch 8514/1000000, d_loss: -99.82454681396484,  g_loss: 115.17279815673828\n",
            "Training epoch 8515/1000000, d_loss: -137.81092834472656,  g_loss: 327.4355163574219\n",
            "Training epoch 8516/1000000, d_loss: -152.2654266357422,  g_loss: 25.06790542602539\n",
            "Training epoch 8517/1000000, d_loss: -162.738037109375,  g_loss: 45.917198181152344\n",
            "Training epoch 8518/1000000, d_loss: -549.183349609375,  g_loss: -58.980712890625\n",
            "Training epoch 8519/1000000, d_loss: -89.89147186279297,  g_loss: 151.8546142578125\n",
            "Training epoch 8520/1000000, d_loss: -149.83168029785156,  g_loss: -16.626432418823242\n",
            "Training epoch 8521/1000000, d_loss: -102.54795837402344,  g_loss: 18.4660587310791\n",
            "Training epoch 8522/1000000, d_loss: 14.595230102539062,  g_loss: 72.98167419433594\n",
            "Training epoch 8523/1000000, d_loss: -139.29257202148438,  g_loss: 128.34097290039062\n",
            "Training epoch 8524/1000000, d_loss: -834.0096435546875,  g_loss: -39.13296127319336\n",
            "Training epoch 8525/1000000, d_loss: -276.0422668457031,  g_loss: -21.08453369140625\n",
            "Training epoch 8526/1000000, d_loss: -36.2171630859375,  g_loss: -11.67556095123291\n",
            "Training epoch 8527/1000000, d_loss: -14.093832969665527,  g_loss: 35.9796257019043\n",
            "Training epoch 8528/1000000, d_loss: -153.1652374267578,  g_loss: 100.26737976074219\n",
            "Training epoch 8529/1000000, d_loss: -153.837158203125,  g_loss: 48.494903564453125\n",
            "Training epoch 8530/1000000, d_loss: -2.7611083984375,  g_loss: 47.08883285522461\n",
            "Training epoch 8531/1000000, d_loss: -51.03976821899414,  g_loss: -14.215017318725586\n",
            "Training epoch 8532/1000000, d_loss: -78.3860855102539,  g_loss: 2.7577152252197266\n",
            "Training epoch 8533/1000000, d_loss: -113.2048568725586,  g_loss: 54.52593994140625\n",
            "Training epoch 8534/1000000, d_loss: -376.737060546875,  g_loss: -209.2069549560547\n",
            "Training epoch 8535/1000000, d_loss: -717.10205078125,  g_loss: -319.21923828125\n",
            "Training epoch 8536/1000000, d_loss: 182.24691772460938,  g_loss: -63.17924880981445\n",
            "Training epoch 8537/1000000, d_loss: -399.9540710449219,  g_loss: -66.62464904785156\n",
            "Training epoch 8538/1000000, d_loss: -46.11181640625,  g_loss: 79.90556335449219\n",
            "Training epoch 8539/1000000, d_loss: -24.327041625976562,  g_loss: 50.4979248046875\n",
            "Training epoch 8540/1000000, d_loss: -123.72769927978516,  g_loss: -10.65402889251709\n",
            "Training epoch 8541/1000000, d_loss: 11.57208251953125,  g_loss: 13.984713554382324\n",
            "Training epoch 8542/1000000, d_loss: -80.10142517089844,  g_loss: 64.3070068359375\n",
            "Training epoch 8543/1000000, d_loss: 60.007232666015625,  g_loss: 151.97549438476562\n",
            "Training epoch 8544/1000000, d_loss: -154.04330444335938,  g_loss: 293.75177001953125\n",
            "Training epoch 8545/1000000, d_loss: -123.30158233642578,  g_loss: -3.98991060256958\n",
            "Training epoch 8546/1000000, d_loss: -192.331787109375,  g_loss: -60.821983337402344\n",
            "Training epoch 8547/1000000, d_loss: -111.7996597290039,  g_loss: -10.418787002563477\n",
            "Training epoch 8548/1000000, d_loss: -123.91581726074219,  g_loss: 24.769927978515625\n",
            "Training epoch 8549/1000000, d_loss: -197.08966064453125,  g_loss: 134.67127990722656\n",
            "Training epoch 8550/1000000, d_loss: -119.63418579101562,  g_loss: 108.50431823730469\n",
            "Training epoch 8551/1000000, d_loss: -1637.650146484375,  g_loss: -446.0135498046875\n",
            "Training epoch 8552/1000000, d_loss: -49.900611877441406,  g_loss: -117.52265167236328\n",
            "Training epoch 8553/1000000, d_loss: 117.3464126586914,  g_loss: -58.433311462402344\n",
            "Training epoch 8554/1000000, d_loss: -30.435340881347656,  g_loss: -53.658935546875\n",
            "Training epoch 8555/1000000, d_loss: -98.51678466796875,  g_loss: -18.728416442871094\n",
            "Training epoch 8556/1000000, d_loss: -71.74766540527344,  g_loss: -16.767501831054688\n",
            "Training epoch 8557/1000000, d_loss: -93.53837585449219,  g_loss: 6.85784912109375\n",
            "Training epoch 8558/1000000, d_loss: -44.46318054199219,  g_loss: 44.29033660888672\n",
            "Training epoch 8559/1000000, d_loss: -47.156978607177734,  g_loss: -12.498695373535156\n",
            "Training epoch 8560/1000000, d_loss: -104.31355285644531,  g_loss: 54.004356384277344\n",
            "Training epoch 8561/1000000, d_loss: -106.376708984375,  g_loss: -21.425872802734375\n",
            "Training epoch 8562/1000000, d_loss: -98.29530334472656,  g_loss: 88.3328628540039\n",
            "Training epoch 8563/1000000, d_loss: -171.29180908203125,  g_loss: 246.27078247070312\n",
            "Training epoch 8564/1000000, d_loss: -37.8782958984375,  g_loss: 39.62189865112305\n",
            "Training epoch 8565/1000000, d_loss: -102.9187240600586,  g_loss: 39.933353424072266\n",
            "Training epoch 8566/1000000, d_loss: -201.60263061523438,  g_loss: 37.096282958984375\n",
            "Training epoch 8567/1000000, d_loss: -58.57818603515625,  g_loss: 17.966999053955078\n",
            "Training epoch 8568/1000000, d_loss: -71.35638427734375,  g_loss: 69.63238525390625\n",
            "Training epoch 8569/1000000, d_loss: -24.302032470703125,  g_loss: 128.08685302734375\n",
            "Training epoch 8570/1000000, d_loss: -56.644439697265625,  g_loss: 125.19856262207031\n",
            "Training epoch 8571/1000000, d_loss: -116.3031005859375,  g_loss: 44.728355407714844\n",
            "Training epoch 8572/1000000, d_loss: -4.0068206787109375,  g_loss: 92.19722747802734\n",
            "Training epoch 8573/1000000, d_loss: -8.401935577392578,  g_loss: 68.04302978515625\n",
            "Training epoch 8574/1000000, d_loss: -117.01029968261719,  g_loss: 8.509929656982422\n",
            "Training epoch 8575/1000000, d_loss: 0.7698631286621094,  g_loss: 1.5604147911071777\n",
            "Training epoch 8576/1000000, d_loss: -194.48300170898438,  g_loss: -62.92023468017578\n",
            "Training epoch 8577/1000000, d_loss: -848.4388427734375,  g_loss: -385.5975646972656\n",
            "Training epoch 8578/1000000, d_loss: 759.274169921875,  g_loss: -77.025146484375\n",
            "Training epoch 8579/1000000, d_loss: -92.93820190429688,  g_loss: 51.413658142089844\n",
            "Training epoch 8580/1000000, d_loss: -88.16517639160156,  g_loss: 140.11761474609375\n",
            "Training epoch 8581/1000000, d_loss: -185.0200958251953,  g_loss: 139.2598419189453\n",
            "Training epoch 8582/1000000, d_loss: -178.8316192626953,  g_loss: 371.64556884765625\n",
            "Training epoch 8583/1000000, d_loss: -66.86738586425781,  g_loss: 131.03622436523438\n",
            "Training epoch 8584/1000000, d_loss: -99.76065063476562,  g_loss: -19.34613609313965\n",
            "Training epoch 8585/1000000, d_loss: -251.44508361816406,  g_loss: -4.245332717895508\n",
            "Training epoch 8586/1000000, d_loss: -27.429927825927734,  g_loss: 90.02201080322266\n",
            "Training epoch 8587/1000000, d_loss: -98.56841278076172,  g_loss: 54.32350158691406\n",
            "Training epoch 8588/1000000, d_loss: -78.6593017578125,  g_loss: 14.61950969696045\n",
            "Training epoch 8589/1000000, d_loss: -16.006202697753906,  g_loss: 31.33564567565918\n",
            "Training epoch 8590/1000000, d_loss: -71.84014129638672,  g_loss: 43.509132385253906\n",
            "Training epoch 8591/1000000, d_loss: -222.8648681640625,  g_loss: -21.062618255615234\n",
            "Training epoch 8592/1000000, d_loss: -233.03489685058594,  g_loss: -134.69644165039062\n",
            "Training epoch 8593/1000000, d_loss: -19.176483154296875,  g_loss: 54.41069412231445\n",
            "Training epoch 8594/1000000, d_loss: -15.829641342163086,  g_loss: 104.01653289794922\n",
            "Training epoch 8595/1000000, d_loss: -182.2025909423828,  g_loss: 57.945613861083984\n",
            "Training epoch 8596/1000000, d_loss: -51.30088806152344,  g_loss: 26.28795051574707\n",
            "Training epoch 8597/1000000, d_loss: -153.97381591796875,  g_loss: 183.4665985107422\n",
            "Training epoch 8598/1000000, d_loss: -105.05908203125,  g_loss: -16.088611602783203\n",
            "Training epoch 8599/1000000, d_loss: -145.00013732910156,  g_loss: 88.23020935058594\n",
            "Training epoch 8600/1000000, d_loss: -106.44160461425781,  g_loss: 45.61909484863281\n",
            "Training epoch 8601/1000000, d_loss: -92.11555480957031,  g_loss: 112.14153289794922\n",
            "Training epoch 8602/1000000, d_loss: -114.84024810791016,  g_loss: 74.59115600585938\n",
            "Training epoch 8603/1000000, d_loss: -314.0836181640625,  g_loss: 14.109410285949707\n",
            "Training epoch 8604/1000000, d_loss: -57.167442321777344,  g_loss: -2.796933650970459\n",
            "Training epoch 8605/1000000, d_loss: -87.44253540039062,  g_loss: 35.22358703613281\n",
            "Training epoch 8606/1000000, d_loss: -57.26532745361328,  g_loss: 31.4183406829834\n",
            "Training epoch 8607/1000000, d_loss: 46.525794982910156,  g_loss: 35.60176467895508\n",
            "Training epoch 8608/1000000, d_loss: -40.82732009887695,  g_loss: 30.975549697875977\n",
            "Training epoch 8609/1000000, d_loss: -182.27493286132812,  g_loss: 59.53633117675781\n",
            "Training epoch 8610/1000000, d_loss: -112.8444595336914,  g_loss: 78.92195892333984\n",
            "Training epoch 8611/1000000, d_loss: -335.8244323730469,  g_loss: -62.04090881347656\n",
            "Training epoch 8612/1000000, d_loss: -116.52658081054688,  g_loss: -14.427103042602539\n",
            "Training epoch 8613/1000000, d_loss: -69.1402816772461,  g_loss: 34.66032791137695\n",
            "Training epoch 8614/1000000, d_loss: -164.59922790527344,  g_loss: 2.9055871963500977\n",
            "Training epoch 8615/1000000, d_loss: -107.97920227050781,  g_loss: -0.6304726600646973\n",
            "Training epoch 8616/1000000, d_loss: -33.523773193359375,  g_loss: 68.60755157470703\n",
            "Training epoch 8617/1000000, d_loss: -204.54258728027344,  g_loss: -31.28232192993164\n",
            "Training epoch 8618/1000000, d_loss: -51.07890701293945,  g_loss: 31.9912166595459\n",
            "Training epoch 8619/1000000, d_loss: -46.97002410888672,  g_loss: 82.98408508300781\n",
            "Training epoch 8620/1000000, d_loss: -166.5902862548828,  g_loss: 147.8170166015625\n",
            "Training epoch 8621/1000000, d_loss: -158.12950134277344,  g_loss: 46.917537689208984\n",
            "Training epoch 8622/1000000, d_loss: -214.77488708496094,  g_loss: 114.65853881835938\n",
            "Training epoch 8623/1000000, d_loss: -130.10595703125,  g_loss: 41.37987518310547\n",
            "Training epoch 8624/1000000, d_loss: -103.844970703125,  g_loss: 52.68601989746094\n",
            "Training epoch 8625/1000000, d_loss: -148.21531677246094,  g_loss: -49.924217224121094\n",
            "Training epoch 8626/1000000, d_loss: -54.35802459716797,  g_loss: 30.777231216430664\n",
            "Training epoch 8627/1000000, d_loss: -58.761741638183594,  g_loss: 53.04875183105469\n",
            "Training epoch 8628/1000000, d_loss: -54.086910247802734,  g_loss: 63.130245208740234\n",
            "Training epoch 8629/1000000, d_loss: -660.5233764648438,  g_loss: -151.275634765625\n",
            "Training epoch 8630/1000000, d_loss: -108.31475830078125,  g_loss: -34.412681579589844\n",
            "Training epoch 8631/1000000, d_loss: -12.370407104492188,  g_loss: 61.787418365478516\n",
            "Training epoch 8632/1000000, d_loss: -138.78323364257812,  g_loss: 75.6480712890625\n",
            "Training epoch 8633/1000000, d_loss: -183.6541290283203,  g_loss: 64.16991424560547\n",
            "Training epoch 8634/1000000, d_loss: -169.26353454589844,  g_loss: 71.44596862792969\n",
            "Training epoch 8635/1000000, d_loss: 6.653360366821289,  g_loss: 93.52506256103516\n",
            "Training epoch 8636/1000000, d_loss: -37.39641571044922,  g_loss: 49.837188720703125\n",
            "Training epoch 8637/1000000, d_loss: -54.47344207763672,  g_loss: 107.43064880371094\n",
            "Training epoch 8638/1000000, d_loss: -34.338130950927734,  g_loss: 87.9649658203125\n",
            "Training epoch 8639/1000000, d_loss: -1060.696044921875,  g_loss: 37.10725402832031\n",
            "Training epoch 8640/1000000, d_loss: -545.6138305664062,  g_loss: -143.5726318359375\n",
            "Training epoch 8641/1000000, d_loss: 9.163629531860352,  g_loss: -45.115386962890625\n",
            "Training epoch 8642/1000000, d_loss: -551.7092895507812,  g_loss: -96.84886169433594\n",
            "Training epoch 8643/1000000, d_loss: 43.06297302246094,  g_loss: 45.726898193359375\n",
            "Training epoch 8644/1000000, d_loss: 44.88011932373047,  g_loss: 60.12152862548828\n",
            "Training epoch 8645/1000000, d_loss: -124.2691650390625,  g_loss: 156.00692749023438\n",
            "Training epoch 8646/1000000, d_loss: -105.3589859008789,  g_loss: 65.81610870361328\n",
            "Training epoch 8647/1000000, d_loss: -81.95857238769531,  g_loss: 55.023048400878906\n",
            "Training epoch 8648/1000000, d_loss: -128.52764892578125,  g_loss: 57.41328811645508\n",
            "Training epoch 8649/1000000, d_loss: -81.46002197265625,  g_loss: 51.89934539794922\n",
            "Training epoch 8650/1000000, d_loss: -110.75426483154297,  g_loss: 165.71878051757812\n",
            "Training epoch 8651/1000000, d_loss: -129.9327392578125,  g_loss: 148.4595489501953\n",
            "Training epoch 8652/1000000, d_loss: -364.6356506347656,  g_loss: 709.440185546875\n",
            "Training epoch 8653/1000000, d_loss: 93.3230972290039,  g_loss: 2.0925145149230957\n",
            "Training epoch 8654/1000000, d_loss: -91.54298400878906,  g_loss: 18.927032470703125\n",
            "Training epoch 8655/1000000, d_loss: -3.5698013305664062,  g_loss: 57.024444580078125\n",
            "Training epoch 8656/1000000, d_loss: -87.2131118774414,  g_loss: -11.12844467163086\n",
            "Training epoch 8657/1000000, d_loss: -72.26556396484375,  g_loss: 23.566425323486328\n",
            "Training epoch 8658/1000000, d_loss: -118.12496948242188,  g_loss: 61.93254852294922\n",
            "Training epoch 8659/1000000, d_loss: -1383.785400390625,  g_loss: -160.35523986816406\n",
            "Training epoch 8660/1000000, d_loss: -4.0830535888671875,  g_loss: 27.488662719726562\n",
            "Training epoch 8661/1000000, d_loss: -91.51410675048828,  g_loss: 11.174371719360352\n",
            "Training epoch 8662/1000000, d_loss: -611.6097412109375,  g_loss: -123.5390396118164\n",
            "Training epoch 8663/1000000, d_loss: 269.7390441894531,  g_loss: -337.55780029296875\n",
            "Training epoch 8664/1000000, d_loss: 3.2403335571289062,  g_loss: 75.40943145751953\n",
            "Training epoch 8665/1000000, d_loss: 67.94638061523438,  g_loss: 41.96580505371094\n",
            "Training epoch 8666/1000000, d_loss: -99.6470947265625,  g_loss: 81.04595947265625\n",
            "Training epoch 8667/1000000, d_loss: -28.041736602783203,  g_loss: 43.52940368652344\n",
            "Training epoch 8668/1000000, d_loss: -315.3113708496094,  g_loss: 24.771909713745117\n",
            "Training epoch 8669/1000000, d_loss: -645.3594970703125,  g_loss: -700.9356079101562\n",
            "Training epoch 8670/1000000, d_loss: 107.07353210449219,  g_loss: -50.568878173828125\n",
            "Training epoch 8671/1000000, d_loss: -227.08450317382812,  g_loss: 243.36166381835938\n",
            "Training epoch 8672/1000000, d_loss: -640.837890625,  g_loss: 1396.9754638671875\n",
            "Training epoch 8673/1000000, d_loss: -114.58830261230469,  g_loss: 361.05316162109375\n",
            "Training epoch 8674/1000000, d_loss: -0.5656051635742188,  g_loss: 167.49285888671875\n",
            "Training epoch 8675/1000000, d_loss: -91.66691589355469,  g_loss: 130.11415100097656\n",
            "Training epoch 8676/1000000, d_loss: -110.4533462524414,  g_loss: 137.53582763671875\n",
            "Training epoch 8677/1000000, d_loss: 13.684898376464844,  g_loss: 148.61328125\n",
            "Training epoch 8678/1000000, d_loss: -190.13824462890625,  g_loss: 101.42789459228516\n",
            "Training epoch 8679/1000000, d_loss: -33.7165641784668,  g_loss: 137.5064239501953\n",
            "Training epoch 8680/1000000, d_loss: 1209.231689453125,  g_loss: 113.23442840576172\n",
            "Training epoch 8681/1000000, d_loss: -25.86431312561035,  g_loss: 112.59449768066406\n",
            "Training epoch 8682/1000000, d_loss: -73.24207305908203,  g_loss: 241.86007690429688\n",
            "Training epoch 8683/1000000, d_loss: -30.62649154663086,  g_loss: 153.99269104003906\n",
            "Training epoch 8684/1000000, d_loss: -68.92228698730469,  g_loss: 81.6880111694336\n",
            "Training epoch 8685/1000000, d_loss: -105.16812896728516,  g_loss: 34.47811508178711\n",
            "Training epoch 8686/1000000, d_loss: -157.36965942382812,  g_loss: 50.23639678955078\n",
            "Training epoch 8687/1000000, d_loss: -6.205739974975586,  g_loss: 88.04534149169922\n",
            "Training epoch 8688/1000000, d_loss: -252.6558837890625,  g_loss: 22.030553817749023\n",
            "Training epoch 8689/1000000, d_loss: -31.161558151245117,  g_loss: 48.10350799560547\n",
            "Training epoch 8690/1000000, d_loss: -165.46426391601562,  g_loss: 12.618623733520508\n",
            "Training epoch 8691/1000000, d_loss: -185.070556640625,  g_loss: 60.845359802246094\n",
            "Training epoch 8692/1000000, d_loss: -16.6871395111084,  g_loss: 57.127037048339844\n",
            "Training epoch 8693/1000000, d_loss: -94.531005859375,  g_loss: 78.94949340820312\n",
            "Training epoch 8694/1000000, d_loss: -58.601966857910156,  g_loss: 131.5673828125\n",
            "Training epoch 8695/1000000, d_loss: -126.4570541381836,  g_loss: 87.04381561279297\n",
            "Training epoch 8696/1000000, d_loss: -205.1287841796875,  g_loss: 107.33547973632812\n",
            "Training epoch 8697/1000000, d_loss: -122.06466674804688,  g_loss: 30.740318298339844\n",
            "Training epoch 8698/1000000, d_loss: -130.78057861328125,  g_loss: 49.84844207763672\n",
            "Training epoch 8699/1000000, d_loss: -422.5779113769531,  g_loss: -31.348705291748047\n",
            "Training epoch 8700/1000000, d_loss: -127.4604721069336,  g_loss: -3.8188982009887695\n",
            "Training epoch 8701/1000000, d_loss: -179.85211181640625,  g_loss: -2.6887643337249756\n",
            "Training epoch 8702/1000000, d_loss: -88.73616790771484,  g_loss: 13.518913269042969\n",
            "Training epoch 8703/1000000, d_loss: -228.2056427001953,  g_loss: 173.09954833984375\n",
            "Training epoch 8704/1000000, d_loss: -412.22235107421875,  g_loss: 69.46023559570312\n",
            "Training epoch 8705/1000000, d_loss: -95.48675537109375,  g_loss: 118.72706604003906\n",
            "Training epoch 8706/1000000, d_loss: -92.1185073852539,  g_loss: 58.31939697265625\n",
            "Training epoch 8707/1000000, d_loss: -71.20146179199219,  g_loss: 43.007667541503906\n",
            "Training epoch 8708/1000000, d_loss: -44.10533142089844,  g_loss: 17.582599639892578\n",
            "Training epoch 8709/1000000, d_loss: -53.35509490966797,  g_loss: 142.4784393310547\n",
            "Training epoch 8710/1000000, d_loss: -155.89964294433594,  g_loss: 98.83619689941406\n",
            "Training epoch 8711/1000000, d_loss: -100.32516479492188,  g_loss: 116.57919311523438\n",
            "Training epoch 8712/1000000, d_loss: -61.574546813964844,  g_loss: 91.6785659790039\n",
            "Training epoch 8713/1000000, d_loss: -68.68108367919922,  g_loss: 49.32970428466797\n",
            "Training epoch 8714/1000000, d_loss: -173.8259735107422,  g_loss: 15.9866943359375\n",
            "Training epoch 8715/1000000, d_loss: -137.807373046875,  g_loss: 19.552310943603516\n",
            "Training epoch 8716/1000000, d_loss: -110.95879364013672,  g_loss: 50.53070068359375\n",
            "Training epoch 8717/1000000, d_loss: -9.308303833007812,  g_loss: 86.00328063964844\n",
            "Training epoch 8718/1000000, d_loss: -122.953125,  g_loss: 197.17079162597656\n",
            "Training epoch 8719/1000000, d_loss: -65.46626281738281,  g_loss: 19.167911529541016\n",
            "Training epoch 8720/1000000, d_loss: -129.80859375,  g_loss: 100.84085083007812\n",
            "Training epoch 8721/1000000, d_loss: -180.43820190429688,  g_loss: 43.55830383300781\n",
            "Training epoch 8722/1000000, d_loss: -77.46060943603516,  g_loss: 5.95845365524292\n",
            "Training epoch 8723/1000000, d_loss: -197.43304443359375,  g_loss: 6.962581634521484\n",
            "Training epoch 8724/1000000, d_loss: -151.94781494140625,  g_loss: -42.034942626953125\n",
            "Training epoch 8725/1000000, d_loss: -18.185338973999023,  g_loss: 75.79856872558594\n",
            "Training epoch 8726/1000000, d_loss: -65.48767852783203,  g_loss: 104.94410705566406\n",
            "Training epoch 8727/1000000, d_loss: -56.309349060058594,  g_loss: 62.83600616455078\n",
            "Training epoch 8728/1000000, d_loss: -127.36923217773438,  g_loss: 30.738426208496094\n",
            "Training epoch 8729/1000000, d_loss: -125.58201599121094,  g_loss: -2.9814600944519043\n",
            "Training epoch 8730/1000000, d_loss: -241.9149169921875,  g_loss: -21.6212100982666\n",
            "Training epoch 8731/1000000, d_loss: -1087.78076171875,  g_loss: -55.2254753112793\n",
            "Training epoch 8732/1000000, d_loss: -327.1759948730469,  g_loss: -314.87548828125\n",
            "Training epoch 8733/1000000, d_loss: 258.7713317871094,  g_loss: -363.75079345703125\n",
            "Training epoch 8734/1000000, d_loss: 275.0951843261719,  g_loss: -65.99282836914062\n",
            "Training epoch 8735/1000000, d_loss: 37.85197067260742,  g_loss: -106.64324951171875\n",
            "Training epoch 8736/1000000, d_loss: -24.87506866455078,  g_loss: 169.7058563232422\n",
            "Training epoch 8737/1000000, d_loss: -40.244747161865234,  g_loss: 62.07722854614258\n",
            "Training epoch 8738/1000000, d_loss: -164.40020751953125,  g_loss: 172.94854736328125\n",
            "Training epoch 8739/1000000, d_loss: -144.65777587890625,  g_loss: 290.3342590332031\n",
            "Training epoch 8740/1000000, d_loss: -89.46627807617188,  g_loss: 86.71249389648438\n",
            "Training epoch 8741/1000000, d_loss: -111.32319641113281,  g_loss: 91.16165161132812\n",
            "Training epoch 8742/1000000, d_loss: -162.884521484375,  g_loss: -33.327449798583984\n",
            "Training epoch 8743/1000000, d_loss: -100.78099060058594,  g_loss: 27.353940963745117\n",
            "Training epoch 8744/1000000, d_loss: 56.13871765136719,  g_loss: -4.9218950271606445\n",
            "Training epoch 8745/1000000, d_loss: -31.45711898803711,  g_loss: 47.9525260925293\n",
            "Training epoch 8746/1000000, d_loss: -113.22804260253906,  g_loss: 199.19801330566406\n",
            "Training epoch 8747/1000000, d_loss: -5.611835479736328,  g_loss: 14.846649169921875\n",
            "Training epoch 8748/1000000, d_loss: 12.363555908203125,  g_loss: 33.02069854736328\n",
            "Training epoch 8749/1000000, d_loss: -58.57060241699219,  g_loss: -7.161657333374023\n",
            "Training epoch 8750/1000000, d_loss: -129.82363891601562,  g_loss: 108.11351013183594\n",
            "Training epoch 8751/1000000, d_loss: -176.04440307617188,  g_loss: -32.567115783691406\n",
            "Training epoch 8752/1000000, d_loss: -224.10943603515625,  g_loss: -27.963054656982422\n",
            "Training epoch 8753/1000000, d_loss: -173.09182739257812,  g_loss: -55.821502685546875\n",
            "Training epoch 8754/1000000, d_loss: -76.08822631835938,  g_loss: 99.12492370605469\n",
            "Training epoch 8755/1000000, d_loss: -89.6475830078125,  g_loss: 56.80403518676758\n",
            "Training epoch 8756/1000000, d_loss: -243.92111206054688,  g_loss: 9.377246856689453\n",
            "Training epoch 8757/1000000, d_loss: -124.14202880859375,  g_loss: 35.03415298461914\n",
            "Training epoch 8758/1000000, d_loss: -56.88710403442383,  g_loss: 30.181278228759766\n",
            "Training epoch 8759/1000000, d_loss: -2252.827880859375,  g_loss: -335.4356689453125\n",
            "Training epoch 8760/1000000, d_loss: 592.8475952148438,  g_loss: -591.930908203125\n",
            "Training epoch 8761/1000000, d_loss: -2.8952560424804688,  g_loss: -348.5718078613281\n",
            "Training epoch 8762/1000000, d_loss: 296.538330078125,  g_loss: -113.64317321777344\n",
            "Training epoch 8763/1000000, d_loss: -67.43992614746094,  g_loss: -27.968536376953125\n",
            "Training epoch 8764/1000000, d_loss: -22.351715087890625,  g_loss: 34.80482482910156\n",
            "Training epoch 8765/1000000, d_loss: -26.03321647644043,  g_loss: 67.85385131835938\n",
            "Training epoch 8766/1000000, d_loss: -60.83076477050781,  g_loss: 70.3986587524414\n",
            "Training epoch 8767/1000000, d_loss: -103.17933654785156,  g_loss: 35.85146713256836\n",
            "Training epoch 8768/1000000, d_loss: -35.607521057128906,  g_loss: 59.06829833984375\n",
            "Training epoch 8769/1000000, d_loss: -1137.8663330078125,  g_loss: 31.56638526916504\n",
            "Training epoch 8770/1000000, d_loss: 2132.5986328125,  g_loss: -206.87899780273438\n",
            "Training epoch 8771/1000000, d_loss: 333.3485107421875,  g_loss: -646.1260986328125\n",
            "Training epoch 8772/1000000, d_loss: 361.8190002441406,  g_loss: 66.77137756347656\n",
            "Training epoch 8773/1000000, d_loss: -78.64450073242188,  g_loss: 93.72946166992188\n",
            "Training epoch 8774/1000000, d_loss: 48.853981018066406,  g_loss: 108.28553771972656\n",
            "Training epoch 8775/1000000, d_loss: -157.39431762695312,  g_loss: 41.461734771728516\n",
            "Training epoch 8776/1000000, d_loss: -88.30001068115234,  g_loss: 107.65310668945312\n",
            "Training epoch 8777/1000000, d_loss: -173.5572052001953,  g_loss: 195.2137451171875\n",
            "Training epoch 8778/1000000, d_loss: -145.9703369140625,  g_loss: 132.37611389160156\n",
            "Training epoch 8779/1000000, d_loss: -149.00733947753906,  g_loss: 126.29385375976562\n",
            "Training epoch 8780/1000000, d_loss: -277.833251953125,  g_loss: -1.3838062286376953\n",
            "Training epoch 8781/1000000, d_loss: -311.8428649902344,  g_loss: -90.38414764404297\n",
            "Training epoch 8782/1000000, d_loss: -79.64505767822266,  g_loss: 51.298004150390625\n",
            "Training epoch 8783/1000000, d_loss: -90.46544647216797,  g_loss: 167.3138427734375\n",
            "Training epoch 8784/1000000, d_loss: -196.7301025390625,  g_loss: 236.98513793945312\n",
            "Training epoch 8785/1000000, d_loss: -194.92526245117188,  g_loss: 10.96737289428711\n",
            "Training epoch 8786/1000000, d_loss: -29.00187110900879,  g_loss: 53.654335021972656\n",
            "Training epoch 8787/1000000, d_loss: 452.9427795410156,  g_loss: -59.25324249267578\n",
            "Training epoch 8788/1000000, d_loss: -97.22691345214844,  g_loss: 89.54438781738281\n",
            "Training epoch 8789/1000000, d_loss: -37.10053253173828,  g_loss: 24.511093139648438\n",
            "Training epoch 8790/1000000, d_loss: -147.10292053222656,  g_loss: 134.57749938964844\n",
            "Training epoch 8791/1000000, d_loss: -260.3820495605469,  g_loss: 155.29150390625\n",
            "Training epoch 8792/1000000, d_loss: -67.97647094726562,  g_loss: 107.60792541503906\n",
            "Training epoch 8793/1000000, d_loss: -50.317840576171875,  g_loss: 128.8846435546875\n",
            "Training epoch 8794/1000000, d_loss: -115.02157592773438,  g_loss: 105.54330444335938\n",
            "Training epoch 8795/1000000, d_loss: -38.30131912231445,  g_loss: 109.97701263427734\n",
            "Training epoch 8796/1000000, d_loss: -23.0380859375,  g_loss: 124.66120147705078\n",
            "Training epoch 8797/1000000, d_loss: -106.1755142211914,  g_loss: 142.8017120361328\n",
            "Training epoch 8798/1000000, d_loss: -479.80828857421875,  g_loss: 24.09713363647461\n",
            "Training epoch 8799/1000000, d_loss: -109.75450897216797,  g_loss: 45.69554138183594\n",
            "Training epoch 8800/1000000, d_loss: -812.2774047851562,  g_loss: -452.8643798828125\n",
            "Training epoch 8801/1000000, d_loss: -43.63861083984375,  g_loss: -31.26055908203125\n",
            "Training epoch 8802/1000000, d_loss: 22.918113708496094,  g_loss: 65.49826049804688\n",
            "Training epoch 8803/1000000, d_loss: 1.3613395690917969,  g_loss: 79.55888366699219\n",
            "Training epoch 8804/1000000, d_loss: -152.29397583007812,  g_loss: 54.45406723022461\n",
            "Training epoch 8805/1000000, d_loss: -32.68701934814453,  g_loss: 47.442596435546875\n",
            "Training epoch 8806/1000000, d_loss: -145.19833374023438,  g_loss: 47.793067932128906\n",
            "Training epoch 8807/1000000, d_loss: -141.42384338378906,  g_loss: 11.903888702392578\n",
            "Training epoch 8808/1000000, d_loss: -52.90041732788086,  g_loss: 6.700284957885742\n",
            "Training epoch 8809/1000000, d_loss: -124.85015106201172,  g_loss: 206.14697265625\n",
            "Training epoch 8810/1000000, d_loss: 6.914833068847656,  g_loss: 63.01579284667969\n",
            "Training epoch 8811/1000000, d_loss: -128.54281616210938,  g_loss: 46.227882385253906\n",
            "Training epoch 8812/1000000, d_loss: 27.130844116210938,  g_loss: 9.70811653137207\n",
            "Training epoch 8813/1000000, d_loss: -52.250633239746094,  g_loss: 43.45906066894531\n",
            "Training epoch 8814/1000000, d_loss: -119.9141845703125,  g_loss: 39.45977020263672\n",
            "Training epoch 8815/1000000, d_loss: -542.6600341796875,  g_loss: -96.0516128540039\n",
            "Training epoch 8816/1000000, d_loss: 16.395320892333984,  g_loss: -97.82625579833984\n",
            "Training epoch 8817/1000000, d_loss: -171.1775360107422,  g_loss: 42.418312072753906\n",
            "Training epoch 8818/1000000, d_loss: -71.17130279541016,  g_loss: 49.75092315673828\n",
            "Training epoch 8819/1000000, d_loss: -32.84736251831055,  g_loss: 10.921476364135742\n",
            "Training epoch 8820/1000000, d_loss: -67.46659088134766,  g_loss: 29.221439361572266\n",
            "Training epoch 8821/1000000, d_loss: -238.2294464111328,  g_loss: 14.13357162475586\n",
            "Training epoch 8822/1000000, d_loss: -112.1395034790039,  g_loss: -42.13098907470703\n",
            "Training epoch 8823/1000000, d_loss: -118.39666748046875,  g_loss: 42.27461242675781\n",
            "Training epoch 8824/1000000, d_loss: -36.33388900756836,  g_loss: 24.89621353149414\n",
            "Training epoch 8825/1000000, d_loss: -62.3597526550293,  g_loss: -2.238646984100342\n",
            "Training epoch 8826/1000000, d_loss: -15.2984619140625,  g_loss: 62.514461517333984\n",
            "Training epoch 8827/1000000, d_loss: -255.35922241210938,  g_loss: -36.818233489990234\n",
            "Training epoch 8828/1000000, d_loss: -40.59406661987305,  g_loss: 49.85774230957031\n",
            "Training epoch 8829/1000000, d_loss: -83.76776123046875,  g_loss: 24.66195297241211\n",
            "Training epoch 8830/1000000, d_loss: 1.3376235961914062,  g_loss: -5.609216690063477\n",
            "Training epoch 8831/1000000, d_loss: -18.87839126586914,  g_loss: 58.15397644042969\n",
            "Training epoch 8832/1000000, d_loss: -635.7865600585938,  g_loss: 23.423423767089844\n",
            "Training epoch 8833/1000000, d_loss: -57.4376106262207,  g_loss: 31.90850830078125\n",
            "Training epoch 8834/1000000, d_loss: -22.176057815551758,  g_loss: -4.109470367431641\n",
            "Training epoch 8835/1000000, d_loss: -69.68424224853516,  g_loss: 49.19092559814453\n",
            "Training epoch 8836/1000000, d_loss: -137.15440368652344,  g_loss: 12.052648544311523\n",
            "Training epoch 8837/1000000, d_loss: -170.24215698242188,  g_loss: 209.30369567871094\n",
            "Training epoch 8838/1000000, d_loss: -239.22280883789062,  g_loss: -16.664443969726562\n",
            "Training epoch 8839/1000000, d_loss: -280.932373046875,  g_loss: -94.43626403808594\n",
            "Training epoch 8840/1000000, d_loss: -97.98567199707031,  g_loss: 5.798778533935547\n",
            "Training epoch 8841/1000000, d_loss: -59.08964538574219,  g_loss: 72.07257080078125\n",
            "Training epoch 8842/1000000, d_loss: -400.115234375,  g_loss: -39.28895568847656\n",
            "Training epoch 8843/1000000, d_loss: -27.54253387451172,  g_loss: 70.87736511230469\n",
            "Training epoch 8844/1000000, d_loss: 20.57903289794922,  g_loss: 85.10485076904297\n",
            "Training epoch 8845/1000000, d_loss: -32.46669006347656,  g_loss: 55.982666015625\n",
            "Training epoch 8846/1000000, d_loss: -55.94719696044922,  g_loss: 86.93020629882812\n",
            "Training epoch 8847/1000000, d_loss: -54.48617935180664,  g_loss: 63.19133758544922\n",
            "Training epoch 8848/1000000, d_loss: -100.37969970703125,  g_loss: 21.383071899414062\n",
            "Training epoch 8849/1000000, d_loss: -92.65451049804688,  g_loss: 54.709190368652344\n",
            "Training epoch 8850/1000000, d_loss: -245.55401611328125,  g_loss: 17.291812896728516\n",
            "Training epoch 8851/1000000, d_loss: -240.98910522460938,  g_loss: -36.03714370727539\n",
            "Training epoch 8852/1000000, d_loss: -54.713348388671875,  g_loss: -11.636510848999023\n",
            "Training epoch 8853/1000000, d_loss: -72.5165786743164,  g_loss: 88.71566772460938\n",
            "Training epoch 8854/1000000, d_loss: 13.958869934082031,  g_loss: 115.92822265625\n",
            "Training epoch 8855/1000000, d_loss: -65.15879821777344,  g_loss: 91.80650329589844\n",
            "Training epoch 8856/1000000, d_loss: -80.4142837524414,  g_loss: 108.13050079345703\n",
            "Training epoch 8857/1000000, d_loss: -86.89606475830078,  g_loss: 74.78726196289062\n",
            "Training epoch 8858/1000000, d_loss: -106.89382934570312,  g_loss: 21.599685668945312\n",
            "Training epoch 8859/1000000, d_loss: -28.974708557128906,  g_loss: 53.58736038208008\n",
            "Training epoch 8860/1000000, d_loss: -89.77949523925781,  g_loss: 0.5958380699157715\n",
            "Training epoch 8861/1000000, d_loss: -251.4187469482422,  g_loss: -26.48810386657715\n",
            "Training epoch 8862/1000000, d_loss: -898.39013671875,  g_loss: -809.3367919921875\n",
            "Training epoch 8863/1000000, d_loss: -868.1952514648438,  g_loss: -530.07373046875\n",
            "Training epoch 8864/1000000, d_loss: 113.96958923339844,  g_loss: -33.16059875488281\n",
            "Training epoch 8865/1000000, d_loss: 22.77657699584961,  g_loss: -99.97798156738281\n",
            "Training epoch 8866/1000000, d_loss: -543.6431274414062,  g_loss: -54.702178955078125\n",
            "Training epoch 8867/1000000, d_loss: -66.71416473388672,  g_loss: 55.070533752441406\n",
            "Training epoch 8868/1000000, d_loss: -189.1290283203125,  g_loss: 149.9386444091797\n",
            "Training epoch 8869/1000000, d_loss: -298.849609375,  g_loss: 64.79485321044922\n",
            "Training epoch 8870/1000000, d_loss: -243.94863891601562,  g_loss: 168.80889892578125\n",
            "Training epoch 8871/1000000, d_loss: -172.86370849609375,  g_loss: 367.2781066894531\n",
            "Training epoch 8872/1000000, d_loss: 74.21408081054688,  g_loss: 33.017364501953125\n",
            "Training epoch 8873/1000000, d_loss: -1.8127517700195312,  g_loss: 63.024696350097656\n",
            "Training epoch 8874/1000000, d_loss: -294.3729248046875,  g_loss: -1.6629009246826172\n",
            "Training epoch 8875/1000000, d_loss: -73.08270263671875,  g_loss: 111.72086334228516\n",
            "Training epoch 8876/1000000, d_loss: -84.12266540527344,  g_loss: 73.7947998046875\n",
            "Training epoch 8877/1000000, d_loss: -61.6683349609375,  g_loss: 51.69055938720703\n",
            "Training epoch 8878/1000000, d_loss: -98.429443359375,  g_loss: 67.53404235839844\n",
            "Training epoch 8879/1000000, d_loss: -72.162841796875,  g_loss: 22.936729431152344\n",
            "Training epoch 8880/1000000, d_loss: -143.5360870361328,  g_loss: 54.779563903808594\n",
            "Training epoch 8881/1000000, d_loss: -139.75814819335938,  g_loss: -117.27265930175781\n",
            "Training epoch 8882/1000000, d_loss: -18.86815643310547,  g_loss: 92.3222885131836\n",
            "Training epoch 8883/1000000, d_loss: -80.1286849975586,  g_loss: 79.08009338378906\n",
            "Training epoch 8884/1000000, d_loss: -361.8773193359375,  g_loss: 87.90592956542969\n",
            "Training epoch 8885/1000000, d_loss: -161.50733947753906,  g_loss: -6.863269805908203\n",
            "Training epoch 8886/1000000, d_loss: 13.636787414550781,  g_loss: 93.18028259277344\n",
            "Training epoch 8887/1000000, d_loss: -50.714195251464844,  g_loss: 106.8168716430664\n",
            "Training epoch 8888/1000000, d_loss: -44.21875762939453,  g_loss: 72.67001342773438\n",
            "Training epoch 8889/1000000, d_loss: -45.15010070800781,  g_loss: 22.695602416992188\n",
            "Training epoch 8890/1000000, d_loss: -125.69161224365234,  g_loss: 167.46865844726562\n",
            "Training epoch 8891/1000000, d_loss: -67.28719329833984,  g_loss: 26.10010528564453\n",
            "Training epoch 8892/1000000, d_loss: -97.70738983154297,  g_loss: 49.736507415771484\n",
            "Training epoch 8893/1000000, d_loss: -103.83827209472656,  g_loss: 35.15983963012695\n",
            "Training epoch 8894/1000000, d_loss: -10.91476058959961,  g_loss: 124.91737365722656\n",
            "Training epoch 8895/1000000, d_loss: -96.11830139160156,  g_loss: 101.42413330078125\n",
            "Training epoch 8896/1000000, d_loss: -120.67829895019531,  g_loss: 140.98748779296875\n",
            "Training epoch 8897/1000000, d_loss: -310.7225036621094,  g_loss: 24.55605697631836\n",
            "Training epoch 8898/1000000, d_loss: -95.29340362548828,  g_loss: -12.606236457824707\n",
            "Training epoch 8899/1000000, d_loss: -106.12322235107422,  g_loss: 18.953126907348633\n",
            "Training epoch 8900/1000000, d_loss: -96.51263427734375,  g_loss: 28.246370315551758\n",
            "Training epoch 8901/1000000, d_loss: -226.900390625,  g_loss: -29.397653579711914\n",
            "Training epoch 8902/1000000, d_loss: -68.63920593261719,  g_loss: 32.0863151550293\n",
            "Training epoch 8903/1000000, d_loss: -251.5918731689453,  g_loss: 73.59405517578125\n",
            "Training epoch 8904/1000000, d_loss: -72.25125885009766,  g_loss: 92.15284729003906\n",
            "Training epoch 8905/1000000, d_loss: -54.221900939941406,  g_loss: 102.73503112792969\n",
            "Training epoch 8906/1000000, d_loss: -28.0828800201416,  g_loss: 81.90182495117188\n",
            "Training epoch 8907/1000000, d_loss: -160.29830932617188,  g_loss: -15.457807540893555\n",
            "Training epoch 8908/1000000, d_loss: -163.18539428710938,  g_loss: -58.295066833496094\n",
            "Training epoch 8909/1000000, d_loss: -80.94538879394531,  g_loss: 103.40214538574219\n",
            "Training epoch 8910/1000000, d_loss: -75.40031433105469,  g_loss: 83.21495056152344\n",
            "Training epoch 8911/1000000, d_loss: -40.626468658447266,  g_loss: 95.20869445800781\n",
            "Training epoch 8912/1000000, d_loss: -131.88760375976562,  g_loss: 59.230224609375\n",
            "Training epoch 8913/1000000, d_loss: -75.08953094482422,  g_loss: 81.5040283203125\n",
            "Training epoch 8914/1000000, d_loss: -109.66670227050781,  g_loss: 88.17991638183594\n",
            "Training epoch 8915/1000000, d_loss: -143.94009399414062,  g_loss: -30.312883377075195\n",
            "Training epoch 8916/1000000, d_loss: -110.55999755859375,  g_loss: 59.0572509765625\n",
            "Training epoch 8917/1000000, d_loss: -70.82052612304688,  g_loss: 75.93119812011719\n",
            "Training epoch 8918/1000000, d_loss: -23.961898803710938,  g_loss: 32.7287483215332\n",
            "Training epoch 8919/1000000, d_loss: -23.128997802734375,  g_loss: 60.99441146850586\n",
            "Training epoch 8920/1000000, d_loss: -99.42235565185547,  g_loss: 102.36927032470703\n",
            "Training epoch 8921/1000000, d_loss: -99.67158508300781,  g_loss: 123.0888900756836\n",
            "Training epoch 8922/1000000, d_loss: -62.417884826660156,  g_loss: 28.519187927246094\n",
            "Training epoch 8923/1000000, d_loss: -42.71565628051758,  g_loss: 18.370285034179688\n",
            "Training epoch 8924/1000000, d_loss: -95.25482940673828,  g_loss: 25.79265785217285\n",
            "Training epoch 8925/1000000, d_loss: -249.911865234375,  g_loss: -8.59400463104248\n",
            "Training epoch 8926/1000000, d_loss: -30.76995849609375,  g_loss: 71.43927001953125\n",
            "Training epoch 8927/1000000, d_loss: -825.329345703125,  g_loss: -65.13385009765625\n",
            "Training epoch 8928/1000000, d_loss: -1106.7509765625,  g_loss: -52.34949493408203\n",
            "Training epoch 8929/1000000, d_loss: 2021.6458740234375,  g_loss: -17.094276428222656\n",
            "Training epoch 8930/1000000, d_loss: 952.0419311523438,  g_loss: -6.802337646484375\n",
            "Training epoch 8931/1000000, d_loss: 206.96380615234375,  g_loss: -205.19921875\n",
            "Training epoch 8932/1000000, d_loss: 217.9969024658203,  g_loss: 9.788188934326172\n",
            "Training epoch 8933/1000000, d_loss: 274.5326843261719,  g_loss: -46.602272033691406\n",
            "Training epoch 8934/1000000, d_loss: 115.64622497558594,  g_loss: -200.83526611328125\n",
            "Training epoch 8935/1000000, d_loss: 253.7354736328125,  g_loss: 77.0713882446289\n",
            "Training epoch 8936/1000000, d_loss: -181.1264190673828,  g_loss: -88.27741241455078\n",
            "Training epoch 8937/1000000, d_loss: -143.06622314453125,  g_loss: 214.0652618408203\n",
            "Training epoch 8938/1000000, d_loss: -11.713020324707031,  g_loss: -33.068809509277344\n",
            "Training epoch 8939/1000000, d_loss: -46.44530487060547,  g_loss: -9.937807083129883\n",
            "Training epoch 8940/1000000, d_loss: -71.03898620605469,  g_loss: -20.368135452270508\n",
            "Training epoch 8941/1000000, d_loss: -160.7156524658203,  g_loss: 148.56723022460938\n",
            "Training epoch 8942/1000000, d_loss: 67.28656768798828,  g_loss: -37.64313888549805\n",
            "Training epoch 8943/1000000, d_loss: 9.245521545410156,  g_loss: -164.99920654296875\n",
            "Training epoch 8944/1000000, d_loss: -137.45248413085938,  g_loss: -90.4326400756836\n",
            "Training epoch 8945/1000000, d_loss: -50.300453186035156,  g_loss: -63.58873748779297\n",
            "Training epoch 8946/1000000, d_loss: -253.42999267578125,  g_loss: -71.5869140625\n",
            "Training epoch 8947/1000000, d_loss: -25.65711212158203,  g_loss: -75.89700317382812\n",
            "Training epoch 8948/1000000, d_loss: 226.36790466308594,  g_loss: -1.7347173690795898\n",
            "Training epoch 8949/1000000, d_loss: -120.54991149902344,  g_loss: 118.9217758178711\n",
            "Training epoch 8950/1000000, d_loss: -59.978458404541016,  g_loss: 42.17511749267578\n",
            "Training epoch 8951/1000000, d_loss: -102.87882995605469,  g_loss: 122.51155090332031\n",
            "Training epoch 8952/1000000, d_loss: -133.828369140625,  g_loss: 167.99728393554688\n",
            "Training epoch 8953/1000000, d_loss: -75.98125457763672,  g_loss: 55.75196838378906\n",
            "Training epoch 8954/1000000, d_loss: -176.850830078125,  g_loss: 94.1229248046875\n",
            "Training epoch 8955/1000000, d_loss: -191.788330078125,  g_loss: 169.23768615722656\n",
            "Training epoch 8956/1000000, d_loss: -591.1044921875,  g_loss: -66.6629638671875\n",
            "Training epoch 8957/1000000, d_loss: 1073.9315185546875,  g_loss: 50.34666061401367\n",
            "Training epoch 8958/1000000, d_loss: -23.878433227539062,  g_loss: 37.04071807861328\n",
            "Training epoch 8959/1000000, d_loss: -40.87781524658203,  g_loss: 59.71266174316406\n",
            "Training epoch 8960/1000000, d_loss: 25.042587280273438,  g_loss: 50.70984649658203\n",
            "Training epoch 8961/1000000, d_loss: -69.06116485595703,  g_loss: 81.37434387207031\n",
            "Training epoch 8962/1000000, d_loss: -15.472503662109375,  g_loss: 105.42352294921875\n",
            "Training epoch 8963/1000000, d_loss: -62.908668518066406,  g_loss: 72.31218719482422\n",
            "Training epoch 8964/1000000, d_loss: -47.21702575683594,  g_loss: 139.12957763671875\n",
            "Training epoch 8965/1000000, d_loss: -95.0023193359375,  g_loss: 50.67359161376953\n",
            "Training epoch 8966/1000000, d_loss: -2.243793487548828,  g_loss: 66.09614562988281\n",
            "Training epoch 8967/1000000, d_loss: -199.52606201171875,  g_loss: 23.704748153686523\n",
            "Training epoch 8968/1000000, d_loss: -70.32676696777344,  g_loss: 56.345069885253906\n",
            "Training epoch 8969/1000000, d_loss: -241.5116729736328,  g_loss: 351.34197998046875\n",
            "Training epoch 8970/1000000, d_loss: -74.00109100341797,  g_loss: -5.096264839172363\n",
            "Training epoch 8971/1000000, d_loss: -83.10737609863281,  g_loss: 9.541419982910156\n",
            "Training epoch 8972/1000000, d_loss: -54.95660400390625,  g_loss: 86.3031005859375\n",
            "Training epoch 8973/1000000, d_loss: -100.30520629882812,  g_loss: 50.56318664550781\n",
            "Training epoch 8974/1000000, d_loss: -193.62014770507812,  g_loss: 3.0068206787109375\n",
            "Training epoch 8975/1000000, d_loss: -74.42760467529297,  g_loss: 79.85575866699219\n",
            "Training epoch 8976/1000000, d_loss: 20.44677734375,  g_loss: 58.059593200683594\n",
            "Training epoch 8977/1000000, d_loss: -25.813724517822266,  g_loss: 77.52999114990234\n",
            "Training epoch 8978/1000000, d_loss: -101.40351104736328,  g_loss: 88.00045013427734\n",
            "Training epoch 8979/1000000, d_loss: -118.6610107421875,  g_loss: 47.68868637084961\n",
            "Training epoch 8980/1000000, d_loss: -0.5762786865234375,  g_loss: 60.05601501464844\n",
            "Training epoch 8981/1000000, d_loss: -40.98876953125,  g_loss: 101.21261596679688\n",
            "Training epoch 8982/1000000, d_loss: -22.87940216064453,  g_loss: 56.34581756591797\n",
            "Training epoch 8983/1000000, d_loss: -125.19685363769531,  g_loss: 31.27170753479004\n",
            "Training epoch 8984/1000000, d_loss: -73.71150970458984,  g_loss: 67.2783203125\n",
            "Training epoch 8985/1000000, d_loss: -56.131011962890625,  g_loss: 71.78205871582031\n",
            "Training epoch 8986/1000000, d_loss: -92.44156646728516,  g_loss: 53.735965728759766\n",
            "Training epoch 8987/1000000, d_loss: -96.23960876464844,  g_loss: 45.81328582763672\n",
            "Training epoch 8988/1000000, d_loss: -135.532958984375,  g_loss: 61.195777893066406\n",
            "Training epoch 8989/1000000, d_loss: -287.21820068359375,  g_loss: -87.2056884765625\n",
            "Training epoch 8990/1000000, d_loss: -838.9578857421875,  g_loss: -57.8292350769043\n",
            "Training epoch 8991/1000000, d_loss: -849.0125732421875,  g_loss: 12.84304428100586\n",
            "Training epoch 8992/1000000, d_loss: -89.1865005493164,  g_loss: -88.30567932128906\n",
            "Training epoch 8993/1000000, d_loss: 26.557390213012695,  g_loss: -67.050537109375\n",
            "Training epoch 8994/1000000, d_loss: 44.999298095703125,  g_loss: 88.00933837890625\n",
            "Training epoch 8995/1000000, d_loss: -53.72601318359375,  g_loss: 94.73548889160156\n",
            "Training epoch 8996/1000000, d_loss: -149.8870086669922,  g_loss: 73.468505859375\n",
            "Training epoch 8997/1000000, d_loss: -349.0845947265625,  g_loss: 100.3599853515625\n",
            "Training epoch 8998/1000000, d_loss: -65.34440612792969,  g_loss: -80.91046142578125\n",
            "Training epoch 8999/1000000, d_loss: -103.36668395996094,  g_loss: 120.39398193359375\n",
            "Training epoch 9000/1000000, d_loss: -297.1038818359375,  g_loss: -47.98735046386719\n",
            "Training epoch 9001/1000000, d_loss: -244.375,  g_loss: -59.3372802734375\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 61/61 [00:00<00:00, 200.56it/s]\n",
            "Meshing: 100%|██████████| 4843/4843 [00:01<00:00, 2657.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_9001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_9001/assets\n",
            "Training epoch 9002/1000000, d_loss: -46.770416259765625,  g_loss: 23.663162231445312\n",
            "Training epoch 9003/1000000, d_loss: -97.9296875,  g_loss: 126.95902252197266\n",
            "Training epoch 9004/1000000, d_loss: -51.3321418762207,  g_loss: 97.31364440917969\n",
            "Training epoch 9005/1000000, d_loss: -157.26651000976562,  g_loss: 6.459301948547363\n",
            "Training epoch 9006/1000000, d_loss: -46.76200866699219,  g_loss: 62.5666618347168\n",
            "Training epoch 9007/1000000, d_loss: -101.93592071533203,  g_loss: 54.33203125\n",
            "Training epoch 9008/1000000, d_loss: -116.47319030761719,  g_loss: -36.85861587524414\n",
            "Training epoch 9009/1000000, d_loss: -103.1759033203125,  g_loss: 53.86577606201172\n",
            "Training epoch 9010/1000000, d_loss: -119.98145294189453,  g_loss: 120.67771911621094\n",
            "Training epoch 9011/1000000, d_loss: -40.89579391479492,  g_loss: 166.218505859375\n",
            "Training epoch 9012/1000000, d_loss: -50.88896560668945,  g_loss: 137.66323852539062\n",
            "Training epoch 9013/1000000, d_loss: -146.48904418945312,  g_loss: 140.15875244140625\n",
            "Training epoch 9014/1000000, d_loss: 4.386524200439453,  g_loss: 160.9450225830078\n",
            "Training epoch 9015/1000000, d_loss: -85.51974487304688,  g_loss: 223.5819854736328\n",
            "Training epoch 9016/1000000, d_loss: -103.85760498046875,  g_loss: 210.7381591796875\n",
            "Training epoch 9017/1000000, d_loss: -151.76316833496094,  g_loss: 336.08489990234375\n",
            "Training epoch 9018/1000000, d_loss: 889.9719848632812,  g_loss: 0.5873756408691406\n",
            "Training epoch 9019/1000000, d_loss: -90.05028533935547,  g_loss: -2.400021553039551\n",
            "Training epoch 9020/1000000, d_loss: -105.99880981445312,  g_loss: -30.080486297607422\n",
            "Training epoch 9021/1000000, d_loss: -45.88138961791992,  g_loss: 6.763954162597656\n",
            "Training epoch 9022/1000000, d_loss: -73.55687713623047,  g_loss: -8.439186096191406\n",
            "Training epoch 9023/1000000, d_loss: -100.21710968017578,  g_loss: 60.29256057739258\n",
            "Training epoch 9024/1000000, d_loss: -106.76728820800781,  g_loss: 131.09783935546875\n",
            "Training epoch 9025/1000000, d_loss: -102.90306854248047,  g_loss: 117.26873779296875\n",
            "Training epoch 9026/1000000, d_loss: -190.6367950439453,  g_loss: 40.77388381958008\n",
            "Training epoch 9027/1000000, d_loss: -28.070209503173828,  g_loss: 75.73473358154297\n",
            "Training epoch 9028/1000000, d_loss: -125.38314819335938,  g_loss: 83.60739135742188\n",
            "Training epoch 9029/1000000, d_loss: -85.83069610595703,  g_loss: 6.878114700317383\n",
            "Training epoch 9030/1000000, d_loss: -33.363243103027344,  g_loss: 19.049331665039062\n",
            "Training epoch 9031/1000000, d_loss: -37.93049621582031,  g_loss: 57.728668212890625\n",
            "Training epoch 9032/1000000, d_loss: -12.562625885009766,  g_loss: 74.25032043457031\n",
            "Training epoch 9033/1000000, d_loss: -15.901779174804688,  g_loss: 64.3764419555664\n",
            "Training epoch 9034/1000000, d_loss: 116.53580474853516,  g_loss: 78.75917053222656\n",
            "Training epoch 9035/1000000, d_loss: -142.67108154296875,  g_loss: 60.855628967285156\n",
            "Training epoch 9036/1000000, d_loss: -44.47869110107422,  g_loss: 45.801841735839844\n",
            "Training epoch 9037/1000000, d_loss: -120.28033447265625,  g_loss: 20.68526840209961\n",
            "Training epoch 9038/1000000, d_loss: -84.14937591552734,  g_loss: 65.66825866699219\n",
            "Training epoch 9039/1000000, d_loss: -80.97093200683594,  g_loss: 71.00355529785156\n",
            "Training epoch 9040/1000000, d_loss: -228.19351196289062,  g_loss: 5.297419548034668\n",
            "Training epoch 9041/1000000, d_loss: -35.869998931884766,  g_loss: 47.59101867675781\n",
            "Training epoch 9042/1000000, d_loss: -148.18727111816406,  g_loss: -24.0288028717041\n",
            "Training epoch 9043/1000000, d_loss: -33.92546081542969,  g_loss: 77.85404968261719\n",
            "Training epoch 9044/1000000, d_loss: -423.59283447265625,  g_loss: -65.25397491455078\n",
            "Training epoch 9045/1000000, d_loss: -53.402984619140625,  g_loss: 27.182518005371094\n",
            "Training epoch 9046/1000000, d_loss: -55.73085021972656,  g_loss: 11.064743041992188\n",
            "Training epoch 9047/1000000, d_loss: -56.05807113647461,  g_loss: 161.95127868652344\n",
            "Training epoch 9048/1000000, d_loss: -41.13807678222656,  g_loss: 113.19564056396484\n",
            "Training epoch 9049/1000000, d_loss: -74.86805725097656,  g_loss: 51.52847671508789\n",
            "Training epoch 9050/1000000, d_loss: -26.078067779541016,  g_loss: 41.315643310546875\n",
            "Training epoch 9051/1000000, d_loss: -68.93394470214844,  g_loss: 26.0450439453125\n",
            "Training epoch 9052/1000000, d_loss: -55.819847106933594,  g_loss: 44.34143829345703\n",
            "Training epoch 9053/1000000, d_loss: -25.35407257080078,  g_loss: 72.51067352294922\n",
            "Training epoch 9054/1000000, d_loss: -84.7926254272461,  g_loss: 11.001373291015625\n",
            "Training epoch 9055/1000000, d_loss: -60.3487548828125,  g_loss: 7.052459239959717\n",
            "Training epoch 9056/1000000, d_loss: -165.11590576171875,  g_loss: -6.216503143310547\n",
            "Training epoch 9057/1000000, d_loss: -160.75296020507812,  g_loss: -14.431193351745605\n",
            "Training epoch 9058/1000000, d_loss: -590.0760498046875,  g_loss: -54.259559631347656\n",
            "Training epoch 9059/1000000, d_loss: -411.3740539550781,  g_loss: -108.84862518310547\n",
            "Training epoch 9060/1000000, d_loss: 259.6659240722656,  g_loss: 30.714797973632812\n",
            "Training epoch 9061/1000000, d_loss: -10.790382385253906,  g_loss: 27.321590423583984\n",
            "Training epoch 9062/1000000, d_loss: -6.0532426834106445,  g_loss: 73.10763549804688\n",
            "Training epoch 9063/1000000, d_loss: -195.20120239257812,  g_loss: 26.33387565612793\n",
            "Training epoch 9064/1000000, d_loss: -74.32810974121094,  g_loss: 30.175865173339844\n",
            "Training epoch 9065/1000000, d_loss: -51.50975036621094,  g_loss: 44.55604553222656\n",
            "Training epoch 9066/1000000, d_loss: -47.569759368896484,  g_loss: 16.699764251708984\n",
            "Training epoch 9067/1000000, d_loss: -150.05389404296875,  g_loss: -12.395923614501953\n",
            "Training epoch 9068/1000000, d_loss: -129.27174377441406,  g_loss: 25.094865798950195\n",
            "Training epoch 9069/1000000, d_loss: -540.2804565429688,  g_loss: -60.97109603881836\n",
            "Training epoch 9070/1000000, d_loss: -8.90018081665039,  g_loss: 17.466564178466797\n",
            "Training epoch 9071/1000000, d_loss: -44.25480651855469,  g_loss: 97.92433166503906\n",
            "Training epoch 9072/1000000, d_loss: -196.5698699951172,  g_loss: 91.32677459716797\n",
            "Training epoch 9073/1000000, d_loss: -50.90127182006836,  g_loss: 43.06435775756836\n",
            "Training epoch 9074/1000000, d_loss: -109.49486541748047,  g_loss: 2.0377426147460938\n",
            "Training epoch 9075/1000000, d_loss: -1.3092498779296875,  g_loss: 70.4179916381836\n",
            "Training epoch 9076/1000000, d_loss: -56.4018669128418,  g_loss: 53.048728942871094\n",
            "Training epoch 9077/1000000, d_loss: 1051.597412109375,  g_loss: 63.82334518432617\n",
            "Training epoch 9078/1000000, d_loss: -59.31178283691406,  g_loss: 63.456016540527344\n",
            "Training epoch 9079/1000000, d_loss: -35.60251235961914,  g_loss: 63.8604850769043\n",
            "Training epoch 9080/1000000, d_loss: -57.61646270751953,  g_loss: 48.93391418457031\n",
            "Training epoch 9081/1000000, d_loss: -161.65365600585938,  g_loss: 44.25136184692383\n",
            "Training epoch 9082/1000000, d_loss: -82.9011001586914,  g_loss: -31.89750099182129\n",
            "Training epoch 9083/1000000, d_loss: -46.95862579345703,  g_loss: 57.06010437011719\n",
            "Training epoch 9084/1000000, d_loss: -59.668941497802734,  g_loss: 76.3302001953125\n",
            "Training epoch 9085/1000000, d_loss: -71.99347686767578,  g_loss: 89.22293090820312\n",
            "Training epoch 9086/1000000, d_loss: -128.609619140625,  g_loss: 68.02667999267578\n",
            "Training epoch 9087/1000000, d_loss: -176.80255126953125,  g_loss: 25.762754440307617\n",
            "Training epoch 9088/1000000, d_loss: -392.2839050292969,  g_loss: -132.72525024414062\n",
            "Training epoch 9089/1000000, d_loss: -121.09698486328125,  g_loss: 93.1419677734375\n",
            "Training epoch 9090/1000000, d_loss: -15.354499816894531,  g_loss: 89.81519317626953\n",
            "Training epoch 9091/1000000, d_loss: -80.20804595947266,  g_loss: 52.173885345458984\n",
            "Training epoch 9092/1000000, d_loss: -153.33827209472656,  g_loss: 5.5859198570251465\n",
            "Training epoch 9093/1000000, d_loss: -36.673744201660156,  g_loss: 57.90631866455078\n",
            "Training epoch 9094/1000000, d_loss: -78.94547271728516,  g_loss: 22.35072135925293\n",
            "Training epoch 9095/1000000, d_loss: -1035.62939453125,  g_loss: -46.89775848388672\n",
            "Training epoch 9096/1000000, d_loss: 19.20696258544922,  g_loss: -22.363256454467773\n",
            "Training epoch 9097/1000000, d_loss: -55.92307662963867,  g_loss: -72.12981414794922\n",
            "Training epoch 9098/1000000, d_loss: -51.3052864074707,  g_loss: -71.63262176513672\n",
            "Training epoch 9099/1000000, d_loss: -151.37066650390625,  g_loss: -84.29850769042969\n",
            "Training epoch 9100/1000000, d_loss: -207.4615936279297,  g_loss: -61.89754104614258\n",
            "Training epoch 9101/1000000, d_loss: -99.33433532714844,  g_loss: -8.612621307373047\n",
            "Training epoch 9102/1000000, d_loss: -174.22169494628906,  g_loss: 118.49412536621094\n",
            "Training epoch 9103/1000000, d_loss: -154.82901000976562,  g_loss: 219.54043579101562\n",
            "Training epoch 9104/1000000, d_loss: -63.7027702331543,  g_loss: 2.829580307006836\n",
            "Training epoch 9105/1000000, d_loss: -19.54990005493164,  g_loss: 32.55120849609375\n",
            "Training epoch 9106/1000000, d_loss: -106.6304931640625,  g_loss: 51.26892852783203\n",
            "Training epoch 9107/1000000, d_loss: -154.89166259765625,  g_loss: 45.285762786865234\n",
            "Training epoch 9108/1000000, d_loss: -774.8285522460938,  g_loss: -164.81369018554688\n",
            "Training epoch 9109/1000000, d_loss: -78.19657135009766,  g_loss: -29.623231887817383\n",
            "Training epoch 9110/1000000, d_loss: -60.84861755371094,  g_loss: -25.562759399414062\n",
            "Training epoch 9111/1000000, d_loss: -100.44026184082031,  g_loss: 0.5280168056488037\n",
            "Training epoch 9112/1000000, d_loss: -260.1146240234375,  g_loss: -16.452749252319336\n",
            "Training epoch 9113/1000000, d_loss: -41.77667236328125,  g_loss: 20.173812866210938\n",
            "Training epoch 9114/1000000, d_loss: -535.0753784179688,  g_loss: -78.17338562011719\n",
            "Training epoch 9115/1000000, d_loss: 8.274654388427734,  g_loss: 44.33226776123047\n",
            "Training epoch 9116/1000000, d_loss: -224.01956176757812,  g_loss: 32.714393615722656\n",
            "Training epoch 9117/1000000, d_loss: -130.26885986328125,  g_loss: 195.64315795898438\n",
            "Training epoch 9118/1000000, d_loss: -445.46844482421875,  g_loss: 153.11976623535156\n",
            "Training epoch 9119/1000000, d_loss: -18.323898315429688,  g_loss: 1.0577125549316406\n",
            "Training epoch 9120/1000000, d_loss: -87.2593994140625,  g_loss: -50.66652297973633\n",
            "Training epoch 9121/1000000, d_loss: -76.9129867553711,  g_loss: 34.61546325683594\n",
            "Training epoch 9122/1000000, d_loss: -172.64041137695312,  g_loss: -23.704360961914062\n",
            "Training epoch 9123/1000000, d_loss: -542.048583984375,  g_loss: -322.13885498046875\n",
            "Training epoch 9124/1000000, d_loss: 29.767471313476562,  g_loss: -26.898828506469727\n",
            "Training epoch 9125/1000000, d_loss: 71.84676361083984,  g_loss: 41.06193542480469\n",
            "Training epoch 9126/1000000, d_loss: -36.74003601074219,  g_loss: 42.59166717529297\n",
            "Training epoch 9127/1000000, d_loss: -87.89895629882812,  g_loss: 108.22262573242188\n",
            "Training epoch 9128/1000000, d_loss: -113.7589111328125,  g_loss: 148.433837890625\n",
            "Training epoch 9129/1000000, d_loss: -73.8426513671875,  g_loss: 110.88937377929688\n",
            "Training epoch 9130/1000000, d_loss: -154.1978302001953,  g_loss: 134.90753173828125\n",
            "Training epoch 9131/1000000, d_loss: -146.74734497070312,  g_loss: 78.55473327636719\n",
            "Training epoch 9132/1000000, d_loss: -99.96636962890625,  g_loss: 103.46810913085938\n",
            "Training epoch 9133/1000000, d_loss: -558.5826416015625,  g_loss: -134.39657592773438\n",
            "Training epoch 9134/1000000, d_loss: -181.49598693847656,  g_loss: -98.2791519165039\n",
            "Training epoch 9135/1000000, d_loss: -282.4243469238281,  g_loss: -76.47772216796875\n",
            "Training epoch 9136/1000000, d_loss: -19.61717987060547,  g_loss: 60.56718826293945\n",
            "Training epoch 9137/1000000, d_loss: -114.3638687133789,  g_loss: 135.68722534179688\n",
            "Training epoch 9138/1000000, d_loss: -4.690269470214844,  g_loss: 31.753145217895508\n",
            "Training epoch 9139/1000000, d_loss: -67.90631866455078,  g_loss: 100.14967346191406\n",
            "Training epoch 9140/1000000, d_loss: -243.19223022460938,  g_loss: 11.63381576538086\n",
            "Training epoch 9141/1000000, d_loss: -51.14189147949219,  g_loss: 43.8027458190918\n",
            "Training epoch 9142/1000000, d_loss: -173.8448944091797,  g_loss: 12.24505615234375\n",
            "Training epoch 9143/1000000, d_loss: 94.61331939697266,  g_loss: 21.11634635925293\n",
            "Training epoch 9144/1000000, d_loss: -103.41768646240234,  g_loss: 33.546470642089844\n",
            "Training epoch 9145/1000000, d_loss: -52.59880828857422,  g_loss: 146.82470703125\n",
            "Training epoch 9146/1000000, d_loss: -77.06192779541016,  g_loss: 107.84890747070312\n",
            "Training epoch 9147/1000000, d_loss: -279.2502746582031,  g_loss: -56.290077209472656\n",
            "Training epoch 9148/1000000, d_loss: 88.59780883789062,  g_loss: 47.73112487792969\n",
            "Training epoch 9149/1000000, d_loss: -43.24751281738281,  g_loss: 104.22613525390625\n",
            "Training epoch 9150/1000000, d_loss: -325.2336120605469,  g_loss: 36.99876403808594\n",
            "Training epoch 9151/1000000, d_loss: -130.91506958007812,  g_loss: 78.39168548583984\n",
            "Training epoch 9152/1000000, d_loss: -1003.4076538085938,  g_loss: -145.33740234375\n",
            "Training epoch 9153/1000000, d_loss: 235.9862823486328,  g_loss: -143.5623321533203\n",
            "Training epoch 9154/1000000, d_loss: -61.45165252685547,  g_loss: -7.78114128112793\n",
            "Training epoch 9155/1000000, d_loss: 154.91064453125,  g_loss: 60.79722595214844\n",
            "Training epoch 9156/1000000, d_loss: -59.51029968261719,  g_loss: 88.28237915039062\n",
            "Training epoch 9157/1000000, d_loss: -64.56626892089844,  g_loss: 24.15212631225586\n",
            "Training epoch 9158/1000000, d_loss: -377.7168884277344,  g_loss: -43.073951721191406\n",
            "Training epoch 9159/1000000, d_loss: 21.624649047851562,  g_loss: -86.71862030029297\n",
            "Training epoch 9160/1000000, d_loss: -169.75572204589844,  g_loss: -11.231834411621094\n",
            "Training epoch 9161/1000000, d_loss: -402.5497131347656,  g_loss: -5.080258369445801\n",
            "Training epoch 9162/1000000, d_loss: -848.7975463867188,  g_loss: -66.64105224609375\n",
            "Training epoch 9163/1000000, d_loss: -144.64068603515625,  g_loss: 29.5196533203125\n",
            "Training epoch 9164/1000000, d_loss: -110.52822875976562,  g_loss: 51.340091705322266\n",
            "Training epoch 9165/1000000, d_loss: -511.596435546875,  g_loss: 2.5881919860839844\n",
            "Training epoch 9166/1000000, d_loss: -67.41090393066406,  g_loss: 22.126209259033203\n",
            "Training epoch 9167/1000000, d_loss: -66.41026306152344,  g_loss: 40.888580322265625\n",
            "Training epoch 9168/1000000, d_loss: -135.69261169433594,  g_loss: 91.57720947265625\n",
            "Training epoch 9169/1000000, d_loss: -110.44764709472656,  g_loss: 164.7518768310547\n",
            "Training epoch 9170/1000000, d_loss: -353.63677978515625,  g_loss: 557.3661499023438\n",
            "Training epoch 9171/1000000, d_loss: -8.551603317260742,  g_loss: -20.40355682373047\n",
            "Training epoch 9172/1000000, d_loss: -56.93598937988281,  g_loss: 42.54540252685547\n",
            "Training epoch 9173/1000000, d_loss: -105.68108367919922,  g_loss: 10.807722091674805\n",
            "Training epoch 9174/1000000, d_loss: -264.9447021484375,  g_loss: -0.08514976501464844\n",
            "Training epoch 9175/1000000, d_loss: 13.404930114746094,  g_loss: 19.967592239379883\n",
            "Training epoch 9176/1000000, d_loss: -202.50997924804688,  g_loss: -118.91680908203125\n",
            "Training epoch 9177/1000000, d_loss: -363.0501708984375,  g_loss: -112.77786254882812\n",
            "Training epoch 9178/1000000, d_loss: -708.6658935546875,  g_loss: -625.3214721679688\n",
            "Training epoch 9179/1000000, d_loss: 346.04522705078125,  g_loss: 31.377140045166016\n",
            "Training epoch 9180/1000000, d_loss: -64.65487670898438,  g_loss: 132.378662109375\n",
            "Training epoch 9181/1000000, d_loss: -292.1552429199219,  g_loss: 85.16090393066406\n",
            "Training epoch 9182/1000000, d_loss: -16.59793472290039,  g_loss: 36.04466247558594\n",
            "Training epoch 9183/1000000, d_loss: -154.7576904296875,  g_loss: 72.85549926757812\n",
            "Training epoch 9184/1000000, d_loss: -200.90679931640625,  g_loss: 235.07904052734375\n",
            "Training epoch 9185/1000000, d_loss: -168.3444061279297,  g_loss: 277.3270568847656\n",
            "Training epoch 9186/1000000, d_loss: -167.02374267578125,  g_loss: 422.10162353515625\n",
            "Training epoch 9187/1000000, d_loss: -72.36819458007812,  g_loss: 217.58016967773438\n",
            "Training epoch 9188/1000000, d_loss: -247.9145050048828,  g_loss: 31.639751434326172\n",
            "Training epoch 9189/1000000, d_loss: -870.2452392578125,  g_loss: 8.370797157287598\n",
            "Training epoch 9190/1000000, d_loss: 105.66678619384766,  g_loss: 0.6294155120849609\n",
            "Training epoch 9191/1000000, d_loss: -18.189430236816406,  g_loss: 13.60384464263916\n",
            "Training epoch 9192/1000000, d_loss: -76.68696594238281,  g_loss: 29.776500701904297\n",
            "Training epoch 9193/1000000, d_loss: 64.43547058105469,  g_loss: 95.22224426269531\n",
            "Training epoch 9194/1000000, d_loss: -132.29348754882812,  g_loss: 109.2571792602539\n",
            "Training epoch 9195/1000000, d_loss: -134.1952667236328,  g_loss: 106.51158142089844\n",
            "Training epoch 9196/1000000, d_loss: 1712.852783203125,  g_loss: 136.41534423828125\n",
            "Training epoch 9197/1000000, d_loss: -64.72692108154297,  g_loss: 118.3556900024414\n",
            "Training epoch 9198/1000000, d_loss: -0.4073677062988281,  g_loss: 105.77066040039062\n",
            "Training epoch 9199/1000000, d_loss: -129.40792846679688,  g_loss: 116.6937255859375\n",
            "Training epoch 9200/1000000, d_loss: -3.3990535736083984,  g_loss: 140.19886779785156\n",
            "Training epoch 9201/1000000, d_loss: -92.75956726074219,  g_loss: 99.9062271118164\n",
            "Training epoch 9202/1000000, d_loss: -22.66321563720703,  g_loss: 115.43896484375\n",
            "Training epoch 9203/1000000, d_loss: -174.0572052001953,  g_loss: 223.748779296875\n",
            "Training epoch 9204/1000000, d_loss: -68.88908386230469,  g_loss: 98.56244659423828\n",
            "Training epoch 9205/1000000, d_loss: -276.7816162109375,  g_loss: 65.96450805664062\n",
            "Training epoch 9206/1000000, d_loss: 60.411346435546875,  g_loss: 76.86825561523438\n",
            "Training epoch 9207/1000000, d_loss: -98.32801055908203,  g_loss: 119.59647369384766\n",
            "Training epoch 9208/1000000, d_loss: -162.85955810546875,  g_loss: 72.0990982055664\n",
            "Training epoch 9209/1000000, d_loss: -86.10120391845703,  g_loss: 43.604400634765625\n",
            "Training epoch 9210/1000000, d_loss: -183.93663024902344,  g_loss: 72.3792724609375\n",
            "Training epoch 9211/1000000, d_loss: -131.6214141845703,  g_loss: 206.31695556640625\n",
            "Training epoch 9212/1000000, d_loss: -73.00003814697266,  g_loss: 180.16885375976562\n",
            "Training epoch 9213/1000000, d_loss: -216.690673828125,  g_loss: 65.29714965820312\n",
            "Training epoch 9214/1000000, d_loss: -47.04431915283203,  g_loss: 82.28795623779297\n",
            "Training epoch 9215/1000000, d_loss: -53.9196662902832,  g_loss: 88.36836242675781\n",
            "Training epoch 9216/1000000, d_loss: -121.2010498046875,  g_loss: 64.93744659423828\n",
            "Training epoch 9217/1000000, d_loss: -123.20809936523438,  g_loss: 67.3891372680664\n",
            "Training epoch 9218/1000000, d_loss: -354.46331787109375,  g_loss: 54.249534606933594\n",
            "Training epoch 9219/1000000, d_loss: 17.605741500854492,  g_loss: 54.12749481201172\n",
            "Training epoch 9220/1000000, d_loss: 76.94831848144531,  g_loss: 78.06010437011719\n",
            "Training epoch 9221/1000000, d_loss: -103.15711975097656,  g_loss: 37.36711120605469\n",
            "Training epoch 9222/1000000, d_loss: -105.62822723388672,  g_loss: 36.815147399902344\n",
            "Training epoch 9223/1000000, d_loss: -195.8500518798828,  g_loss: 37.0969352722168\n",
            "Training epoch 9224/1000000, d_loss: -54.39383316040039,  g_loss: 0.1995832920074463\n",
            "Training epoch 9225/1000000, d_loss: -57.380340576171875,  g_loss: 46.237548828125\n",
            "Training epoch 9226/1000000, d_loss: -365.56494140625,  g_loss: -30.660369873046875\n",
            "Training epoch 9227/1000000, d_loss: 24.315044403076172,  g_loss: 42.222652435302734\n",
            "Training epoch 9228/1000000, d_loss: -97.5960922241211,  g_loss: 32.629295349121094\n",
            "Training epoch 9229/1000000, d_loss: -43.55890655517578,  g_loss: 14.729120254516602\n",
            "Training epoch 9230/1000000, d_loss: -49.39459991455078,  g_loss: 29.614948272705078\n",
            "Training epoch 9231/1000000, d_loss: -51.332088470458984,  g_loss: 50.231407165527344\n",
            "Training epoch 9232/1000000, d_loss: -472.9372863769531,  g_loss: -308.51324462890625\n",
            "Training epoch 9233/1000000, d_loss: -43.34259033203125,  g_loss: 45.508419036865234\n",
            "Training epoch 9234/1000000, d_loss: -183.80056762695312,  g_loss: -16.804784774780273\n",
            "Training epoch 9235/1000000, d_loss: -119.09027862548828,  g_loss: 6.748455047607422\n",
            "Training epoch 9236/1000000, d_loss: -378.1591491699219,  g_loss: 275.4248046875\n",
            "Training epoch 9237/1000000, d_loss: -487.3103942871094,  g_loss: -185.2471923828125\n",
            "Training epoch 9238/1000000, d_loss: 36.22332763671875,  g_loss: 77.46199035644531\n",
            "Training epoch 9239/1000000, d_loss: -101.69349670410156,  g_loss: 157.64569091796875\n",
            "Training epoch 9240/1000000, d_loss: -118.50740051269531,  g_loss: 200.86538696289062\n",
            "Training epoch 9241/1000000, d_loss: -239.95919799804688,  g_loss: 67.28250122070312\n",
            "Training epoch 9242/1000000, d_loss: -718.5437622070312,  g_loss: -222.13661193847656\n",
            "Training epoch 9243/1000000, d_loss: -70.10262298583984,  g_loss: 91.47807312011719\n",
            "Training epoch 9244/1000000, d_loss: -417.71832275390625,  g_loss: 340.6286315917969\n",
            "Training epoch 9245/1000000, d_loss: -33.70564270019531,  g_loss: -9.877025604248047\n",
            "Training epoch 9246/1000000, d_loss: -121.51353454589844,  g_loss: 6.21977424621582\n",
            "Training epoch 9247/1000000, d_loss: -106.20008087158203,  g_loss: 68.11901092529297\n",
            "Training epoch 9248/1000000, d_loss: -90.90806579589844,  g_loss: 44.169189453125\n",
            "Training epoch 9249/1000000, d_loss: -121.1323471069336,  g_loss: 128.52796936035156\n",
            "Training epoch 9250/1000000, d_loss: -151.6845703125,  g_loss: 1.1276774406433105\n",
            "Training epoch 9251/1000000, d_loss: -119.43254852294922,  g_loss: -46.50768280029297\n",
            "Training epoch 9252/1000000, d_loss: -94.39533233642578,  g_loss: -40.554168701171875\n",
            "Training epoch 9253/1000000, d_loss: -78.97946166992188,  g_loss: -2.2799549102783203\n",
            "Training epoch 9254/1000000, d_loss: -359.4862976074219,  g_loss: -175.6702880859375\n",
            "Training epoch 9255/1000000, d_loss: -73.51301574707031,  g_loss: 124.48283386230469\n",
            "Training epoch 9256/1000000, d_loss: -68.71487426757812,  g_loss: 98.43446350097656\n",
            "Training epoch 9257/1000000, d_loss: -213.24114990234375,  g_loss: 22.393644332885742\n",
            "Training epoch 9258/1000000, d_loss: -283.8322448730469,  g_loss: 331.8816223144531\n",
            "Training epoch 9259/1000000, d_loss: 253.75323486328125,  g_loss: 348.580322265625\n",
            "Training epoch 9260/1000000, d_loss: -67.35723114013672,  g_loss: 37.11228561401367\n",
            "Training epoch 9261/1000000, d_loss: -128.06268310546875,  g_loss: 83.6589126586914\n",
            "Training epoch 9262/1000000, d_loss: -65.08147430419922,  g_loss: 29.409934997558594\n",
            "Training epoch 9263/1000000, d_loss: -102.83283996582031,  g_loss: 45.5382194519043\n",
            "Training epoch 9264/1000000, d_loss: -85.13753509521484,  g_loss: 86.92708587646484\n",
            "Training epoch 9265/1000000, d_loss: -49.466712951660156,  g_loss: 78.85824584960938\n",
            "Training epoch 9266/1000000, d_loss: -178.11880493164062,  g_loss: 74.55732727050781\n",
            "Training epoch 9267/1000000, d_loss: -67.26289367675781,  g_loss: 52.21622848510742\n",
            "Training epoch 9268/1000000, d_loss: -54.62944030761719,  g_loss: 72.54478454589844\n",
            "Training epoch 9269/1000000, d_loss: -84.93841552734375,  g_loss: 79.7809066772461\n",
            "Training epoch 9270/1000000, d_loss: -465.0938415527344,  g_loss: 7.4844279289245605\n",
            "Training epoch 9271/1000000, d_loss: -17.382421493530273,  g_loss: 75.74247741699219\n",
            "Training epoch 9272/1000000, d_loss: -13.877304077148438,  g_loss: 75.26616668701172\n",
            "Training epoch 9273/1000000, d_loss: -59.25090408325195,  g_loss: 63.45929718017578\n",
            "Training epoch 9274/1000000, d_loss: -39.242332458496094,  g_loss: 82.02729034423828\n",
            "Training epoch 9275/1000000, d_loss: -143.4084930419922,  g_loss: 29.950071334838867\n",
            "Training epoch 9276/1000000, d_loss: -39.84680938720703,  g_loss: 35.47926330566406\n",
            "Training epoch 9277/1000000, d_loss: -17.790733337402344,  g_loss: 65.41553497314453\n",
            "Training epoch 9278/1000000, d_loss: -209.93609619140625,  g_loss: 24.69390869140625\n",
            "Training epoch 9279/1000000, d_loss: -130.4931640625,  g_loss: 20.089731216430664\n",
            "Training epoch 9280/1000000, d_loss: -452.64788818359375,  g_loss: -207.9562530517578\n",
            "Training epoch 9281/1000000, d_loss: -38.43819808959961,  g_loss: 64.47315979003906\n",
            "Training epoch 9282/1000000, d_loss: -23.354476928710938,  g_loss: 35.66841125488281\n",
            "Training epoch 9283/1000000, d_loss: -84.85832214355469,  g_loss: 53.33652877807617\n",
            "Training epoch 9284/1000000, d_loss: -137.63970947265625,  g_loss: 19.104211807250977\n",
            "Training epoch 9285/1000000, d_loss: -79.84011840820312,  g_loss: 13.114013671875\n",
            "Training epoch 9286/1000000, d_loss: -99.52511596679688,  g_loss: 57.691070556640625\n",
            "Training epoch 9287/1000000, d_loss: -94.68799591064453,  g_loss: 42.494693756103516\n",
            "Training epoch 9288/1000000, d_loss: -145.35671997070312,  g_loss: -21.452383041381836\n",
            "Training epoch 9289/1000000, d_loss: -70.39774322509766,  g_loss: 63.97604751586914\n",
            "Training epoch 9290/1000000, d_loss: -80.201416015625,  g_loss: 76.96560668945312\n",
            "Training epoch 9291/1000000, d_loss: -165.22091674804688,  g_loss: 16.455013275146484\n",
            "Training epoch 9292/1000000, d_loss: -45.38084411621094,  g_loss: 74.77275848388672\n",
            "Training epoch 9293/1000000, d_loss: -195.60421752929688,  g_loss: 233.974365234375\n",
            "Training epoch 9294/1000000, d_loss: -93.15106201171875,  g_loss: 199.88946533203125\n",
            "Training epoch 9295/1000000, d_loss: -456.6133117675781,  g_loss: -126.2990493774414\n",
            "Training epoch 9296/1000000, d_loss: -12.323670387268066,  g_loss: -20.485763549804688\n",
            "Training epoch 9297/1000000, d_loss: -44.94285202026367,  g_loss: 17.730087280273438\n",
            "Training epoch 9298/1000000, d_loss: -50.46240997314453,  g_loss: 17.972257614135742\n",
            "Training epoch 9299/1000000, d_loss: -322.4599609375,  g_loss: -106.82711791992188\n",
            "Training epoch 9300/1000000, d_loss: -49.37269592285156,  g_loss: 24.833423614501953\n",
            "Training epoch 9301/1000000, d_loss: -150.35496520996094,  g_loss: 12.135602951049805\n",
            "Training epoch 9302/1000000, d_loss: -501.5422668457031,  g_loss: -392.4994812011719\n",
            "Training epoch 9303/1000000, d_loss: -177.10137939453125,  g_loss: 13.079243659973145\n",
            "Training epoch 9304/1000000, d_loss: -588.30517578125,  g_loss: -474.04608154296875\n",
            "Training epoch 9305/1000000, d_loss: 421.81158447265625,  g_loss: 58.67096710205078\n",
            "Training epoch 9306/1000000, d_loss: -213.45460510253906,  g_loss: 239.0803680419922\n",
            "Training epoch 9307/1000000, d_loss: -297.66937255859375,  g_loss: 6.073668956756592\n",
            "Training epoch 9308/1000000, d_loss: -432.5285949707031,  g_loss: 525.2901611328125\n",
            "Training epoch 9309/1000000, d_loss: -298.23980712890625,  g_loss: 702.1805419921875\n",
            "Training epoch 9310/1000000, d_loss: -75.16915893554688,  g_loss: 74.51737976074219\n",
            "Training epoch 9311/1000000, d_loss: 9.074410438537598,  g_loss: -9.639779090881348\n",
            "Training epoch 9312/1000000, d_loss: -51.059608459472656,  g_loss: 21.671493530273438\n",
            "Training epoch 9313/1000000, d_loss: -239.3592987060547,  g_loss: 135.0606231689453\n",
            "Training epoch 9314/1000000, d_loss: -53.185096740722656,  g_loss: 33.58085632324219\n",
            "Training epoch 9315/1000000, d_loss: -248.45436096191406,  g_loss: -43.37980270385742\n",
            "Training epoch 9316/1000000, d_loss: -38.415428161621094,  g_loss: 1.2511367797851562\n",
            "Training epoch 9317/1000000, d_loss: -166.92913818359375,  g_loss: -62.59183883666992\n",
            "Training epoch 9318/1000000, d_loss: -121.01913452148438,  g_loss: 30.571168899536133\n",
            "Training epoch 9319/1000000, d_loss: -713.54638671875,  g_loss: -380.1732482910156\n",
            "Training epoch 9320/1000000, d_loss: -107.07564544677734,  g_loss: 88.7925796508789\n",
            "Training epoch 9321/1000000, d_loss: -171.24058532714844,  g_loss: 240.3764190673828\n",
            "Training epoch 9322/1000000, d_loss: -235.82366943359375,  g_loss: -153.93862915039062\n",
            "Training epoch 9323/1000000, d_loss: -475.78564453125,  g_loss: -147.0852813720703\n",
            "Training epoch 9324/1000000, d_loss: -109.17548370361328,  g_loss: 145.1842041015625\n",
            "Training epoch 9325/1000000, d_loss: -151.43008422851562,  g_loss: 52.46290588378906\n",
            "Training epoch 9326/1000000, d_loss: -199.27554321289062,  g_loss: 242.36019897460938\n",
            "Training epoch 9327/1000000, d_loss: -525.6188354492188,  g_loss: 1011.5137939453125\n",
            "Training epoch 9328/1000000, d_loss: -14.974571228027344,  g_loss: 210.19186401367188\n",
            "Training epoch 9329/1000000, d_loss: -248.63809204101562,  g_loss: 34.02450180053711\n",
            "Training epoch 9330/1000000, d_loss: -180.0450439453125,  g_loss: 141.54916381835938\n",
            "Training epoch 9331/1000000, d_loss: -67.71856689453125,  g_loss: 317.0701904296875\n",
            "Training epoch 9332/1000000, d_loss: -37.09172821044922,  g_loss: 91.57563781738281\n",
            "Training epoch 9333/1000000, d_loss: -0.2790069580078125,  g_loss: 78.21345520019531\n",
            "Training epoch 9334/1000000, d_loss: -113.24279022216797,  g_loss: 230.015625\n",
            "Training epoch 9335/1000000, d_loss: -47.67707061767578,  g_loss: 127.85813903808594\n",
            "Training epoch 9336/1000000, d_loss: -117.21356201171875,  g_loss: 43.502281188964844\n",
            "Training epoch 9337/1000000, d_loss: -178.66806030273438,  g_loss: 62.79896545410156\n",
            "Training epoch 9338/1000000, d_loss: 293.61248779296875,  g_loss: 99.66542053222656\n",
            "Training epoch 9339/1000000, d_loss: -174.34840393066406,  g_loss: 92.8994140625\n",
            "Training epoch 9340/1000000, d_loss: -22.107311248779297,  g_loss: 49.43278503417969\n",
            "Training epoch 9341/1000000, d_loss: -178.60911560058594,  g_loss: 18.031078338623047\n",
            "Training epoch 9342/1000000, d_loss: -42.797142028808594,  g_loss: 57.3104248046875\n",
            "Training epoch 9343/1000000, d_loss: -305.7106628417969,  g_loss: -30.39324951171875\n",
            "Training epoch 9344/1000000, d_loss: -143.52061462402344,  g_loss: 111.90254974365234\n",
            "Training epoch 9345/1000000, d_loss: -50.95989227294922,  g_loss: 135.53591918945312\n",
            "Training epoch 9346/1000000, d_loss: 0.868804931640625,  g_loss: 86.6278305053711\n",
            "Training epoch 9347/1000000, d_loss: -75.32038116455078,  g_loss: 77.12065124511719\n",
            "Training epoch 9348/1000000, d_loss: -44.59479522705078,  g_loss: 100.6041259765625\n",
            "Training epoch 9349/1000000, d_loss: -260.9849853515625,  g_loss: -2.562915802001953\n",
            "Training epoch 9350/1000000, d_loss: -46.58524703979492,  g_loss: 138.49122619628906\n",
            "Training epoch 9351/1000000, d_loss: -89.8334732055664,  g_loss: 127.59632873535156\n",
            "Training epoch 9352/1000000, d_loss: -145.52711486816406,  g_loss: 69.46864318847656\n",
            "Training epoch 9353/1000000, d_loss: -124.84154510498047,  g_loss: 111.49317932128906\n",
            "Training epoch 9354/1000000, d_loss: -337.248291015625,  g_loss: -6.9480133056640625\n",
            "Training epoch 9355/1000000, d_loss: -140.96371459960938,  g_loss: 65.43785095214844\n",
            "Training epoch 9356/1000000, d_loss: -234.27064514160156,  g_loss: 415.91156005859375\n",
            "Training epoch 9357/1000000, d_loss: -30.30663299560547,  g_loss: 72.62830352783203\n",
            "Training epoch 9358/1000000, d_loss: -101.44683837890625,  g_loss: 40.67138671875\n",
            "Training epoch 9359/1000000, d_loss: -256.70928955078125,  g_loss: -55.75190734863281\n",
            "Training epoch 9360/1000000, d_loss: -901.2234497070312,  g_loss: -496.0556945800781\n",
            "Training epoch 9361/1000000, d_loss: -106.68484497070312,  g_loss: 23.063518524169922\n",
            "Training epoch 9362/1000000, d_loss: -237.4636993408203,  g_loss: 557.98876953125\n",
            "Training epoch 9363/1000000, d_loss: -107.83405303955078,  g_loss: 148.58609008789062\n",
            "Training epoch 9364/1000000, d_loss: -77.77648162841797,  g_loss: 223.90924072265625\n",
            "Training epoch 9365/1000000, d_loss: -244.09628295898438,  g_loss: 376.1951904296875\n",
            "Training epoch 9366/1000000, d_loss: -53.14503479003906,  g_loss: 115.19621276855469\n",
            "Training epoch 9367/1000000, d_loss: -258.5390930175781,  g_loss: 100.39061737060547\n",
            "Training epoch 9368/1000000, d_loss: -57.46638488769531,  g_loss: 118.03160858154297\n",
            "Training epoch 9369/1000000, d_loss: -65.51052856445312,  g_loss: 81.59495544433594\n",
            "Training epoch 9370/1000000, d_loss: -111.08456420898438,  g_loss: 82.04668426513672\n",
            "Training epoch 9371/1000000, d_loss: -192.91590881347656,  g_loss: 70.23436737060547\n",
            "Training epoch 9372/1000000, d_loss: -66.48880004882812,  g_loss: 156.8707733154297\n",
            "Training epoch 9373/1000000, d_loss: 39.644561767578125,  g_loss: 67.09245300292969\n",
            "Training epoch 9374/1000000, d_loss: -95.86418914794922,  g_loss: 77.22258758544922\n",
            "Training epoch 9375/1000000, d_loss: -157.3379364013672,  g_loss: -3.197010040283203\n",
            "Training epoch 9376/1000000, d_loss: -181.82534790039062,  g_loss: 370.736328125\n",
            "Training epoch 9377/1000000, d_loss: -63.32930374145508,  g_loss: 46.25566482543945\n",
            "Training epoch 9378/1000000, d_loss: -585.3010864257812,  g_loss: -222.00753784179688\n",
            "Training epoch 9379/1000000, d_loss: -53.86103439331055,  g_loss: 15.18777084350586\n",
            "Training epoch 9380/1000000, d_loss: -15.479165077209473,  g_loss: 11.617485046386719\n",
            "Training epoch 9381/1000000, d_loss: -77.30531311035156,  g_loss: 30.51079559326172\n",
            "Training epoch 9382/1000000, d_loss: -118.35271453857422,  g_loss: -11.965564727783203\n",
            "Training epoch 9383/1000000, d_loss: -31.80060577392578,  g_loss: -10.271937370300293\n",
            "Training epoch 9384/1000000, d_loss: -19.955713272094727,  g_loss: 27.902156829833984\n",
            "Training epoch 9385/1000000, d_loss: -34.147674560546875,  g_loss: 3.5774950981140137\n",
            "Training epoch 9386/1000000, d_loss: -124.46748352050781,  g_loss: 61.4471549987793\n",
            "Training epoch 9387/1000000, d_loss: -333.5873718261719,  g_loss: -76.18043518066406\n",
            "Training epoch 9388/1000000, d_loss: -1423.72802734375,  g_loss: -270.2581787109375\n",
            "Training epoch 9389/1000000, d_loss: 22.42336654663086,  g_loss: 58.790374755859375\n",
            "Training epoch 9390/1000000, d_loss: -33.04287338256836,  g_loss: 14.54867935180664\n",
            "Training epoch 9391/1000000, d_loss: -39.423004150390625,  g_loss: 23.899951934814453\n",
            "Training epoch 9392/1000000, d_loss: -1264.318115234375,  g_loss: -159.52679443359375\n",
            "Training epoch 9393/1000000, d_loss: 63.72681427001953,  g_loss: 31.299589157104492\n",
            "Training epoch 9394/1000000, d_loss: -23.323280334472656,  g_loss: 40.37714767456055\n",
            "Training epoch 9395/1000000, d_loss: -578.3660888671875,  g_loss: -199.30429077148438\n",
            "Training epoch 9396/1000000, d_loss: 39.65729904174805,  g_loss: -29.098634719848633\n",
            "Training epoch 9397/1000000, d_loss: 68.84385681152344,  g_loss: -0.556427001953125\n",
            "Training epoch 9398/1000000, d_loss: -78.30146789550781,  g_loss: 84.04629516601562\n",
            "Training epoch 9399/1000000, d_loss: -130.21144104003906,  g_loss: 64.74958801269531\n",
            "Training epoch 9400/1000000, d_loss: -329.3394775390625,  g_loss: 435.97857666015625\n",
            "Training epoch 9401/1000000, d_loss: -198.44668579101562,  g_loss: 214.2404022216797\n",
            "Training epoch 9402/1000000, d_loss: -83.18463134765625,  g_loss: 90.12979125976562\n",
            "Training epoch 9403/1000000, d_loss: -134.09495544433594,  g_loss: 192.73439025878906\n",
            "Training epoch 9404/1000000, d_loss: 2.549213409423828,  g_loss: 368.79693603515625\n",
            "Training epoch 9405/1000000, d_loss: 20.805084228515625,  g_loss: 81.49404907226562\n",
            "Training epoch 9406/1000000, d_loss: -144.6676025390625,  g_loss: 292.9541931152344\n",
            "Training epoch 9407/1000000, d_loss: 18.089641571044922,  g_loss: -11.859864234924316\n",
            "Training epoch 9408/1000000, d_loss: -23.578916549682617,  g_loss: 7.8773040771484375\n",
            "Training epoch 9409/1000000, d_loss: -31.163780212402344,  g_loss: -0.3811807632446289\n",
            "Training epoch 9410/1000000, d_loss: -117.4925537109375,  g_loss: -3.012698173522949\n",
            "Training epoch 9411/1000000, d_loss: -21.058860778808594,  g_loss: -5.157939434051514\n",
            "Training epoch 9412/1000000, d_loss: -735.6715087890625,  g_loss: -56.24842071533203\n",
            "Training epoch 9413/1000000, d_loss: -1098.9078369140625,  g_loss: -424.0494079589844\n",
            "Training epoch 9414/1000000, d_loss: -34.99562072753906,  g_loss: -27.992431640625\n",
            "Training epoch 9415/1000000, d_loss: 1618.1590576171875,  g_loss: 5.536559104919434\n",
            "Training epoch 9416/1000000, d_loss: -75.39555358886719,  g_loss: 37.07855224609375\n",
            "Training epoch 9417/1000000, d_loss: -45.57777404785156,  g_loss: 7.038817882537842\n",
            "Training epoch 9418/1000000, d_loss: -474.2804260253906,  g_loss: 3.096432685852051\n",
            "Training epoch 9419/1000000, d_loss: -60.419090270996094,  g_loss: -18.975265502929688\n",
            "Training epoch 9420/1000000, d_loss: -39.72773742675781,  g_loss: 63.54375457763672\n",
            "Training epoch 9421/1000000, d_loss: -10.101165771484375,  g_loss: 83.8702392578125\n",
            "Training epoch 9422/1000000, d_loss: -77.09331512451172,  g_loss: 79.42180633544922\n",
            "Training epoch 9423/1000000, d_loss: -102.11698150634766,  g_loss: 70.60957336425781\n",
            "Training epoch 9424/1000000, d_loss: -345.16253662109375,  g_loss: 60.851463317871094\n",
            "Training epoch 9425/1000000, d_loss: -148.186279296875,  g_loss: 58.98475646972656\n",
            "Training epoch 9426/1000000, d_loss: -63.74032211303711,  g_loss: -94.35456085205078\n",
            "Training epoch 9427/1000000, d_loss: -47.579620361328125,  g_loss: 55.82552719116211\n",
            "Training epoch 9428/1000000, d_loss: -20.167266845703125,  g_loss: 52.25567626953125\n",
            "Training epoch 9429/1000000, d_loss: -121.40957641601562,  g_loss: 64.28770446777344\n",
            "Training epoch 9430/1000000, d_loss: 58.908538818359375,  g_loss: 106.05386352539062\n",
            "Training epoch 9431/1000000, d_loss: -89.46331024169922,  g_loss: 61.66509246826172\n",
            "Training epoch 9432/1000000, d_loss: -89.40937805175781,  g_loss: 158.45225524902344\n",
            "Training epoch 9433/1000000, d_loss: -76.23320007324219,  g_loss: 39.27153778076172\n",
            "Training epoch 9434/1000000, d_loss: -114.63053894042969,  g_loss: 128.64573669433594\n",
            "Training epoch 9435/1000000, d_loss: -275.36614990234375,  g_loss: -2.7134947776794434\n",
            "Training epoch 9436/1000000, d_loss: -28.361371994018555,  g_loss: 17.941368103027344\n",
            "Training epoch 9437/1000000, d_loss: -191.16258239746094,  g_loss: 31.820091247558594\n",
            "Training epoch 9438/1000000, d_loss: -46.58583068847656,  g_loss: 22.537025451660156\n",
            "Training epoch 9439/1000000, d_loss: -42.85804748535156,  g_loss: 30.531848907470703\n",
            "Training epoch 9440/1000000, d_loss: -462.0953674316406,  g_loss: -34.1384391784668\n",
            "Training epoch 9441/1000000, d_loss: -5990.3125,  g_loss: -411.5816955566406\n",
            "Training epoch 9442/1000000, d_loss: 917.6395263671875,  g_loss: -353.0671081542969\n",
            "Training epoch 9443/1000000, d_loss: 1421.619873046875,  g_loss: -609.1144409179688\n",
            "Training epoch 9444/1000000, d_loss: 304.8077087402344,  g_loss: -1032.8521728515625\n",
            "Training epoch 9445/1000000, d_loss: 339.37139892578125,  g_loss: -94.28081512451172\n",
            "Training epoch 9446/1000000, d_loss: 36.183685302734375,  g_loss: 125.96663665771484\n",
            "Training epoch 9447/1000000, d_loss: 88.07951354980469,  g_loss: 114.93196105957031\n",
            "Training epoch 9448/1000000, d_loss: -181.52139282226562,  g_loss: 205.30859375\n",
            "Training epoch 9449/1000000, d_loss: -167.6064453125,  g_loss: 149.6889190673828\n",
            "Training epoch 9450/1000000, d_loss: -305.65673828125,  g_loss: 304.5782165527344\n",
            "Training epoch 9451/1000000, d_loss: -104.70368957519531,  g_loss: 234.07736206054688\n",
            "Training epoch 9452/1000000, d_loss: -255.59506225585938,  g_loss: 264.76995849609375\n",
            "Training epoch 9453/1000000, d_loss: -53.80401611328125,  g_loss: 89.89373016357422\n",
            "Training epoch 9454/1000000, d_loss: -172.15545654296875,  g_loss: 12.677997589111328\n",
            "Training epoch 9455/1000000, d_loss: 66.27057647705078,  g_loss: -20.152427673339844\n",
            "Training epoch 9456/1000000, d_loss: -3.1684188842773438,  g_loss: -74.60810089111328\n",
            "Training epoch 9457/1000000, d_loss: -65.10433959960938,  g_loss: -29.888357162475586\n",
            "Training epoch 9458/1000000, d_loss: -137.04347229003906,  g_loss: -87.09678649902344\n",
            "Training epoch 9459/1000000, d_loss: -32.79583740234375,  g_loss: 1.9988975524902344\n",
            "Training epoch 9460/1000000, d_loss: -246.4518280029297,  g_loss: -26.015560150146484\n",
            "Training epoch 9461/1000000, d_loss: -60.720916748046875,  g_loss: -10.525535583496094\n",
            "Training epoch 9462/1000000, d_loss: -61.85939025878906,  g_loss: 28.84035873413086\n",
            "Training epoch 9463/1000000, d_loss: -236.5627899169922,  g_loss: 28.62468910217285\n",
            "Training epoch 9464/1000000, d_loss: -248.65768432617188,  g_loss: 63.576332092285156\n",
            "Training epoch 9465/1000000, d_loss: -67.14169311523438,  g_loss: 33.142173767089844\n",
            "Training epoch 9466/1000000, d_loss: -242.33523559570312,  g_loss: -7.845033645629883\n",
            "Training epoch 9467/1000000, d_loss: -42.72638702392578,  g_loss: 147.44198608398438\n",
            "Training epoch 9468/1000000, d_loss: -261.62548828125,  g_loss: 188.07852172851562\n",
            "Training epoch 9469/1000000, d_loss: -233.7161102294922,  g_loss: 420.1177978515625\n",
            "Training epoch 9470/1000000, d_loss: -88.29914855957031,  g_loss: 37.129905700683594\n",
            "Training epoch 9471/1000000, d_loss: -110.32693481445312,  g_loss: 29.084796905517578\n",
            "Training epoch 9472/1000000, d_loss: -51.162925720214844,  g_loss: 35.65433120727539\n",
            "Training epoch 9473/1000000, d_loss: -1264.10888671875,  g_loss: -75.2416000366211\n",
            "Training epoch 9474/1000000, d_loss: 100.7779541015625,  g_loss: -11.372199058532715\n",
            "Training epoch 9475/1000000, d_loss: -16.86020851135254,  g_loss: 55.52970886230469\n",
            "Training epoch 9476/1000000, d_loss: -144.030517578125,  g_loss: 88.77653503417969\n",
            "Training epoch 9477/1000000, d_loss: -103.66021728515625,  g_loss: 61.55558776855469\n",
            "Training epoch 9478/1000000, d_loss: -246.58047485351562,  g_loss: 278.3877258300781\n",
            "Training epoch 9479/1000000, d_loss: -94.25617980957031,  g_loss: 97.38731384277344\n",
            "Training epoch 9480/1000000, d_loss: -61.980159759521484,  g_loss: 109.38145446777344\n",
            "Training epoch 9481/1000000, d_loss: -13.522636413574219,  g_loss: 49.64092254638672\n",
            "Training epoch 9482/1000000, d_loss: -168.43560791015625,  g_loss: 37.19167709350586\n",
            "Training epoch 9483/1000000, d_loss: -24.808273315429688,  g_loss: 72.57258605957031\n",
            "Training epoch 9484/1000000, d_loss: -57.3167839050293,  g_loss: 96.65374755859375\n",
            "Training epoch 9485/1000000, d_loss: -129.31163024902344,  g_loss: 74.41111755371094\n",
            "Training epoch 9486/1000000, d_loss: -324.31854248046875,  g_loss: 118.25276947021484\n",
            "Training epoch 9487/1000000, d_loss: 17.06182098388672,  g_loss: 73.00495147705078\n",
            "Training epoch 9488/1000000, d_loss: -54.879364013671875,  g_loss: 50.26811981201172\n",
            "Training epoch 9489/1000000, d_loss: -62.160797119140625,  g_loss: 113.51895141601562\n",
            "Training epoch 9490/1000000, d_loss: -78.37701416015625,  g_loss: 31.556663513183594\n",
            "Training epoch 9491/1000000, d_loss: -230.5909423828125,  g_loss: 85.71226501464844\n",
            "Training epoch 9492/1000000, d_loss: -98.28040313720703,  g_loss: 74.02737426757812\n",
            "Training epoch 9493/1000000, d_loss: -109.11930084228516,  g_loss: 62.82763671875\n",
            "Training epoch 9494/1000000, d_loss: -209.04434204101562,  g_loss: 24.62228775024414\n",
            "Training epoch 9495/1000000, d_loss: -41.873409271240234,  g_loss: 58.392120361328125\n",
            "Training epoch 9496/1000000, d_loss: -112.16566467285156,  g_loss: 56.8758544921875\n",
            "Training epoch 9497/1000000, d_loss: -72.74625396728516,  g_loss: 73.51264953613281\n",
            "Training epoch 9498/1000000, d_loss: -164.33888244628906,  g_loss: 48.86654281616211\n",
            "Training epoch 9499/1000000, d_loss: -261.89337158203125,  g_loss: 13.102391242980957\n",
            "Training epoch 9500/1000000, d_loss: -348.30712890625,  g_loss: -17.474695205688477\n",
            "Training epoch 9501/1000000, d_loss: -429.19842529296875,  g_loss: -51.4527587890625\n",
            "Training epoch 9502/1000000, d_loss: -137.72389221191406,  g_loss: -40.93959045410156\n",
            "Training epoch 9503/1000000, d_loss: -1232.487060546875,  g_loss: -148.95086669921875\n",
            "Training epoch 9504/1000000, d_loss: -14.256269454956055,  g_loss: -64.57792663574219\n",
            "Training epoch 9505/1000000, d_loss: 56.07911682128906,  g_loss: -16.80305290222168\n",
            "Training epoch 9506/1000000, d_loss: -49.27249526977539,  g_loss: 80.98027801513672\n",
            "Training epoch 9507/1000000, d_loss: -193.342041015625,  g_loss: 181.10032653808594\n",
            "Training epoch 9508/1000000, d_loss: -110.72234344482422,  g_loss: 124.14234924316406\n",
            "Training epoch 9509/1000000, d_loss: -33.317344665527344,  g_loss: -32.703834533691406\n",
            "Training epoch 9510/1000000, d_loss: -721.8541259765625,  g_loss: -300.71337890625\n",
            "Training epoch 9511/1000000, d_loss: -430.2461853027344,  g_loss: -163.6878662109375\n",
            "Training epoch 9512/1000000, d_loss: -34.62274932861328,  g_loss: 38.645835876464844\n",
            "Training epoch 9513/1000000, d_loss: -200.94586181640625,  g_loss: 361.9225769042969\n",
            "Training epoch 9514/1000000, d_loss: -108.73419189453125,  g_loss: 133.935791015625\n",
            "Training epoch 9515/1000000, d_loss: -246.62600708007812,  g_loss: 150.27491760253906\n",
            "Training epoch 9516/1000000, d_loss: -222.66465759277344,  g_loss: 258.80609130859375\n",
            "Training epoch 9517/1000000, d_loss: -21.34912109375,  g_loss: 25.36947250366211\n",
            "Training epoch 9518/1000000, d_loss: -138.27865600585938,  g_loss: 225.98336791992188\n",
            "Training epoch 9519/1000000, d_loss: -198.51516723632812,  g_loss: 282.6990966796875\n",
            "Training epoch 9520/1000000, d_loss: 6.4097442626953125,  g_loss: 100.58053588867188\n",
            "Training epoch 9521/1000000, d_loss: -4.205352783203125,  g_loss: 43.279518127441406\n",
            "Training epoch 9522/1000000, d_loss: -102.3698501586914,  g_loss: 91.28933715820312\n",
            "Training epoch 9523/1000000, d_loss: -56.57145309448242,  g_loss: -37.774532318115234\n",
            "Training epoch 9524/1000000, d_loss: -134.8447265625,  g_loss: -44.67235565185547\n",
            "Training epoch 9525/1000000, d_loss: -186.5699005126953,  g_loss: 41.21450424194336\n",
            "Training epoch 9526/1000000, d_loss: -117.61310577392578,  g_loss: 27.874309539794922\n",
            "Training epoch 9527/1000000, d_loss: -601.5044555664062,  g_loss: -204.9317169189453\n",
            "Training epoch 9528/1000000, d_loss: -1054.8685302734375,  g_loss: -125.90682983398438\n",
            "Training epoch 9529/1000000, d_loss: 21.93115234375,  g_loss: -97.86473083496094\n",
            "Training epoch 9530/1000000, d_loss: -48.337188720703125,  g_loss: 13.731368064880371\n",
            "Training epoch 9531/1000000, d_loss: 47.9229736328125,  g_loss: 49.315425872802734\n",
            "Training epoch 9532/1000000, d_loss: -81.8870620727539,  g_loss: 87.24441528320312\n",
            "Training epoch 9533/1000000, d_loss: -350.7174377441406,  g_loss: 25.83018684387207\n",
            "Training epoch 9534/1000000, d_loss: -804.9602661132812,  g_loss: -117.7221908569336\n",
            "Training epoch 9535/1000000, d_loss: -168.14552307128906,  g_loss: 55.7856330871582\n",
            "Training epoch 9536/1000000, d_loss: -258.98724365234375,  g_loss: 398.049072265625\n",
            "Training epoch 9537/1000000, d_loss: -160.80557250976562,  g_loss: 220.69290161132812\n",
            "Training epoch 9538/1000000, d_loss: -132.4036407470703,  g_loss: 40.00543975830078\n",
            "Training epoch 9539/1000000, d_loss: -73.5048828125,  g_loss: 23.32878875732422\n",
            "Training epoch 9540/1000000, d_loss: -34.56645584106445,  g_loss: 93.45513916015625\n",
            "Training epoch 9541/1000000, d_loss: -88.01053619384766,  g_loss: 161.34994506835938\n",
            "Training epoch 9542/1000000, d_loss: -151.91688537597656,  g_loss: 142.98529052734375\n",
            "Training epoch 9543/1000000, d_loss: -61.17255401611328,  g_loss: 145.9268035888672\n",
            "Training epoch 9544/1000000, d_loss: -78.16619873046875,  g_loss: 60.27438735961914\n",
            "Training epoch 9545/1000000, d_loss: -86.2256088256836,  g_loss: -0.014582157135009766\n",
            "Training epoch 9546/1000000, d_loss: -387.5851135253906,  g_loss: -59.438716888427734\n",
            "Training epoch 9547/1000000, d_loss: -821.3223266601562,  g_loss: -97.94287109375\n",
            "Training epoch 9548/1000000, d_loss: -432.9447021484375,  g_loss: -2.158015727996826\n",
            "Training epoch 9549/1000000, d_loss: -893.6551513671875,  g_loss: -80.30233764648438\n",
            "Training epoch 9550/1000000, d_loss: -38.7528076171875,  g_loss: -207.0955352783203\n",
            "Training epoch 9551/1000000, d_loss: 326.0323486328125,  g_loss: -34.68629455566406\n",
            "Training epoch 9552/1000000, d_loss: -136.13182067871094,  g_loss: 16.887252807617188\n",
            "Training epoch 9553/1000000, d_loss: -48.86362838745117,  g_loss: -27.87116241455078\n",
            "Training epoch 9554/1000000, d_loss: -99.846923828125,  g_loss: 18.874126434326172\n",
            "Training epoch 9555/1000000, d_loss: -69.41870880126953,  g_loss: -50.20015335083008\n",
            "Training epoch 9556/1000000, d_loss: -102.4419937133789,  g_loss: 1.802438735961914\n",
            "Training epoch 9557/1000000, d_loss: -11.332855224609375,  g_loss: 89.23988342285156\n",
            "Training epoch 9558/1000000, d_loss: -27.842002868652344,  g_loss: 43.723018646240234\n",
            "Training epoch 9559/1000000, d_loss: -244.95901489257812,  g_loss: -4.66413688659668\n",
            "Training epoch 9560/1000000, d_loss: 153.31634521484375,  g_loss: 45.17576217651367\n",
            "Training epoch 9561/1000000, d_loss: -205.11663818359375,  g_loss: 78.54843139648438\n",
            "Training epoch 9562/1000000, d_loss: -238.3854217529297,  g_loss: -7.296868324279785\n",
            "Training epoch 9563/1000000, d_loss: -65.77013397216797,  g_loss: 178.53956604003906\n",
            "Training epoch 9564/1000000, d_loss: -121.73674011230469,  g_loss: 111.95721435546875\n",
            "Training epoch 9565/1000000, d_loss: -158.22299194335938,  g_loss: 157.12564086914062\n",
            "Training epoch 9566/1000000, d_loss: -239.5780487060547,  g_loss: 3.6745147705078125\n",
            "Training epoch 9567/1000000, d_loss: -133.8375244140625,  g_loss: 85.35905456542969\n",
            "Training epoch 9568/1000000, d_loss: -316.3274230957031,  g_loss: -45.0918083190918\n",
            "Training epoch 9569/1000000, d_loss: -157.0794219970703,  g_loss: 7.520992279052734\n",
            "Training epoch 9570/1000000, d_loss: -129.60569763183594,  g_loss: -1.4696121215820312\n",
            "Training epoch 9571/1000000, d_loss: -107.84703063964844,  g_loss: 141.3409423828125\n",
            "Training epoch 9572/1000000, d_loss: -195.04713439941406,  g_loss: 216.18026733398438\n",
            "Training epoch 9573/1000000, d_loss: -85.95790100097656,  g_loss: 40.87086486816406\n",
            "Training epoch 9574/1000000, d_loss: -192.94485473632812,  g_loss: 111.15948486328125\n",
            "Training epoch 9575/1000000, d_loss: -57.60349655151367,  g_loss: 50.499778747558594\n",
            "Training epoch 9576/1000000, d_loss: -311.82080078125,  g_loss: -3.4400196075439453\n",
            "Training epoch 9577/1000000, d_loss: -229.3804931640625,  g_loss: 227.56527709960938\n",
            "Training epoch 9578/1000000, d_loss: -108.78866577148438,  g_loss: 62.21937942504883\n",
            "Training epoch 9579/1000000, d_loss: -263.84368896484375,  g_loss: 219.71119689941406\n",
            "Training epoch 9580/1000000, d_loss: -762.2875366210938,  g_loss: -264.9827575683594\n",
            "Training epoch 9581/1000000, d_loss: -105.59823608398438,  g_loss: 119.44953918457031\n",
            "Training epoch 9582/1000000, d_loss: -256.00811767578125,  g_loss: 38.626548767089844\n",
            "Training epoch 9583/1000000, d_loss: -77.87391662597656,  g_loss: 36.63853454589844\n",
            "Training epoch 9584/1000000, d_loss: -169.59519958496094,  g_loss: 101.59846496582031\n",
            "Training epoch 9585/1000000, d_loss: -579.2420043945312,  g_loss: -113.58895111083984\n",
            "Training epoch 9586/1000000, d_loss: -24.255218505859375,  g_loss: 45.279693603515625\n",
            "Training epoch 9587/1000000, d_loss: -85.26437377929688,  g_loss: 170.82659912109375\n",
            "Training epoch 9588/1000000, d_loss: -74.80697631835938,  g_loss: 95.84858703613281\n",
            "Training epoch 9589/1000000, d_loss: -116.11524200439453,  g_loss: 3.4248733520507812\n",
            "Training epoch 9590/1000000, d_loss: -776.232421875,  g_loss: -227.27316284179688\n",
            "Training epoch 9591/1000000, d_loss: -60.22004318237305,  g_loss: 35.002010345458984\n",
            "Training epoch 9592/1000000, d_loss: -105.09075164794922,  g_loss: 123.32440185546875\n",
            "Training epoch 9593/1000000, d_loss: -78.89665985107422,  g_loss: 71.83155059814453\n",
            "Training epoch 9594/1000000, d_loss: -138.22381591796875,  g_loss: 63.637054443359375\n",
            "Training epoch 9595/1000000, d_loss: -229.7325897216797,  g_loss: -10.618782043457031\n",
            "Training epoch 9596/1000000, d_loss: -135.94931030273438,  g_loss: 86.3731689453125\n",
            "Training epoch 9597/1000000, d_loss: -116.43553161621094,  g_loss: 124.97911834716797\n",
            "Training epoch 9598/1000000, d_loss: 156.37838745117188,  g_loss: 50.665382385253906\n",
            "Training epoch 9599/1000000, d_loss: -213.06288146972656,  g_loss: 33.289955139160156\n",
            "Training epoch 9600/1000000, d_loss: 12.857147216796875,  g_loss: 110.36204528808594\n",
            "Training epoch 9601/1000000, d_loss: -37.32927322387695,  g_loss: 183.412841796875\n",
            "Training epoch 9602/1000000, d_loss: -485.13543701171875,  g_loss: 119.45263671875\n",
            "Training epoch 9603/1000000, d_loss: -20.3310604095459,  g_loss: 124.42254638671875\n",
            "Training epoch 9604/1000000, d_loss: -79.80233764648438,  g_loss: 167.8270721435547\n",
            "Training epoch 9605/1000000, d_loss: -24.093055725097656,  g_loss: 133.86178588867188\n",
            "Training epoch 9606/1000000, d_loss: -2.8965530395507812,  g_loss: 124.77229309082031\n",
            "Training epoch 9607/1000000, d_loss: -45.39500427246094,  g_loss: 132.16482543945312\n",
            "Training epoch 9608/1000000, d_loss: -53.06275177001953,  g_loss: 75.65046691894531\n",
            "Training epoch 9609/1000000, d_loss: -960.3590698242188,  g_loss: -59.40481185913086\n",
            "Training epoch 9610/1000000, d_loss: -134.9422607421875,  g_loss: 4.777312278747559\n",
            "Training epoch 9611/1000000, d_loss: -13.670980453491211,  g_loss: 42.79317855834961\n",
            "Training epoch 9612/1000000, d_loss: -265.3457946777344,  g_loss: 105.85140228271484\n",
            "Training epoch 9613/1000000, d_loss: 15.752517700195312,  g_loss: 32.366634368896484\n",
            "Training epoch 9614/1000000, d_loss: 6567.8701171875,  g_loss: 33.42390441894531\n",
            "Training epoch 9615/1000000, d_loss: -34.0515251159668,  g_loss: 91.8563232421875\n",
            "Training epoch 9616/1000000, d_loss: -78.11924743652344,  g_loss: 77.5556640625\n",
            "Training epoch 9617/1000000, d_loss: -40.46575927734375,  g_loss: 73.96516418457031\n",
            "Training epoch 9618/1000000, d_loss: -30.45651626586914,  g_loss: 65.2182846069336\n",
            "Training epoch 9619/1000000, d_loss: 31.237476348876953,  g_loss: 90.06369018554688\n",
            "Training epoch 9620/1000000, d_loss: -197.1746368408203,  g_loss: 69.5948486328125\n",
            "Training epoch 9621/1000000, d_loss: -121.34675598144531,  g_loss: 96.92398071289062\n",
            "Training epoch 9622/1000000, d_loss: -99.33891296386719,  g_loss: 124.31591796875\n",
            "Training epoch 9623/1000000, d_loss: -228.28038024902344,  g_loss: 72.95695495605469\n",
            "Training epoch 9624/1000000, d_loss: -176.55926513671875,  g_loss: 81.26282501220703\n",
            "Training epoch 9625/1000000, d_loss: -51.507476806640625,  g_loss: 41.622413635253906\n",
            "Training epoch 9626/1000000, d_loss: -90.95500183105469,  g_loss: 85.98088836669922\n",
            "Training epoch 9627/1000000, d_loss: -493.2039794921875,  g_loss: 45.16407012939453\n",
            "Training epoch 9628/1000000, d_loss: -1583.3243408203125,  g_loss: -301.4275817871094\n",
            "Training epoch 9629/1000000, d_loss: 1193.815185546875,  g_loss: -124.80462646484375\n",
            "Training epoch 9630/1000000, d_loss: -238.8817138671875,  g_loss: -93.50932312011719\n",
            "Training epoch 9631/1000000, d_loss: 53.355369567871094,  g_loss: 8.55862045288086\n",
            "Training epoch 9632/1000000, d_loss: 130.12026977539062,  g_loss: -252.24847412109375\n",
            "Training epoch 9633/1000000, d_loss: 77.18893432617188,  g_loss: -194.21958923339844\n",
            "Training epoch 9634/1000000, d_loss: 252.5858154296875,  g_loss: 160.5980682373047\n",
            "Training epoch 9635/1000000, d_loss: -105.44041442871094,  g_loss: -66.97142028808594\n",
            "Training epoch 9636/1000000, d_loss: -236.352783203125,  g_loss: 212.22283935546875\n",
            "Training epoch 9637/1000000, d_loss: -378.20904541015625,  g_loss: 450.3725280761719\n",
            "Training epoch 9638/1000000, d_loss: -167.44998168945312,  g_loss: 261.2913818359375\n",
            "Training epoch 9639/1000000, d_loss: 71.5306396484375,  g_loss: -37.613128662109375\n",
            "Training epoch 9640/1000000, d_loss: 58.31480407714844,  g_loss: 27.581073760986328\n",
            "Training epoch 9641/1000000, d_loss: -57.84370803833008,  g_loss: 62.837772369384766\n",
            "Training epoch 9642/1000000, d_loss: -97.5894546508789,  g_loss: 120.09709167480469\n",
            "Training epoch 9643/1000000, d_loss: -39.73950958251953,  g_loss: 72.22451782226562\n",
            "Training epoch 9644/1000000, d_loss: -95.3001480102539,  g_loss: 48.844093322753906\n",
            "Training epoch 9645/1000000, d_loss: -192.78549194335938,  g_loss: 242.02035522460938\n",
            "Training epoch 9646/1000000, d_loss: 49.000484466552734,  g_loss: 105.71324920654297\n",
            "Training epoch 9647/1000000, d_loss: -114.61878204345703,  g_loss: 47.49834060668945\n",
            "Training epoch 9648/1000000, d_loss: -171.4535369873047,  g_loss: 59.216522216796875\n",
            "Training epoch 9649/1000000, d_loss: -57.62956237792969,  g_loss: 122.55047607421875\n",
            "Training epoch 9650/1000000, d_loss: -100.77804565429688,  g_loss: 178.10675048828125\n",
            "Training epoch 9651/1000000, d_loss: -161.6712646484375,  g_loss: 163.7362060546875\n",
            "Training epoch 9652/1000000, d_loss: -54.49361801147461,  g_loss: 106.16043090820312\n",
            "Training epoch 9653/1000000, d_loss: -221.37344360351562,  g_loss: 38.200233459472656\n",
            "Training epoch 9654/1000000, d_loss: -343.970947265625,  g_loss: 90.73974609375\n",
            "Training epoch 9655/1000000, d_loss: -68.52923583984375,  g_loss: 89.25332641601562\n",
            "Training epoch 9656/1000000, d_loss: -92.03337097167969,  g_loss: 164.15890502929688\n",
            "Training epoch 9657/1000000, d_loss: -55.765682220458984,  g_loss: 123.21434020996094\n",
            "Training epoch 9658/1000000, d_loss: -143.6767578125,  g_loss: 96.92926025390625\n",
            "Training epoch 9659/1000000, d_loss: -1132.1981201171875,  g_loss: -31.63766098022461\n",
            "Training epoch 9660/1000000, d_loss: -41.104644775390625,  g_loss: 108.70014190673828\n",
            "Training epoch 9661/1000000, d_loss: 36.91905975341797,  g_loss: 103.63374328613281\n",
            "Training epoch 9662/1000000, d_loss: -104.49420166015625,  g_loss: 155.9813232421875\n",
            "Training epoch 9663/1000000, d_loss: 63.14274597167969,  g_loss: 132.7990264892578\n",
            "Training epoch 9664/1000000, d_loss: -151.85626220703125,  g_loss: 92.92977905273438\n",
            "Training epoch 9665/1000000, d_loss: -237.07752990722656,  g_loss: 44.286720275878906\n",
            "Training epoch 9666/1000000, d_loss: -381.556884765625,  g_loss: -1.437479019165039\n",
            "Training epoch 9667/1000000, d_loss: -633.61376953125,  g_loss: -244.41958618164062\n",
            "Training epoch 9668/1000000, d_loss: 17.421138763427734,  g_loss: -112.44606018066406\n",
            "Training epoch 9669/1000000, d_loss: -15.441452026367188,  g_loss: 13.310735702514648\n",
            "Training epoch 9670/1000000, d_loss: -1281.5606689453125,  g_loss: -104.9041748046875\n",
            "Training epoch 9671/1000000, d_loss: 144.1717987060547,  g_loss: 100.31300354003906\n",
            "Training epoch 9672/1000000, d_loss: -207.64210510253906,  g_loss: 131.23023986816406\n",
            "Training epoch 9673/1000000, d_loss: 6.807624816894531,  g_loss: 182.29916381835938\n",
            "Training epoch 9674/1000000, d_loss: -191.0542755126953,  g_loss: 240.24412536621094\n",
            "Training epoch 9675/1000000, d_loss: -435.493408203125,  g_loss: 423.4115905761719\n",
            "Training epoch 9676/1000000, d_loss: 9.986724853515625,  g_loss: 117.13331604003906\n",
            "Training epoch 9677/1000000, d_loss: 62.38874816894531,  g_loss: 34.63245391845703\n",
            "Training epoch 9678/1000000, d_loss: -236.26715087890625,  g_loss: 28.988224029541016\n",
            "Training epoch 9679/1000000, d_loss: -48.28891372680664,  g_loss: 49.04661560058594\n",
            "Training epoch 9680/1000000, d_loss: -34.602699279785156,  g_loss: 51.27539825439453\n",
            "Training epoch 9681/1000000, d_loss: -31.734285354614258,  g_loss: 42.562686920166016\n",
            "Training epoch 9682/1000000, d_loss: -332.08428955078125,  g_loss: 24.479036331176758\n",
            "Training epoch 9683/1000000, d_loss: -96.63359069824219,  g_loss: 8.79322624206543\n",
            "Training epoch 9684/1000000, d_loss: -196.59426879882812,  g_loss: 123.85362243652344\n",
            "Training epoch 9685/1000000, d_loss: -111.35652160644531,  g_loss: 285.8834228515625\n",
            "Training epoch 9686/1000000, d_loss: -53.56216049194336,  g_loss: 164.0438995361328\n",
            "Training epoch 9687/1000000, d_loss: -121.11919403076172,  g_loss: 289.28900146484375\n",
            "Training epoch 9688/1000000, d_loss: -312.5054931640625,  g_loss: 66.49800109863281\n",
            "Training epoch 9689/1000000, d_loss: 18.897003173828125,  g_loss: 68.96114349365234\n",
            "Training epoch 9690/1000000, d_loss: -358.8076477050781,  g_loss: -50.48185729980469\n",
            "Training epoch 9691/1000000, d_loss: -156.96084594726562,  g_loss: 80.29254150390625\n",
            "Training epoch 9692/1000000, d_loss: -113.0189437866211,  g_loss: 223.60052490234375\n",
            "Training epoch 9693/1000000, d_loss: -84.41558074951172,  g_loss: 51.34287643432617\n",
            "Training epoch 9694/1000000, d_loss: 4.442474365234375,  g_loss: 74.14372253417969\n",
            "Training epoch 9695/1000000, d_loss: -60.13325119018555,  g_loss: 76.52789306640625\n",
            "Training epoch 9696/1000000, d_loss: 0.9915714263916016,  g_loss: 115.42591094970703\n",
            "Training epoch 9697/1000000, d_loss: -108.49481201171875,  g_loss: 125.69310760498047\n",
            "Training epoch 9698/1000000, d_loss: -107.26126861572266,  g_loss: 44.46223068237305\n",
            "Training epoch 9699/1000000, d_loss: -421.6873474121094,  g_loss: 40.6463623046875\n",
            "Training epoch 9700/1000000, d_loss: -85.81552124023438,  g_loss: 161.99769592285156\n",
            "Training epoch 9701/1000000, d_loss: -91.12810516357422,  g_loss: 147.9259490966797\n",
            "Training epoch 9702/1000000, d_loss: -64.75393676757812,  g_loss: 140.76922607421875\n",
            "Training epoch 9703/1000000, d_loss: -125.48152923583984,  g_loss: 80.69824981689453\n",
            "Training epoch 9704/1000000, d_loss: -151.93402099609375,  g_loss: 74.88409423828125\n",
            "Training epoch 9705/1000000, d_loss: -54.19593811035156,  g_loss: 62.23780822753906\n",
            "Training epoch 9706/1000000, d_loss: -27.637977600097656,  g_loss: 88.93315887451172\n",
            "Training epoch 9707/1000000, d_loss: -141.64781188964844,  g_loss: 90.40130615234375\n",
            "Training epoch 9708/1000000, d_loss: 2.5849552154541016,  g_loss: 50.41977310180664\n",
            "Training epoch 9709/1000000, d_loss: -206.94400024414062,  g_loss: 71.63114166259766\n",
            "Training epoch 9710/1000000, d_loss: -94.09308624267578,  g_loss: 53.51590347290039\n",
            "Training epoch 9711/1000000, d_loss: -894.5003051757812,  g_loss: -205.6071319580078\n",
            "Training epoch 9712/1000000, d_loss: -875.6973876953125,  g_loss: 3.701040267944336\n",
            "Training epoch 9713/1000000, d_loss: -2646.795166015625,  g_loss: -1321.9383544921875\n",
            "Training epoch 9714/1000000, d_loss: 823.6395263671875,  g_loss: -499.73272705078125\n",
            "Training epoch 9715/1000000, d_loss: -59.4556884765625,  g_loss: 253.0306396484375\n",
            "Training epoch 9716/1000000, d_loss: 41.357391357421875,  g_loss: 137.37271118164062\n",
            "Training epoch 9717/1000000, d_loss: -96.76516723632812,  g_loss: 162.83653259277344\n",
            "Training epoch 9718/1000000, d_loss: -238.52413940429688,  g_loss: 235.65142822265625\n",
            "Training epoch 9719/1000000, d_loss: -189.53695678710938,  g_loss: 353.93450927734375\n",
            "Training epoch 9720/1000000, d_loss: -155.47918701171875,  g_loss: 127.45899963378906\n",
            "Training epoch 9721/1000000, d_loss: -31.60179901123047,  g_loss: 73.93687438964844\n",
            "Training epoch 9722/1000000, d_loss: -100.32838439941406,  g_loss: 198.5479736328125\n",
            "Training epoch 9723/1000000, d_loss: -182.6552734375,  g_loss: 211.54238891601562\n",
            "Training epoch 9724/1000000, d_loss: -91.98193359375,  g_loss: 177.36624145507812\n",
            "Training epoch 9725/1000000, d_loss: -407.8711853027344,  g_loss: 40.120361328125\n",
            "Training epoch 9726/1000000, d_loss: -278.040771484375,  g_loss: -1.007371425628662\n",
            "Training epoch 9727/1000000, d_loss: 124.29301452636719,  g_loss: 22.711204528808594\n",
            "Training epoch 9728/1000000, d_loss: -95.09102630615234,  g_loss: 69.10819244384766\n",
            "Training epoch 9729/1000000, d_loss: -36.60863494873047,  g_loss: 37.385902404785156\n",
            "Training epoch 9730/1000000, d_loss: -142.2058868408203,  g_loss: 42.50144958496094\n",
            "Training epoch 9731/1000000, d_loss: -1292.778076171875,  g_loss: -953.8261108398438\n",
            "Training epoch 9732/1000000, d_loss: 342.2250061035156,  g_loss: 102.340576171875\n",
            "Training epoch 9733/1000000, d_loss: -402.7733154296875,  g_loss: -52.076805114746094\n",
            "Training epoch 9734/1000000, d_loss: 94.11143493652344,  g_loss: 109.85047912597656\n",
            "Training epoch 9735/1000000, d_loss: -167.31057739257812,  g_loss: 183.07595825195312\n",
            "Training epoch 9736/1000000, d_loss: 16.162506103515625,  g_loss: 219.33956909179688\n",
            "Training epoch 9737/1000000, d_loss: -233.13417053222656,  g_loss: 435.3953552246094\n",
            "Training epoch 9738/1000000, d_loss: -830.7774047851562,  g_loss: 1199.326904296875\n",
            "Training epoch 9739/1000000, d_loss: 371.77435302734375,  g_loss: 84.5497055053711\n",
            "Training epoch 9740/1000000, d_loss: -16.024381637573242,  g_loss: 109.40872955322266\n",
            "Training epoch 9741/1000000, d_loss: -120.84956359863281,  g_loss: 193.20480346679688\n",
            "Training epoch 9742/1000000, d_loss: -50.096866607666016,  g_loss: 125.13613891601562\n",
            "Training epoch 9743/1000000, d_loss: -101.28878021240234,  g_loss: 174.6916046142578\n",
            "Training epoch 9744/1000000, d_loss: -138.15432739257812,  g_loss: 162.30154418945312\n",
            "Training epoch 9745/1000000, d_loss: -107.02676391601562,  g_loss: 150.847412109375\n",
            "Training epoch 9746/1000000, d_loss: -56.620357513427734,  g_loss: 186.4484100341797\n",
            "Training epoch 9747/1000000, d_loss: -195.81021118164062,  g_loss: 151.39938354492188\n",
            "Training epoch 9748/1000000, d_loss: -78.59709167480469,  g_loss: 7.327157020568848\n",
            "Training epoch 9749/1000000, d_loss: -45.78355407714844,  g_loss: 95.95819091796875\n",
            "Training epoch 9750/1000000, d_loss: -200.16322326660156,  g_loss: -21.021841049194336\n",
            "Training epoch 9751/1000000, d_loss: -180.71998596191406,  g_loss: 52.68782424926758\n",
            "Training epoch 9752/1000000, d_loss: -226.2158966064453,  g_loss: 9.12104606628418\n",
            "Training epoch 9753/1000000, d_loss: -121.98432922363281,  g_loss: 95.1722412109375\n",
            "Training epoch 9754/1000000, d_loss: -403.1837463378906,  g_loss: 36.546546936035156\n",
            "Training epoch 9755/1000000, d_loss: -5.916074752807617,  g_loss: 89.69701385498047\n",
            "Training epoch 9756/1000000, d_loss: -136.3944549560547,  g_loss: 137.61798095703125\n",
            "Training epoch 9757/1000000, d_loss: -187.71383666992188,  g_loss: 207.3985595703125\n",
            "Training epoch 9758/1000000, d_loss: -7.15704345703125,  g_loss: 65.58714294433594\n",
            "Training epoch 9759/1000000, d_loss: -73.47596740722656,  g_loss: 133.52969360351562\n",
            "Training epoch 9760/1000000, d_loss: -189.73944091796875,  g_loss: 51.07234573364258\n",
            "Training epoch 9761/1000000, d_loss: -156.15411376953125,  g_loss: 66.59599304199219\n",
            "Training epoch 9762/1000000, d_loss: -966.0335693359375,  g_loss: -694.3535766601562\n",
            "Training epoch 9763/1000000, d_loss: -65.59036254882812,  g_loss: 25.40549087524414\n",
            "Training epoch 9764/1000000, d_loss: -37.23201370239258,  g_loss: 90.94232940673828\n",
            "Training epoch 9765/1000000, d_loss: -260.2088928222656,  g_loss: 208.80349731445312\n",
            "Training epoch 9766/1000000, d_loss: 142.8682861328125,  g_loss: 43.494529724121094\n",
            "Training epoch 9767/1000000, d_loss: -258.31524658203125,  g_loss: -1.5921778678894043\n",
            "Training epoch 9768/1000000, d_loss: -89.97119903564453,  g_loss: 48.547950744628906\n",
            "Training epoch 9769/1000000, d_loss: -273.0034484863281,  g_loss: 280.3441467285156\n",
            "Training epoch 9770/1000000, d_loss: -964.2745971679688,  g_loss: -370.9273376464844\n",
            "Training epoch 9771/1000000, d_loss: -181.8553466796875,  g_loss: 75.21405029296875\n",
            "Training epoch 9772/1000000, d_loss: 17.79766082763672,  g_loss: 64.99383544921875\n",
            "Training epoch 9773/1000000, d_loss: -82.53443908691406,  g_loss: -39.149837493896484\n",
            "Training epoch 9774/1000000, d_loss: -114.05806732177734,  g_loss: 67.51747131347656\n",
            "Training epoch 9775/1000000, d_loss: -150.97508239746094,  g_loss: 101.00799560546875\n",
            "Training epoch 9776/1000000, d_loss: -138.826416015625,  g_loss: 96.92617797851562\n",
            "Training epoch 9777/1000000, d_loss: -319.7019958496094,  g_loss: 142.81710815429688\n",
            "Training epoch 9778/1000000, d_loss: 6.216827392578125,  g_loss: 40.37422180175781\n",
            "Training epoch 9779/1000000, d_loss: -73.60118103027344,  g_loss: 63.055580139160156\n",
            "Training epoch 9780/1000000, d_loss: -90.70382690429688,  g_loss: 84.44515991210938\n",
            "Training epoch 9781/1000000, d_loss: -42.426429748535156,  g_loss: 54.84239959716797\n",
            "Training epoch 9782/1000000, d_loss: -74.29652404785156,  g_loss: 44.324462890625\n",
            "Training epoch 9783/1000000, d_loss: -44.05559539794922,  g_loss: 268.8714904785156\n",
            "Training epoch 9784/1000000, d_loss: -125.3743896484375,  g_loss: 188.07968139648438\n",
            "Training epoch 9785/1000000, d_loss: -66.83830261230469,  g_loss: 116.69695281982422\n",
            "Training epoch 9786/1000000, d_loss: -71.75626373291016,  g_loss: 123.65953063964844\n",
            "Training epoch 9787/1000000, d_loss: -242.60751342773438,  g_loss: 75.96890258789062\n",
            "Training epoch 9788/1000000, d_loss: -127.7966537475586,  g_loss: 81.19888305664062\n",
            "Training epoch 9789/1000000, d_loss: -33.75482940673828,  g_loss: 95.10895538330078\n",
            "Training epoch 9790/1000000, d_loss: -19.480377197265625,  g_loss: 67.24059295654297\n",
            "Training epoch 9791/1000000, d_loss: -306.0339660644531,  g_loss: -163.14794921875\n",
            "Training epoch 9792/1000000, d_loss: -52.82582092285156,  g_loss: 155.58221435546875\n",
            "Training epoch 9793/1000000, d_loss: -209.40513610839844,  g_loss: 51.83254623413086\n",
            "Training epoch 9794/1000000, d_loss: -92.751953125,  g_loss: 188.75485229492188\n",
            "Training epoch 9795/1000000, d_loss: -88.3416748046875,  g_loss: 203.97032165527344\n",
            "Training epoch 9796/1000000, d_loss: -95.89839935302734,  g_loss: 139.29833984375\n",
            "Training epoch 9797/1000000, d_loss: -613.4969482421875,  g_loss: -267.30218505859375\n",
            "Training epoch 9798/1000000, d_loss: -71.63040161132812,  g_loss: 0.05077362060546875\n",
            "Training epoch 9799/1000000, d_loss: -14.084705352783203,  g_loss: 78.54573059082031\n",
            "Training epoch 9800/1000000, d_loss: -393.1558837890625,  g_loss: -83.38311004638672\n",
            "Training epoch 9801/1000000, d_loss: -79.71330261230469,  g_loss: 135.70407104492188\n",
            "Training epoch 9802/1000000, d_loss: -1026.25830078125,  g_loss: -342.11187744140625\n",
            "Training epoch 9803/1000000, d_loss: -941.1417236328125,  g_loss: -986.914306640625\n",
            "Training epoch 9804/1000000, d_loss: 5.8634033203125,  g_loss: -24.732080459594727\n",
            "Training epoch 9805/1000000, d_loss: -326.3866271972656,  g_loss: -71.06100463867188\n",
            "Training epoch 9806/1000000, d_loss: 393.05633544921875,  g_loss: -168.65396118164062\n",
            "Training epoch 9807/1000000, d_loss: 33.64129638671875,  g_loss: -37.790496826171875\n",
            "Training epoch 9808/1000000, d_loss: -75.00660705566406,  g_loss: 98.03984832763672\n",
            "Training epoch 9809/1000000, d_loss: -125.79386901855469,  g_loss: 193.8568115234375\n",
            "Training epoch 9810/1000000, d_loss: -13.3531494140625,  g_loss: 89.88185119628906\n",
            "Training epoch 9811/1000000, d_loss: -254.06158447265625,  g_loss: 301.9530029296875\n",
            "Training epoch 9812/1000000, d_loss: -276.4910888671875,  g_loss: 443.1982116699219\n",
            "Training epoch 9813/1000000, d_loss: -399.8689880371094,  g_loss: 546.0009765625\n",
            "Training epoch 9814/1000000, d_loss: 23.516754150390625,  g_loss: 342.6798095703125\n",
            "Training epoch 9815/1000000, d_loss: 13.980758666992188,  g_loss: 118.34361267089844\n",
            "Training epoch 9816/1000000, d_loss: -97.9404296875,  g_loss: 45.49127197265625\n",
            "Training epoch 9817/1000000, d_loss: -34.575294494628906,  g_loss: -125.25140380859375\n",
            "Training epoch 9818/1000000, d_loss: -164.98272705078125,  g_loss: 70.18839263916016\n",
            "Training epoch 9819/1000000, d_loss: -71.90764617919922,  g_loss: 5.718879699707031\n",
            "Training epoch 9820/1000000, d_loss: -296.6678161621094,  g_loss: 353.8553466796875\n",
            "Training epoch 9821/1000000, d_loss: -107.29692840576172,  g_loss: 133.2764434814453\n",
            "Training epoch 9822/1000000, d_loss: -15.152470588684082,  g_loss: 0.7125558853149414\n",
            "Training epoch 9823/1000000, d_loss: -339.3856201171875,  g_loss: 496.32147216796875\n",
            "Training epoch 9824/1000000, d_loss: -108.54156494140625,  g_loss: 162.90737915039062\n",
            "Training epoch 9825/1000000, d_loss: -125.30757141113281,  g_loss: 38.533050537109375\n",
            "Training epoch 9826/1000000, d_loss: -145.8253173828125,  g_loss: 81.75204467773438\n",
            "Training epoch 9827/1000000, d_loss: -37.861236572265625,  g_loss: -1.4629831314086914\n",
            "Training epoch 9828/1000000, d_loss: -262.63946533203125,  g_loss: 106.0267562866211\n",
            "Training epoch 9829/1000000, d_loss: -62.49729537963867,  g_loss: -104.56654357910156\n",
            "Training epoch 9830/1000000, d_loss: -76.9442138671875,  g_loss: -2.1551971435546875\n",
            "Training epoch 9831/1000000, d_loss: -158.2960205078125,  g_loss: 150.329833984375\n",
            "Training epoch 9832/1000000, d_loss: -21.37434196472168,  g_loss: 85.47697448730469\n",
            "Training epoch 9833/1000000, d_loss: -52.431941986083984,  g_loss: 24.920196533203125\n",
            "Training epoch 9834/1000000, d_loss: -103.19744110107422,  g_loss: 113.59768676757812\n",
            "Training epoch 9835/1000000, d_loss: 4.486110687255859,  g_loss: 90.00472259521484\n",
            "Training epoch 9836/1000000, d_loss: -253.0458984375,  g_loss: 15.87993335723877\n",
            "Training epoch 9837/1000000, d_loss: -61.2320442199707,  g_loss: 63.724639892578125\n",
            "Training epoch 9838/1000000, d_loss: -705.76220703125,  g_loss: -12.12978744506836\n",
            "Training epoch 9839/1000000, d_loss: -252.85009765625,  g_loss: -206.72389221191406\n",
            "Training epoch 9840/1000000, d_loss: -53.54970169067383,  g_loss: 61.812583923339844\n",
            "Training epoch 9841/1000000, d_loss: 1075.7274169921875,  g_loss: 19.49645233154297\n",
            "Training epoch 9842/1000000, d_loss: -35.817352294921875,  g_loss: 100.45430755615234\n",
            "Training epoch 9843/1000000, d_loss: -93.20402526855469,  g_loss: 205.7579345703125\n",
            "Training epoch 9844/1000000, d_loss: -136.06985473632812,  g_loss: 324.85333251953125\n",
            "Training epoch 9845/1000000, d_loss: -56.394500732421875,  g_loss: 91.60697174072266\n",
            "Training epoch 9846/1000000, d_loss: -1028.858642578125,  g_loss: -105.046142578125\n",
            "Training epoch 9847/1000000, d_loss: -448.42901611328125,  g_loss: 43.42781448364258\n",
            "Training epoch 9848/1000000, d_loss: -25.10954475402832,  g_loss: 62.88594055175781\n",
            "Training epoch 9849/1000000, d_loss: 16.49859619140625,  g_loss: 36.47230529785156\n",
            "Training epoch 9850/1000000, d_loss: -320.7825927734375,  g_loss: -6.664876937866211\n",
            "Training epoch 9851/1000000, d_loss: -252.45516967773438,  g_loss: 44.4438591003418\n",
            "Training epoch 9852/1000000, d_loss: -31.923921585083008,  g_loss: 48.189781188964844\n",
            "Training epoch 9853/1000000, d_loss: 83.19180297851562,  g_loss: 43.0921745300293\n",
            "Training epoch 9854/1000000, d_loss: -21.75244140625,  g_loss: 75.79835510253906\n",
            "Training epoch 9855/1000000, d_loss: -111.80765533447266,  g_loss: 134.37428283691406\n",
            "Training epoch 9856/1000000, d_loss: -56.94178771972656,  g_loss: 21.759716033935547\n",
            "Training epoch 9857/1000000, d_loss: 136.04737854003906,  g_loss: 99.31448364257812\n",
            "Training epoch 9858/1000000, d_loss: -163.14694213867188,  g_loss: 131.46047973632812\n",
            "Training epoch 9859/1000000, d_loss: -117.3997802734375,  g_loss: 144.4219970703125\n",
            "Training epoch 9860/1000000, d_loss: -261.80926513671875,  g_loss: 58.763084411621094\n",
            "Training epoch 9861/1000000, d_loss: -4.201205253601074,  g_loss: 42.782752990722656\n",
            "Training epoch 9862/1000000, d_loss: -236.2978057861328,  g_loss: 42.17642593383789\n",
            "Training epoch 9863/1000000, d_loss: -590.2679443359375,  g_loss: -274.28485107421875\n",
            "Training epoch 9864/1000000, d_loss: -140.77554321289062,  g_loss: 219.94717407226562\n",
            "Training epoch 9865/1000000, d_loss: -45.88152313232422,  g_loss: 121.02001953125\n",
            "Training epoch 9866/1000000, d_loss: -93.35308837890625,  g_loss: 179.86888122558594\n",
            "Training epoch 9867/1000000, d_loss: -182.24716186523438,  g_loss: 163.76527404785156\n",
            "Training epoch 9868/1000000, d_loss: 155.03677368164062,  g_loss: 14.80841064453125\n",
            "Training epoch 9869/1000000, d_loss: -236.52224731445312,  g_loss: 140.23974609375\n",
            "Training epoch 9870/1000000, d_loss: -290.5166015625,  g_loss: 103.9055404663086\n",
            "Training epoch 9871/1000000, d_loss: 16.031280517578125,  g_loss: 172.65359497070312\n",
            "Training epoch 9872/1000000, d_loss: -101.66734313964844,  g_loss: 303.134033203125\n",
            "Training epoch 9873/1000000, d_loss: 13.409515380859375,  g_loss: 125.0522232055664\n",
            "Training epoch 9874/1000000, d_loss: -3.8792953491210938,  g_loss: 42.58283996582031\n",
            "Training epoch 9875/1000000, d_loss: -122.72747039794922,  g_loss: 75.11521911621094\n",
            "Training epoch 9876/1000000, d_loss: -57.01890182495117,  g_loss: 156.89712524414062\n",
            "Training epoch 9877/1000000, d_loss: -10.289264678955078,  g_loss: 90.18363952636719\n",
            "Training epoch 9878/1000000, d_loss: -122.66963195800781,  g_loss: 122.67007446289062\n",
            "Training epoch 9879/1000000, d_loss: -53.26409912109375,  g_loss: 141.4352264404297\n",
            "Training epoch 9880/1000000, d_loss: -143.8385467529297,  g_loss: 66.64933013916016\n",
            "Training epoch 9881/1000000, d_loss: -132.38656616210938,  g_loss: 129.79193115234375\n",
            "Training epoch 9882/1000000, d_loss: -374.8414611816406,  g_loss: 59.598270416259766\n",
            "Training epoch 9883/1000000, d_loss: -82.2900390625,  g_loss: 95.47993469238281\n",
            "Training epoch 9884/1000000, d_loss: -162.51087951660156,  g_loss: 82.98234558105469\n",
            "Training epoch 9885/1000000, d_loss: -100.52820587158203,  g_loss: 55.100711822509766\n",
            "Training epoch 9886/1000000, d_loss: -98.74446105957031,  g_loss: 50.96259689331055\n",
            "Training epoch 9887/1000000, d_loss: -228.531494140625,  g_loss: 116.10371398925781\n",
            "Training epoch 9888/1000000, d_loss: -1549.257568359375,  g_loss: -75.75424194335938\n",
            "Training epoch 9889/1000000, d_loss: 271.90789794921875,  g_loss: -189.06997680664062\n",
            "Training epoch 9890/1000000, d_loss: 183.05783081054688,  g_loss: -139.3308868408203\n",
            "Training epoch 9891/1000000, d_loss: 5.642341613769531,  g_loss: 40.25444412231445\n",
            "Training epoch 9892/1000000, d_loss: -125.61245727539062,  g_loss: 172.83056640625\n",
            "Training epoch 9893/1000000, d_loss: -109.93476867675781,  g_loss: 156.18609619140625\n",
            "Training epoch 9894/1000000, d_loss: -216.34628295898438,  g_loss: 166.23416137695312\n",
            "Training epoch 9895/1000000, d_loss: -68.8155517578125,  g_loss: 329.9891052246094\n",
            "Training epoch 9896/1000000, d_loss: -212.08860778808594,  g_loss: 218.71426391601562\n",
            "Training epoch 9897/1000000, d_loss: -12.728134155273438,  g_loss: 108.515869140625\n",
            "Training epoch 9898/1000000, d_loss: -181.29006958007812,  g_loss: 199.70550537109375\n",
            "Training epoch 9899/1000000, d_loss: -119.32084655761719,  g_loss: 267.35968017578125\n",
            "Training epoch 9900/1000000, d_loss: -550.8594360351562,  g_loss: -66.73746490478516\n",
            "Training epoch 9901/1000000, d_loss: -157.8166961669922,  g_loss: 64.12897491455078\n",
            "Training epoch 9902/1000000, d_loss: 31.673545837402344,  g_loss: 128.65286254882812\n",
            "Training epoch 9903/1000000, d_loss: -49.049678802490234,  g_loss: 82.88922119140625\n",
            "Training epoch 9904/1000000, d_loss: -114.2184066772461,  g_loss: 107.15946960449219\n",
            "Training epoch 9905/1000000, d_loss: -102.74906921386719,  g_loss: 113.59315490722656\n",
            "Training epoch 9906/1000000, d_loss: -71.37640380859375,  g_loss: 109.78909301757812\n",
            "Training epoch 9907/1000000, d_loss: -211.50961303710938,  g_loss: 153.20840454101562\n",
            "Training epoch 9908/1000000, d_loss: -194.29039001464844,  g_loss: 22.842222213745117\n",
            "Training epoch 9909/1000000, d_loss: -96.88175201416016,  g_loss: 10.687907218933105\n",
            "Training epoch 9910/1000000, d_loss: -215.45703125,  g_loss: 179.850341796875\n",
            "Training epoch 9911/1000000, d_loss: 3.382963180541992,  g_loss: 64.61605072021484\n",
            "Training epoch 9912/1000000, d_loss: -126.79546356201172,  g_loss: 34.305572509765625\n",
            "Training epoch 9913/1000000, d_loss: -74.18927001953125,  g_loss: 44.105682373046875\n",
            "Training epoch 9914/1000000, d_loss: -68.75696563720703,  g_loss: 84.30889892578125\n",
            "Training epoch 9915/1000000, d_loss: -30.737285614013672,  g_loss: 81.6675033569336\n",
            "Training epoch 9916/1000000, d_loss: -169.06155395507812,  g_loss: 50.61346435546875\n",
            "Training epoch 9917/1000000, d_loss: -231.69281005859375,  g_loss: 9.541597366333008\n",
            "Training epoch 9918/1000000, d_loss: -111.30681610107422,  g_loss: 36.777442932128906\n",
            "Training epoch 9919/1000000, d_loss: -134.35888671875,  g_loss: 37.884315490722656\n",
            "Training epoch 9920/1000000, d_loss: 171.25643920898438,  g_loss: 118.3614501953125\n",
            "Training epoch 9921/1000000, d_loss: -37.26322937011719,  g_loss: 124.52363586425781\n",
            "Training epoch 9922/1000000, d_loss: 42.49559020996094,  g_loss: 166.39013671875\n",
            "Training epoch 9923/1000000, d_loss: -115.28269958496094,  g_loss: 150.82644653320312\n",
            "Training epoch 9924/1000000, d_loss: 5.184883117675781,  g_loss: 107.35322570800781\n",
            "Training epoch 9925/1000000, d_loss: -126.9862060546875,  g_loss: 111.27384948730469\n",
            "Training epoch 9926/1000000, d_loss: -48.739105224609375,  g_loss: 106.98526000976562\n",
            "Training epoch 9927/1000000, d_loss: -295.312744140625,  g_loss: 20.905628204345703\n",
            "Training epoch 9928/1000000, d_loss: -33.31089782714844,  g_loss: 86.13126373291016\n",
            "Training epoch 9929/1000000, d_loss: -35.09114456176758,  g_loss: 44.904510498046875\n",
            "Training epoch 9930/1000000, d_loss: -326.22039794921875,  g_loss: 29.512235641479492\n",
            "Training epoch 9931/1000000, d_loss: -113.87507629394531,  g_loss: 88.84648132324219\n",
            "Training epoch 9932/1000000, d_loss: -3231.7880859375,  g_loss: -778.5311889648438\n",
            "Training epoch 9933/1000000, d_loss: 733.932861328125,  g_loss: -264.9158935546875\n",
            "Training epoch 9934/1000000, d_loss: 97.54922485351562,  g_loss: -204.22601318359375\n",
            "Training epoch 9935/1000000, d_loss: 53.660377502441406,  g_loss: -420.4337158203125\n",
            "Training epoch 9936/1000000, d_loss: -44.049781799316406,  g_loss: 165.6978759765625\n",
            "Training epoch 9937/1000000, d_loss: -66.10872650146484,  g_loss: -17.766590118408203\n",
            "Training epoch 9938/1000000, d_loss: -103.39300537109375,  g_loss: 46.39679718017578\n",
            "Training epoch 9939/1000000, d_loss: -118.2780532836914,  g_loss: 108.35603332519531\n",
            "Training epoch 9940/1000000, d_loss: -170.90005493164062,  g_loss: 197.0174560546875\n",
            "Training epoch 9941/1000000, d_loss: 0.7520904541015625,  g_loss: 58.49446105957031\n",
            "Training epoch 9942/1000000, d_loss: -240.01760864257812,  g_loss: 439.6448669433594\n",
            "Training epoch 9943/1000000, d_loss: -66.93982696533203,  g_loss: 3.4103641510009766\n",
            "Training epoch 9944/1000000, d_loss: -152.59925842285156,  g_loss: 54.36012268066406\n",
            "Training epoch 9945/1000000, d_loss: -100.7718734741211,  g_loss: 89.28082275390625\n",
            "Training epoch 9946/1000000, d_loss: -57.3562126159668,  g_loss: 48.65853500366211\n",
            "Training epoch 9947/1000000, d_loss: -173.36758422851562,  g_loss: 37.463409423828125\n",
            "Training epoch 9948/1000000, d_loss: -142.99075317382812,  g_loss: 44.25354766845703\n",
            "Training epoch 9949/1000000, d_loss: -225.8329620361328,  g_loss: 21.38656997680664\n",
            "Training epoch 9950/1000000, d_loss: -364.76751708984375,  g_loss: 56.6287841796875\n",
            "Training epoch 9951/1000000, d_loss: -33.850547790527344,  g_loss: 114.9020004272461\n",
            "Training epoch 9952/1000000, d_loss: -164.84317016601562,  g_loss: 339.7903747558594\n",
            "Training epoch 9953/1000000, d_loss: 68.27561950683594,  g_loss: 91.52124786376953\n",
            "Training epoch 9954/1000000, d_loss: -190.30284118652344,  g_loss: 80.35662841796875\n",
            "Training epoch 9955/1000000, d_loss: -210.52792358398438,  g_loss: -4.732860565185547\n",
            "Training epoch 9956/1000000, d_loss: -174.46673583984375,  g_loss: 75.50162506103516\n",
            "Training epoch 9957/1000000, d_loss: -45.432861328125,  g_loss: 91.83230590820312\n",
            "Training epoch 9958/1000000, d_loss: -46.10134506225586,  g_loss: 72.77926635742188\n",
            "Training epoch 9959/1000000, d_loss: 723.1775512695312,  g_loss: 88.50962829589844\n",
            "Training epoch 9960/1000000, d_loss: 46.339447021484375,  g_loss: 105.04714965820312\n",
            "Training epoch 9961/1000000, d_loss: -174.02479553222656,  g_loss: 135.7276611328125\n",
            "Training epoch 9962/1000000, d_loss: -38.3792839050293,  g_loss: 102.51115417480469\n",
            "Training epoch 9963/1000000, d_loss: -140.42337036132812,  g_loss: 110.41690063476562\n",
            "Training epoch 9964/1000000, d_loss: -54.414974212646484,  g_loss: 93.23350524902344\n",
            "Training epoch 9965/1000000, d_loss: -1240.2587890625,  g_loss: -60.738929748535156\n",
            "Training epoch 9966/1000000, d_loss: 69.74795532226562,  g_loss: 58.73255920410156\n",
            "Training epoch 9967/1000000, d_loss: 27.37509536743164,  g_loss: 35.31706619262695\n",
            "Training epoch 9968/1000000, d_loss: -74.64685821533203,  g_loss: 75.03888702392578\n",
            "Training epoch 9969/1000000, d_loss: -57.97411346435547,  g_loss: 118.82853698730469\n",
            "Training epoch 9970/1000000, d_loss: -101.68130493164062,  g_loss: 163.71511840820312\n",
            "Training epoch 9971/1000000, d_loss: -62.96696090698242,  g_loss: 99.32344055175781\n",
            "Training epoch 9972/1000000, d_loss: -84.36052703857422,  g_loss: 189.49154663085938\n",
            "Training epoch 9973/1000000, d_loss: -92.90577697753906,  g_loss: 69.9019775390625\n",
            "Training epoch 9974/1000000, d_loss: -146.2273712158203,  g_loss: 56.50067901611328\n",
            "Training epoch 9975/1000000, d_loss: -233.9971923828125,  g_loss: -63.30535888671875\n",
            "Training epoch 9976/1000000, d_loss: -248.66690063476562,  g_loss: -785.7440185546875\n",
            "Training epoch 9977/1000000, d_loss: -123.17237854003906,  g_loss: 394.3321228027344\n",
            "Training epoch 9978/1000000, d_loss: -576.7272338867188,  g_loss: 1279.9219970703125\n",
            "Training epoch 9979/1000000, d_loss: 25.7176513671875,  g_loss: 52.07054138183594\n",
            "Training epoch 9980/1000000, d_loss: -54.415733337402344,  g_loss: -17.871538162231445\n",
            "Training epoch 9981/1000000, d_loss: -249.1477508544922,  g_loss: 238.05099487304688\n",
            "Training epoch 9982/1000000, d_loss: -106.36970520019531,  g_loss: 114.4876937866211\n",
            "Training epoch 9983/1000000, d_loss: -191.21192932128906,  g_loss: 188.72607421875\n",
            "Training epoch 9984/1000000, d_loss: -200.67324829101562,  g_loss: 219.54058837890625\n",
            "Training epoch 9985/1000000, d_loss: -183.45643615722656,  g_loss: 72.50505065917969\n",
            "Training epoch 9986/1000000, d_loss: -1009.8849487304688,  g_loss: -107.5949935913086\n",
            "Training epoch 9987/1000000, d_loss: 465.16314697265625,  g_loss: 80.08001708984375\n",
            "Training epoch 9988/1000000, d_loss: -18.433250427246094,  g_loss: 43.913108825683594\n",
            "Training epoch 9989/1000000, d_loss: 55.13960266113281,  g_loss: 79.4256820678711\n",
            "Training epoch 9990/1000000, d_loss: 56.68229675292969,  g_loss: -64.78016662597656\n",
            "Training epoch 9991/1000000, d_loss: 52.40054702758789,  g_loss: 40.240386962890625\n",
            "Training epoch 9992/1000000, d_loss: -92.07662963867188,  g_loss: 163.15159606933594\n",
            "Training epoch 9993/1000000, d_loss: 32.840179443359375,  g_loss: 83.9100112915039\n",
            "Training epoch 9994/1000000, d_loss: -236.05169677734375,  g_loss: 431.1192932128906\n",
            "Training epoch 9995/1000000, d_loss: -47.63844299316406,  g_loss: 161.853515625\n",
            "Training epoch 9996/1000000, d_loss: -65.36122131347656,  g_loss: 168.8929901123047\n",
            "Training epoch 9997/1000000, d_loss: -107.4162826538086,  g_loss: 109.17742156982422\n",
            "Training epoch 9998/1000000, d_loss: -359.6768798828125,  g_loss: 40.21044921875\n",
            "Training epoch 9999/1000000, d_loss: 2.5027923583984375,  g_loss: 121.24683380126953\n",
            "Training epoch 10000/1000000, d_loss: -199.73880004882812,  g_loss: 252.5336456298828\n",
            "Training epoch 10001/1000000, d_loss: -135.44845581054688,  g_loss: 263.3407897949219\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 17/17 [00:00<00:00, 737.35it/s]\n",
            "Meshing: 100%|██████████| 832/832 [00:00<00:00, 5728.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_10001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_10001/assets\n",
            "Training epoch 10002/1000000, d_loss: -1.3692626953125,  g_loss: 151.09884643554688\n",
            "Training epoch 10003/1000000, d_loss: -159.22573852539062,  g_loss: 401.5539245605469\n",
            "Training epoch 10004/1000000, d_loss: -63.55502700805664,  g_loss: 169.64527893066406\n",
            "Training epoch 10005/1000000, d_loss: -109.58580017089844,  g_loss: 89.6048583984375\n",
            "Training epoch 10006/1000000, d_loss: -203.58187866210938,  g_loss: 89.49615478515625\n",
            "Training epoch 10007/1000000, d_loss: -721.386962890625,  g_loss: 83.06105041503906\n",
            "Training epoch 10008/1000000, d_loss: -77.90625,  g_loss: -1.4321906566619873\n",
            "Training epoch 10009/1000000, d_loss: -114.06532287597656,  g_loss: -46.708282470703125\n",
            "Training epoch 10010/1000000, d_loss: 6.914421081542969,  g_loss: 32.837284088134766\n",
            "Training epoch 10011/1000000, d_loss: 4.335285186767578,  g_loss: 78.05162048339844\n",
            "Training epoch 10012/1000000, d_loss: 256.35626220703125,  g_loss: 91.76982879638672\n",
            "Training epoch 10013/1000000, d_loss: -15.463207244873047,  g_loss: 60.99755096435547\n",
            "Training epoch 10014/1000000, d_loss: -136.83242797851562,  g_loss: 32.843040466308594\n",
            "Training epoch 10015/1000000, d_loss: -86.85285186767578,  g_loss: 48.540550231933594\n",
            "Training epoch 10016/1000000, d_loss: -78.78816223144531,  g_loss: 161.57020568847656\n",
            "Training epoch 10017/1000000, d_loss: -50.62370300292969,  g_loss: 128.03948974609375\n",
            "Training epoch 10018/1000000, d_loss: -238.3170928955078,  g_loss: 81.04949188232422\n",
            "Training epoch 10019/1000000, d_loss: -68.38336181640625,  g_loss: 111.61639404296875\n",
            "Training epoch 10020/1000000, d_loss: -1792.2117919921875,  g_loss: 9.39833927154541\n",
            "Training epoch 10021/1000000, d_loss: -209.36378479003906,  g_loss: -148.58897399902344\n",
            "Training epoch 10022/1000000, d_loss: -24.978668212890625,  g_loss: -98.01884460449219\n",
            "Training epoch 10023/1000000, d_loss: -297.33245849609375,  g_loss: -82.63255310058594\n",
            "Training epoch 10024/1000000, d_loss: 444.69927978515625,  g_loss: -22.87367820739746\n",
            "Training epoch 10025/1000000, d_loss: -89.34368896484375,  g_loss: 12.102336883544922\n",
            "Training epoch 10026/1000000, d_loss: -162.449951171875,  g_loss: 126.21622467041016\n",
            "Training epoch 10027/1000000, d_loss: -61.133148193359375,  g_loss: 310.34869384765625\n",
            "Training epoch 10028/1000000, d_loss: -7.690147399902344,  g_loss: 81.27396392822266\n",
            "Training epoch 10029/1000000, d_loss: 1107.9970703125,  g_loss: 135.64016723632812\n",
            "Training epoch 10030/1000000, d_loss: -125.34895324707031,  g_loss: 114.10797119140625\n",
            "Training epoch 10031/1000000, d_loss: -51.574798583984375,  g_loss: 94.49134063720703\n",
            "Training epoch 10032/1000000, d_loss: -104.62657165527344,  g_loss: 141.78236389160156\n",
            "Training epoch 10033/1000000, d_loss: -234.98629760742188,  g_loss: 97.99586486816406\n",
            "Training epoch 10034/1000000, d_loss: -301.67547607421875,  g_loss: 295.2682189941406\n",
            "Training epoch 10035/1000000, d_loss: -69.50086212158203,  g_loss: 127.4708251953125\n",
            "Training epoch 10036/1000000, d_loss: -57.182472229003906,  g_loss: 163.19834899902344\n",
            "Training epoch 10037/1000000, d_loss: -98.7075424194336,  g_loss: 120.17317199707031\n",
            "Training epoch 10038/1000000, d_loss: -397.95892333984375,  g_loss: 705.65185546875\n",
            "Training epoch 10039/1000000, d_loss: -4.827938079833984,  g_loss: 73.02757263183594\n",
            "Training epoch 10040/1000000, d_loss: -206.84397888183594,  g_loss: 40.361572265625\n",
            "Training epoch 10041/1000000, d_loss: -49.38505935668945,  g_loss: 77.11746215820312\n",
            "Training epoch 10042/1000000, d_loss: -64.46467590332031,  g_loss: 71.79507446289062\n",
            "Training epoch 10043/1000000, d_loss: 47.51268768310547,  g_loss: 79.59539794921875\n",
            "Training epoch 10044/1000000, d_loss: -126.95585632324219,  g_loss: 92.16120910644531\n",
            "Training epoch 10045/1000000, d_loss: -305.58929443359375,  g_loss: -136.53016662597656\n",
            "Training epoch 10046/1000000, d_loss: -167.4501953125,  g_loss: 204.5411834716797\n",
            "Training epoch 10047/1000000, d_loss: -80.62472534179688,  g_loss: -0.10410904884338379\n",
            "Training epoch 10048/1000000, d_loss: -430.32659912109375,  g_loss: -335.47174072265625\n",
            "Training epoch 10049/1000000, d_loss: 236.38265991210938,  g_loss: -5.5732421875\n",
            "Training epoch 10050/1000000, d_loss: -1256.3902587890625,  g_loss: -224.06906127929688\n",
            "Training epoch 10051/1000000, d_loss: -149.18551635742188,  g_loss: 382.3553466796875\n",
            "Training epoch 10052/1000000, d_loss: -129.29180908203125,  g_loss: 160.6532440185547\n",
            "Training epoch 10053/1000000, d_loss: -123.96216583251953,  g_loss: 124.61298370361328\n",
            "Training epoch 10054/1000000, d_loss: 153.3229522705078,  g_loss: 31.37247657775879\n",
            "Training epoch 10055/1000000, d_loss: -123.63406372070312,  g_loss: 52.573631286621094\n",
            "Training epoch 10056/1000000, d_loss: -72.91514587402344,  g_loss: 52.25742721557617\n",
            "Training epoch 10057/1000000, d_loss: -107.05000305175781,  g_loss: 41.12663650512695\n",
            "Training epoch 10058/1000000, d_loss: -91.41346740722656,  g_loss: 185.79603576660156\n",
            "Training epoch 10059/1000000, d_loss: -195.22401428222656,  g_loss: 45.88998794555664\n",
            "Training epoch 10060/1000000, d_loss: -282.7503967285156,  g_loss: 356.0118408203125\n",
            "Training epoch 10061/1000000, d_loss: -162.86077880859375,  g_loss: 37.7970085144043\n",
            "Training epoch 10062/1000000, d_loss: -150.97586059570312,  g_loss: -22.60362434387207\n",
            "Training epoch 10063/1000000, d_loss: -71.5323257446289,  g_loss: 38.32523727416992\n",
            "Training epoch 10064/1000000, d_loss: -165.28753662109375,  g_loss: 26.432689666748047\n",
            "Training epoch 10065/1000000, d_loss: -36.834075927734375,  g_loss: 104.37139892578125\n",
            "Training epoch 10066/1000000, d_loss: -73.11347961425781,  g_loss: 131.2585906982422\n",
            "Training epoch 10067/1000000, d_loss: 99.55867004394531,  g_loss: 93.18450927734375\n",
            "Training epoch 10068/1000000, d_loss: -70.8963394165039,  g_loss: 80.6485595703125\n",
            "Training epoch 10069/1000000, d_loss: -92.95173645019531,  g_loss: 100.41029357910156\n",
            "Training epoch 10070/1000000, d_loss: -144.87042236328125,  g_loss: 62.73426818847656\n",
            "Training epoch 10071/1000000, d_loss: -45.8099365234375,  g_loss: 116.3236312866211\n",
            "Training epoch 10072/1000000, d_loss: -105.05401611328125,  g_loss: 83.49015808105469\n",
            "Training epoch 10073/1000000, d_loss: -128.0160369873047,  g_loss: 195.48907470703125\n",
            "Training epoch 10074/1000000, d_loss: -214.53591918945312,  g_loss: 61.62232208251953\n",
            "Training epoch 10075/1000000, d_loss: -5.276893615722656,  g_loss: 140.15945434570312\n",
            "Training epoch 10076/1000000, d_loss: -182.6646728515625,  g_loss: 36.04644012451172\n",
            "Training epoch 10077/1000000, d_loss: -78.92044067382812,  g_loss: 43.57933044433594\n",
            "Training epoch 10078/1000000, d_loss: -89.28495025634766,  g_loss: 8.929732322692871\n",
            "Training epoch 10079/1000000, d_loss: -83.81094360351562,  g_loss: 54.858001708984375\n",
            "Training epoch 10080/1000000, d_loss: -424.74237060546875,  g_loss: -50.41036605834961\n",
            "Training epoch 10081/1000000, d_loss: 34.07670593261719,  g_loss: 68.85816955566406\n",
            "Training epoch 10082/1000000, d_loss: -134.62417602539062,  g_loss: 18.239887237548828\n",
            "Training epoch 10083/1000000, d_loss: -260.06378173828125,  g_loss: -98.52014923095703\n",
            "Training epoch 10084/1000000, d_loss: -207.91477966308594,  g_loss: -29.387575149536133\n",
            "Training epoch 10085/1000000, d_loss: 290.19189453125,  g_loss: 44.76122283935547\n",
            "Training epoch 10086/1000000, d_loss: -119.80709838867188,  g_loss: 86.37557220458984\n",
            "Training epoch 10087/1000000, d_loss: -28.76276397705078,  g_loss: 96.58589935302734\n",
            "Training epoch 10088/1000000, d_loss: -448.7774353027344,  g_loss: -91.22759246826172\n",
            "Training epoch 10089/1000000, d_loss: -33.30267333984375,  g_loss: 136.15744018554688\n",
            "Training epoch 10090/1000000, d_loss: -181.95806884765625,  g_loss: 379.6837158203125\n",
            "Training epoch 10091/1000000, d_loss: -74.40872192382812,  g_loss: 21.630855560302734\n",
            "Training epoch 10092/1000000, d_loss: -178.8480224609375,  g_loss: 20.388431549072266\n",
            "Training epoch 10093/1000000, d_loss: -461.0594482421875,  g_loss: -148.65625\n",
            "Training epoch 10094/1000000, d_loss: -15.966262817382812,  g_loss: 54.06250762939453\n",
            "Training epoch 10095/1000000, d_loss: -11.3792724609375,  g_loss: 113.96398162841797\n",
            "Training epoch 10096/1000000, d_loss: -100.86389923095703,  g_loss: 151.72769165039062\n",
            "Training epoch 10097/1000000, d_loss: -12.889663696289062,  g_loss: 170.492431640625\n",
            "Training epoch 10098/1000000, d_loss: -25.72266387939453,  g_loss: 129.2881317138672\n",
            "Training epoch 10099/1000000, d_loss: -45.05183410644531,  g_loss: 162.654052734375\n",
            "Training epoch 10100/1000000, d_loss: -84.8917236328125,  g_loss: 110.76577758789062\n",
            "Training epoch 10101/1000000, d_loss: -54.48594665527344,  g_loss: 174.85043334960938\n",
            "Training epoch 10102/1000000, d_loss: -42.85480499267578,  g_loss: 101.42312622070312\n",
            "Training epoch 10103/1000000, d_loss: -90.7689437866211,  g_loss: 122.64490509033203\n",
            "Training epoch 10104/1000000, d_loss: -94.109375,  g_loss: 47.64903259277344\n",
            "Training epoch 10105/1000000, d_loss: -129.58255004882812,  g_loss: 110.7196044921875\n",
            "Training epoch 10106/1000000, d_loss: -60.78346252441406,  g_loss: 66.91262817382812\n",
            "Training epoch 10107/1000000, d_loss: -93.88735961914062,  g_loss: 72.7137222290039\n",
            "Training epoch 10108/1000000, d_loss: -383.306884765625,  g_loss: -169.8201141357422\n",
            "Training epoch 10109/1000000, d_loss: -130.7299346923828,  g_loss: 96.7005615234375\n",
            "Training epoch 10110/1000000, d_loss: 62.05633544921875,  g_loss: 194.69927978515625\n",
            "Training epoch 10111/1000000, d_loss: -63.41265869140625,  g_loss: 261.9091796875\n",
            "Training epoch 10112/1000000, d_loss: -77.20109558105469,  g_loss: 143.42721557617188\n",
            "Training epoch 10113/1000000, d_loss: -72.82975769042969,  g_loss: 69.3010025024414\n",
            "Training epoch 10114/1000000, d_loss: -155.41180419921875,  g_loss: 4.871503829956055\n",
            "Training epoch 10115/1000000, d_loss: -272.47015380859375,  g_loss: 142.82391357421875\n",
            "Training epoch 10116/1000000, d_loss: -73.32772064208984,  g_loss: 141.59580993652344\n",
            "Training epoch 10117/1000000, d_loss: -164.5210418701172,  g_loss: 39.942420959472656\n",
            "Training epoch 10118/1000000, d_loss: -199.06295776367188,  g_loss: 46.71134948730469\n",
            "Training epoch 10119/1000000, d_loss: -117.23894500732422,  g_loss: 43.0162353515625\n",
            "Training epoch 10120/1000000, d_loss: -206.86581420898438,  g_loss: -0.3649435043334961\n",
            "Training epoch 10121/1000000, d_loss: -4.704620361328125,  g_loss: 130.70974731445312\n",
            "Training epoch 10122/1000000, d_loss: -248.55702209472656,  g_loss: 88.74813842773438\n",
            "Training epoch 10123/1000000, d_loss: -328.4832763671875,  g_loss: 80.9154052734375\n",
            "Training epoch 10124/1000000, d_loss: -38.526859283447266,  g_loss: 124.03761291503906\n",
            "Training epoch 10125/1000000, d_loss: -133.21792602539062,  g_loss: 94.96168518066406\n",
            "Training epoch 10126/1000000, d_loss: -114.37956237792969,  g_loss: 191.3137969970703\n",
            "Training epoch 10127/1000000, d_loss: -72.38416290283203,  g_loss: 95.62120819091797\n",
            "Training epoch 10128/1000000, d_loss: -295.1906433105469,  g_loss: 29.59282684326172\n",
            "Training epoch 10129/1000000, d_loss: -12.056449890136719,  g_loss: 131.65966796875\n",
            "Training epoch 10130/1000000, d_loss: -291.3894348144531,  g_loss: 35.14717102050781\n",
            "Training epoch 10131/1000000, d_loss: 50395.171875,  g_loss: -45.51689529418945\n",
            "Training epoch 10132/1000000, d_loss: 219.62335205078125,  g_loss: -72.9232177734375\n",
            "Training epoch 10133/1000000, d_loss: 544.6113891601562,  g_loss: -6.0084638595581055\n",
            "Training epoch 10134/1000000, d_loss: -52.610206604003906,  g_loss: -28.37377166748047\n",
            "Training epoch 10135/1000000, d_loss: -85.93795776367188,  g_loss: -23.185016632080078\n",
            "Training epoch 10136/1000000, d_loss: 142.62747192382812,  g_loss: -12.00113296508789\n",
            "Training epoch 10137/1000000, d_loss: -225.3280029296875,  g_loss: 105.47137451171875\n",
            "Training epoch 10138/1000000, d_loss: -162.62435913085938,  g_loss: -12.373664855957031\n",
            "Training epoch 10139/1000000, d_loss: -174.43589782714844,  g_loss: 64.90373229980469\n",
            "Training epoch 10140/1000000, d_loss: -24.807907104492188,  g_loss: -93.38137817382812\n",
            "Training epoch 10141/1000000, d_loss: -33.07743835449219,  g_loss: 254.92300415039062\n",
            "Training epoch 10142/1000000, d_loss: -118.97550201416016,  g_loss: -20.001373291015625\n",
            "Training epoch 10143/1000000, d_loss: -289.7945861816406,  g_loss: -112.30555725097656\n",
            "Training epoch 10144/1000000, d_loss: -19.354415893554688,  g_loss: 52.379695892333984\n",
            "Training epoch 10145/1000000, d_loss: -210.0889892578125,  g_loss: 1.8436708450317383\n",
            "Training epoch 10146/1000000, d_loss: -67.13623809814453,  g_loss: 17.06936264038086\n",
            "Training epoch 10147/1000000, d_loss: -75.44295501708984,  g_loss: 43.14596939086914\n",
            "Training epoch 10148/1000000, d_loss: -128.07969665527344,  g_loss: -13.07855224609375\n",
            "Training epoch 10149/1000000, d_loss: -154.38645935058594,  g_loss: 98.48292541503906\n",
            "Training epoch 10150/1000000, d_loss: -44.113006591796875,  g_loss: 63.14755630493164\n",
            "Training epoch 10151/1000000, d_loss: 95.80516815185547,  g_loss: 140.732421875\n",
            "Training epoch 10152/1000000, d_loss: -97.1947021484375,  g_loss: 149.71392822265625\n",
            "Training epoch 10153/1000000, d_loss: -70.97455596923828,  g_loss: 83.95176696777344\n",
            "Training epoch 10154/1000000, d_loss: -171.273681640625,  g_loss: 47.741539001464844\n",
            "Training epoch 10155/1000000, d_loss: -258.6009521484375,  g_loss: -13.441669464111328\n",
            "Training epoch 10156/1000000, d_loss: -113.50133514404297,  g_loss: 11.943769454956055\n",
            "Training epoch 10157/1000000, d_loss: -197.32254028320312,  g_loss: -59.90786361694336\n",
            "Training epoch 10158/1000000, d_loss: -75.37925720214844,  g_loss: 90.70616912841797\n",
            "Training epoch 10159/1000000, d_loss: -422.5474853515625,  g_loss: -67.05223083496094\n",
            "Training epoch 10160/1000000, d_loss: -133.37779235839844,  g_loss: 87.79454803466797\n",
            "Training epoch 10161/1000000, d_loss: -243.43612670898438,  g_loss: 49.261505126953125\n",
            "Training epoch 10162/1000000, d_loss: -10.805709838867188,  g_loss: 77.33890533447266\n",
            "Training epoch 10163/1000000, d_loss: -335.93328857421875,  g_loss: -13.456225395202637\n",
            "Training epoch 10164/1000000, d_loss: -281.4852294921875,  g_loss: -30.698383331298828\n",
            "Training epoch 10165/1000000, d_loss: -112.90214538574219,  g_loss: 47.187255859375\n",
            "Training epoch 10166/1000000, d_loss: -113.9837875366211,  g_loss: 131.83480834960938\n",
            "Training epoch 10167/1000000, d_loss: -14.258506774902344,  g_loss: 103.80502319335938\n",
            "Training epoch 10168/1000000, d_loss: -132.3018035888672,  g_loss: 108.83732604980469\n",
            "Training epoch 10169/1000000, d_loss: -102.05675506591797,  g_loss: 4.887063980102539\n",
            "Training epoch 10170/1000000, d_loss: -79.50019073486328,  g_loss: 40.56615447998047\n",
            "Training epoch 10171/1000000, d_loss: -102.6623764038086,  g_loss: 71.57359313964844\n",
            "Training epoch 10172/1000000, d_loss: 3640.96923828125,  g_loss: 63.12979507446289\n",
            "Training epoch 10173/1000000, d_loss: 26.789939880371094,  g_loss: 26.3909969329834\n",
            "Training epoch 10174/1000000, d_loss: -72.80799865722656,  g_loss: 53.98902893066406\n",
            "Training epoch 10175/1000000, d_loss: -28.332996368408203,  g_loss: 84.33677673339844\n",
            "Training epoch 10176/1000000, d_loss: -168.76023864746094,  g_loss: 54.07695388793945\n",
            "Training epoch 10177/1000000, d_loss: -262.9822998046875,  g_loss: 68.77978515625\n",
            "Training epoch 10178/1000000, d_loss: -66.57477569580078,  g_loss: 55.66796112060547\n",
            "Training epoch 10179/1000000, d_loss: -488.8167724609375,  g_loss: 25.79816436767578\n",
            "Training epoch 10180/1000000, d_loss: -111.28914642333984,  g_loss: 62.312171936035156\n",
            "Training epoch 10181/1000000, d_loss: -194.5506134033203,  g_loss: -16.56972885131836\n",
            "Training epoch 10182/1000000, d_loss: -169.94537353515625,  g_loss: 60.723384857177734\n",
            "Training epoch 10183/1000000, d_loss: -139.98333740234375,  g_loss: -7.977410316467285\n",
            "Training epoch 10184/1000000, d_loss: -70.9344482421875,  g_loss: 14.677730560302734\n",
            "Training epoch 10185/1000000, d_loss: -62.32191467285156,  g_loss: 17.100685119628906\n",
            "Training epoch 10186/1000000, d_loss: -100.5864486694336,  g_loss: 153.24224853515625\n",
            "Training epoch 10187/1000000, d_loss: -87.06208801269531,  g_loss: 64.86726379394531\n",
            "Training epoch 10188/1000000, d_loss: -244.667236328125,  g_loss: 84.1846694946289\n",
            "Training epoch 10189/1000000, d_loss: -74.93296813964844,  g_loss: -19.717899322509766\n",
            "Training epoch 10190/1000000, d_loss: -69.77845001220703,  g_loss: -70.57249450683594\n",
            "Training epoch 10191/1000000, d_loss: -358.4184265136719,  g_loss: -203.18911743164062\n",
            "Training epoch 10192/1000000, d_loss: 47.512603759765625,  g_loss: 52.59852981567383\n",
            "Training epoch 10193/1000000, d_loss: -95.74270629882812,  g_loss: 125.35887908935547\n",
            "Training epoch 10194/1000000, d_loss: -104.13519287109375,  g_loss: 110.93963623046875\n",
            "Training epoch 10195/1000000, d_loss: -406.33953857421875,  g_loss: 116.91679382324219\n",
            "Training epoch 10196/1000000, d_loss: -257.1693420410156,  g_loss: 98.89761352539062\n",
            "Training epoch 10197/1000000, d_loss: -287.59320068359375,  g_loss: 155.15013122558594\n",
            "Training epoch 10198/1000000, d_loss: -361.0194091796875,  g_loss: 40.254554748535156\n",
            "Training epoch 10199/1000000, d_loss: -975.6085205078125,  g_loss: -148.273681640625\n",
            "Training epoch 10200/1000000, d_loss: -261.5005798339844,  g_loss: 114.96424102783203\n",
            "Training epoch 10201/1000000, d_loss: -53.79314041137695,  g_loss: 19.979373931884766\n",
            "Training epoch 10202/1000000, d_loss: -277.553955078125,  g_loss: 1.1901674270629883\n",
            "Training epoch 10203/1000000, d_loss: -183.37652587890625,  g_loss: 245.57333374023438\n",
            "Training epoch 10204/1000000, d_loss: -75.62113952636719,  g_loss: 66.02639770507812\n",
            "Training epoch 10205/1000000, d_loss: -173.13406372070312,  g_loss: 124.05342864990234\n",
            "Training epoch 10206/1000000, d_loss: -250.67263793945312,  g_loss: 41.44358444213867\n",
            "Training epoch 10207/1000000, d_loss: -113.48699951171875,  g_loss: -41.35301971435547\n",
            "Training epoch 10208/1000000, d_loss: -438.8779296875,  g_loss: -55.95581817626953\n",
            "Training epoch 10209/1000000, d_loss: -1021.1129760742188,  g_loss: -492.0560302734375\n",
            "Training epoch 10210/1000000, d_loss: 69.2706069946289,  g_loss: -41.98987579345703\n",
            "Training epoch 10211/1000000, d_loss: -163.0461883544922,  g_loss: 246.01280212402344\n",
            "Training epoch 10212/1000000, d_loss: -40.37063217163086,  g_loss: 128.93580627441406\n",
            "Training epoch 10213/1000000, d_loss: -132.45040893554688,  g_loss: 126.44068908691406\n",
            "Training epoch 10214/1000000, d_loss: -105.11625671386719,  g_loss: 183.69430541992188\n",
            "Training epoch 10215/1000000, d_loss: -14.288360595703125,  g_loss: 121.49314880371094\n",
            "Training epoch 10216/1000000, d_loss: -47.872581481933594,  g_loss: 81.85601806640625\n",
            "Training epoch 10217/1000000, d_loss: -378.75103759765625,  g_loss: 430.5530700683594\n",
            "Training epoch 10218/1000000, d_loss: -132.3550262451172,  g_loss: 497.7667236328125\n",
            "Training epoch 10219/1000000, d_loss: -161.15682983398438,  g_loss: 525.3790283203125\n",
            "Training epoch 10220/1000000, d_loss: -44.83346176147461,  g_loss: 39.59796142578125\n",
            "Training epoch 10221/1000000, d_loss: -1.4955825805664062,  g_loss: 66.06352996826172\n",
            "Training epoch 10222/1000000, d_loss: -46.26005554199219,  g_loss: 108.04214477539062\n",
            "Training epoch 10223/1000000, d_loss: -229.45248413085938,  g_loss: 184.19696044921875\n",
            "Training epoch 10224/1000000, d_loss: -173.36849975585938,  g_loss: 99.61335754394531\n",
            "Training epoch 10225/1000000, d_loss: -105.61929321289062,  g_loss: 60.43937301635742\n",
            "Training epoch 10226/1000000, d_loss: -263.6768493652344,  g_loss: 429.5447998046875\n",
            "Training epoch 10227/1000000, d_loss: -368.6817321777344,  g_loss: 101.15995788574219\n",
            "Training epoch 10228/1000000, d_loss: -243.92105102539062,  g_loss: 8.259912490844727\n",
            "Training epoch 10229/1000000, d_loss: 44.25519943237305,  g_loss: 100.65028381347656\n",
            "Training epoch 10230/1000000, d_loss: -107.70829010009766,  g_loss: 107.2930679321289\n",
            "Training epoch 10231/1000000, d_loss: -118.67578125,  g_loss: 61.42613983154297\n",
            "Training epoch 10232/1000000, d_loss: -22.383590698242188,  g_loss: 119.24778747558594\n",
            "Training epoch 10233/1000000, d_loss: -31.638505935668945,  g_loss: 118.39733123779297\n",
            "Training epoch 10234/1000000, d_loss: -23.559505462646484,  g_loss: 126.19445037841797\n",
            "Training epoch 10235/1000000, d_loss: -60.796451568603516,  g_loss: 142.1665802001953\n",
            "Training epoch 10236/1000000, d_loss: -147.5816650390625,  g_loss: 118.71449279785156\n",
            "Training epoch 10237/1000000, d_loss: -71.36247253417969,  g_loss: 88.63177490234375\n",
            "Training epoch 10238/1000000, d_loss: -109.32315063476562,  g_loss: 68.61637878417969\n",
            "Training epoch 10239/1000000, d_loss: -65.17237854003906,  g_loss: 60.98871612548828\n",
            "Training epoch 10240/1000000, d_loss: -62.53586959838867,  g_loss: 55.21532440185547\n",
            "Training epoch 10241/1000000, d_loss: -229.47454833984375,  g_loss: 82.25507354736328\n",
            "Training epoch 10242/1000000, d_loss: -1166.7652587890625,  g_loss: -343.2685852050781\n",
            "Training epoch 10243/1000000, d_loss: -216.08395385742188,  g_loss: -59.52163314819336\n",
            "Training epoch 10244/1000000, d_loss: -354.58197021484375,  g_loss: -135.9385223388672\n",
            "Training epoch 10245/1000000, d_loss: 73.34840393066406,  g_loss: 8.402227401733398\n",
            "Training epoch 10246/1000000, d_loss: -36.576454162597656,  g_loss: 34.1891975402832\n",
            "Training epoch 10247/1000000, d_loss: -158.44871520996094,  g_loss: 48.65554428100586\n",
            "Training epoch 10248/1000000, d_loss: -34.40272521972656,  g_loss: 51.498958587646484\n",
            "Training epoch 10249/1000000, d_loss: 28.981613159179688,  g_loss: 27.76299476623535\n",
            "Training epoch 10250/1000000, d_loss: -86.03971862792969,  g_loss: 41.98640441894531\n",
            "Training epoch 10251/1000000, d_loss: -245.00643920898438,  g_loss: 22.449100494384766\n",
            "Training epoch 10252/1000000, d_loss: -81.65486907958984,  g_loss: 56.41497039794922\n",
            "Training epoch 10253/1000000, d_loss: -58.68959045410156,  g_loss: 60.989505767822266\n",
            "Training epoch 10254/1000000, d_loss: -204.57797241210938,  g_loss: 65.43303680419922\n",
            "Training epoch 10255/1000000, d_loss: 18.282516479492188,  g_loss: 85.13370513916016\n",
            "Training epoch 10256/1000000, d_loss: -67.45144653320312,  g_loss: 59.90416717529297\n",
            "Training epoch 10257/1000000, d_loss: -152.62680053710938,  g_loss: 73.87308502197266\n",
            "Training epoch 10258/1000000, d_loss: -49.05976867675781,  g_loss: 62.09199523925781\n",
            "Training epoch 10259/1000000, d_loss: -52.8223876953125,  g_loss: 63.012847900390625\n",
            "Training epoch 10260/1000000, d_loss: -132.703857421875,  g_loss: 102.52964782714844\n",
            "Training epoch 10261/1000000, d_loss: -718.197998046875,  g_loss: -298.23382568359375\n",
            "Training epoch 10262/1000000, d_loss: 35.230316162109375,  g_loss: 4.565731048583984\n",
            "Training epoch 10263/1000000, d_loss: 34.48247528076172,  g_loss: 109.28713989257812\n",
            "Training epoch 10264/1000000, d_loss: -18.200347900390625,  g_loss: 129.61581420898438\n",
            "Training epoch 10265/1000000, d_loss: -42.966949462890625,  g_loss: 97.54852294921875\n",
            "Training epoch 10266/1000000, d_loss: -466.320068359375,  g_loss: -24.52724838256836\n",
            "Training epoch 10267/1000000, d_loss: -77.4749526977539,  g_loss: 50.41551971435547\n",
            "Training epoch 10268/1000000, d_loss: -51.487510681152344,  g_loss: 73.91258239746094\n",
            "Training epoch 10269/1000000, d_loss: -1968.1290283203125,  g_loss: -482.3973388671875\n",
            "Training epoch 10270/1000000, d_loss: -443.6106872558594,  g_loss: 99.77178955078125\n",
            "Training epoch 10271/1000000, d_loss: 477.2777099609375,  g_loss: -57.29950714111328\n",
            "Training epoch 10272/1000000, d_loss: -129.0752410888672,  g_loss: -388.326416015625\n",
            "Training epoch 10273/1000000, d_loss: 7.312232971191406,  g_loss: 45.77753829956055\n",
            "Training epoch 10274/1000000, d_loss: -118.34329223632812,  g_loss: 28.098712921142578\n",
            "Training epoch 10275/1000000, d_loss: 322.7861633300781,  g_loss: -189.87913513183594\n",
            "Training epoch 10276/1000000, d_loss: 95.35903930664062,  g_loss: 203.21688842773438\n",
            "Training epoch 10277/1000000, d_loss: -493.7342224121094,  g_loss: 657.8582153320312\n",
            "Training epoch 10278/1000000, d_loss: -595.6712646484375,  g_loss: 858.374755859375\n",
            "Training epoch 10279/1000000, d_loss: -548.8583984375,  g_loss: 928.4732666015625\n",
            "Training epoch 10280/1000000, d_loss: 147.42214965820312,  g_loss: -18.644044876098633\n",
            "Training epoch 10281/1000000, d_loss: -83.95712280273438,  g_loss: -55.28190612792969\n",
            "Training epoch 10282/1000000, d_loss: 7.760128021240234,  g_loss: 3.3054351806640625\n",
            "Training epoch 10283/1000000, d_loss: -21.55274200439453,  g_loss: -300.54034423828125\n",
            "Training epoch 10284/1000000, d_loss: -50.6794548034668,  g_loss: -99.34213256835938\n",
            "Training epoch 10285/1000000, d_loss: -39.93434143066406,  g_loss: -59.60603713989258\n",
            "Training epoch 10286/1000000, d_loss: -93.46261596679688,  g_loss: 10.726485252380371\n",
            "Training epoch 10287/1000000, d_loss: -44.05878829956055,  g_loss: 8.001291275024414\n",
            "Training epoch 10288/1000000, d_loss: -6.071070671081543,  g_loss: 28.137290954589844\n",
            "Training epoch 10289/1000000, d_loss: -97.91557312011719,  g_loss: 47.232566833496094\n",
            "Training epoch 10290/1000000, d_loss: -150.8376922607422,  g_loss: 97.34371185302734\n",
            "Training epoch 10291/1000000, d_loss: -173.0915069580078,  g_loss: 139.72613525390625\n",
            "Training epoch 10292/1000000, d_loss: -36.478294372558594,  g_loss: 28.68870735168457\n",
            "Training epoch 10293/1000000, d_loss: -86.53596496582031,  g_loss: 47.37750244140625\n",
            "Training epoch 10294/1000000, d_loss: -108.04512786865234,  g_loss: 132.28173828125\n",
            "Training epoch 10295/1000000, d_loss: -57.21037673950195,  g_loss: 123.18412017822266\n",
            "Training epoch 10296/1000000, d_loss: -103.14204406738281,  g_loss: 172.13706970214844\n",
            "Training epoch 10297/1000000, d_loss: -35.46510314941406,  g_loss: 50.36127853393555\n",
            "Training epoch 10298/1000000, d_loss: -96.76620483398438,  g_loss: 46.52027893066406\n",
            "Training epoch 10299/1000000, d_loss: 22.297882080078125,  g_loss: 77.29933166503906\n",
            "Training epoch 10300/1000000, d_loss: -56.847267150878906,  g_loss: 30.579994201660156\n",
            "Training epoch 10301/1000000, d_loss: -100.01786041259766,  g_loss: 96.82337951660156\n",
            "Training epoch 10302/1000000, d_loss: -166.28672790527344,  g_loss: 42.384822845458984\n",
            "Training epoch 10303/1000000, d_loss: -215.59918212890625,  g_loss: -19.511333465576172\n",
            "Training epoch 10304/1000000, d_loss: -127.66255187988281,  g_loss: 50.48877716064453\n",
            "Training epoch 10305/1000000, d_loss: -215.93179321289062,  g_loss: 23.996654510498047\n",
            "Training epoch 10306/1000000, d_loss: -232.67337036132812,  g_loss: 32.53288650512695\n",
            "Training epoch 10307/1000000, d_loss: -8.503219604492188,  g_loss: 57.88617706298828\n",
            "Training epoch 10308/1000000, d_loss: -56.41425323486328,  g_loss: 69.22660827636719\n",
            "Training epoch 10309/1000000, d_loss: 4.633491516113281,  g_loss: 110.6145248413086\n",
            "Training epoch 10310/1000000, d_loss: -209.30984497070312,  g_loss: 65.48989868164062\n",
            "Training epoch 10311/1000000, d_loss: -88.8966064453125,  g_loss: 125.59453582763672\n",
            "Training epoch 10312/1000000, d_loss: -100.533447265625,  g_loss: 97.96742248535156\n",
            "Training epoch 10313/1000000, d_loss: 64.119384765625,  g_loss: 49.501407623291016\n",
            "Training epoch 10314/1000000, d_loss: -772.711669921875,  g_loss: -86.12765502929688\n",
            "Training epoch 10315/1000000, d_loss: 41.591705322265625,  g_loss: 8.489099502563477\n",
            "Training epoch 10316/1000000, d_loss: -1247.9991455078125,  g_loss: -517.460205078125\n",
            "Training epoch 10317/1000000, d_loss: 131.8326416015625,  g_loss: 51.92494201660156\n",
            "Training epoch 10318/1000000, d_loss: -325.90435791015625,  g_loss: 132.0442352294922\n",
            "Training epoch 10319/1000000, d_loss: 7.50982666015625,  g_loss: 184.01852416992188\n",
            "Training epoch 10320/1000000, d_loss: -74.2007827758789,  g_loss: 300.40924072265625\n",
            "Training epoch 10321/1000000, d_loss: -151.52127075195312,  g_loss: 200.7550811767578\n",
            "Training epoch 10322/1000000, d_loss: -166.45187377929688,  g_loss: 403.26190185546875\n",
            "Training epoch 10323/1000000, d_loss: -39.75851821899414,  g_loss: 224.76028442382812\n",
            "Training epoch 10324/1000000, d_loss: -147.3948516845703,  g_loss: 261.24432373046875\n",
            "Training epoch 10325/1000000, d_loss: -238.95462036132812,  g_loss: 471.9869384765625\n",
            "Training epoch 10326/1000000, d_loss: -62.27495574951172,  g_loss: 69.50035095214844\n",
            "Training epoch 10327/1000000, d_loss: -76.6775131225586,  g_loss: 44.18974304199219\n",
            "Training epoch 10328/1000000, d_loss: -106.62981414794922,  g_loss: -1.1131820678710938\n",
            "Training epoch 10329/1000000, d_loss: -84.98151397705078,  g_loss: -5.134037017822266\n",
            "Training epoch 10330/1000000, d_loss: -63.42703628540039,  g_loss: 57.038814544677734\n",
            "Training epoch 10331/1000000, d_loss: -367.2188720703125,  g_loss: -103.09239196777344\n",
            "Training epoch 10332/1000000, d_loss: -1155.5780029296875,  g_loss: -158.15444946289062\n",
            "Training epoch 10333/1000000, d_loss: -221.0404052734375,  g_loss: 22.51142692565918\n",
            "Training epoch 10334/1000000, d_loss: 125.28155517578125,  g_loss: 2.2850418090820312\n",
            "Training epoch 10335/1000000, d_loss: -63.645477294921875,  g_loss: 9.026267051696777\n",
            "Training epoch 10336/1000000, d_loss: -10.01021957397461,  g_loss: 42.188602447509766\n",
            "Training epoch 10337/1000000, d_loss: 77.67598724365234,  g_loss: 81.12821197509766\n",
            "Training epoch 10338/1000000, d_loss: -119.30363464355469,  g_loss: 86.75456237792969\n",
            "Training epoch 10339/1000000, d_loss: -31.901260375976562,  g_loss: 164.04031372070312\n",
            "Training epoch 10340/1000000, d_loss: -99.86847686767578,  g_loss: 149.81509399414062\n",
            "Training epoch 10341/1000000, d_loss: 2.2267532348632812,  g_loss: 231.18788146972656\n",
            "Training epoch 10342/1000000, d_loss: -75.58840942382812,  g_loss: 219.80224609375\n",
            "Training epoch 10343/1000000, d_loss: -206.53904724121094,  g_loss: 29.739721298217773\n",
            "Training epoch 10344/1000000, d_loss: -309.0376892089844,  g_loss: 11.202468872070312\n",
            "Training epoch 10345/1000000, d_loss: -686.9014282226562,  g_loss: -223.67373657226562\n",
            "Training epoch 10346/1000000, d_loss: -73.93502807617188,  g_loss: 154.95957946777344\n",
            "Training epoch 10347/1000000, d_loss: -331.4678649902344,  g_loss: 281.65142822265625\n",
            "Training epoch 10348/1000000, d_loss: 36.33966064453125,  g_loss: 81.39862823486328\n",
            "Training epoch 10349/1000000, d_loss: -318.2522277832031,  g_loss: 436.3524169921875\n",
            "Training epoch 10350/1000000, d_loss: -77.98284912109375,  g_loss: 148.8880615234375\n",
            "Training epoch 10351/1000000, d_loss: -45.09898376464844,  g_loss: 278.8169250488281\n",
            "Training epoch 10352/1000000, d_loss: -105.93058013916016,  g_loss: 28.072683334350586\n",
            "Training epoch 10353/1000000, d_loss: -109.8327407836914,  g_loss: 59.95362854003906\n",
            "Training epoch 10354/1000000, d_loss: -203.09310913085938,  g_loss: 21.17337417602539\n",
            "Training epoch 10355/1000000, d_loss: -116.38642883300781,  g_loss: 222.44793701171875\n",
            "Training epoch 10356/1000000, d_loss: -67.02836608886719,  g_loss: 49.3531494140625\n",
            "Training epoch 10357/1000000, d_loss: -110.44805908203125,  g_loss: 102.37577056884766\n",
            "Training epoch 10358/1000000, d_loss: -23.176258087158203,  g_loss: 79.9146499633789\n",
            "Training epoch 10359/1000000, d_loss: -1941.4063720703125,  g_loss: -483.24249267578125\n",
            "Training epoch 10360/1000000, d_loss: -1.8711767196655273,  g_loss: -126.47726440429688\n",
            "Training epoch 10361/1000000, d_loss: 128.95291137695312,  g_loss: 71.01248168945312\n",
            "Training epoch 10362/1000000, d_loss: -50.269493103027344,  g_loss: 63.65068817138672\n",
            "Training epoch 10363/1000000, d_loss: -30.76284408569336,  g_loss: 99.23121643066406\n",
            "Training epoch 10364/1000000, d_loss: -251.74771118164062,  g_loss: 44.99943542480469\n",
            "Training epoch 10365/1000000, d_loss: -55.195159912109375,  g_loss: 50.09397888183594\n",
            "Training epoch 10366/1000000, d_loss: -362.78125,  g_loss: -147.73800659179688\n",
            "Training epoch 10367/1000000, d_loss: 46.69708251953125,  g_loss: 76.13134002685547\n",
            "Training epoch 10368/1000000, d_loss: -225.01895141601562,  g_loss: 35.9088134765625\n",
            "Training epoch 10369/1000000, d_loss: 46.558624267578125,  g_loss: 154.32449340820312\n",
            "Training epoch 10370/1000000, d_loss: -205.7720947265625,  g_loss: 165.99781799316406\n",
            "Training epoch 10371/1000000, d_loss: -657.05712890625,  g_loss: -202.40603637695312\n",
            "Training epoch 10372/1000000, d_loss: -125.55095672607422,  g_loss: -38.244834899902344\n",
            "Training epoch 10373/1000000, d_loss: -307.2847595214844,  g_loss: -1.5624284744262695\n",
            "Training epoch 10374/1000000, d_loss: -69.53655242919922,  g_loss: 170.25311279296875\n",
            "Training epoch 10375/1000000, d_loss: 18.568283081054688,  g_loss: 127.58769226074219\n",
            "Training epoch 10376/1000000, d_loss: -227.4419708251953,  g_loss: 143.7032928466797\n",
            "Training epoch 10377/1000000, d_loss: -198.18377685546875,  g_loss: 482.3861999511719\n",
            "Training epoch 10378/1000000, d_loss: 28.20388412475586,  g_loss: 95.5966796875\n",
            "Training epoch 10379/1000000, d_loss: -94.20692443847656,  g_loss: 136.82000732421875\n",
            "Training epoch 10380/1000000, d_loss: -54.04420471191406,  g_loss: 133.71495056152344\n",
            "Training epoch 10381/1000000, d_loss: -601.7463989257812,  g_loss: 476.01910400390625\n",
            "Training epoch 10382/1000000, d_loss: -25.245269775390625,  g_loss: 232.0863037109375\n",
            "Training epoch 10383/1000000, d_loss: -141.50991821289062,  g_loss: 26.013946533203125\n",
            "Training epoch 10384/1000000, d_loss: -51.263160705566406,  g_loss: 42.12358093261719\n",
            "Training epoch 10385/1000000, d_loss: -100.18794250488281,  g_loss: 106.98954772949219\n",
            "Training epoch 10386/1000000, d_loss: -63.404457092285156,  g_loss: 131.68910217285156\n",
            "Training epoch 10387/1000000, d_loss: -4.283359527587891,  g_loss: 152.91200256347656\n",
            "Training epoch 10388/1000000, d_loss: -474.1905517578125,  g_loss: -9.353103637695312\n",
            "Training epoch 10389/1000000, d_loss: -140.1710205078125,  g_loss: 25.600513458251953\n",
            "Training epoch 10390/1000000, d_loss: 12.232778549194336,  g_loss: 50.88190460205078\n",
            "Training epoch 10391/1000000, d_loss: 22.9202880859375,  g_loss: 69.71397399902344\n",
            "Training epoch 10392/1000000, d_loss: -68.70767974853516,  g_loss: 86.43290710449219\n",
            "Training epoch 10393/1000000, d_loss: -796.06884765625,  g_loss: -18.793285369873047\n",
            "Training epoch 10394/1000000, d_loss: 20.586883544921875,  g_loss: -11.903244018554688\n",
            "Training epoch 10395/1000000, d_loss: -113.88105010986328,  g_loss: 36.656009674072266\n",
            "Training epoch 10396/1000000, d_loss: -264.42657470703125,  g_loss: 0.18208742141723633\n",
            "Training epoch 10397/1000000, d_loss: -3.6663818359375,  g_loss: -6.067561149597168\n",
            "Training epoch 10398/1000000, d_loss: 2558.9580078125,  g_loss: -62.78644561767578\n",
            "Training epoch 10399/1000000, d_loss: -40.86518859863281,  g_loss: 0.9577178955078125\n",
            "Training epoch 10400/1000000, d_loss: -21.7001895904541,  g_loss: -22.75064468383789\n",
            "Training epoch 10401/1000000, d_loss: -363.0888977050781,  g_loss: -158.49090576171875\n",
            "Training epoch 10402/1000000, d_loss: 159.20559692382812,  g_loss: 26.16371726989746\n",
            "Training epoch 10403/1000000, d_loss: -113.51160430908203,  g_loss: 111.93167877197266\n",
            "Training epoch 10404/1000000, d_loss: -140.96395874023438,  g_loss: -71.84097290039062\n",
            "Training epoch 10405/1000000, d_loss: -206.79681396484375,  g_loss: -10.699487686157227\n",
            "Training epoch 10406/1000000, d_loss: -20.081130981445312,  g_loss: 95.21656799316406\n",
            "Training epoch 10407/1000000, d_loss: -85.09188842773438,  g_loss: 146.46902465820312\n",
            "Training epoch 10408/1000000, d_loss: -102.50614929199219,  g_loss: 138.2389373779297\n",
            "Training epoch 10409/1000000, d_loss: -88.31710052490234,  g_loss: 176.6143341064453\n",
            "Training epoch 10410/1000000, d_loss: -268.568115234375,  g_loss: 237.6205596923828\n",
            "Training epoch 10411/1000000, d_loss: -167.9632568359375,  g_loss: 248.6014404296875\n",
            "Training epoch 10412/1000000, d_loss: -76.01248931884766,  g_loss: 99.9801254272461\n",
            "Training epoch 10413/1000000, d_loss: -168.2336883544922,  g_loss: 98.32693481445312\n",
            "Training epoch 10414/1000000, d_loss: -68.77608489990234,  g_loss: 47.808135986328125\n",
            "Training epoch 10415/1000000, d_loss: -342.03802490234375,  g_loss: -123.32294464111328\n",
            "Training epoch 10416/1000000, d_loss: -157.8947296142578,  g_loss: 121.43922424316406\n",
            "Training epoch 10417/1000000, d_loss: -35.40930938720703,  g_loss: 44.86640548706055\n",
            "Training epoch 10418/1000000, d_loss: -250.34519958496094,  g_loss: 33.4769287109375\n",
            "Training epoch 10419/1000000, d_loss: -808.4293212890625,  g_loss: -654.0626831054688\n",
            "Training epoch 10420/1000000, d_loss: 113.68206787109375,  g_loss: -13.526229858398438\n",
            "Training epoch 10421/1000000, d_loss: -235.8334197998047,  g_loss: 122.77086639404297\n",
            "Training epoch 10422/1000000, d_loss: -308.8109436035156,  g_loss: 53.06640625\n",
            "Training epoch 10423/1000000, d_loss: -66.48565673828125,  g_loss: 116.28360748291016\n",
            "Training epoch 10424/1000000, d_loss: -458.8318176269531,  g_loss: 89.55532836914062\n",
            "Training epoch 10425/1000000, d_loss: -229.14112854003906,  g_loss: 149.53564453125\n",
            "Training epoch 10426/1000000, d_loss: -83.05882263183594,  g_loss: 106.5101318359375\n",
            "Training epoch 10427/1000000, d_loss: -104.46395111083984,  g_loss: 103.44599914550781\n",
            "Training epoch 10428/1000000, d_loss: -34.350826263427734,  g_loss: 150.20138549804688\n",
            "Training epoch 10429/1000000, d_loss: -214.53431701660156,  g_loss: 810.4901733398438\n",
            "Training epoch 10430/1000000, d_loss: -268.29144287109375,  g_loss: 180.47177124023438\n",
            "Training epoch 10431/1000000, d_loss: -278.0853576660156,  g_loss: 426.54302978515625\n",
            "Training epoch 10432/1000000, d_loss: -498.345458984375,  g_loss: -236.67929077148438\n",
            "Training epoch 10433/1000000, d_loss: 23.96607208251953,  g_loss: 71.38632202148438\n",
            "Training epoch 10434/1000000, d_loss: -176.1611785888672,  g_loss: 56.92604446411133\n",
            "Training epoch 10435/1000000, d_loss: -156.50457763671875,  g_loss: 102.45304107666016\n",
            "Training epoch 10436/1000000, d_loss: -385.8365783691406,  g_loss: -3.45074462890625\n",
            "Training epoch 10437/1000000, d_loss: 53.043914794921875,  g_loss: 69.41942596435547\n",
            "Training epoch 10438/1000000, d_loss: -80.27039337158203,  g_loss: 169.98696899414062\n",
            "Training epoch 10439/1000000, d_loss: 115.79725646972656,  g_loss: 154.66128540039062\n",
            "Training epoch 10440/1000000, d_loss: 80.41635131835938,  g_loss: 109.85189056396484\n",
            "Training epoch 10441/1000000, d_loss: -103.20924377441406,  g_loss: 161.20062255859375\n",
            "Training epoch 10442/1000000, d_loss: -83.44878387451172,  g_loss: 228.8306121826172\n",
            "Training epoch 10443/1000000, d_loss: -141.94284057617188,  g_loss: 327.5494384765625\n",
            "Training epoch 10444/1000000, d_loss: -115.87310791015625,  g_loss: 126.23823547363281\n",
            "Training epoch 10445/1000000, d_loss: -86.17019653320312,  g_loss: 120.05915069580078\n",
            "Training epoch 10446/1000000, d_loss: -19.73638153076172,  g_loss: 129.29342651367188\n",
            "Training epoch 10447/1000000, d_loss: -183.14077758789062,  g_loss: 97.41995239257812\n",
            "Training epoch 10448/1000000, d_loss: -136.65765380859375,  g_loss: 115.58866882324219\n",
            "Training epoch 10449/1000000, d_loss: -84.55121612548828,  g_loss: 158.0296173095703\n",
            "Training epoch 10450/1000000, d_loss: -433.36907958984375,  g_loss: 65.477783203125\n",
            "Training epoch 10451/1000000, d_loss: -93.07508087158203,  g_loss: -17.855072021484375\n",
            "Training epoch 10452/1000000, d_loss: -652.6055908203125,  g_loss: -414.8485107421875\n",
            "Training epoch 10453/1000000, d_loss: 5.3607940673828125,  g_loss: 143.58889770507812\n",
            "Training epoch 10454/1000000, d_loss: -355.42401123046875,  g_loss: 693.3914794921875\n",
            "Training epoch 10455/1000000, d_loss: -105.70494079589844,  g_loss: 166.6732177734375\n",
            "Training epoch 10456/1000000, d_loss: -242.62217712402344,  g_loss: 91.49964904785156\n",
            "Training epoch 10457/1000000, d_loss: -256.40521240234375,  g_loss: 193.84439086914062\n",
            "Training epoch 10458/1000000, d_loss: -506.13031005859375,  g_loss: 910.0830078125\n",
            "Training epoch 10459/1000000, d_loss: -50.02568817138672,  g_loss: 10.07398796081543\n",
            "Training epoch 10460/1000000, d_loss: -121.17889404296875,  g_loss: 47.52978515625\n",
            "Training epoch 10461/1000000, d_loss: -86.15594482421875,  g_loss: 53.879486083984375\n",
            "Training epoch 10462/1000000, d_loss: -624.779052734375,  g_loss: -124.76724243164062\n",
            "Training epoch 10463/1000000, d_loss: -624.6173095703125,  g_loss: -108.36082458496094\n",
            "Training epoch 10464/1000000, d_loss: -83.25294494628906,  g_loss: 90.6761703491211\n",
            "Training epoch 10465/1000000, d_loss: 50.10455322265625,  g_loss: 91.94125366210938\n",
            "Training epoch 10466/1000000, d_loss: -168.55865478515625,  g_loss: 56.75860595703125\n",
            "Training epoch 10467/1000000, d_loss: -28.276779174804688,  g_loss: 45.058868408203125\n",
            "Training epoch 10468/1000000, d_loss: -149.75900268554688,  g_loss: 32.823577880859375\n",
            "Training epoch 10469/1000000, d_loss: -62.68017578125,  g_loss: 121.61161804199219\n",
            "Training epoch 10470/1000000, d_loss: -79.84583282470703,  g_loss: 65.70040130615234\n",
            "Training epoch 10471/1000000, d_loss: -499.082763671875,  g_loss: -158.33236694335938\n",
            "Training epoch 10472/1000000, d_loss: -245.82064819335938,  g_loss: -61.35838317871094\n",
            "Training epoch 10473/1000000, d_loss: 122.392822265625,  g_loss: 82.15885925292969\n",
            "Training epoch 10474/1000000, d_loss: -44.3839111328125,  g_loss: 135.87713623046875\n",
            "Training epoch 10475/1000000, d_loss: -23.51556396484375,  g_loss: 219.72994995117188\n",
            "Training epoch 10476/1000000, d_loss: -85.91567993164062,  g_loss: 204.477783203125\n",
            "Training epoch 10477/1000000, d_loss: -44.710731506347656,  g_loss: 142.29086303710938\n",
            "Training epoch 10478/1000000, d_loss: -92.60334777832031,  g_loss: 147.86965942382812\n",
            "Training epoch 10479/1000000, d_loss: -274.98199462890625,  g_loss: 92.42527770996094\n",
            "Training epoch 10480/1000000, d_loss: -93.61349487304688,  g_loss: 77.86410522460938\n",
            "Training epoch 10481/1000000, d_loss: -35.40376281738281,  g_loss: 119.12132263183594\n",
            "Training epoch 10482/1000000, d_loss: -52.168678283691406,  g_loss: 128.24722290039062\n",
            "Training epoch 10483/1000000, d_loss: -79.43763732910156,  g_loss: 132.9019012451172\n",
            "Training epoch 10484/1000000, d_loss: -152.6590576171875,  g_loss: 172.298583984375\n",
            "Training epoch 10485/1000000, d_loss: -38.244441986083984,  g_loss: 109.52507019042969\n",
            "Training epoch 10486/1000000, d_loss: 59.649810791015625,  g_loss: 31.64521026611328\n",
            "Training epoch 10487/1000000, d_loss: -307.28509521484375,  g_loss: 4.989162445068359\n",
            "Training epoch 10488/1000000, d_loss: -96.4525375366211,  g_loss: 81.90643310546875\n",
            "Training epoch 10489/1000000, d_loss: -209.87014770507812,  g_loss: 39.879695892333984\n",
            "Training epoch 10490/1000000, d_loss: -54.547672271728516,  g_loss: 42.450775146484375\n",
            "Training epoch 10491/1000000, d_loss: -931.95654296875,  g_loss: 7.1259307861328125\n",
            "Training epoch 10492/1000000, d_loss: -66.95586395263672,  g_loss: -37.73485565185547\n",
            "Training epoch 10493/1000000, d_loss: -48.00884246826172,  g_loss: 103.37518310546875\n",
            "Training epoch 10494/1000000, d_loss: -56.08570098876953,  g_loss: 78.25759887695312\n",
            "Training epoch 10495/1000000, d_loss: -90.8489990234375,  g_loss: 56.85945129394531\n",
            "Training epoch 10496/1000000, d_loss: 29.309364318847656,  g_loss: 48.486236572265625\n",
            "Training epoch 10497/1000000, d_loss: -48.31904602050781,  g_loss: 87.25051879882812\n",
            "Training epoch 10498/1000000, d_loss: -126.63211059570312,  g_loss: 5.69200325012207\n",
            "Training epoch 10499/1000000, d_loss: -221.8657989501953,  g_loss: 33.18167495727539\n",
            "Training epoch 10500/1000000, d_loss: -114.05795288085938,  g_loss: 138.29331970214844\n",
            "Training epoch 10501/1000000, d_loss: -143.94317626953125,  g_loss: 168.06170654296875\n",
            "Training epoch 10502/1000000, d_loss: -289.1066589355469,  g_loss: 24.58930206298828\n",
            "Training epoch 10503/1000000, d_loss: 4.346735000610352,  g_loss: 75.79244995117188\n",
            "Training epoch 10504/1000000, d_loss: 1050.99267578125,  g_loss: 97.52649688720703\n",
            "Training epoch 10505/1000000, d_loss: -148.06082153320312,  g_loss: 59.629371643066406\n",
            "Training epoch 10506/1000000, d_loss: -78.88383483886719,  g_loss: 122.46440124511719\n",
            "Training epoch 10507/1000000, d_loss: -83.65904235839844,  g_loss: 70.86104583740234\n",
            "Training epoch 10508/1000000, d_loss: -50.61943054199219,  g_loss: 101.93518829345703\n",
            "Training epoch 10509/1000000, d_loss: -76.62107849121094,  g_loss: 32.77301788330078\n",
            "Training epoch 10510/1000000, d_loss: -43.01836395263672,  g_loss: 38.29579162597656\n",
            "Training epoch 10511/1000000, d_loss: 59.702640533447266,  g_loss: 53.807861328125\n",
            "Training epoch 10512/1000000, d_loss: 8.1834716796875,  g_loss: 73.04664611816406\n",
            "Training epoch 10513/1000000, d_loss: -234.46971130371094,  g_loss: 43.1140251159668\n",
            "Training epoch 10514/1000000, d_loss: -518.7969970703125,  g_loss: -10.526870727539062\n",
            "Training epoch 10515/1000000, d_loss: 112504.6015625,  g_loss: -86.81002044677734\n",
            "Training epoch 10516/1000000, d_loss: 200.39308166503906,  g_loss: -447.684814453125\n",
            "Training epoch 10517/1000000, d_loss: 309.6280212402344,  g_loss: -394.4306335449219\n",
            "Training epoch 10518/1000000, d_loss: 7.978546142578125,  g_loss: -275.9085388183594\n",
            "Training epoch 10519/1000000, d_loss: 246.3609619140625,  g_loss: -231.10943603515625\n",
            "Training epoch 10520/1000000, d_loss: 374.3354187011719,  g_loss: -433.5809326171875\n",
            "Training epoch 10521/1000000, d_loss: 401.5331115722656,  g_loss: 202.02813720703125\n",
            "Training epoch 10522/1000000, d_loss: -129.884033203125,  g_loss: 108.62679290771484\n",
            "Training epoch 10523/1000000, d_loss: 128.55047607421875,  g_loss: 216.88055419921875\n",
            "Training epoch 10524/1000000, d_loss: 0.102386474609375,  g_loss: 29.00177764892578\n",
            "Training epoch 10525/1000000, d_loss: -597.992431640625,  g_loss: 328.6260986328125\n",
            "Training epoch 10526/1000000, d_loss: -179.44017028808594,  g_loss: 143.328369140625\n",
            "Training epoch 10527/1000000, d_loss: -236.37542724609375,  g_loss: 220.9110565185547\n",
            "Training epoch 10528/1000000, d_loss: -230.09786987304688,  g_loss: 3.3043060302734375\n",
            "Training epoch 10529/1000000, d_loss: -210.23770141601562,  g_loss: 189.38790893554688\n",
            "Training epoch 10530/1000000, d_loss: -437.24835205078125,  g_loss: 1.1142349243164062\n",
            "Training epoch 10531/1000000, d_loss: -458.449462890625,  g_loss: 320.91363525390625\n",
            "Training epoch 10532/1000000, d_loss: 102.12677001953125,  g_loss: -408.6531066894531\n",
            "Training epoch 10533/1000000, d_loss: -15.957660675048828,  g_loss: -384.81671142578125\n",
            "Training epoch 10534/1000000, d_loss: -40.720550537109375,  g_loss: -282.75762939453125\n",
            "Training epoch 10535/1000000, d_loss: -23.86236572265625,  g_loss: -295.9557189941406\n",
            "Training epoch 10536/1000000, d_loss: -42.97384262084961,  g_loss: -217.15325927734375\n",
            "Training epoch 10537/1000000, d_loss: 61.995887756347656,  g_loss: -110.98727416992188\n",
            "Training epoch 10538/1000000, d_loss: -57.8831672668457,  g_loss: -56.74289321899414\n",
            "Training epoch 10539/1000000, d_loss: -140.70159912109375,  g_loss: -5.063323974609375\n",
            "Training epoch 10540/1000000, d_loss: -33.554718017578125,  g_loss: -17.931961059570312\n",
            "Training epoch 10541/1000000, d_loss: -140.0082244873047,  g_loss: -27.11455535888672\n",
            "Training epoch 10542/1000000, d_loss: -547.818603515625,  g_loss: -93.14550018310547\n",
            "Training epoch 10543/1000000, d_loss: -675.669677734375,  g_loss: -177.27951049804688\n",
            "Training epoch 10544/1000000, d_loss: -23.547016143798828,  g_loss: -10.536170959472656\n",
            "Training epoch 10545/1000000, d_loss: 46.58763122558594,  g_loss: 18.067859649658203\n",
            "Training epoch 10546/1000000, d_loss: -72.24060821533203,  g_loss: -38.18809509277344\n",
            "Training epoch 10547/1000000, d_loss: 15.713958740234375,  g_loss: -0.7113246917724609\n",
            "Training epoch 10548/1000000, d_loss: 7.681859970092773,  g_loss: 20.13549041748047\n",
            "Training epoch 10549/1000000, d_loss: -204.02853393554688,  g_loss: 9.636468887329102\n",
            "Training epoch 10550/1000000, d_loss: -63.444454193115234,  g_loss: 47.19336700439453\n",
            "Training epoch 10551/1000000, d_loss: -56.42341995239258,  g_loss: 43.79431915283203\n",
            "Training epoch 10552/1000000, d_loss: -47.78657913208008,  g_loss: 42.16358947753906\n",
            "Training epoch 10553/1000000, d_loss: -240.8257293701172,  g_loss: 187.70108032226562\n",
            "Training epoch 10554/1000000, d_loss: -285.2412414550781,  g_loss: 5.741623878479004\n",
            "Training epoch 10555/1000000, d_loss: -77.98214721679688,  g_loss: -38.509132385253906\n",
            "Training epoch 10556/1000000, d_loss: -33.326385498046875,  g_loss: -43.215606689453125\n",
            "Training epoch 10557/1000000, d_loss: -79.43840026855469,  g_loss: -59.30976867675781\n",
            "Training epoch 10558/1000000, d_loss: -342.175048828125,  g_loss: -65.67401885986328\n",
            "Training epoch 10559/1000000, d_loss: -144.5072479248047,  g_loss: -20.466659545898438\n",
            "Training epoch 10560/1000000, d_loss: 19.512672424316406,  g_loss: 100.54373168945312\n",
            "Training epoch 10561/1000000, d_loss: -85.77299499511719,  g_loss: 48.01182174682617\n",
            "Training epoch 10562/1000000, d_loss: -45.71723937988281,  g_loss: 26.99195098876953\n",
            "Training epoch 10563/1000000, d_loss: -111.51901245117188,  g_loss: -4.671587944030762\n",
            "Training epoch 10564/1000000, d_loss: -75.73362731933594,  g_loss: 43.52202606201172\n",
            "Training epoch 10565/1000000, d_loss: -71.25611877441406,  g_loss: 87.22987365722656\n",
            "Training epoch 10566/1000000, d_loss: -677.00244140625,  g_loss: -87.54025268554688\n",
            "Training epoch 10567/1000000, d_loss: -10.915977478027344,  g_loss: -1.4655470848083496\n",
            "Training epoch 10568/1000000, d_loss: -112.73662567138672,  g_loss: 53.00697708129883\n",
            "Training epoch 10569/1000000, d_loss: -386.0009460449219,  g_loss: -3.899176597595215\n",
            "Training epoch 10570/1000000, d_loss: -40.663814544677734,  g_loss: -2.801711082458496\n",
            "Training epoch 10571/1000000, d_loss: -90.7973403930664,  g_loss: 10.367709159851074\n",
            "Training epoch 10572/1000000, d_loss: -446.3758850097656,  g_loss: -148.54598999023438\n",
            "Training epoch 10573/1000000, d_loss: -65.75798797607422,  g_loss: -115.58648681640625\n",
            "Training epoch 10574/1000000, d_loss: -83.45001983642578,  g_loss: 108.08851623535156\n",
            "Training epoch 10575/1000000, d_loss: -752.0353393554688,  g_loss: 140.42747497558594\n",
            "Training epoch 10576/1000000, d_loss: -269.861083984375,  g_loss: -14.271211624145508\n",
            "Training epoch 10577/1000000, d_loss: -319.8409729003906,  g_loss: -152.7795867919922\n",
            "Training epoch 10578/1000000, d_loss: -171.12335205078125,  g_loss: 357.5089111328125\n",
            "Training epoch 10579/1000000, d_loss: -145.2945556640625,  g_loss: 50.72840118408203\n",
            "Training epoch 10580/1000000, d_loss: -363.1981201171875,  g_loss: 128.03457641601562\n",
            "Training epoch 10581/1000000, d_loss: -77.4384536743164,  g_loss: 24.672622680664062\n",
            "Training epoch 10582/1000000, d_loss: -309.5068359375,  g_loss: -141.08413696289062\n",
            "Training epoch 10583/1000000, d_loss: -24.121906280517578,  g_loss: 75.41004943847656\n",
            "Training epoch 10584/1000000, d_loss: -490.44268798828125,  g_loss: -215.87335205078125\n",
            "Training epoch 10585/1000000, d_loss: 24.062000274658203,  g_loss: 30.782302856445312\n",
            "Training epoch 10586/1000000, d_loss: -703.2576904296875,  g_loss: -223.0550994873047\n",
            "Training epoch 10587/1000000, d_loss: 92.23464965820312,  g_loss: 162.5756378173828\n",
            "Training epoch 10588/1000000, d_loss: -91.20358276367188,  g_loss: 131.19354248046875\n",
            "Training epoch 10589/1000000, d_loss: -354.4978942871094,  g_loss: -60.956817626953125\n",
            "Training epoch 10590/1000000, d_loss: -92.02833557128906,  g_loss: 23.491592407226562\n",
            "Training epoch 10591/1000000, d_loss: -49.62653732299805,  g_loss: 17.348339080810547\n",
            "Training epoch 10592/1000000, d_loss: -100.23722839355469,  g_loss: 77.67955017089844\n",
            "Training epoch 10593/1000000, d_loss: -146.8687744140625,  g_loss: 151.60104370117188\n",
            "Training epoch 10594/1000000, d_loss: 251.2798309326172,  g_loss: 168.43515014648438\n",
            "Training epoch 10595/1000000, d_loss: -84.17704010009766,  g_loss: 163.98414611816406\n",
            "Training epoch 10596/1000000, d_loss: -102.71942138671875,  g_loss: 266.63653564453125\n",
            "Training epoch 10597/1000000, d_loss: -76.07606506347656,  g_loss: 190.74945068359375\n",
            "Training epoch 10598/1000000, d_loss: -105.76237487792969,  g_loss: 184.7799072265625\n",
            "Training epoch 10599/1000000, d_loss: -81.63056945800781,  g_loss: 213.47320556640625\n",
            "Training epoch 10600/1000000, d_loss: -69.45227813720703,  g_loss: 127.2025146484375\n",
            "Training epoch 10601/1000000, d_loss: -214.5330047607422,  g_loss: 161.12171936035156\n",
            "Training epoch 10602/1000000, d_loss: -62.8350715637207,  g_loss: 163.06993103027344\n",
            "Training epoch 10603/1000000, d_loss: -68.18853759765625,  g_loss: 233.17068481445312\n",
            "Training epoch 10604/1000000, d_loss: -35.64886474609375,  g_loss: 166.6678466796875\n",
            "Training epoch 10605/1000000, d_loss: -73.66847229003906,  g_loss: 215.81982421875\n",
            "Training epoch 10606/1000000, d_loss: -163.20928955078125,  g_loss: 364.4041748046875\n",
            "Training epoch 10607/1000000, d_loss: -35.373321533203125,  g_loss: 148.6485595703125\n",
            "Training epoch 10608/1000000, d_loss: -146.8558807373047,  g_loss: 144.49932861328125\n",
            "Training epoch 10609/1000000, d_loss: -94.73865509033203,  g_loss: 161.01300048828125\n",
            "Training epoch 10610/1000000, d_loss: -366.287353515625,  g_loss: 13.079957962036133\n",
            "Training epoch 10611/1000000, d_loss: -76.15656280517578,  g_loss: 68.51964569091797\n",
            "Training epoch 10612/1000000, d_loss: -144.21319580078125,  g_loss: 106.29100799560547\n",
            "Training epoch 10613/1000000, d_loss: -63.126380920410156,  g_loss: 190.75428771972656\n",
            "Training epoch 10614/1000000, d_loss: -113.51210021972656,  g_loss: 178.00387573242188\n",
            "Training epoch 10615/1000000, d_loss: -1441.06640625,  g_loss: -315.5387878417969\n",
            "Training epoch 10616/1000000, d_loss: 350.372314453125,  g_loss: 97.05912780761719\n",
            "Training epoch 10617/1000000, d_loss: 59.91827392578125,  g_loss: 127.99650573730469\n",
            "Training epoch 10618/1000000, d_loss: -45.845458984375,  g_loss: 113.95787811279297\n",
            "Training epoch 10619/1000000, d_loss: -113.17469787597656,  g_loss: 95.78988647460938\n",
            "Training epoch 10620/1000000, d_loss: -6.664112091064453,  g_loss: 97.28611755371094\n",
            "Training epoch 10621/1000000, d_loss: -126.00276184082031,  g_loss: 87.51896667480469\n",
            "Training epoch 10622/1000000, d_loss: -152.84542846679688,  g_loss: 113.65689086914062\n",
            "Training epoch 10623/1000000, d_loss: -258.3177185058594,  g_loss: 20.2872314453125\n",
            "Training epoch 10624/1000000, d_loss: -89.30596160888672,  g_loss: 100.82719421386719\n",
            "Training epoch 10625/1000000, d_loss: -13.672809600830078,  g_loss: -41.04014205932617\n",
            "Training epoch 10626/1000000, d_loss: -87.78939056396484,  g_loss: -31.026939392089844\n",
            "Training epoch 10627/1000000, d_loss: -66.8416748046875,  g_loss: 85.52376556396484\n",
            "Training epoch 10628/1000000, d_loss: -281.59814453125,  g_loss: 218.42852783203125\n",
            "Training epoch 10629/1000000, d_loss: -397.2406005859375,  g_loss: 267.61590576171875\n",
            "Training epoch 10630/1000000, d_loss: -152.24533081054688,  g_loss: 528.1612548828125\n",
            "Training epoch 10631/1000000, d_loss: -0.123016357421875,  g_loss: 149.18740844726562\n",
            "Training epoch 10632/1000000, d_loss: -130.4373779296875,  g_loss: 173.3296356201172\n",
            "Training epoch 10633/1000000, d_loss: -113.70478057861328,  g_loss: 150.96524047851562\n",
            "Training epoch 10634/1000000, d_loss: -94.63436126708984,  g_loss: 284.13714599609375\n",
            "Training epoch 10635/1000000, d_loss: -89.40373229980469,  g_loss: 153.87863159179688\n",
            "Training epoch 10636/1000000, d_loss: -113.21199798583984,  g_loss: 164.74380493164062\n",
            "Training epoch 10637/1000000, d_loss: -560.3115234375,  g_loss: -266.9803161621094\n",
            "Training epoch 10638/1000000, d_loss: 556.4171752929688,  g_loss: 65.4630355834961\n",
            "Training epoch 10639/1000000, d_loss: -31.02082633972168,  g_loss: 87.70763397216797\n",
            "Training epoch 10640/1000000, d_loss: -57.78862380981445,  g_loss: 131.7422637939453\n",
            "Training epoch 10641/1000000, d_loss: -17.82130241394043,  g_loss: 154.14273071289062\n",
            "Training epoch 10642/1000000, d_loss: -437.5263366699219,  g_loss: -15.395366668701172\n",
            "Training epoch 10643/1000000, d_loss: -116.48870849609375,  g_loss: 88.79203033447266\n",
            "Training epoch 10644/1000000, d_loss: 733.4357299804688,  g_loss: 174.736083984375\n",
            "Training epoch 10645/1000000, d_loss: -242.41648864746094,  g_loss: 137.16204833984375\n",
            "Training epoch 10646/1000000, d_loss: 106.70616149902344,  g_loss: 157.65643310546875\n",
            "Training epoch 10647/1000000, d_loss: -29.61319351196289,  g_loss: 137.59353637695312\n",
            "Training epoch 10648/1000000, d_loss: -247.09011840820312,  g_loss: 145.14437866210938\n",
            "Training epoch 10649/1000000, d_loss: 45.11951446533203,  g_loss: 164.98440551757812\n",
            "Training epoch 10650/1000000, d_loss: -124.65126037597656,  g_loss: 154.53530883789062\n",
            "Training epoch 10651/1000000, d_loss: -268.53485107421875,  g_loss: 174.58316040039062\n",
            "Training epoch 10652/1000000, d_loss: -136.3064422607422,  g_loss: 148.45762634277344\n",
            "Training epoch 10653/1000000, d_loss: -50.59199523925781,  g_loss: 218.6035614013672\n",
            "Training epoch 10654/1000000, d_loss: -177.587158203125,  g_loss: 195.87969970703125\n",
            "Training epoch 10655/1000000, d_loss: -141.66000366210938,  g_loss: 193.3606414794922\n",
            "Training epoch 10656/1000000, d_loss: 33.98173522949219,  g_loss: 193.2574462890625\n",
            "Training epoch 10657/1000000, d_loss: -163.98190307617188,  g_loss: 129.12417602539062\n",
            "Training epoch 10658/1000000, d_loss: -532.8407592773438,  g_loss: 81.45346069335938\n",
            "Training epoch 10659/1000000, d_loss: -265.3594055175781,  g_loss: -99.66118621826172\n",
            "Training epoch 10660/1000000, d_loss: -49.24713134765625,  g_loss: 300.99261474609375\n",
            "Training epoch 10661/1000000, d_loss: -258.37078857421875,  g_loss: 411.72064208984375\n",
            "Training epoch 10662/1000000, d_loss: -267.4151306152344,  g_loss: 243.2775115966797\n",
            "Training epoch 10663/1000000, d_loss: -284.49835205078125,  g_loss: 409.3401794433594\n",
            "Training epoch 10664/1000000, d_loss: -203.38172912597656,  g_loss: 173.06484985351562\n",
            "Training epoch 10665/1000000, d_loss: -148.87750244140625,  g_loss: 91.39436340332031\n",
            "Training epoch 10666/1000000, d_loss: -131.5029296875,  g_loss: 25.455665588378906\n",
            "Training epoch 10667/1000000, d_loss: 52.661415100097656,  g_loss: 108.26884460449219\n",
            "Training epoch 10668/1000000, d_loss: -11.134109497070312,  g_loss: 132.46560668945312\n",
            "Training epoch 10669/1000000, d_loss: -225.6871337890625,  g_loss: 101.23808288574219\n",
            "Training epoch 10670/1000000, d_loss: -176.39657592773438,  g_loss: 45.88968276977539\n",
            "Training epoch 10671/1000000, d_loss: -46.662200927734375,  g_loss: 191.24227905273438\n",
            "Training epoch 10672/1000000, d_loss: -66.18174743652344,  g_loss: 166.96083068847656\n",
            "Training epoch 10673/1000000, d_loss: 1.9525108337402344,  g_loss: 69.46204376220703\n",
            "Training epoch 10674/1000000, d_loss: -89.09506225585938,  g_loss: 96.38252258300781\n",
            "Training epoch 10675/1000000, d_loss: -134.41134643554688,  g_loss: 232.9812774658203\n",
            "Training epoch 10676/1000000, d_loss: -153.50070190429688,  g_loss: 118.13345336914062\n",
            "Training epoch 10677/1000000, d_loss: -199.96119689941406,  g_loss: 80.60662841796875\n",
            "Training epoch 10678/1000000, d_loss: -190.62400817871094,  g_loss: 27.145238876342773\n",
            "Training epoch 10679/1000000, d_loss: 2114.11962890625,  g_loss: 69.8160400390625\n",
            "Training epoch 10680/1000000, d_loss: -221.2254638671875,  g_loss: 35.15928649902344\n",
            "Training epoch 10681/1000000, d_loss: -140.39193725585938,  g_loss: 9.076188087463379\n",
            "Training epoch 10682/1000000, d_loss: 69.6842041015625,  g_loss: 25.767913818359375\n",
            "Training epoch 10683/1000000, d_loss: -19.032211303710938,  g_loss: 73.34131622314453\n",
            "Training epoch 10684/1000000, d_loss: -2763.88623046875,  g_loss: -30.048213958740234\n",
            "Training epoch 10685/1000000, d_loss: 50.287147521972656,  g_loss: 27.36048126220703\n",
            "Training epoch 10686/1000000, d_loss: 29.30712890625,  g_loss: 1.247640609741211\n",
            "Training epoch 10687/1000000, d_loss: -255.9495849609375,  g_loss: 22.115779876708984\n",
            "Training epoch 10688/1000000, d_loss: -35.79331588745117,  g_loss: 49.83380889892578\n",
            "Training epoch 10689/1000000, d_loss: -168.9956512451172,  g_loss: 81.79170227050781\n",
            "Training epoch 10690/1000000, d_loss: -28.19662094116211,  g_loss: 88.01834869384766\n",
            "Training epoch 10691/1000000, d_loss: -12.59222412109375,  g_loss: 109.34769439697266\n",
            "Training epoch 10692/1000000, d_loss: -102.52021026611328,  g_loss: 111.24177551269531\n",
            "Training epoch 10693/1000000, d_loss: -303.2674255371094,  g_loss: 373.25604248046875\n",
            "Training epoch 10694/1000000, d_loss: 34.829437255859375,  g_loss: -94.53108215332031\n",
            "Training epoch 10695/1000000, d_loss: -9.898260116577148,  g_loss: -86.28221893310547\n",
            "Training epoch 10696/1000000, d_loss: -25.139814376831055,  g_loss: -48.81025314331055\n",
            "Training epoch 10697/1000000, d_loss: -121.99324035644531,  g_loss: 13.56791877746582\n",
            "Training epoch 10698/1000000, d_loss: -55.97114562988281,  g_loss: 70.13063049316406\n",
            "Training epoch 10699/1000000, d_loss: -33.635562896728516,  g_loss: 40.98170852661133\n",
            "Training epoch 10700/1000000, d_loss: -25.81755828857422,  g_loss: 47.75709915161133\n",
            "Training epoch 10701/1000000, d_loss: -54.862735748291016,  g_loss: 31.14544677734375\n",
            "Training epoch 10702/1000000, d_loss: -142.52249145507812,  g_loss: 35.337188720703125\n",
            "Training epoch 10703/1000000, d_loss: -131.44357299804688,  g_loss: 35.96510696411133\n",
            "Training epoch 10704/1000000, d_loss: -161.03146362304688,  g_loss: 275.2962646484375\n",
            "Training epoch 10705/1000000, d_loss: -1598.0751953125,  g_loss: -107.94625091552734\n",
            "Training epoch 10706/1000000, d_loss: -15.249065399169922,  g_loss: 72.7200927734375\n",
            "Training epoch 10707/1000000, d_loss: -349.80084228515625,  g_loss: 41.1170654296875\n",
            "Training epoch 10708/1000000, d_loss: -72.10682678222656,  g_loss: 20.63141632080078\n",
            "Training epoch 10709/1000000, d_loss: -5.407722473144531,  g_loss: 49.264488220214844\n",
            "Training epoch 10710/1000000, d_loss: -56.36576843261719,  g_loss: 58.541683197021484\n",
            "Training epoch 10711/1000000, d_loss: -119.79264831542969,  g_loss: 71.25405883789062\n",
            "Training epoch 10712/1000000, d_loss: -152.46981811523438,  g_loss: 64.69808959960938\n",
            "Training epoch 10713/1000000, d_loss: -276.161865234375,  g_loss: -65.08322143554688\n",
            "Training epoch 10714/1000000, d_loss: -47.494564056396484,  g_loss: 70.75550842285156\n",
            "Training epoch 10715/1000000, d_loss: -116.45177459716797,  g_loss: 160.87088012695312\n",
            "Training epoch 10716/1000000, d_loss: -53.386802673339844,  g_loss: 108.74380493164062\n",
            "Training epoch 10717/1000000, d_loss: -431.119873046875,  g_loss: 14.645711898803711\n",
            "Training epoch 10718/1000000, d_loss: -1.1803512573242188,  g_loss: 133.71987915039062\n",
            "Training epoch 10719/1000000, d_loss: -65.21581268310547,  g_loss: 146.6710205078125\n",
            "Training epoch 10720/1000000, d_loss: 18.027862548828125,  g_loss: 101.53557586669922\n",
            "Training epoch 10721/1000000, d_loss: -75.76508331298828,  g_loss: 150.85885620117188\n",
            "Training epoch 10722/1000000, d_loss: -145.3419189453125,  g_loss: 112.85051727294922\n",
            "Training epoch 10723/1000000, d_loss: -301.18853759765625,  g_loss: 26.789770126342773\n",
            "Training epoch 10724/1000000, d_loss: -75.51575469970703,  g_loss: 50.51453399658203\n",
            "Training epoch 10725/1000000, d_loss: 15.328102111816406,  g_loss: 100.06520080566406\n",
            "Training epoch 10726/1000000, d_loss: -773.71337890625,  g_loss: -89.94560241699219\n",
            "Training epoch 10727/1000000, d_loss: 163.53338623046875,  g_loss: 30.095849990844727\n",
            "Training epoch 10728/1000000, d_loss: -126.91182708740234,  g_loss: 71.66802978515625\n",
            "Training epoch 10729/1000000, d_loss: -151.26014709472656,  g_loss: 171.87197875976562\n",
            "Training epoch 10730/1000000, d_loss: -972.9310302734375,  g_loss: -35.0740966796875\n",
            "Training epoch 10731/1000000, d_loss: -420.9478454589844,  g_loss: -282.5207214355469\n",
            "Training epoch 10732/1000000, d_loss: 356.40167236328125,  g_loss: 87.12333679199219\n",
            "Training epoch 10733/1000000, d_loss: -119.88052368164062,  g_loss: 78.170166015625\n",
            "Training epoch 10734/1000000, d_loss: -147.01214599609375,  g_loss: 62.987735748291016\n",
            "Training epoch 10735/1000000, d_loss: -138.69630432128906,  g_loss: 82.42092895507812\n",
            "Training epoch 10736/1000000, d_loss: -39.057411193847656,  g_loss: 162.37265014648438\n",
            "Training epoch 10737/1000000, d_loss: -137.525390625,  g_loss: 110.60220336914062\n",
            "Training epoch 10738/1000000, d_loss: -79.513671875,  g_loss: 37.61419677734375\n",
            "Training epoch 10739/1000000, d_loss: -62.04400634765625,  g_loss: 7.266558647155762\n",
            "Training epoch 10740/1000000, d_loss: -16.840145111083984,  g_loss: -36.222557067871094\n",
            "Training epoch 10741/1000000, d_loss: -118.81874084472656,  g_loss: 132.20782470703125\n",
            "Training epoch 10742/1000000, d_loss: -33.31085968017578,  g_loss: 17.75366973876953\n",
            "Training epoch 10743/1000000, d_loss: -66.97148132324219,  g_loss: 42.332515716552734\n",
            "Training epoch 10744/1000000, d_loss: -327.0207824707031,  g_loss: -18.258636474609375\n",
            "Training epoch 10745/1000000, d_loss: -297.201904296875,  g_loss: -54.6329231262207\n",
            "Training epoch 10746/1000000, d_loss: -21.755311965942383,  g_loss: 16.44402313232422\n",
            "Training epoch 10747/1000000, d_loss: -442.18524169921875,  g_loss: -65.92233276367188\n",
            "Training epoch 10748/1000000, d_loss: 100.09319305419922,  g_loss: 114.98241424560547\n",
            "Training epoch 10749/1000000, d_loss: -29.255878448486328,  g_loss: 93.23468017578125\n",
            "Training epoch 10750/1000000, d_loss: -116.4490966796875,  g_loss: 76.1248779296875\n",
            "Training epoch 10751/1000000, d_loss: -81.41342163085938,  g_loss: 69.58047485351562\n",
            "Training epoch 10752/1000000, d_loss: -359.3121337890625,  g_loss: 42.56385803222656\n",
            "Training epoch 10753/1000000, d_loss: -139.97816467285156,  g_loss: 148.00936889648438\n",
            "Training epoch 10754/1000000, d_loss: 32.038597106933594,  g_loss: 147.37669372558594\n",
            "Training epoch 10755/1000000, d_loss: -275.0010681152344,  g_loss: 24.28369140625\n",
            "Training epoch 10756/1000000, d_loss: -102.08610534667969,  g_loss: 46.64832305908203\n",
            "Training epoch 10757/1000000, d_loss: -98.50552368164062,  g_loss: 79.58429718017578\n",
            "Training epoch 10758/1000000, d_loss: -196.10626220703125,  g_loss: -1.4450969696044922\n",
            "Training epoch 10759/1000000, d_loss: -117.90483093261719,  g_loss: 21.535512924194336\n",
            "Training epoch 10760/1000000, d_loss: -58.74435043334961,  g_loss: 77.40129089355469\n",
            "Training epoch 10761/1000000, d_loss: -38.45046615600586,  g_loss: 62.15813064575195\n",
            "Training epoch 10762/1000000, d_loss: -31.039766311645508,  g_loss: 71.07125854492188\n",
            "Training epoch 10763/1000000, d_loss: -98.4015121459961,  g_loss: 84.65106201171875\n",
            "Training epoch 10764/1000000, d_loss: 8.207645416259766,  g_loss: -21.27567481994629\n",
            "Training epoch 10765/1000000, d_loss: -419.1845397949219,  g_loss: -42.90138626098633\n",
            "Training epoch 10766/1000000, d_loss: -116.64698791503906,  g_loss: -3.688183307647705\n",
            "Training epoch 10767/1000000, d_loss: -121.49703216552734,  g_loss: 39.99102020263672\n",
            "Training epoch 10768/1000000, d_loss: -86.72383880615234,  g_loss: 138.30618286132812\n",
            "Training epoch 10769/1000000, d_loss: -67.43099975585938,  g_loss: 122.38407897949219\n",
            "Training epoch 10770/1000000, d_loss: -142.26327514648438,  g_loss: 44.55342483520508\n",
            "Training epoch 10771/1000000, d_loss: -222.87973022460938,  g_loss: 124.11262512207031\n",
            "Training epoch 10772/1000000, d_loss: -59.94887161254883,  g_loss: 29.33715057373047\n",
            "Training epoch 10773/1000000, d_loss: -160.03118896484375,  g_loss: -23.468658447265625\n",
            "Training epoch 10774/1000000, d_loss: -15.239303588867188,  g_loss: 83.65750122070312\n",
            "Training epoch 10775/1000000, d_loss: -81.09742736816406,  g_loss: 68.07135009765625\n",
            "Training epoch 10776/1000000, d_loss: -6.3400115966796875,  g_loss: 66.82960510253906\n",
            "Training epoch 10777/1000000, d_loss: -99.27669525146484,  g_loss: 84.38304138183594\n",
            "Training epoch 10778/1000000, d_loss: -28.983474731445312,  g_loss: -7.42573881149292\n",
            "Training epoch 10779/1000000, d_loss: -45.20425796508789,  g_loss: -4.763007164001465\n",
            "Training epoch 10780/1000000, d_loss: 10.213066101074219,  g_loss: 15.521140098571777\n",
            "Training epoch 10781/1000000, d_loss: -94.58616638183594,  g_loss: 50.14629364013672\n",
            "Training epoch 10782/1000000, d_loss: -95.83856201171875,  g_loss: 43.05784225463867\n",
            "Training epoch 10783/1000000, d_loss: -87.54037475585938,  g_loss: 68.27992248535156\n",
            "Training epoch 10784/1000000, d_loss: -110.36849212646484,  g_loss: 260.8726501464844\n",
            "Training epoch 10785/1000000, d_loss: -46.700279235839844,  g_loss: 73.15283203125\n",
            "Training epoch 10786/1000000, d_loss: -37.60028839111328,  g_loss: 49.1963996887207\n",
            "Training epoch 10787/1000000, d_loss: -37.09136962890625,  g_loss: 37.336334228515625\n",
            "Training epoch 10788/1000000, d_loss: -523.8695678710938,  g_loss: -46.723548889160156\n",
            "Training epoch 10789/1000000, d_loss: -366.4173278808594,  g_loss: -53.1157112121582\n",
            "Training epoch 10790/1000000, d_loss: -645.4462890625,  g_loss: -248.17434692382812\n",
            "Training epoch 10791/1000000, d_loss: -74.84788513183594,  g_loss: 7.519158363342285\n",
            "Training epoch 10792/1000000, d_loss: -8.451658248901367,  g_loss: -21.474380493164062\n",
            "Training epoch 10793/1000000, d_loss: -324.96038818359375,  g_loss: -230.46505737304688\n",
            "Training epoch 10794/1000000, d_loss: -1023.084228515625,  g_loss: -203.74172973632812\n",
            "Training epoch 10795/1000000, d_loss: 440.51373291015625,  g_loss: -82.29312133789062\n",
            "Training epoch 10796/1000000, d_loss: -25.057861328125,  g_loss: -97.55113220214844\n",
            "Training epoch 10797/1000000, d_loss: -90.54179382324219,  g_loss: 15.9371337890625\n",
            "Training epoch 10798/1000000, d_loss: -91.91056823730469,  g_loss: 143.8323211669922\n",
            "Training epoch 10799/1000000, d_loss: -129.36839294433594,  g_loss: 53.65664291381836\n",
            "Training epoch 10800/1000000, d_loss: 31.879653930664062,  g_loss: -55.0601921081543\n",
            "Training epoch 10801/1000000, d_loss: -74.31623840332031,  g_loss: -112.52328491210938\n",
            "Training epoch 10802/1000000, d_loss: 262.2281799316406,  g_loss: 33.56114959716797\n",
            "Training epoch 10803/1000000, d_loss: 6.7161712646484375,  g_loss: -24.134056091308594\n",
            "Training epoch 10804/1000000, d_loss: -22.807209014892578,  g_loss: -57.680213928222656\n",
            "Training epoch 10805/1000000, d_loss: -65.73554229736328,  g_loss: -41.73506546020508\n",
            "Training epoch 10806/1000000, d_loss: -117.38180541992188,  g_loss: -2.098036766052246\n",
            "Training epoch 10807/1000000, d_loss: -122.62692260742188,  g_loss: 7.316154956817627\n",
            "Training epoch 10808/1000000, d_loss: -39.949764251708984,  g_loss: -32.28571319580078\n",
            "Training epoch 10809/1000000, d_loss: -146.52520751953125,  g_loss: 26.63669776916504\n",
            "Training epoch 10810/1000000, d_loss: -257.8206787109375,  g_loss: -76.08494567871094\n",
            "Training epoch 10811/1000000, d_loss: -78.32756805419922,  g_loss: -69.21427917480469\n",
            "Training epoch 10812/1000000, d_loss: -113.35310363769531,  g_loss: -6.215333461761475\n",
            "Training epoch 10813/1000000, d_loss: -298.9853210449219,  g_loss: -46.8376350402832\n",
            "Training epoch 10814/1000000, d_loss: -22.852876663208008,  g_loss: 68.19774627685547\n",
            "Training epoch 10815/1000000, d_loss: -139.091064453125,  g_loss: 38.55523681640625\n",
            "Training epoch 10816/1000000, d_loss: -189.55422973632812,  g_loss: -36.21923828125\n",
            "Training epoch 10817/1000000, d_loss: 0.2981834411621094,  g_loss: 51.66844940185547\n",
            "Training epoch 10818/1000000, d_loss: -93.4542236328125,  g_loss: 33.10334396362305\n",
            "Training epoch 10819/1000000, d_loss: -177.44979858398438,  g_loss: 17.094884872436523\n",
            "Training epoch 10820/1000000, d_loss: -51.705665588378906,  g_loss: 0.9242050647735596\n",
            "Training epoch 10821/1000000, d_loss: -96.74678039550781,  g_loss: -13.121145248413086\n",
            "Training epoch 10822/1000000, d_loss: -106.41339111328125,  g_loss: 24.830860137939453\n",
            "Training epoch 10823/1000000, d_loss: -112.76490783691406,  g_loss: 108.25668334960938\n",
            "Training epoch 10824/1000000, d_loss: -885.3492431640625,  g_loss: -144.13864135742188\n",
            "Training epoch 10825/1000000, d_loss: 8446.73828125,  g_loss: -33.01985549926758\n",
            "Training epoch 10826/1000000, d_loss: -300.32818603515625,  g_loss: -60.287742614746094\n",
            "Training epoch 10827/1000000, d_loss: -242.873779296875,  g_loss: -13.327278137207031\n",
            "Training epoch 10828/1000000, d_loss: 161.0093994140625,  g_loss: -73.45658874511719\n",
            "Training epoch 10829/1000000, d_loss: 20.52031707763672,  g_loss: -62.641693115234375\n",
            "Training epoch 10830/1000000, d_loss: -51.652156829833984,  g_loss: -118.3377685546875\n",
            "Training epoch 10831/1000000, d_loss: -47.37139129638672,  g_loss: -88.32417297363281\n",
            "Training epoch 10832/1000000, d_loss: -31.60378646850586,  g_loss: -40.65315246582031\n",
            "Training epoch 10833/1000000, d_loss: -101.54999542236328,  g_loss: 20.16461181640625\n",
            "Training epoch 10834/1000000, d_loss: -97.68006896972656,  g_loss: 6.971680641174316\n",
            "Training epoch 10835/1000000, d_loss: -48.91517639160156,  g_loss: -16.970996856689453\n",
            "Training epoch 10836/1000000, d_loss: -0.7364654541015625,  g_loss: -21.985092163085938\n",
            "Training epoch 10837/1000000, d_loss: -188.18026733398438,  g_loss: -64.85842895507812\n",
            "Training epoch 10838/1000000, d_loss: -76.41494750976562,  g_loss: -15.078990936279297\n",
            "Training epoch 10839/1000000, d_loss: -106.76838684082031,  g_loss: -227.35720825195312\n",
            "Training epoch 10840/1000000, d_loss: -108.2883071899414,  g_loss: -107.31008911132812\n",
            "Training epoch 10841/1000000, d_loss: -134.18655395507812,  g_loss: -100.43708801269531\n",
            "Training epoch 10842/1000000, d_loss: -388.5855712890625,  g_loss: -103.08946228027344\n",
            "Training epoch 10843/1000000, d_loss: -1363.89013671875,  g_loss: -622.5071411132812\n",
            "Training epoch 10844/1000000, d_loss: -293.25604248046875,  g_loss: -96.3141860961914\n",
            "Training epoch 10845/1000000, d_loss: 10.755805969238281,  g_loss: 155.99972534179688\n",
            "Training epoch 10846/1000000, d_loss: -62.39178466796875,  g_loss: 173.3106689453125\n",
            "Training epoch 10847/1000000, d_loss: -194.481201171875,  g_loss: 8.394268035888672\n",
            "Training epoch 10848/1000000, d_loss: -45.87193298339844,  g_loss: 61.347808837890625\n",
            "Training epoch 10849/1000000, d_loss: 195.70803833007812,  g_loss: 107.83404541015625\n",
            "Training epoch 10850/1000000, d_loss: -184.82583618164062,  g_loss: 139.9486083984375\n",
            "Training epoch 10851/1000000, d_loss: -749.7974853515625,  g_loss: -168.5933837890625\n",
            "Training epoch 10852/1000000, d_loss: -339.7966613769531,  g_loss: -196.9052734375\n",
            "Training epoch 10853/1000000, d_loss: -69.42782592773438,  g_loss: -6.944073677062988\n",
            "Training epoch 10854/1000000, d_loss: -155.98770141601562,  g_loss: -55.83757781982422\n",
            "Training epoch 10855/1000000, d_loss: 104.86048889160156,  g_loss: 43.173648834228516\n",
            "Training epoch 10856/1000000, d_loss: -70.21641540527344,  g_loss: -18.98904037475586\n",
            "Training epoch 10857/1000000, d_loss: -178.25930786132812,  g_loss: 236.51080322265625\n",
            "Training epoch 10858/1000000, d_loss: -70.52853393554688,  g_loss: 437.9983825683594\n",
            "Training epoch 10859/1000000, d_loss: -69.5832290649414,  g_loss: -4.87270450592041\n",
            "Training epoch 10860/1000000, d_loss: -29.845245361328125,  g_loss: 62.134525299072266\n",
            "Training epoch 10861/1000000, d_loss: -585.4444580078125,  g_loss: -55.21735763549805\n",
            "Training epoch 10862/1000000, d_loss: -70.56730651855469,  g_loss: 1.84608793258667\n",
            "Training epoch 10863/1000000, d_loss: 48.69853973388672,  g_loss: -18.239566802978516\n",
            "Training epoch 10864/1000000, d_loss: -107.21643829345703,  g_loss: 17.707975387573242\n",
            "Training epoch 10865/1000000, d_loss: -125.9278335571289,  g_loss: 59.25825500488281\n",
            "Training epoch 10866/1000000, d_loss: -265.4020690917969,  g_loss: 374.5572204589844\n",
            "Training epoch 10867/1000000, d_loss: -49.674598693847656,  g_loss: -46.93533706665039\n",
            "Training epoch 10868/1000000, d_loss: -13.293270111083984,  g_loss: -137.0126953125\n",
            "Training epoch 10869/1000000, d_loss: -76.24610900878906,  g_loss: -38.66417694091797\n",
            "Training epoch 10870/1000000, d_loss: -164.03787231445312,  g_loss: -43.690818786621094\n",
            "Training epoch 10871/1000000, d_loss: -753.6386108398438,  g_loss: -291.0317077636719\n",
            "Training epoch 10872/1000000, d_loss: 13.4271240234375,  g_loss: -5.195269584655762\n",
            "Training epoch 10873/1000000, d_loss: -787.3468017578125,  g_loss: -207.02719116210938\n",
            "Training epoch 10874/1000000, d_loss: -1231.7794189453125,  g_loss: -1393.209228515625\n",
            "Training epoch 10875/1000000, d_loss: 429.051513671875,  g_loss: -536.1875\n",
            "Training epoch 10876/1000000, d_loss: -292.75445556640625,  g_loss: -219.18284606933594\n",
            "Training epoch 10877/1000000, d_loss: -276.035888671875,  g_loss: 84.98738861083984\n",
            "Training epoch 10878/1000000, d_loss: -314.9748229980469,  g_loss: 404.8216552734375\n",
            "Training epoch 10879/1000000, d_loss: -43.10157775878906,  g_loss: -9.308591842651367\n",
            "Training epoch 10880/1000000, d_loss: -37.708656311035156,  g_loss: 43.19340515136719\n",
            "Training epoch 10881/1000000, d_loss: 29.10576629638672,  g_loss: 28.170427322387695\n",
            "Training epoch 10882/1000000, d_loss: -522.1823120117188,  g_loss: 502.6454162597656\n",
            "Training epoch 10883/1000000, d_loss: -185.07669067382812,  g_loss: 148.65606689453125\n",
            "Training epoch 10884/1000000, d_loss: -199.28338623046875,  g_loss: 248.1934356689453\n",
            "Training epoch 10885/1000000, d_loss: -240.19427490234375,  g_loss: -31.824941635131836\n",
            "Training epoch 10886/1000000, d_loss: -300.77337646484375,  g_loss: 488.64874267578125\n",
            "Training epoch 10887/1000000, d_loss: -278.96026611328125,  g_loss: 645.1162719726562\n",
            "Training epoch 10888/1000000, d_loss: -271.8048400878906,  g_loss: 683.8432006835938\n",
            "Training epoch 10889/1000000, d_loss: -91.04959106445312,  g_loss: 69.93742370605469\n",
            "Training epoch 10890/1000000, d_loss: -815.626708984375,  g_loss: -10.404147148132324\n",
            "Training epoch 10891/1000000, d_loss: -1858.478515625,  g_loss: -346.2957458496094\n",
            "Training epoch 10892/1000000, d_loss: 575.0194091796875,  g_loss: -13.670358657836914\n",
            "Training epoch 10893/1000000, d_loss: -922.77783203125,  g_loss: -656.6072998046875\n",
            "Training epoch 10894/1000000, d_loss: 313.6146545410156,  g_loss: -400.8747253417969\n",
            "Training epoch 10895/1000000, d_loss: -234.91929626464844,  g_loss: -1.0645713806152344\n",
            "Training epoch 10896/1000000, d_loss: 803.5485229492188,  g_loss: -207.66159057617188\n",
            "Training epoch 10897/1000000, d_loss: 157.1458282470703,  g_loss: 68.66741943359375\n",
            "Training epoch 10898/1000000, d_loss: -442.29913330078125,  g_loss: 695.4884033203125\n",
            "Training epoch 10899/1000000, d_loss: 207.8114013671875,  g_loss: 293.3899230957031\n",
            "Training epoch 10900/1000000, d_loss: -141.5719451904297,  g_loss: -79.48748016357422\n",
            "Training epoch 10901/1000000, d_loss: 48.81022644042969,  g_loss: -214.49307250976562\n",
            "Training epoch 10902/1000000, d_loss: -126.68048095703125,  g_loss: -27.03236961364746\n",
            "Training epoch 10903/1000000, d_loss: -72.84442138671875,  g_loss: 157.40115356445312\n",
            "Training epoch 10904/1000000, d_loss: -127.60723876953125,  g_loss: 81.27980041503906\n",
            "Training epoch 10905/1000000, d_loss: -177.67910766601562,  g_loss: 93.95079040527344\n",
            "Training epoch 10906/1000000, d_loss: -57.13563919067383,  g_loss: -25.45392417907715\n",
            "Training epoch 10907/1000000, d_loss: -133.1580352783203,  g_loss: -40.02071762084961\n",
            "Training epoch 10908/1000000, d_loss: -61.64942932128906,  g_loss: 83.52257537841797\n",
            "Training epoch 10909/1000000, d_loss: 4.74542236328125,  g_loss: -40.174896240234375\n",
            "Training epoch 10910/1000000, d_loss: -33.883575439453125,  g_loss: -32.64620590209961\n",
            "Training epoch 10911/1000000, d_loss: -89.66693115234375,  g_loss: 11.660578727722168\n",
            "Training epoch 10912/1000000, d_loss: 1632.0030517578125,  g_loss: 29.619712829589844\n",
            "Training epoch 10913/1000000, d_loss: -122.6260986328125,  g_loss: 97.16454315185547\n",
            "Training epoch 10914/1000000, d_loss: -0.85455322265625,  g_loss: 24.400558471679688\n",
            "Training epoch 10915/1000000, d_loss: -24.710159301757812,  g_loss: 1.6496162414550781\n",
            "Training epoch 10916/1000000, d_loss: -49.921295166015625,  g_loss: -122.96879577636719\n",
            "Training epoch 10917/1000000, d_loss: -4.079057693481445,  g_loss: -35.0062255859375\n",
            "Training epoch 10918/1000000, d_loss: -94.11912536621094,  g_loss: -30.423542022705078\n",
            "Training epoch 10919/1000000, d_loss: -143.47747802734375,  g_loss: -48.68418884277344\n",
            "Training epoch 10920/1000000, d_loss: -107.28775024414062,  g_loss: -31.630292892456055\n",
            "Training epoch 10921/1000000, d_loss: -186.15625,  g_loss: -64.84925842285156\n",
            "Training epoch 10922/1000000, d_loss: -74.521484375,  g_loss: 10.333080291748047\n",
            "Training epoch 10923/1000000, d_loss: -33.436004638671875,  g_loss: 22.312110900878906\n",
            "Training epoch 10924/1000000, d_loss: -203.6556854248047,  g_loss: -46.05815124511719\n",
            "Training epoch 10925/1000000, d_loss: -216.1079559326172,  g_loss: -173.47061157226562\n",
            "Training epoch 10926/1000000, d_loss: -17.380638122558594,  g_loss: 124.14605712890625\n",
            "Training epoch 10927/1000000, d_loss: -66.99868774414062,  g_loss: 123.39716339111328\n",
            "Training epoch 10928/1000000, d_loss: -156.01339721679688,  g_loss: 225.05987548828125\n",
            "Training epoch 10929/1000000, d_loss: -97.61882019042969,  g_loss: 47.41358184814453\n",
            "Training epoch 10930/1000000, d_loss: -60.080135345458984,  g_loss: 79.63936614990234\n",
            "Training epoch 10931/1000000, d_loss: -94.59246063232422,  g_loss: 108.96179962158203\n",
            "Training epoch 10932/1000000, d_loss: -129.48855590820312,  g_loss: 73.1572265625\n",
            "Training epoch 10933/1000000, d_loss: -2177.97265625,  g_loss: -162.71072387695312\n",
            "Training epoch 10934/1000000, d_loss: -1052.7657470703125,  g_loss: -182.1806640625\n",
            "Training epoch 10935/1000000, d_loss: 90.05646514892578,  g_loss: -65.26277160644531\n",
            "Training epoch 10936/1000000, d_loss: 3.29803466796875,  g_loss: 74.63471984863281\n",
            "Training epoch 10937/1000000, d_loss: -350.88665771484375,  g_loss: -50.44007110595703\n",
            "Training epoch 10938/1000000, d_loss: -44.744956970214844,  g_loss: 10.134406089782715\n",
            "Training epoch 10939/1000000, d_loss: -429.07427978515625,  g_loss: -51.563724517822266\n",
            "Training epoch 10940/1000000, d_loss: 879.0450439453125,  g_loss: 74.26129150390625\n",
            "Training epoch 10941/1000000, d_loss: -71.24263763427734,  g_loss: 40.85460662841797\n",
            "Training epoch 10942/1000000, d_loss: -182.04698181152344,  g_loss: 192.48487854003906\n",
            "Training epoch 10943/1000000, d_loss: -110.95902252197266,  g_loss: 145.5289306640625\n",
            "Training epoch 10944/1000000, d_loss: -193.26055908203125,  g_loss: 202.41928100585938\n",
            "Training epoch 10945/1000000, d_loss: 64.79620361328125,  g_loss: 43.601409912109375\n",
            "Training epoch 10946/1000000, d_loss: -604.85205078125,  g_loss: 23.5671329498291\n",
            "Training epoch 10947/1000000, d_loss: 1531.419189453125,  g_loss: 42.52650833129883\n",
            "Training epoch 10948/1000000, d_loss: 548.973388671875,  g_loss: 62.604366302490234\n",
            "Training epoch 10949/1000000, d_loss: -40.22903060913086,  g_loss: 6.795720100402832\n",
            "Training epoch 10950/1000000, d_loss: -133.31234741210938,  g_loss: 120.58802032470703\n",
            "Training epoch 10951/1000000, d_loss: -110.93476867675781,  g_loss: 42.824771881103516\n",
            "Training epoch 10952/1000000, d_loss: -114.1126480102539,  g_loss: 129.17214965820312\n",
            "Training epoch 10953/1000000, d_loss: -119.80412292480469,  g_loss: 33.94288635253906\n",
            "Training epoch 10954/1000000, d_loss: -589.0858154296875,  g_loss: -102.80156707763672\n",
            "Training epoch 10955/1000000, d_loss: -90.50698852539062,  g_loss: -42.18509292602539\n",
            "Training epoch 10956/1000000, d_loss: -197.9302978515625,  g_loss: -58.653480529785156\n",
            "Training epoch 10957/1000000, d_loss: 127.98101043701172,  g_loss: 20.428333282470703\n",
            "Training epoch 10958/1000000, d_loss: -59.77410125732422,  g_loss: 39.23438262939453\n",
            "Training epoch 10959/1000000, d_loss: -110.65716552734375,  g_loss: -4.632767677307129\n",
            "Training epoch 10960/1000000, d_loss: -150.0233154296875,  g_loss: 40.305362701416016\n",
            "Training epoch 10961/1000000, d_loss: -158.9971466064453,  g_loss: 31.775304794311523\n",
            "Training epoch 10962/1000000, d_loss: -224.27197265625,  g_loss: 11.548236846923828\n",
            "Training epoch 10963/1000000, d_loss: -156.205322265625,  g_loss: 90.3674087524414\n",
            "Training epoch 10964/1000000, d_loss: -62.7856330871582,  g_loss: 29.141704559326172\n",
            "Training epoch 10965/1000000, d_loss: -1597.5247802734375,  g_loss: -255.4370880126953\n",
            "Training epoch 10966/1000000, d_loss: 6696.1640625,  g_loss: 14.311067581176758\n",
            "Training epoch 10967/1000000, d_loss: 214.1929473876953,  g_loss: -1.212770938873291\n",
            "Training epoch 10968/1000000, d_loss: -31.59307861328125,  g_loss: 31.349056243896484\n",
            "Training epoch 10969/1000000, d_loss: 0.4399375915527344,  g_loss: 24.198862075805664\n",
            "Training epoch 10970/1000000, d_loss: -92.1507568359375,  g_loss: 18.710872650146484\n",
            "Training epoch 10971/1000000, d_loss: -77.98406982421875,  g_loss: -9.12905502319336\n",
            "Training epoch 10972/1000000, d_loss: -217.71875,  g_loss: -21.70513343811035\n",
            "Training epoch 10973/1000000, d_loss: -298.5318603515625,  g_loss: 7.252400875091553\n",
            "Training epoch 10974/1000000, d_loss: 38.05824279785156,  g_loss: -18.397390365600586\n",
            "Training epoch 10975/1000000, d_loss: -369.23858642578125,  g_loss: 288.8045959472656\n",
            "Training epoch 10976/1000000, d_loss: -86.16141510009766,  g_loss: 77.60979461669922\n",
            "Training epoch 10977/1000000, d_loss: -23.108718872070312,  g_loss: 80.25960540771484\n",
            "Training epoch 10978/1000000, d_loss: -70.31208038330078,  g_loss: 121.76937103271484\n",
            "Training epoch 10979/1000000, d_loss: -339.36322021484375,  g_loss: 47.52854919433594\n",
            "Training epoch 10980/1000000, d_loss: -624.4500732421875,  g_loss: 12.420487403869629\n",
            "Training epoch 10981/1000000, d_loss: -26.45566177368164,  g_loss: 40.178863525390625\n",
            "Training epoch 10982/1000000, d_loss: -61.33497619628906,  g_loss: 0.18628406524658203\n",
            "Training epoch 10983/1000000, d_loss: -80.5484390258789,  g_loss: 110.89507293701172\n",
            "Training epoch 10984/1000000, d_loss: -102.88484191894531,  g_loss: 42.860870361328125\n",
            "Training epoch 10985/1000000, d_loss: -63.43851089477539,  g_loss: 5.834427356719971\n",
            "Training epoch 10986/1000000, d_loss: -112.63885498046875,  g_loss: -8.59084701538086\n",
            "Training epoch 10987/1000000, d_loss: -73.16812896728516,  g_loss: -46.07596969604492\n",
            "Training epoch 10988/1000000, d_loss: -152.3497772216797,  g_loss: -23.261871337890625\n",
            "Training epoch 10989/1000000, d_loss: -748.0376586914062,  g_loss: -133.3402862548828\n",
            "Training epoch 10990/1000000, d_loss: -302.3071594238281,  g_loss: -30.731239318847656\n",
            "Training epoch 10991/1000000, d_loss: -191.32028198242188,  g_loss: -88.36905670166016\n",
            "Training epoch 10992/1000000, d_loss: -2.5056838989257812,  g_loss: 14.064165115356445\n",
            "Training epoch 10993/1000000, d_loss: -137.17816162109375,  g_loss: 159.3295135498047\n",
            "Training epoch 10994/1000000, d_loss: -129.60174560546875,  g_loss: 88.54346466064453\n",
            "Training epoch 10995/1000000, d_loss: -506.0776062011719,  g_loss: -28.90290069580078\n",
            "Training epoch 10996/1000000, d_loss: -113.87747192382812,  g_loss: 25.410778045654297\n",
            "Training epoch 10997/1000000, d_loss: 7.886920928955078,  g_loss: 42.282127380371094\n",
            "Training epoch 10998/1000000, d_loss: -260.608154296875,  g_loss: -38.23270034790039\n",
            "Training epoch 10999/1000000, d_loss: -143.24989318847656,  g_loss: 48.40596008300781\n",
            "Training epoch 11000/1000000, d_loss: -79.09931182861328,  g_loss: 17.18307113647461\n",
            "Training epoch 11001/1000000, d_loss: -28.468307495117188,  g_loss: 78.96861267089844\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 26ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 64/64 [00:00<00:00, 131.81it/s]\n",
            "Meshing: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not create voxel model... Continuing training\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_11001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_11001/assets\n",
            "Training epoch 11002/1000000, d_loss: -2295.640380859375,  g_loss: -168.81564331054688\n",
            "Training epoch 11003/1000000, d_loss: 214.57676696777344,  g_loss: -9.764827728271484\n",
            "Training epoch 11004/1000000, d_loss: -23.06221580505371,  g_loss: 48.67669677734375\n",
            "Training epoch 11005/1000000, d_loss: -82.24299621582031,  g_loss: 251.58033752441406\n",
            "Training epoch 11006/1000000, d_loss: -195.81375122070312,  g_loss: 208.88558959960938\n",
            "Training epoch 11007/1000000, d_loss: -121.48136901855469,  g_loss: 64.47563171386719\n",
            "Training epoch 11008/1000000, d_loss: 39.52727508544922,  g_loss: -3.714430332183838\n",
            "Training epoch 11009/1000000, d_loss: -955.1123657226562,  g_loss: -79.61006164550781\n",
            "Training epoch 11010/1000000, d_loss: 0.4130096435546875,  g_loss: 91.88841247558594\n",
            "Training epoch 11011/1000000, d_loss: -527.38427734375,  g_loss: -334.7803039550781\n",
            "Training epoch 11012/1000000, d_loss: 55.19132614135742,  g_loss: -73.22954559326172\n",
            "Training epoch 11013/1000000, d_loss: 380.359130859375,  g_loss: 14.478426933288574\n",
            "Training epoch 11014/1000000, d_loss: -123.9148941040039,  g_loss: -86.65721893310547\n",
            "Training epoch 11015/1000000, d_loss: -1275.0823974609375,  g_loss: -77.98078918457031\n",
            "Training epoch 11016/1000000, d_loss: 20624.3984375,  g_loss: 29.5284481048584\n",
            "Training epoch 11017/1000000, d_loss: -50.960662841796875,  g_loss: 90.6708984375\n",
            "Training epoch 11018/1000000, d_loss: -42.183013916015625,  g_loss: 73.93498229980469\n",
            "Training epoch 11019/1000000, d_loss: -146.08680725097656,  g_loss: 148.05471801757812\n",
            "Training epoch 11020/1000000, d_loss: -67.78955841064453,  g_loss: 64.60673522949219\n",
            "Training epoch 11021/1000000, d_loss: -23.912818908691406,  g_loss: 112.29541778564453\n",
            "Training epoch 11022/1000000, d_loss: -210.2751007080078,  g_loss: 243.3458709716797\n",
            "Training epoch 11023/1000000, d_loss: -127.03965759277344,  g_loss: 182.5673828125\n",
            "Training epoch 11024/1000000, d_loss: -65.63639831542969,  g_loss: 101.85179901123047\n",
            "Training epoch 11025/1000000, d_loss: 16.69222068786621,  g_loss: 69.99446868896484\n",
            "Training epoch 11026/1000000, d_loss: -66.26045227050781,  g_loss: 31.545734405517578\n",
            "Training epoch 11027/1000000, d_loss: -63.35813903808594,  g_loss: 177.87310791015625\n",
            "Training epoch 11028/1000000, d_loss: -96.00398254394531,  g_loss: 28.7742919921875\n",
            "Training epoch 11029/1000000, d_loss: -49.54420852661133,  g_loss: 81.9938735961914\n",
            "Training epoch 11030/1000000, d_loss: -193.81048583984375,  g_loss: 52.82693862915039\n",
            "Training epoch 11031/1000000, d_loss: -112.08489990234375,  g_loss: -17.30248260498047\n",
            "Training epoch 11032/1000000, d_loss: -150.24658203125,  g_loss: 9.21323299407959\n",
            "Training epoch 11033/1000000, d_loss: -1480.149658203125,  g_loss: -130.349365234375\n",
            "Training epoch 11034/1000000, d_loss: -92.24517822265625,  g_loss: -14.340932846069336\n",
            "Training epoch 11035/1000000, d_loss: -99.5841064453125,  g_loss: -77.6537857055664\n",
            "Training epoch 11036/1000000, d_loss: -1.3481693267822266,  g_loss: 32.58870315551758\n",
            "Training epoch 11037/1000000, d_loss: 374.2105712890625,  g_loss: 41.39409255981445\n",
            "Training epoch 11038/1000000, d_loss: -56.73460006713867,  g_loss: 36.38335418701172\n",
            "Training epoch 11039/1000000, d_loss: -64.66915130615234,  g_loss: 25.002656936645508\n",
            "Training epoch 11040/1000000, d_loss: -58.441192626953125,  g_loss: -1.1164264678955078\n",
            "Training epoch 11041/1000000, d_loss: -447.2379455566406,  g_loss: -77.36070251464844\n",
            "Training epoch 11042/1000000, d_loss: -459.5090637207031,  g_loss: -139.2477569580078\n",
            "Training epoch 11043/1000000, d_loss: 22.568674087524414,  g_loss: -29.693275451660156\n",
            "Training epoch 11044/1000000, d_loss: -55.73596954345703,  g_loss: 35.402618408203125\n",
            "Training epoch 11045/1000000, d_loss: -61.51200866699219,  g_loss: 4.161835670471191\n",
            "Training epoch 11046/1000000, d_loss: 2.7588043212890625,  g_loss: 18.292316436767578\n",
            "Training epoch 11047/1000000, d_loss: -58.407814025878906,  g_loss: -42.23060989379883\n",
            "Training epoch 11048/1000000, d_loss: -134.1956024169922,  g_loss: 29.59964942932129\n",
            "Training epoch 11049/1000000, d_loss: -8.84857177734375,  g_loss: -115.50955200195312\n",
            "Training epoch 11050/1000000, d_loss: -153.6293487548828,  g_loss: 104.77352142333984\n",
            "Training epoch 11051/1000000, d_loss: -193.07861328125,  g_loss: -43.25105667114258\n",
            "Training epoch 11052/1000000, d_loss: -131.93299865722656,  g_loss: -47.66184616088867\n",
            "Training epoch 11053/1000000, d_loss: -346.2597351074219,  g_loss: -113.28203582763672\n",
            "Training epoch 11054/1000000, d_loss: -75.76844024658203,  g_loss: -90.55240631103516\n",
            "Training epoch 11055/1000000, d_loss: -105.92051696777344,  g_loss: -52.188087463378906\n",
            "Training epoch 11056/1000000, d_loss: -74.8695068359375,  g_loss: -28.151874542236328\n",
            "Training epoch 11057/1000000, d_loss: -39.914005279541016,  g_loss: 46.860897064208984\n",
            "Training epoch 11058/1000000, d_loss: -36.37066650390625,  g_loss: 59.432395935058594\n",
            "Training epoch 11059/1000000, d_loss: -389.9133605957031,  g_loss: -55.92841339111328\n",
            "Training epoch 11060/1000000, d_loss: -136.38494873046875,  g_loss: 70.10714721679688\n",
            "Training epoch 11061/1000000, d_loss: 5.8225555419921875,  g_loss: 72.3781967163086\n",
            "Training epoch 11062/1000000, d_loss: -123.08433532714844,  g_loss: 73.11961364746094\n",
            "Training epoch 11063/1000000, d_loss: -39.75869369506836,  g_loss: 51.170257568359375\n",
            "Training epoch 11064/1000000, d_loss: -102.84219360351562,  g_loss: 26.169662475585938\n",
            "Training epoch 11065/1000000, d_loss: -349.0468444824219,  g_loss: -52.705108642578125\n",
            "Training epoch 11066/1000000, d_loss: -1214.351318359375,  g_loss: -235.00253295898438\n",
            "Training epoch 11067/1000000, d_loss: -48.599395751953125,  g_loss: -186.93411254882812\n",
            "Training epoch 11068/1000000, d_loss: -63.48662185668945,  g_loss: 5.3709564208984375\n",
            "Training epoch 11069/1000000, d_loss: -28.790603637695312,  g_loss: 26.323944091796875\n",
            "Training epoch 11070/1000000, d_loss: -178.3827667236328,  g_loss: 83.1874008178711\n",
            "Training epoch 11071/1000000, d_loss: -289.9891357421875,  g_loss: 169.72320556640625\n",
            "Training epoch 11072/1000000, d_loss: -145.7608184814453,  g_loss: 94.80315399169922\n",
            "Training epoch 11073/1000000, d_loss: -137.23133850097656,  g_loss: 27.132068634033203\n",
            "Training epoch 11074/1000000, d_loss: -39.97770690917969,  g_loss: 19.739423751831055\n",
            "Training epoch 11075/1000000, d_loss: -99.00043487548828,  g_loss: 25.5134334564209\n",
            "Training epoch 11076/1000000, d_loss: 258.3229675292969,  g_loss: -33.61669158935547\n",
            "Training epoch 11077/1000000, d_loss: -158.386962890625,  g_loss: -3.3632678985595703\n",
            "Training epoch 11078/1000000, d_loss: -178.4258270263672,  g_loss: -15.309548377990723\n",
            "Training epoch 11079/1000000, d_loss: -101.47831726074219,  g_loss: 66.10503387451172\n",
            "Training epoch 11080/1000000, d_loss: -144.85386657714844,  g_loss: 170.32778930664062\n",
            "Training epoch 11081/1000000, d_loss: -4.584758758544922,  g_loss: -39.75220489501953\n",
            "Training epoch 11082/1000000, d_loss: -901.2500610351562,  g_loss: -114.98374938964844\n",
            "Training epoch 11083/1000000, d_loss: -81.23062133789062,  g_loss: -17.518890380859375\n",
            "Training epoch 11084/1000000, d_loss: -31.178876876831055,  g_loss: -39.62294387817383\n",
            "Training epoch 11085/1000000, d_loss: -29.399696350097656,  g_loss: 61.55876922607422\n",
            "Training epoch 11086/1000000, d_loss: -323.10589599609375,  g_loss: 369.67578125\n",
            "Training epoch 11087/1000000, d_loss: 316.8827209472656,  g_loss: 28.65860366821289\n",
            "Training epoch 11088/1000000, d_loss: -96.31353759765625,  g_loss: 63.872676849365234\n",
            "Training epoch 11089/1000000, d_loss: -93.04537200927734,  g_loss: 50.656036376953125\n",
            "Training epoch 11090/1000000, d_loss: -72.86955261230469,  g_loss: 51.497772216796875\n",
            "Training epoch 11091/1000000, d_loss: -98.57180786132812,  g_loss: 21.594072341918945\n",
            "Training epoch 11092/1000000, d_loss: -125.51422882080078,  g_loss: 20.552940368652344\n",
            "Training epoch 11093/1000000, d_loss: -68.70281982421875,  g_loss: 28.43170928955078\n",
            "Training epoch 11094/1000000, d_loss: -57.882080078125,  g_loss: 5.322627067565918\n",
            "Training epoch 11095/1000000, d_loss: -290.9333190917969,  g_loss: -59.73828125\n",
            "Training epoch 11096/1000000, d_loss: -11.161968231201172,  g_loss: -50.620643615722656\n",
            "Training epoch 11097/1000000, d_loss: 73922.2890625,  g_loss: -50.72989273071289\n",
            "Training epoch 11098/1000000, d_loss: 73.64977264404297,  g_loss: -138.562255859375\n",
            "Training epoch 11099/1000000, d_loss: 98.11558532714844,  g_loss: -237.71835327148438\n",
            "Training epoch 11100/1000000, d_loss: -281.83526611328125,  g_loss: -240.32427978515625\n",
            "Training epoch 11101/1000000, d_loss: 201.985595703125,  g_loss: -88.00663757324219\n",
            "Training epoch 11102/1000000, d_loss: -7.305267333984375,  g_loss: -58.87863540649414\n",
            "Training epoch 11103/1000000, d_loss: -149.98345947265625,  g_loss: -25.730735778808594\n",
            "Training epoch 11104/1000000, d_loss: -119.53483581542969,  g_loss: -114.90461730957031\n",
            "Training epoch 11105/1000000, d_loss: -152.04800415039062,  g_loss: 70.86921691894531\n",
            "Training epoch 11106/1000000, d_loss: -258.2989501953125,  g_loss: -15.539970397949219\n",
            "Training epoch 11107/1000000, d_loss: -368.063720703125,  g_loss: 30.95730209350586\n",
            "Training epoch 11108/1000000, d_loss: -799.3327026367188,  g_loss: 460.22955322265625\n",
            "Training epoch 11109/1000000, d_loss: 17.125991821289062,  g_loss: -284.16070556640625\n",
            "Training epoch 11110/1000000, d_loss: 88.85563659667969,  g_loss: -607.6544799804688\n",
            "Training epoch 11111/1000000, d_loss: -86.65274047851562,  g_loss: -606.2860107421875\n",
            "Training epoch 11112/1000000, d_loss: -15.960689544677734,  g_loss: -654.713134765625\n",
            "Training epoch 11113/1000000, d_loss: -83.14533996582031,  g_loss: -476.78680419921875\n",
            "Training epoch 11114/1000000, d_loss: -20.807876586914062,  g_loss: -499.733154296875\n",
            "Training epoch 11115/1000000, d_loss: -264.7331237792969,  g_loss: -505.1949157714844\n",
            "Training epoch 11116/1000000, d_loss: 41.85655975341797,  g_loss: -453.12689208984375\n",
            "Training epoch 11117/1000000, d_loss: 6.132476329803467,  g_loss: -427.4527893066406\n",
            "Training epoch 11118/1000000, d_loss: -69.16244506835938,  g_loss: -361.7219543457031\n",
            "Training epoch 11119/1000000, d_loss: -15.250516891479492,  g_loss: -281.7611389160156\n",
            "Training epoch 11120/1000000, d_loss: -292.02777099609375,  g_loss: -260.4490051269531\n",
            "Training epoch 11121/1000000, d_loss: 49.2362174987793,  g_loss: -112.81502532958984\n",
            "Training epoch 11122/1000000, d_loss: -20.173906326293945,  g_loss: -42.860469818115234\n",
            "Training epoch 11123/1000000, d_loss: -228.50558471679688,  g_loss: -48.62305450439453\n",
            "Training epoch 11124/1000000, d_loss: -994.9195556640625,  g_loss: -223.9178009033203\n",
            "Training epoch 11125/1000000, d_loss: -49.247398376464844,  g_loss: -38.513702392578125\n",
            "Training epoch 11126/1000000, d_loss: -120.2412338256836,  g_loss: -50.34324645996094\n",
            "Training epoch 11127/1000000, d_loss: -341.3878173828125,  g_loss: -46.005191802978516\n",
            "Training epoch 11128/1000000, d_loss: -34.267303466796875,  g_loss: 0.7268104553222656\n",
            "Training epoch 11129/1000000, d_loss: -122.70124816894531,  g_loss: -16.587465286254883\n",
            "Training epoch 11130/1000000, d_loss: -136.95855712890625,  g_loss: -46.64177703857422\n",
            "Training epoch 11131/1000000, d_loss: -68.23381805419922,  g_loss: 40.40645217895508\n",
            "Training epoch 11132/1000000, d_loss: -108.84483337402344,  g_loss: -12.61062240600586\n",
            "Training epoch 11133/1000000, d_loss: -98.71145629882812,  g_loss: 14.31375503540039\n",
            "Training epoch 11134/1000000, d_loss: -1081.55517578125,  g_loss: -99.5771713256836\n",
            "Training epoch 11135/1000000, d_loss: -8.555681228637695,  g_loss: -5.472136497497559\n",
            "Training epoch 11136/1000000, d_loss: -30.901836395263672,  g_loss: 90.28569030761719\n",
            "Training epoch 11137/1000000, d_loss: -70.9296875,  g_loss: 69.07394409179688\n",
            "Training epoch 11138/1000000, d_loss: 17.75391387939453,  g_loss: 88.47940063476562\n",
            "Training epoch 11139/1000000, d_loss: -96.9260025024414,  g_loss: 134.4044647216797\n",
            "Training epoch 11140/1000000, d_loss: -108.38446044921875,  g_loss: 65.93576049804688\n",
            "Training epoch 11141/1000000, d_loss: -98.6001968383789,  g_loss: -91.72105407714844\n",
            "Training epoch 11142/1000000, d_loss: 10.904169082641602,  g_loss: -23.8138427734375\n",
            "Training epoch 11143/1000000, d_loss: -63.400489807128906,  g_loss: 8.845324516296387\n",
            "Training epoch 11144/1000000, d_loss: -153.68487548828125,  g_loss: -36.93218231201172\n",
            "Training epoch 11145/1000000, d_loss: -140.0839385986328,  g_loss: -111.43734741210938\n",
            "Training epoch 11146/1000000, d_loss: -62.881351470947266,  g_loss: 5.924615383148193\n",
            "Training epoch 11147/1000000, d_loss: -825.9952392578125,  g_loss: -315.37750244140625\n",
            "Training epoch 11148/1000000, d_loss: -75.32131958007812,  g_loss: 38.81013107299805\n",
            "Training epoch 11149/1000000, d_loss: -94.44296264648438,  g_loss: 8.743626594543457\n",
            "Training epoch 11150/1000000, d_loss: -907.0426025390625,  g_loss: -76.06080627441406\n",
            "Training epoch 11151/1000000, d_loss: -16.554916381835938,  g_loss: 26.71673011779785\n",
            "Training epoch 11152/1000000, d_loss: -293.5648193359375,  g_loss: -72.33456420898438\n",
            "Training epoch 11153/1000000, d_loss: 230.53768920898438,  g_loss: 57.0681266784668\n",
            "Training epoch 11154/1000000, d_loss: -170.48175048828125,  g_loss: 84.71627807617188\n",
            "Training epoch 11155/1000000, d_loss: -216.91122436523438,  g_loss: -17.73175811767578\n",
            "Training epoch 11156/1000000, d_loss: -136.90093994140625,  g_loss: 29.31928062438965\n",
            "Training epoch 11157/1000000, d_loss: -383.93621826171875,  g_loss: -39.583858489990234\n",
            "Training epoch 11158/1000000, d_loss: -310.0430603027344,  g_loss: -63.42755126953125\n",
            "Training epoch 11159/1000000, d_loss: -143.23280334472656,  g_loss: 57.123538970947266\n",
            "Training epoch 11160/1000000, d_loss: -135.10165405273438,  g_loss: 65.83990478515625\n",
            "Training epoch 11161/1000000, d_loss: -130.7188720703125,  g_loss: 15.634129524230957\n",
            "Training epoch 11162/1000000, d_loss: -575.5855712890625,  g_loss: 124.91465759277344\n",
            "Training epoch 11163/1000000, d_loss: -76.63087463378906,  g_loss: -54.754302978515625\n",
            "Training epoch 11164/1000000, d_loss: -4.664386749267578,  g_loss: -2.433255672454834\n",
            "Training epoch 11165/1000000, d_loss: -19.893234252929688,  g_loss: -23.883056640625\n",
            "Training epoch 11166/1000000, d_loss: -195.97369384765625,  g_loss: -78.254638671875\n",
            "Training epoch 11167/1000000, d_loss: -169.2146453857422,  g_loss: -91.64965057373047\n",
            "Training epoch 11168/1000000, d_loss: -80.43347930908203,  g_loss: 24.181495666503906\n",
            "Training epoch 11169/1000000, d_loss: -180.96849060058594,  g_loss: 3.1859941482543945\n",
            "Training epoch 11170/1000000, d_loss: -94.16050720214844,  g_loss: 34.03895568847656\n",
            "Training epoch 11171/1000000, d_loss: -54.506431579589844,  g_loss: 46.61170196533203\n",
            "Training epoch 11172/1000000, d_loss: -114.82684326171875,  g_loss: 9.791277885437012\n",
            "Training epoch 11173/1000000, d_loss: -168.37649536132812,  g_loss: -38.70112609863281\n",
            "Training epoch 11174/1000000, d_loss: -211.63121032714844,  g_loss: 37.37688446044922\n",
            "Training epoch 11175/1000000, d_loss: -63.82805252075195,  g_loss: 118.52066802978516\n",
            "Training epoch 11176/1000000, d_loss: -56.56726837158203,  g_loss: 126.53981018066406\n",
            "Training epoch 11177/1000000, d_loss: -66.0135498046875,  g_loss: 36.51891326904297\n",
            "Training epoch 11178/1000000, d_loss: -236.19680786132812,  g_loss: 20.633594512939453\n",
            "Training epoch 11179/1000000, d_loss: -131.53326416015625,  g_loss: -9.850759506225586\n",
            "Training epoch 11180/1000000, d_loss: -252.6304931640625,  g_loss: 19.883506774902344\n",
            "Training epoch 11181/1000000, d_loss: -66.36299896240234,  g_loss: 125.50639343261719\n",
            "Training epoch 11182/1000000, d_loss: -127.77767181396484,  g_loss: 84.4866714477539\n",
            "Training epoch 11183/1000000, d_loss: -131.1181182861328,  g_loss: 70.51664733886719\n",
            "Training epoch 11184/1000000, d_loss: -756.0534057617188,  g_loss: 5.477528095245361\n",
            "Training epoch 11185/1000000, d_loss: -159.59771728515625,  g_loss: -152.14878845214844\n",
            "Training epoch 11186/1000000, d_loss: -68.67218780517578,  g_loss: -15.702425003051758\n",
            "Training epoch 11187/1000000, d_loss: -89.32457733154297,  g_loss: 15.414745330810547\n",
            "Training epoch 11188/1000000, d_loss: -116.7493667602539,  g_loss: -28.207319259643555\n",
            "Training epoch 11189/1000000, d_loss: 152.83506774902344,  g_loss: 42.53593826293945\n",
            "Training epoch 11190/1000000, d_loss: -243.4475555419922,  g_loss: -11.647075653076172\n",
            "Training epoch 11191/1000000, d_loss: -119.02360534667969,  g_loss: 16.52996826171875\n",
            "Training epoch 11192/1000000, d_loss: -91.86797332763672,  g_loss: 27.744029998779297\n",
            "Training epoch 11193/1000000, d_loss: -56.878692626953125,  g_loss: 60.197166442871094\n",
            "Training epoch 11194/1000000, d_loss: -73.19976806640625,  g_loss: -44.55120086669922\n",
            "Training epoch 11195/1000000, d_loss: -134.6110382080078,  g_loss: 16.058284759521484\n",
            "Training epoch 11196/1000000, d_loss: -72.00576782226562,  g_loss: -49.19505310058594\n",
            "Training epoch 11197/1000000, d_loss: -111.13931274414062,  g_loss: 1.1663833856582642\n",
            "Training epoch 11198/1000000, d_loss: -32.5829963684082,  g_loss: 30.81011390686035\n",
            "Training epoch 11199/1000000, d_loss: -137.7951202392578,  g_loss: 72.56743621826172\n",
            "Training epoch 11200/1000000, d_loss: -73.93507385253906,  g_loss: 73.71121215820312\n",
            "Training epoch 11201/1000000, d_loss: -361.83447265625,  g_loss: -137.97067260742188\n",
            "Training epoch 11202/1000000, d_loss: -89.18334197998047,  g_loss: 135.9581298828125\n",
            "Training epoch 11203/1000000, d_loss: -1054.6416015625,  g_loss: -534.8535766601562\n",
            "Training epoch 11204/1000000, d_loss: 1194.537841796875,  g_loss: -118.72544860839844\n",
            "Training epoch 11205/1000000, d_loss: -68.85466766357422,  g_loss: -46.263710021972656\n",
            "Training epoch 11206/1000000, d_loss: -27.19310760498047,  g_loss: 29.173242568969727\n",
            "Training epoch 11207/1000000, d_loss: -149.91033935546875,  g_loss: 23.7412166595459\n",
            "Training epoch 11208/1000000, d_loss: -783.6722412109375,  g_loss: -139.92530822753906\n",
            "Training epoch 11209/1000000, d_loss: -159.3282012939453,  g_loss: 43.75559997558594\n",
            "Training epoch 11210/1000000, d_loss: -117.91517639160156,  g_loss: 89.62521362304688\n",
            "Training epoch 11211/1000000, d_loss: -207.87954711914062,  g_loss: 58.87220764160156\n",
            "Training epoch 11212/1000000, d_loss: -15.38592529296875,  g_loss: 58.44084548950195\n",
            "Training epoch 11213/1000000, d_loss: -74.35675811767578,  g_loss: 37.24454116821289\n",
            "Training epoch 11214/1000000, d_loss: -52.09391784667969,  g_loss: 114.8700180053711\n",
            "Training epoch 11215/1000000, d_loss: -121.78459167480469,  g_loss: 46.36040115356445\n",
            "Training epoch 11216/1000000, d_loss: -79.58306884765625,  g_loss: 61.11766815185547\n",
            "Training epoch 11217/1000000, d_loss: -89.30841064453125,  g_loss: 89.53540802001953\n",
            "Training epoch 11218/1000000, d_loss: -147.71249389648438,  g_loss: 211.91204833984375\n",
            "Training epoch 11219/1000000, d_loss: -187.80514526367188,  g_loss: 31.013477325439453\n",
            "Training epoch 11220/1000000, d_loss: -402.4542236328125,  g_loss: -72.84391784667969\n",
            "Training epoch 11221/1000000, d_loss: 174.88922119140625,  g_loss: 140.97708129882812\n",
            "Training epoch 11222/1000000, d_loss: -120.08443450927734,  g_loss: 60.025909423828125\n",
            "Training epoch 11223/1000000, d_loss: -346.5137023925781,  g_loss: -39.825565338134766\n",
            "Training epoch 11224/1000000, d_loss: -457.73309326171875,  g_loss: -237.89051818847656\n",
            "Training epoch 11225/1000000, d_loss: 64.12353515625,  g_loss: 78.64057159423828\n",
            "Training epoch 11226/1000000, d_loss: -201.33399963378906,  g_loss: 138.46307373046875\n",
            "Training epoch 11227/1000000, d_loss: -76.50505065917969,  g_loss: 29.227745056152344\n",
            "Training epoch 11228/1000000, d_loss: -123.41993713378906,  g_loss: 59.752838134765625\n",
            "Training epoch 11229/1000000, d_loss: -245.78823852539062,  g_loss: -49.03541564941406\n",
            "Training epoch 11230/1000000, d_loss: -86.45896911621094,  g_loss: 116.65739440917969\n",
            "Training epoch 11231/1000000, d_loss: -1208.697021484375,  g_loss: -163.27175903320312\n",
            "Training epoch 11232/1000000, d_loss: 23.588088989257812,  g_loss: -113.03642272949219\n",
            "Training epoch 11233/1000000, d_loss: -285.5994873046875,  g_loss: 22.202165603637695\n",
            "Training epoch 11234/1000000, d_loss: -133.2096710205078,  g_loss: 47.73497009277344\n",
            "Training epoch 11235/1000000, d_loss: -102.4393539428711,  g_loss: 51.08716583251953\n",
            "Training epoch 11236/1000000, d_loss: -159.9476318359375,  g_loss: 116.2362289428711\n",
            "Training epoch 11237/1000000, d_loss: -335.8669738769531,  g_loss: -79.35875701904297\n",
            "Training epoch 11238/1000000, d_loss: 14.129127502441406,  g_loss: -44.34884262084961\n",
            "Training epoch 11239/1000000, d_loss: -36.602542877197266,  g_loss: -12.641395568847656\n",
            "Training epoch 11240/1000000, d_loss: -68.00035095214844,  g_loss: -19.62447166442871\n",
            "Training epoch 11241/1000000, d_loss: -223.62364196777344,  g_loss: -6.72210693359375\n",
            "Training epoch 11242/1000000, d_loss: -85.72161102294922,  g_loss: 22.004535675048828\n",
            "Training epoch 11243/1000000, d_loss: -77.75802612304688,  g_loss: 19.57360076904297\n",
            "Training epoch 11244/1000000, d_loss: -185.44790649414062,  g_loss: 203.76242065429688\n",
            "Training epoch 11245/1000000, d_loss: -94.59783935546875,  g_loss: 185.25051879882812\n",
            "Training epoch 11246/1000000, d_loss: -186.02377319335938,  g_loss: -27.30962562561035\n",
            "Training epoch 11247/1000000, d_loss: -380.993896484375,  g_loss: -118.27415466308594\n",
            "Training epoch 11248/1000000, d_loss: -140.05303955078125,  g_loss: -3.841787815093994\n",
            "Training epoch 11249/1000000, d_loss: -80.37373352050781,  g_loss: 153.0980224609375\n",
            "Training epoch 11250/1000000, d_loss: -332.16497802734375,  g_loss: -32.00690841674805\n",
            "Training epoch 11251/1000000, d_loss: -295.8301696777344,  g_loss: -130.57354736328125\n",
            "Training epoch 11252/1000000, d_loss: 112.93461608886719,  g_loss: -7.80930757522583\n",
            "Training epoch 11253/1000000, d_loss: -31.907249450683594,  g_loss: 54.74932861328125\n",
            "Training epoch 11254/1000000, d_loss: -196.6465301513672,  g_loss: -7.454536437988281\n",
            "Training epoch 11255/1000000, d_loss: -131.4109649658203,  g_loss: 119.43382263183594\n",
            "Training epoch 11256/1000000, d_loss: -22.77812957763672,  g_loss: 24.149364471435547\n",
            "Training epoch 11257/1000000, d_loss: -78.90444946289062,  g_loss: 208.62942504882812\n",
            "Training epoch 11258/1000000, d_loss: -97.49810028076172,  g_loss: 215.80975341796875\n",
            "Training epoch 11259/1000000, d_loss: -253.44210815429688,  g_loss: -4.057995796203613\n",
            "Training epoch 11260/1000000, d_loss: -181.20791625976562,  g_loss: -90.48561096191406\n",
            "Training epoch 11261/1000000, d_loss: 7.80889892578125,  g_loss: 26.503890991210938\n",
            "Training epoch 11262/1000000, d_loss: -394.40753173828125,  g_loss: 15.91573429107666\n",
            "Training epoch 11263/1000000, d_loss: -90.69912719726562,  g_loss: 16.592561721801758\n",
            "Training epoch 11264/1000000, d_loss: -167.38790893554688,  g_loss: -3.310445785522461\n",
            "Training epoch 11265/1000000, d_loss: -97.06443786621094,  g_loss: 6.38884162902832\n",
            "Training epoch 11266/1000000, d_loss: -94.16409301757812,  g_loss: 25.406715393066406\n",
            "Training epoch 11267/1000000, d_loss: -21.231075286865234,  g_loss: 15.043949127197266\n",
            "Training epoch 11268/1000000, d_loss: -30.781940460205078,  g_loss: 7.278388500213623\n",
            "Training epoch 11269/1000000, d_loss: -41.40879821777344,  g_loss: 7.326314926147461\n",
            "Training epoch 11270/1000000, d_loss: -198.7738037109375,  g_loss: 9.838615417480469\n",
            "Training epoch 11271/1000000, d_loss: -117.15216827392578,  g_loss: 38.811927795410156\n",
            "Training epoch 11272/1000000, d_loss: -496.2909851074219,  g_loss: -104.69775390625\n",
            "Training epoch 11273/1000000, d_loss: -41.031185150146484,  g_loss: 22.90526580810547\n",
            "Training epoch 11274/1000000, d_loss: 63.113525390625,  g_loss: -90.04161071777344\n",
            "Training epoch 11275/1000000, d_loss: 123.22119903564453,  g_loss: 10.6891508102417\n",
            "Training epoch 11276/1000000, d_loss: -40.96544647216797,  g_loss: 34.642662048339844\n",
            "Training epoch 11277/1000000, d_loss: 157.2991485595703,  g_loss: 16.200056076049805\n",
            "Training epoch 11278/1000000, d_loss: -75.45531463623047,  g_loss: 45.88129425048828\n",
            "Training epoch 11279/1000000, d_loss: -91.9483642578125,  g_loss: 49.26581573486328\n",
            "Training epoch 11280/1000000, d_loss: -96.28759002685547,  g_loss: -5.3906450271606445\n",
            "Training epoch 11281/1000000, d_loss: -71.83570861816406,  g_loss: 39.900596618652344\n",
            "Training epoch 11282/1000000, d_loss: -59.20407485961914,  g_loss: 38.06803894042969\n",
            "Training epoch 11283/1000000, d_loss: -81.12391662597656,  g_loss: 36.88896942138672\n",
            "Training epoch 11284/1000000, d_loss: -83.93318176269531,  g_loss: 77.0316162109375\n",
            "Training epoch 11285/1000000, d_loss: -29.292007446289062,  g_loss: 73.8487319946289\n",
            "Training epoch 11286/1000000, d_loss: -90.83317565917969,  g_loss: 67.41450500488281\n",
            "Training epoch 11287/1000000, d_loss: -152.2827911376953,  g_loss: 28.32423210144043\n",
            "Training epoch 11288/1000000, d_loss: -154.06556701660156,  g_loss: 58.79521179199219\n",
            "Training epoch 11289/1000000, d_loss: -864.953125,  g_loss: -314.46551513671875\n",
            "Training epoch 11290/1000000, d_loss: 92.87641906738281,  g_loss: 53.47694396972656\n",
            "Training epoch 11291/1000000, d_loss: -66.90128326416016,  g_loss: 64.3102798461914\n",
            "Training epoch 11292/1000000, d_loss: -116.71377563476562,  g_loss: 323.67877197265625\n",
            "Training epoch 11293/1000000, d_loss: -134.19422912597656,  g_loss: 219.60400390625\n",
            "Training epoch 11294/1000000, d_loss: 39.88494873046875,  g_loss: 763.7852172851562\n",
            "Training epoch 11295/1000000, d_loss: -50.404335021972656,  g_loss: 49.22559356689453\n",
            "Training epoch 11296/1000000, d_loss: -59.22930908203125,  g_loss: -3.026744842529297\n",
            "Training epoch 11297/1000000, d_loss: -64.41984558105469,  g_loss: 44.726104736328125\n",
            "Training epoch 11298/1000000, d_loss: 7.240577697753906,  g_loss: 43.29500961303711\n",
            "Training epoch 11299/1000000, d_loss: -305.91729736328125,  g_loss: -52.85559844970703\n",
            "Training epoch 11300/1000000, d_loss: -381.62615966796875,  g_loss: -434.68292236328125\n",
            "Training epoch 11301/1000000, d_loss: -50.16078186035156,  g_loss: -67.89225006103516\n",
            "Training epoch 11302/1000000, d_loss: 49.84711456298828,  g_loss: -110.00418853759766\n",
            "Training epoch 11303/1000000, d_loss: -826.711181640625,  g_loss: -68.75479888916016\n",
            "Training epoch 11304/1000000, d_loss: -260.8312683105469,  g_loss: -810.7107543945312\n",
            "Training epoch 11305/1000000, d_loss: 5.8128662109375,  g_loss: 19.70777130126953\n",
            "Training epoch 11306/1000000, d_loss: -29.03363037109375,  g_loss: 164.5199432373047\n",
            "Training epoch 11307/1000000, d_loss: -532.6029663085938,  g_loss: 636.3947143554688\n",
            "Training epoch 11308/1000000, d_loss: 115.52482604980469,  g_loss: 352.6121826171875\n",
            "Training epoch 11309/1000000, d_loss: 267.386962890625,  g_loss: 77.52044677734375\n",
            "Training epoch 11310/1000000, d_loss: -94.15471649169922,  g_loss: 7.805060863494873\n",
            "Training epoch 11311/1000000, d_loss: -77.7153549194336,  g_loss: 121.47663879394531\n",
            "Training epoch 11312/1000000, d_loss: -144.6116943359375,  g_loss: 93.03887939453125\n",
            "Training epoch 11313/1000000, d_loss: -288.87847900390625,  g_loss: 293.7858581542969\n",
            "Training epoch 11314/1000000, d_loss: -462.455322265625,  g_loss: -2.095576286315918\n",
            "Training epoch 11315/1000000, d_loss: -409.55914306640625,  g_loss: -10.448236465454102\n",
            "Training epoch 11316/1000000, d_loss: -79.72472381591797,  g_loss: 10.044013977050781\n",
            "Training epoch 11317/1000000, d_loss: -78.65980529785156,  g_loss: 60.50518798828125\n",
            "Training epoch 11318/1000000, d_loss: -189.46583557128906,  g_loss: 27.253488540649414\n",
            "Training epoch 11319/1000000, d_loss: -81.10504150390625,  g_loss: 25.170759201049805\n",
            "Training epoch 11320/1000000, d_loss: -176.2313232421875,  g_loss: 117.84783172607422\n",
            "Training epoch 11321/1000000, d_loss: -49.80014419555664,  g_loss: 126.92437744140625\n",
            "Training epoch 11322/1000000, d_loss: -101.37911987304688,  g_loss: 7.862773895263672\n",
            "Training epoch 11323/1000000, d_loss: -118.26840209960938,  g_loss: 135.5873260498047\n",
            "Training epoch 11324/1000000, d_loss: -239.57937622070312,  g_loss: 133.81773376464844\n",
            "Training epoch 11325/1000000, d_loss: -40.19596862792969,  g_loss: -6.689815521240234\n",
            "Training epoch 11326/1000000, d_loss: -390.3765869140625,  g_loss: -53.90826416015625\n",
            "Training epoch 11327/1000000, d_loss: -358.4943542480469,  g_loss: -98.59635925292969\n",
            "Training epoch 11328/1000000, d_loss: -791.8955078125,  g_loss: -95.63873291015625\n",
            "Training epoch 11329/1000000, d_loss: 461.38916015625,  g_loss: -81.90841674804688\n",
            "Training epoch 11330/1000000, d_loss: -204.8767547607422,  g_loss: -60.51260757446289\n",
            "Training epoch 11331/1000000, d_loss: 87.51526641845703,  g_loss: -33.8692741394043\n",
            "Training epoch 11332/1000000, d_loss: -7.7164459228515625,  g_loss: 19.345115661621094\n",
            "Training epoch 11333/1000000, d_loss: -35.29261016845703,  g_loss: 44.077388763427734\n",
            "Training epoch 11334/1000000, d_loss: -144.2016143798828,  g_loss: 54.52885437011719\n",
            "Training epoch 11335/1000000, d_loss: -146.50082397460938,  g_loss: 40.201011657714844\n",
            "Training epoch 11336/1000000, d_loss: -291.52593994140625,  g_loss: 197.68661499023438\n",
            "Training epoch 11337/1000000, d_loss: -111.540283203125,  g_loss: 46.75669860839844\n",
            "Training epoch 11338/1000000, d_loss: -69.08679962158203,  g_loss: 48.901611328125\n",
            "Training epoch 11339/1000000, d_loss: -87.6650619506836,  g_loss: 45.865230560302734\n",
            "Training epoch 11340/1000000, d_loss: -404.238037109375,  g_loss: -95.60830688476562\n",
            "Training epoch 11341/1000000, d_loss: -663.4921264648438,  g_loss: -147.9584197998047\n",
            "Training epoch 11342/1000000, d_loss: -553.2020874023438,  g_loss: -674.4534912109375\n",
            "Training epoch 11343/1000000, d_loss: -87.07563781738281,  g_loss: 313.5896301269531\n",
            "Training epoch 11344/1000000, d_loss: -17.003665924072266,  g_loss: 82.895751953125\n",
            "Training epoch 11345/1000000, d_loss: -128.90625,  g_loss: 49.10934829711914\n",
            "Training epoch 11346/1000000, d_loss: -34.02980041503906,  g_loss: 140.536376953125\n",
            "Training epoch 11347/1000000, d_loss: -64.08234405517578,  g_loss: 72.0189208984375\n",
            "Training epoch 11348/1000000, d_loss: -428.21905517578125,  g_loss: 904.5145263671875\n",
            "Training epoch 11349/1000000, d_loss: 107.70246887207031,  g_loss: 9.476020812988281\n",
            "Training epoch 11350/1000000, d_loss: -84.64456176757812,  g_loss: 2.430495262145996\n",
            "Training epoch 11351/1000000, d_loss: -171.03817749023438,  g_loss: 26.92715835571289\n",
            "Training epoch 11352/1000000, d_loss: 154.3192138671875,  g_loss: 26.339147567749023\n",
            "Training epoch 11353/1000000, d_loss: -33.61829376220703,  g_loss: 40.38095474243164\n",
            "Training epoch 11354/1000000, d_loss: -36.19856262207031,  g_loss: 29.815242767333984\n",
            "Training epoch 11355/1000000, d_loss: -217.69993591308594,  g_loss: -3.7718114852905273\n",
            "Training epoch 11356/1000000, d_loss: -48.919254302978516,  g_loss: 8.774679183959961\n",
            "Training epoch 11357/1000000, d_loss: -80.85299682617188,  g_loss: 26.86346435546875\n",
            "Training epoch 11358/1000000, d_loss: -156.33314514160156,  g_loss: -6.678266525268555\n",
            "Training epoch 11359/1000000, d_loss: -64.93852996826172,  g_loss: -24.19573402404785\n",
            "Training epoch 11360/1000000, d_loss: -257.7122497558594,  g_loss: 24.25326156616211\n",
            "Training epoch 11361/1000000, d_loss: -137.5944366455078,  g_loss: -20.639949798583984\n",
            "Training epoch 11362/1000000, d_loss: -90.27437591552734,  g_loss: -20.368593215942383\n",
            "Training epoch 11363/1000000, d_loss: -329.4258728027344,  g_loss: -88.46401977539062\n",
            "Training epoch 11364/1000000, d_loss: -295.9469299316406,  g_loss: -124.08804321289062\n",
            "Training epoch 11365/1000000, d_loss: -48.130714416503906,  g_loss: 20.881290435791016\n",
            "Training epoch 11366/1000000, d_loss: -114.10404968261719,  g_loss: -14.27855396270752\n",
            "Training epoch 11367/1000000, d_loss: -17.756454467773438,  g_loss: 147.09219360351562\n",
            "Training epoch 11368/1000000, d_loss: -59.635986328125,  g_loss: 90.82137298583984\n",
            "Training epoch 11369/1000000, d_loss: 26.484146118164062,  g_loss: 51.579872131347656\n",
            "Training epoch 11370/1000000, d_loss: -134.16065979003906,  g_loss: 37.37983322143555\n",
            "Training epoch 11371/1000000, d_loss: -128.97035217285156,  g_loss: 124.77915954589844\n",
            "Training epoch 11372/1000000, d_loss: -85.37380981445312,  g_loss: 82.98905944824219\n",
            "Training epoch 11373/1000000, d_loss: -191.30783081054688,  g_loss: 136.05252075195312\n",
            "Training epoch 11374/1000000, d_loss: -1032.554443359375,  g_loss: -114.94718933105469\n",
            "Training epoch 11375/1000000, d_loss: -126.07810974121094,  g_loss: 1.2559680938720703\n",
            "Training epoch 11376/1000000, d_loss: -525.5361938476562,  g_loss: -84.10671997070312\n",
            "Training epoch 11377/1000000, d_loss: -120.29959106445312,  g_loss: -110.98515319824219\n",
            "Training epoch 11378/1000000, d_loss: -352.5492248535156,  g_loss: -50.87657165527344\n",
            "Training epoch 11379/1000000, d_loss: -1794.388916015625,  g_loss: -569.3181762695312\n",
            "Training epoch 11380/1000000, d_loss: -528.27392578125,  g_loss: -734.5216064453125\n",
            "Training epoch 11381/1000000, d_loss: 225.748779296875,  g_loss: -52.51298904418945\n",
            "Training epoch 11382/1000000, d_loss: 75.81910705566406,  g_loss: 32.390262603759766\n",
            "Training epoch 11383/1000000, d_loss: -81.86083984375,  g_loss: 129.12457275390625\n",
            "Training epoch 11384/1000000, d_loss: -39.63917541503906,  g_loss: 63.48186492919922\n",
            "Training epoch 11385/1000000, d_loss: -203.7836151123047,  g_loss: 218.54202270507812\n",
            "Training epoch 11386/1000000, d_loss: -305.9525451660156,  g_loss: 341.7900390625\n",
            "Training epoch 11387/1000000, d_loss: -98.03966522216797,  g_loss: 114.36093139648438\n",
            "Training epoch 11388/1000000, d_loss: -122.93709564208984,  g_loss: 52.040771484375\n",
            "Training epoch 11389/1000000, d_loss: -95.52426147460938,  g_loss: 84.29148864746094\n",
            "Training epoch 11390/1000000, d_loss: -354.4317626953125,  g_loss: 255.57171630859375\n",
            "Training epoch 11391/1000000, d_loss: -258.19464111328125,  g_loss: 122.86425018310547\n",
            "Training epoch 11392/1000000, d_loss: -249.81918334960938,  g_loss: 450.8541259765625\n",
            "Training epoch 11393/1000000, d_loss: -184.5115966796875,  g_loss: 199.2100372314453\n",
            "Training epoch 11394/1000000, d_loss: 50.92137145996094,  g_loss: -38.86217498779297\n",
            "Training epoch 11395/1000000, d_loss: -135.02987670898438,  g_loss: -7.640481948852539\n",
            "Training epoch 11396/1000000, d_loss: -379.6236572265625,  g_loss: -155.64205932617188\n",
            "Training epoch 11397/1000000, d_loss: -1373.67236328125,  g_loss: -276.0500793457031\n",
            "Training epoch 11398/1000000, d_loss: 2162.099365234375,  g_loss: -52.624488830566406\n",
            "Training epoch 11399/1000000, d_loss: -188.96273803710938,  g_loss: 46.480567932128906\n",
            "Training epoch 11400/1000000, d_loss: 107.39248657226562,  g_loss: 1.0300278663635254\n",
            "Training epoch 11401/1000000, d_loss: -82.32513427734375,  g_loss: 18.08132553100586\n",
            "Training epoch 11402/1000000, d_loss: -196.33352661132812,  g_loss: -150.254150390625\n",
            "Training epoch 11403/1000000, d_loss: 42.65271759033203,  g_loss: 76.92317199707031\n",
            "Training epoch 11404/1000000, d_loss: -224.12213134765625,  g_loss: 17.48491668701172\n",
            "Training epoch 11405/1000000, d_loss: 30.136749267578125,  g_loss: 102.3431396484375\n",
            "Training epoch 11406/1000000, d_loss: -212.7274169921875,  g_loss: 107.044921875\n",
            "Training epoch 11407/1000000, d_loss: -20.265670776367188,  g_loss: 46.826969146728516\n",
            "Training epoch 11408/1000000, d_loss: -74.80902099609375,  g_loss: 14.843429565429688\n",
            "Training epoch 11409/1000000, d_loss: -86.05193328857422,  g_loss: 6.032140731811523\n",
            "Training epoch 11410/1000000, d_loss: -155.7023162841797,  g_loss: 55.10456085205078\n",
            "Training epoch 11411/1000000, d_loss: -178.3790283203125,  g_loss: -48.354766845703125\n",
            "Training epoch 11412/1000000, d_loss: -114.38188171386719,  g_loss: -14.194662094116211\n",
            "Training epoch 11413/1000000, d_loss: -77.5526351928711,  g_loss: 62.58460235595703\n",
            "Training epoch 11414/1000000, d_loss: -308.8729248046875,  g_loss: -155.48797607421875\n",
            "Training epoch 11415/1000000, d_loss: -76.14692687988281,  g_loss: 33.11940383911133\n",
            "Training epoch 11416/1000000, d_loss: -61.43181610107422,  g_loss: 126.20108032226562\n",
            "Training epoch 11417/1000000, d_loss: -115.97480010986328,  g_loss: 100.2374496459961\n",
            "Training epoch 11418/1000000, d_loss: -117.9994888305664,  g_loss: 90.0291976928711\n",
            "Training epoch 11419/1000000, d_loss: 7.1213836669921875,  g_loss: 73.0716552734375\n",
            "Training epoch 11420/1000000, d_loss: -75.40518951416016,  g_loss: 110.64295959472656\n",
            "Training epoch 11421/1000000, d_loss: -32.04890441894531,  g_loss: 17.76352882385254\n",
            "Training epoch 11422/1000000, d_loss: -125.57131958007812,  g_loss: 93.77384948730469\n",
            "Training epoch 11423/1000000, d_loss: -83.90830993652344,  g_loss: 55.55307388305664\n",
            "Training epoch 11424/1000000, d_loss: -121.89280700683594,  g_loss: 67.31059265136719\n",
            "Training epoch 11425/1000000, d_loss: -813.910888671875,  g_loss: -208.22547912597656\n",
            "Training epoch 11426/1000000, d_loss: -1650.286865234375,  g_loss: -431.2811279296875\n",
            "Training epoch 11427/1000000, d_loss: 186.98020935058594,  g_loss: -120.35356140136719\n",
            "Training epoch 11428/1000000, d_loss: -85.03707885742188,  g_loss: -198.88113403320312\n",
            "Training epoch 11429/1000000, d_loss: 32.342063903808594,  g_loss: -157.7574462890625\n",
            "Training epoch 11430/1000000, d_loss: -57.632869720458984,  g_loss: -52.037559509277344\n",
            "Training epoch 11431/1000000, d_loss: -44.136043548583984,  g_loss: -51.472408294677734\n",
            "Training epoch 11432/1000000, d_loss: -47.64599609375,  g_loss: -7.0940704345703125\n",
            "Training epoch 11433/1000000, d_loss: -269.7376403808594,  g_loss: -54.17060470581055\n",
            "Training epoch 11434/1000000, d_loss: -280.02740478515625,  g_loss: -107.17915344238281\n",
            "Training epoch 11435/1000000, d_loss: -70.26083374023438,  g_loss: 22.81631088256836\n",
            "Training epoch 11436/1000000, d_loss: -76.26881408691406,  g_loss: 5.418083190917969\n",
            "Training epoch 11437/1000000, d_loss: -202.32626342773438,  g_loss: -38.714942932128906\n",
            "Training epoch 11438/1000000, d_loss: -93.88858795166016,  g_loss: 72.59686279296875\n",
            "Training epoch 11439/1000000, d_loss: -109.97749328613281,  g_loss: 57.40206527709961\n",
            "Training epoch 11440/1000000, d_loss: -214.8275909423828,  g_loss: 91.04752349853516\n",
            "Training epoch 11441/1000000, d_loss: -227.349609375,  g_loss: -111.37322998046875\n",
            "Training epoch 11442/1000000, d_loss: 427.97412109375,  g_loss: 27.028480529785156\n",
            "Training epoch 11443/1000000, d_loss: -27.24604034423828,  g_loss: 18.47318458557129\n",
            "Training epoch 11444/1000000, d_loss: -84.61469268798828,  g_loss: 26.802017211914062\n",
            "Training epoch 11445/1000000, d_loss: -78.14112854003906,  g_loss: 8.563881874084473\n",
            "Training epoch 11446/1000000, d_loss: -20.441192626953125,  g_loss: 52.84583282470703\n",
            "Training epoch 11447/1000000, d_loss: -498.5521545410156,  g_loss: -61.75725555419922\n",
            "Training epoch 11448/1000000, d_loss: -8.824235916137695,  g_loss: 77.37577819824219\n",
            "Training epoch 11449/1000000, d_loss: -11.183479309082031,  g_loss: -17.72830581665039\n",
            "Training epoch 11450/1000000, d_loss: -197.18751525878906,  g_loss: 75.57228088378906\n",
            "Training epoch 11451/1000000, d_loss: -42.88996887207031,  g_loss: -4.063897609710693\n",
            "Training epoch 11452/1000000, d_loss: -149.3194580078125,  g_loss: 47.43623733520508\n",
            "Training epoch 11453/1000000, d_loss: -29.243438720703125,  g_loss: 55.66560363769531\n",
            "Training epoch 11454/1000000, d_loss: -196.75927734375,  g_loss: 1.7773494720458984\n",
            "Training epoch 11455/1000000, d_loss: -87.48165893554688,  g_loss: 106.81033325195312\n",
            "Training epoch 11456/1000000, d_loss: -1029.7359619140625,  g_loss: 29.148658752441406\n",
            "Training epoch 11457/1000000, d_loss: -1238.192138671875,  g_loss: -77.85174560546875\n",
            "Training epoch 11458/1000000, d_loss: 236.7993621826172,  g_loss: -31.014509201049805\n",
            "Training epoch 11459/1000000, d_loss: 21.180309295654297,  g_loss: -84.11398315429688\n",
            "Training epoch 11460/1000000, d_loss: 318.97650146484375,  g_loss: -123.73286437988281\n",
            "Training epoch 11461/1000000, d_loss: -69.7705078125,  g_loss: -92.85181427001953\n",
            "Training epoch 11462/1000000, d_loss: 50.69287872314453,  g_loss: -189.9828338623047\n",
            "Training epoch 11463/1000000, d_loss: 3.124471664428711,  g_loss: -74.06126403808594\n",
            "Training epoch 11464/1000000, d_loss: -156.39797973632812,  g_loss: 226.71983337402344\n",
            "Training epoch 11465/1000000, d_loss: -151.51400756835938,  g_loss: 127.54679870605469\n",
            "Training epoch 11466/1000000, d_loss: -172.34889221191406,  g_loss: -24.343318939208984\n",
            "Training epoch 11467/1000000, d_loss: -70.05388641357422,  g_loss: -4.781341552734375\n",
            "Training epoch 11468/1000000, d_loss: -75.6958999633789,  g_loss: -55.03268051147461\n",
            "Training epoch 11469/1000000, d_loss: 136.38397216796875,  g_loss: 60.265647888183594\n",
            "Training epoch 11470/1000000, d_loss: 123.8453369140625,  g_loss: 19.102741241455078\n",
            "Training epoch 11471/1000000, d_loss: -159.06529235839844,  g_loss: 58.086097717285156\n",
            "Training epoch 11472/1000000, d_loss: -57.425201416015625,  g_loss: -8.825987815856934\n",
            "Training epoch 11473/1000000, d_loss: -137.59356689453125,  g_loss: -4.005542755126953\n",
            "Training epoch 11474/1000000, d_loss: -1288.4281005859375,  g_loss: -715.6663208007812\n",
            "Training epoch 11475/1000000, d_loss: 103.7378158569336,  g_loss: -355.1455993652344\n",
            "Training epoch 11476/1000000, d_loss: -429.6665344238281,  g_loss: -11.695053100585938\n",
            "Training epoch 11477/1000000, d_loss: -2985.4833984375,  g_loss: -1442.704345703125\n",
            "Training epoch 11478/1000000, d_loss: 6311.24658203125,  g_loss: -63.80775451660156\n",
            "Training epoch 11479/1000000, d_loss: 265.6444396972656,  g_loss: -10.265748023986816\n",
            "Training epoch 11480/1000000, d_loss: 41.92657470703125,  g_loss: -71.95478820800781\n",
            "Training epoch 11481/1000000, d_loss: -108.18704223632812,  g_loss: 77.77304077148438\n",
            "Training epoch 11482/1000000, d_loss: -17.65435791015625,  g_loss: 108.9020004272461\n",
            "Training epoch 11483/1000000, d_loss: 271.24530029296875,  g_loss: -37.06169128417969\n",
            "Training epoch 11484/1000000, d_loss: -140.4897003173828,  g_loss: -45.031822204589844\n",
            "Training epoch 11485/1000000, d_loss: 64.74254608154297,  g_loss: 95.7374038696289\n",
            "Training epoch 11486/1000000, d_loss: -24.257537841796875,  g_loss: 102.35750579833984\n",
            "Training epoch 11487/1000000, d_loss: -298.28472900390625,  g_loss: 547.2659301757812\n",
            "Training epoch 11488/1000000, d_loss: -70.49565887451172,  g_loss: 23.619808197021484\n",
            "Training epoch 11489/1000000, d_loss: -245.8614501953125,  g_loss: 171.6211395263672\n",
            "Training epoch 11490/1000000, d_loss: -18.299728393554688,  g_loss: 197.01060485839844\n",
            "Training epoch 11491/1000000, d_loss: 108.69502258300781,  g_loss: 80.34021759033203\n",
            "Training epoch 11492/1000000, d_loss: -45.77782440185547,  g_loss: 76.29928588867188\n",
            "Training epoch 11493/1000000, d_loss: -246.65066528320312,  g_loss: 316.975830078125\n",
            "Training epoch 11494/1000000, d_loss: -4.5999908447265625,  g_loss: -87.71647644042969\n",
            "Training epoch 11495/1000000, d_loss: -79.54847717285156,  g_loss: -50.038856506347656\n",
            "Training epoch 11496/1000000, d_loss: -39.982032775878906,  g_loss: -32.03388977050781\n",
            "Training epoch 11497/1000000, d_loss: -186.8950958251953,  g_loss: 28.878814697265625\n",
            "Training epoch 11498/1000000, d_loss: -78.90292358398438,  g_loss: 32.56360626220703\n",
            "Training epoch 11499/1000000, d_loss: -53.9731330871582,  g_loss: -22.101089477539062\n",
            "Training epoch 11500/1000000, d_loss: -88.663818359375,  g_loss: 5.856599807739258\n",
            "Training epoch 11501/1000000, d_loss: -47.4462776184082,  g_loss: 92.88734436035156\n",
            "Training epoch 11502/1000000, d_loss: -455.1370849609375,  g_loss: -85.14645385742188\n",
            "Training epoch 11503/1000000, d_loss: -12.85379409790039,  g_loss: 63.31611251831055\n",
            "Training epoch 11504/1000000, d_loss: -356.7371826171875,  g_loss: 36.34288024902344\n",
            "Training epoch 11505/1000000, d_loss: 73.13489532470703,  g_loss: 160.10342407226562\n",
            "Training epoch 11506/1000000, d_loss: -54.86029052734375,  g_loss: 149.32666015625\n",
            "Training epoch 11507/1000000, d_loss: -149.74322509765625,  g_loss: 97.59263610839844\n",
            "Training epoch 11508/1000000, d_loss: -16.522031784057617,  g_loss: 62.263946533203125\n",
            "Training epoch 11509/1000000, d_loss: 55.975746154785156,  g_loss: 70.31059265136719\n",
            "Training epoch 11510/1000000, d_loss: -98.31865692138672,  g_loss: 105.02671813964844\n",
            "Training epoch 11511/1000000, d_loss: -95.3970947265625,  g_loss: 106.72618865966797\n",
            "Training epoch 11512/1000000, d_loss: -40.38938903808594,  g_loss: 93.58436584472656\n",
            "Training epoch 11513/1000000, d_loss: -93.57249450683594,  g_loss: 99.98295593261719\n",
            "Training epoch 11514/1000000, d_loss: -197.26158142089844,  g_loss: 24.374000549316406\n",
            "Training epoch 11515/1000000, d_loss: -2794.40869140625,  g_loss: -718.2672729492188\n",
            "Training epoch 11516/1000000, d_loss: -101.38623046875,  g_loss: -522.57568359375\n",
            "Training epoch 11517/1000000, d_loss: 287.2959899902344,  g_loss: -358.0561828613281\n",
            "Training epoch 11518/1000000, d_loss: -160.04942321777344,  g_loss: -283.8071594238281\n",
            "Training epoch 11519/1000000, d_loss: -103.18836212158203,  g_loss: -137.05255126953125\n",
            "Training epoch 11520/1000000, d_loss: -181.2192840576172,  g_loss: -242.073974609375\n",
            "Training epoch 11521/1000000, d_loss: -45.512847900390625,  g_loss: 82.0513916015625\n",
            "Training epoch 11522/1000000, d_loss: -83.20584106445312,  g_loss: 138.34767150878906\n",
            "Training epoch 11523/1000000, d_loss: 665.87255859375,  g_loss: 166.58087158203125\n",
            "Training epoch 11524/1000000, d_loss: -211.55999755859375,  g_loss: 266.89178466796875\n",
            "Training epoch 11525/1000000, d_loss: -358.5028076171875,  g_loss: 697.4300537109375\n",
            "Training epoch 11526/1000000, d_loss: -4.297863006591797,  g_loss: 19.209835052490234\n",
            "Training epoch 11527/1000000, d_loss: -34.164573669433594,  g_loss: 110.18753814697266\n",
            "Training epoch 11528/1000000, d_loss: 6.547088623046875,  g_loss: 83.954345703125\n",
            "Training epoch 11529/1000000, d_loss: -173.25135803222656,  g_loss: 131.55218505859375\n",
            "Training epoch 11530/1000000, d_loss: -111.28630065917969,  g_loss: 122.51220703125\n",
            "Training epoch 11531/1000000, d_loss: -57.31529998779297,  g_loss: 78.9303207397461\n",
            "Training epoch 11532/1000000, d_loss: -76.81262969970703,  g_loss: 17.195999145507812\n",
            "Training epoch 11533/1000000, d_loss: -283.0552673339844,  g_loss: 61.19411849975586\n",
            "Training epoch 11534/1000000, d_loss: -30.52899169921875,  g_loss: 52.548187255859375\n",
            "Training epoch 11535/1000000, d_loss: -72.57783508300781,  g_loss: 122.1668930053711\n",
            "Training epoch 11536/1000000, d_loss: -191.6577911376953,  g_loss: 303.11041259765625\n",
            "Training epoch 11537/1000000, d_loss: -60.56175994873047,  g_loss: 135.41729736328125\n",
            "Training epoch 11538/1000000, d_loss: -88.83912658691406,  g_loss: 91.39430236816406\n",
            "Training epoch 11539/1000000, d_loss: -168.087158203125,  g_loss: 30.70306396484375\n",
            "Training epoch 11540/1000000, d_loss: -47.83135986328125,  g_loss: 83.0780029296875\n",
            "Training epoch 11541/1000000, d_loss: -224.79592895507812,  g_loss: 345.726318359375\n",
            "Training epoch 11542/1000000, d_loss: -191.13238525390625,  g_loss: 179.96713256835938\n",
            "Training epoch 11543/1000000, d_loss: -86.77320861816406,  g_loss: 205.1646728515625\n",
            "Training epoch 11544/1000000, d_loss: -71.86421203613281,  g_loss: 191.33619689941406\n",
            "Training epoch 11545/1000000, d_loss: -27.892227172851562,  g_loss: 161.36473083496094\n",
            "Training epoch 11546/1000000, d_loss: -28.62221908569336,  g_loss: 133.6290283203125\n",
            "Training epoch 11547/1000000, d_loss: -705.3412475585938,  g_loss: 23.900318145751953\n",
            "Training epoch 11548/1000000, d_loss: -3.089092254638672,  g_loss: 52.90374755859375\n",
            "Training epoch 11549/1000000, d_loss: -235.29605102539062,  g_loss: 58.10457229614258\n",
            "Training epoch 11550/1000000, d_loss: -77.03143310546875,  g_loss: 55.11400604248047\n",
            "Training epoch 11551/1000000, d_loss: -626.9890747070312,  g_loss: -372.1407470703125\n",
            "Training epoch 11552/1000000, d_loss: -18.830238342285156,  g_loss: 74.95195007324219\n",
            "Training epoch 11553/1000000, d_loss: -70.28146362304688,  g_loss: 88.97388458251953\n",
            "Training epoch 11554/1000000, d_loss: -617.0631713867188,  g_loss: -32.68924331665039\n",
            "Training epoch 11555/1000000, d_loss: -134.08502197265625,  g_loss: 231.53665161132812\n",
            "Training epoch 11556/1000000, d_loss: -132.0646209716797,  g_loss: 95.7751693725586\n",
            "Training epoch 11557/1000000, d_loss: -164.26329040527344,  g_loss: 413.25860595703125\n",
            "Training epoch 11558/1000000, d_loss: -61.306392669677734,  g_loss: 101.60935974121094\n",
            "Training epoch 11559/1000000, d_loss: -106.67784118652344,  g_loss: 86.16091918945312\n",
            "Training epoch 11560/1000000, d_loss: -190.41378784179688,  g_loss: 52.43699645996094\n",
            "Training epoch 11561/1000000, d_loss: -235.33941650390625,  g_loss: 110.38336181640625\n",
            "Training epoch 11562/1000000, d_loss: -104.29507446289062,  g_loss: 104.53104400634766\n",
            "Training epoch 11563/1000000, d_loss: -86.91816711425781,  g_loss: 36.98976516723633\n",
            "Training epoch 11564/1000000, d_loss: -36.109153747558594,  g_loss: 91.37532806396484\n",
            "Training epoch 11565/1000000, d_loss: 83.59078979492188,  g_loss: 117.04729461669922\n",
            "Training epoch 11566/1000000, d_loss: -151.54405212402344,  g_loss: 35.50092697143555\n",
            "Training epoch 11567/1000000, d_loss: -263.8368225097656,  g_loss: 194.7576446533203\n",
            "Training epoch 11568/1000000, d_loss: -227.78993225097656,  g_loss: 91.67078399658203\n",
            "Training epoch 11569/1000000, d_loss: -333.78411865234375,  g_loss: -103.77085876464844\n",
            "Training epoch 11570/1000000, d_loss: -89.90565490722656,  g_loss: 34.882415771484375\n",
            "Training epoch 11571/1000000, d_loss: 81.16532135009766,  g_loss: 112.55851745605469\n",
            "Training epoch 11572/1000000, d_loss: -71.47793579101562,  g_loss: 63.01735305786133\n",
            "Training epoch 11573/1000000, d_loss: -130.45359802246094,  g_loss: 75.99269104003906\n",
            "Training epoch 11574/1000000, d_loss: -71.93289184570312,  g_loss: 80.47705078125\n",
            "Training epoch 11575/1000000, d_loss: -70.96137237548828,  g_loss: 116.66165924072266\n",
            "Training epoch 11576/1000000, d_loss: -75.10140991210938,  g_loss: 205.84487915039062\n",
            "Training epoch 11577/1000000, d_loss: 25.554824829101562,  g_loss: 40.89811325073242\n",
            "Training epoch 11578/1000000, d_loss: -87.73538208007812,  g_loss: 12.987804412841797\n",
            "Training epoch 11579/1000000, d_loss: -145.81504821777344,  g_loss: 11.783374786376953\n",
            "Training epoch 11580/1000000, d_loss: -295.9380187988281,  g_loss: -65.25067901611328\n",
            "Training epoch 11581/1000000, d_loss: -83.35032653808594,  g_loss: 6.551204681396484\n",
            "Training epoch 11582/1000000, d_loss: -66.79017639160156,  g_loss: -1.0878419876098633\n",
            "Training epoch 11583/1000000, d_loss: -24.47565269470215,  g_loss: 11.107719421386719\n",
            "Training epoch 11584/1000000, d_loss: -97.7569580078125,  g_loss: 90.50016784667969\n",
            "Training epoch 11585/1000000, d_loss: -1018.2510986328125,  g_loss: -212.70921325683594\n",
            "Training epoch 11586/1000000, d_loss: -271.6780700683594,  g_loss: -281.619384765625\n",
            "Training epoch 11587/1000000, d_loss: 3.7082786560058594,  g_loss: 21.832807540893555\n",
            "Training epoch 11588/1000000, d_loss: -197.96603393554688,  g_loss: 174.81283569335938\n",
            "Training epoch 11589/1000000, d_loss: -21.843917846679688,  g_loss: 125.88826751708984\n",
            "Training epoch 11590/1000000, d_loss: -23.121295928955078,  g_loss: 125.7545166015625\n",
            "Training epoch 11591/1000000, d_loss: 17.04034423828125,  g_loss: 74.85235595703125\n",
            "Training epoch 11592/1000000, d_loss: -432.3240966796875,  g_loss: -139.2452392578125\n",
            "Training epoch 11593/1000000, d_loss: -49.664833068847656,  g_loss: 80.51008605957031\n",
            "Training epoch 11594/1000000, d_loss: -366.659912109375,  g_loss: -21.047649383544922\n",
            "Training epoch 11595/1000000, d_loss: -121.7479248046875,  g_loss: 115.87775421142578\n",
            "Training epoch 11596/1000000, d_loss: -238.31344604492188,  g_loss: 528.163330078125\n",
            "Training epoch 11597/1000000, d_loss: -25.132095336914062,  g_loss: 30.251346588134766\n",
            "Training epoch 11598/1000000, d_loss: -34.73029327392578,  g_loss: 34.09189987182617\n",
            "Training epoch 11599/1000000, d_loss: -93.96772766113281,  g_loss: 2.089625358581543\n",
            "Training epoch 11600/1000000, d_loss: 33.21720886230469,  g_loss: 68.52032470703125\n",
            "Training epoch 11601/1000000, d_loss: -82.00865936279297,  g_loss: 57.249488830566406\n",
            "Training epoch 11602/1000000, d_loss: -772.8606567382812,  g_loss: 12.105737686157227\n",
            "Training epoch 11603/1000000, d_loss: -57.84223556518555,  g_loss: 56.66455841064453\n",
            "Training epoch 11604/1000000, d_loss: -143.1837158203125,  g_loss: 39.99769973754883\n",
            "Training epoch 11605/1000000, d_loss: 17.135208129882812,  g_loss: 47.573020935058594\n",
            "Training epoch 11606/1000000, d_loss: -184.17141723632812,  g_loss: 134.83688354492188\n",
            "Training epoch 11607/1000000, d_loss: -102.64988708496094,  g_loss: 104.71432495117188\n",
            "Training epoch 11608/1000000, d_loss: -92.10391998291016,  g_loss: 160.54489135742188\n",
            "Training epoch 11609/1000000, d_loss: -118.37130737304688,  g_loss: 151.23739624023438\n",
            "Training epoch 11610/1000000, d_loss: -76.35224151611328,  g_loss: 62.39993667602539\n",
            "Training epoch 11611/1000000, d_loss: -734.497802734375,  g_loss: 48.729583740234375\n",
            "Training epoch 11612/1000000, d_loss: -282.5147705078125,  g_loss: -24.78917694091797\n",
            "Training epoch 11613/1000000, d_loss: -64.03677368164062,  g_loss: 62.76588439941406\n",
            "Training epoch 11614/1000000, d_loss: -207.18899536132812,  g_loss: 220.34609985351562\n",
            "Training epoch 11615/1000000, d_loss: -118.52417755126953,  g_loss: 267.5140380859375\n",
            "Training epoch 11616/1000000, d_loss: -103.21659851074219,  g_loss: 134.26834106445312\n",
            "Training epoch 11617/1000000, d_loss: -68.60321807861328,  g_loss: 161.74942016601562\n",
            "Training epoch 11618/1000000, d_loss: -90.63848114013672,  g_loss: 109.11725616455078\n",
            "Training epoch 11619/1000000, d_loss: -398.5643310546875,  g_loss: -10.34087085723877\n",
            "Training epoch 11620/1000000, d_loss: -61.75453186035156,  g_loss: 59.83340835571289\n",
            "Training epoch 11621/1000000, d_loss: -498.3747253417969,  g_loss: -387.1487731933594\n",
            "Training epoch 11622/1000000, d_loss: -613.2418823242188,  g_loss: -32.569732666015625\n",
            "Training epoch 11623/1000000, d_loss: 44.37101745605469,  g_loss: 56.5415153503418\n",
            "Training epoch 11624/1000000, d_loss: -36.73202896118164,  g_loss: 49.52765655517578\n",
            "Training epoch 11625/1000000, d_loss: -190.82504272460938,  g_loss: 189.0255584716797\n",
            "Training epoch 11626/1000000, d_loss: -92.5840835571289,  g_loss: 132.42221069335938\n",
            "Training epoch 11627/1000000, d_loss: 79.10404968261719,  g_loss: 127.79087829589844\n",
            "Training epoch 11628/1000000, d_loss: -252.6332550048828,  g_loss: 82.22905731201172\n",
            "Training epoch 11629/1000000, d_loss: -107.3353271484375,  g_loss: 105.05865478515625\n",
            "Training epoch 11630/1000000, d_loss: -68.00157165527344,  g_loss: 106.34732055664062\n",
            "Training epoch 11631/1000000, d_loss: -48.38462448120117,  g_loss: 85.92289733886719\n",
            "Training epoch 11632/1000000, d_loss: -106.48857116699219,  g_loss: 174.17759704589844\n",
            "Training epoch 11633/1000000, d_loss: -366.37646484375,  g_loss: 25.398900985717773\n",
            "Training epoch 11634/1000000, d_loss: -961.9325561523438,  g_loss: -41.44559097290039\n",
            "Training epoch 11635/1000000, d_loss: 25.638275146484375,  g_loss: 150.40310668945312\n",
            "Training epoch 11636/1000000, d_loss: -18.997737884521484,  g_loss: 105.69850158691406\n",
            "Training epoch 11637/1000000, d_loss: 66.8715591430664,  g_loss: 139.07437133789062\n",
            "Training epoch 11638/1000000, d_loss: -23.6009521484375,  g_loss: 152.37733459472656\n",
            "Training epoch 11639/1000000, d_loss: -63.430076599121094,  g_loss: 123.32485961914062\n",
            "Training epoch 11640/1000000, d_loss: -48.80447769165039,  g_loss: 176.41490173339844\n",
            "Training epoch 11641/1000000, d_loss: -11.594329833984375,  g_loss: 118.3948974609375\n",
            "Training epoch 11642/1000000, d_loss: -87.88832092285156,  g_loss: 204.49163818359375\n",
            "Training epoch 11643/1000000, d_loss: -117.57894897460938,  g_loss: 185.054443359375\n",
            "Training epoch 11644/1000000, d_loss: -121.03437805175781,  g_loss: 123.01211547851562\n",
            "Training epoch 11645/1000000, d_loss: -9.11822509765625,  g_loss: 154.15005493164062\n",
            "Training epoch 11646/1000000, d_loss: -72.42622375488281,  g_loss: 145.35618591308594\n",
            "Training epoch 11647/1000000, d_loss: -144.61239624023438,  g_loss: 221.29942321777344\n",
            "Training epoch 11648/1000000, d_loss: -113.58839416503906,  g_loss: 109.54900360107422\n",
            "Training epoch 11649/1000000, d_loss: -107.61946105957031,  g_loss: 154.2354736328125\n",
            "Training epoch 11650/1000000, d_loss: -200.33038330078125,  g_loss: 103.9223403930664\n",
            "Training epoch 11651/1000000, d_loss: -135.97030639648438,  g_loss: 70.36418151855469\n",
            "Training epoch 11652/1000000, d_loss: -168.73880004882812,  g_loss: 27.060670852661133\n",
            "Training epoch 11653/1000000, d_loss: -66.34590148925781,  g_loss: 93.62570190429688\n",
            "Training epoch 11654/1000000, d_loss: -224.7071990966797,  g_loss: 56.38035583496094\n",
            "Training epoch 11655/1000000, d_loss: -31.229835510253906,  g_loss: 119.91448974609375\n",
            "Training epoch 11656/1000000, d_loss: -15.796642303466797,  g_loss: 142.00303649902344\n",
            "Training epoch 11657/1000000, d_loss: -213.9925079345703,  g_loss: 85.93225860595703\n",
            "Training epoch 11658/1000000, d_loss: -452.4702453613281,  g_loss: -40.41443634033203\n",
            "Training epoch 11659/1000000, d_loss: -288.26617431640625,  g_loss: 14.342361450195312\n",
            "Training epoch 11660/1000000, d_loss: -90.48125457763672,  g_loss: 294.252197265625\n",
            "Training epoch 11661/1000000, d_loss: -5.106086730957031,  g_loss: -20.94139862060547\n",
            "Training epoch 11662/1000000, d_loss: -254.4095458984375,  g_loss: -74.40878295898438\n",
            "Training epoch 11663/1000000, d_loss: -358.3753662109375,  g_loss: 59.60270309448242\n",
            "Training epoch 11664/1000000, d_loss: -157.4522705078125,  g_loss: 8.58481502532959\n",
            "Training epoch 11665/1000000, d_loss: -27.611103057861328,  g_loss: 95.638427734375\n",
            "Training epoch 11666/1000000, d_loss: -152.00390625,  g_loss: 95.46307373046875\n",
            "Training epoch 11667/1000000, d_loss: -404.500732421875,  g_loss: -48.43024444580078\n",
            "Training epoch 11668/1000000, d_loss: -108.37374114990234,  g_loss: 169.31195068359375\n",
            "Training epoch 11669/1000000, d_loss: -217.8452911376953,  g_loss: 62.299888610839844\n",
            "Training epoch 11670/1000000, d_loss: 61.137489318847656,  g_loss: 132.5163116455078\n",
            "Training epoch 11671/1000000, d_loss: -124.09464263916016,  g_loss: 149.48052978515625\n",
            "Training epoch 11672/1000000, d_loss: -237.1656494140625,  g_loss: 76.32177734375\n",
            "Training epoch 11673/1000000, d_loss: -431.5283203125,  g_loss: -17.416873931884766\n",
            "Training epoch 11674/1000000, d_loss: -101.13906860351562,  g_loss: 102.79042053222656\n",
            "Training epoch 11675/1000000, d_loss: -185.0059051513672,  g_loss: 2.7303619384765625\n",
            "Training epoch 11676/1000000, d_loss: -206.95059204101562,  g_loss: 49.95942687988281\n",
            "Training epoch 11677/1000000, d_loss: -60.20166015625,  g_loss: 93.8707275390625\n",
            "Training epoch 11678/1000000, d_loss: -419.5847473144531,  g_loss: -13.515860557556152\n",
            "Training epoch 11679/1000000, d_loss: -172.54248046875,  g_loss: -112.37327575683594\n",
            "Training epoch 11680/1000000, d_loss: -50.67182922363281,  g_loss: 185.1914825439453\n",
            "Training epoch 11681/1000000, d_loss: -69.31217956542969,  g_loss: 201.6842803955078\n",
            "Training epoch 11682/1000000, d_loss: -164.60720825195312,  g_loss: 98.30657958984375\n",
            "Training epoch 11683/1000000, d_loss: -13.851302146911621,  g_loss: 156.22264099121094\n",
            "Training epoch 11684/1000000, d_loss: -186.25640869140625,  g_loss: 278.98187255859375\n",
            "Training epoch 11685/1000000, d_loss: -89.55709838867188,  g_loss: 130.86834716796875\n",
            "Training epoch 11686/1000000, d_loss: -6.490853309631348,  g_loss: 128.33657836914062\n",
            "Training epoch 11687/1000000, d_loss: -237.24798583984375,  g_loss: 91.53404998779297\n",
            "Training epoch 11688/1000000, d_loss: -78.61056518554688,  g_loss: 144.171630859375\n",
            "Training epoch 11689/1000000, d_loss: -76.96731567382812,  g_loss: 138.7612762451172\n",
            "Training epoch 11690/1000000, d_loss: -88.9953842163086,  g_loss: 66.67031860351562\n",
            "Training epoch 11691/1000000, d_loss: -91.4365463256836,  g_loss: 65.39738464355469\n",
            "Training epoch 11692/1000000, d_loss: -45.51845169067383,  g_loss: 122.96156311035156\n",
            "Training epoch 11693/1000000, d_loss: -220.54495239257812,  g_loss: 71.20384216308594\n",
            "Training epoch 11694/1000000, d_loss: -50.261962890625,  g_loss: 142.12161254882812\n",
            "Training epoch 11695/1000000, d_loss: -109.11197662353516,  g_loss: 90.51652526855469\n",
            "Training epoch 11696/1000000, d_loss: -29.535829544067383,  g_loss: 65.0750732421875\n",
            "Training epoch 11697/1000000, d_loss: -29.816219329833984,  g_loss: 25.64439582824707\n",
            "Training epoch 11698/1000000, d_loss: -57.59364700317383,  g_loss: 64.57069396972656\n",
            "Training epoch 11699/1000000, d_loss: -35.38555145263672,  g_loss: 48.60334014892578\n",
            "Training epoch 11700/1000000, d_loss: -82.28495788574219,  g_loss: 95.77063751220703\n",
            "Training epoch 11701/1000000, d_loss: -64.16035461425781,  g_loss: 121.83828735351562\n",
            "Training epoch 11702/1000000, d_loss: -73.13186645507812,  g_loss: 94.6790542602539\n",
            "Training epoch 11703/1000000, d_loss: -733.1524047851562,  g_loss: -39.891441345214844\n",
            "Training epoch 11704/1000000, d_loss: -80.56363677978516,  g_loss: -2.7178843021392822\n",
            "Training epoch 11705/1000000, d_loss: -38.438880920410156,  g_loss: 25.01983070373535\n",
            "Training epoch 11706/1000000, d_loss: -642.2951049804688,  g_loss: -38.194496154785156\n",
            "Training epoch 11707/1000000, d_loss: -230.34303283691406,  g_loss: 41.11275100708008\n",
            "Training epoch 11708/1000000, d_loss: -237.32122802734375,  g_loss: -44.48764419555664\n",
            "Training epoch 11709/1000000, d_loss: 12.020254135131836,  g_loss: -45.08342742919922\n",
            "Training epoch 11710/1000000, d_loss: -577.2017822265625,  g_loss: -107.21739196777344\n",
            "Training epoch 11711/1000000, d_loss: -414.17572021484375,  g_loss: 30.898120880126953\n",
            "Training epoch 11712/1000000, d_loss: -275.6900329589844,  g_loss: -68.9933090209961\n",
            "Training epoch 11713/1000000, d_loss: -35.92210388183594,  g_loss: 143.75990295410156\n",
            "Training epoch 11714/1000000, d_loss: -203.08917236328125,  g_loss: 96.67028045654297\n",
            "Training epoch 11715/1000000, d_loss: -257.3397521972656,  g_loss: 104.91676330566406\n",
            "Training epoch 11716/1000000, d_loss: -178.048583984375,  g_loss: 110.96475219726562\n",
            "Training epoch 11717/1000000, d_loss: -179.54713439941406,  g_loss: 178.09576416015625\n",
            "Training epoch 11718/1000000, d_loss: -255.31240844726562,  g_loss: 370.65325927734375\n",
            "Training epoch 11719/1000000, d_loss: -335.96893310546875,  g_loss: 396.4376525878906\n",
            "Training epoch 11720/1000000, d_loss: -42.42420959472656,  g_loss: 217.69161987304688\n",
            "Training epoch 11721/1000000, d_loss: -319.697265625,  g_loss: 41.35600280761719\n",
            "Training epoch 11722/1000000, d_loss: -65.83694458007812,  g_loss: 46.29486846923828\n",
            "Training epoch 11723/1000000, d_loss: -59.62028503417969,  g_loss: 66.02599334716797\n",
            "Training epoch 11724/1000000, d_loss: -12.904190063476562,  g_loss: -48.6498908996582\n",
            "Training epoch 11725/1000000, d_loss: -213.1270294189453,  g_loss: -36.009986877441406\n",
            "Training epoch 11726/1000000, d_loss: -55.88145065307617,  g_loss: 19.82440948486328\n",
            "Training epoch 11727/1000000, d_loss: -113.22457122802734,  g_loss: 13.110655784606934\n",
            "Training epoch 11728/1000000, d_loss: -98.02017211914062,  g_loss: 98.79324340820312\n",
            "Training epoch 11729/1000000, d_loss: -111.72412109375,  g_loss: 51.80168914794922\n",
            "Training epoch 11730/1000000, d_loss: -330.4298400878906,  g_loss: 61.527183532714844\n",
            "Training epoch 11731/1000000, d_loss: -178.76324462890625,  g_loss: -30.095943450927734\n",
            "Training epoch 11732/1000000, d_loss: -52.838314056396484,  g_loss: 115.50682067871094\n",
            "Training epoch 11733/1000000, d_loss: -152.93251037597656,  g_loss: 92.67407989501953\n",
            "Training epoch 11734/1000000, d_loss: -194.40402221679688,  g_loss: 62.66419982910156\n",
            "Training epoch 11735/1000000, d_loss: -48.286075592041016,  g_loss: 59.60417175292969\n",
            "Training epoch 11736/1000000, d_loss: -111.66854858398438,  g_loss: 82.94967651367188\n",
            "Training epoch 11737/1000000, d_loss: -1342.4522705078125,  g_loss: -216.4880828857422\n",
            "Training epoch 11738/1000000, d_loss: -276.8194885253906,  g_loss: -229.027099609375\n",
            "Training epoch 11739/1000000, d_loss: -202.81686401367188,  g_loss: -333.9995422363281\n",
            "Training epoch 11740/1000000, d_loss: 38.571380615234375,  g_loss: 16.819255828857422\n",
            "Training epoch 11741/1000000, d_loss: -113.46488189697266,  g_loss: 79.4710693359375\n",
            "Training epoch 11742/1000000, d_loss: -87.32793426513672,  g_loss: 174.09213256835938\n",
            "Training epoch 11743/1000000, d_loss: -127.53345489501953,  g_loss: 302.62530517578125\n",
            "Training epoch 11744/1000000, d_loss: -144.65628051757812,  g_loss: 71.26707458496094\n",
            "Training epoch 11745/1000000, d_loss: -107.27961730957031,  g_loss: 115.38506317138672\n",
            "Training epoch 11746/1000000, d_loss: -89.23609924316406,  g_loss: 91.81845092773438\n",
            "Training epoch 11747/1000000, d_loss: -327.6318359375,  g_loss: -139.23455810546875\n",
            "Training epoch 11748/1000000, d_loss: -108.1127700805664,  g_loss: 264.8063659667969\n",
            "Training epoch 11749/1000000, d_loss: -75.61744689941406,  g_loss: -43.42788314819336\n",
            "Training epoch 11750/1000000, d_loss: -82.03392791748047,  g_loss: -5.433113098144531\n",
            "Training epoch 11751/1000000, d_loss: -186.63446044921875,  g_loss: 61.52304458618164\n",
            "Training epoch 11752/1000000, d_loss: -1117.37158203125,  g_loss: -140.21131896972656\n",
            "Training epoch 11753/1000000, d_loss: -12.248786926269531,  g_loss: -38.316768646240234\n",
            "Training epoch 11754/1000000, d_loss: 37.369415283203125,  g_loss: -91.27819061279297\n",
            "Training epoch 11755/1000000, d_loss: -95.24462890625,  g_loss: 8.561355590820312\n",
            "Training epoch 11756/1000000, d_loss: -182.2354736328125,  g_loss: 25.145904541015625\n",
            "Training epoch 11757/1000000, d_loss: -125.87201690673828,  g_loss: -6.08201789855957\n",
            "Training epoch 11758/1000000, d_loss: -163.9852294921875,  g_loss: 59.60272216796875\n",
            "Training epoch 11759/1000000, d_loss: -210.7388458251953,  g_loss: 44.70730972290039\n",
            "Training epoch 11760/1000000, d_loss: -187.54364013671875,  g_loss: -31.838577270507812\n",
            "Training epoch 11761/1000000, d_loss: -40.689666748046875,  g_loss: -12.184835433959961\n",
            "Training epoch 11762/1000000, d_loss: -131.45883178710938,  g_loss: 4.270158767700195\n",
            "Training epoch 11763/1000000, d_loss: 6.5042572021484375,  g_loss: -14.064886093139648\n",
            "Training epoch 11764/1000000, d_loss: -48.018367767333984,  g_loss: 29.462617874145508\n",
            "Training epoch 11765/1000000, d_loss: -412.0384826660156,  g_loss: -73.0399169921875\n",
            "Training epoch 11766/1000000, d_loss: -88.980712890625,  g_loss: -22.435455322265625\n",
            "Training epoch 11767/1000000, d_loss: -135.92691040039062,  g_loss: 51.990596771240234\n",
            "Training epoch 11768/1000000, d_loss: -145.94058227539062,  g_loss: 87.0914077758789\n",
            "Training epoch 11769/1000000, d_loss: -145.81796264648438,  g_loss: 123.81302642822266\n",
            "Training epoch 11770/1000000, d_loss: -84.52462768554688,  g_loss: 115.19065856933594\n",
            "Training epoch 11771/1000000, d_loss: -103.0535888671875,  g_loss: 51.113487243652344\n",
            "Training epoch 11772/1000000, d_loss: -169.61880493164062,  g_loss: 102.86662292480469\n",
            "Training epoch 11773/1000000, d_loss: -81.8194351196289,  g_loss: 68.30982971191406\n",
            "Training epoch 11774/1000000, d_loss: -82.3934326171875,  g_loss: 54.25852584838867\n",
            "Training epoch 11775/1000000, d_loss: -187.6932373046875,  g_loss: 25.0538387298584\n",
            "Training epoch 11776/1000000, d_loss: -99.89636993408203,  g_loss: 28.432910919189453\n",
            "Training epoch 11777/1000000, d_loss: -148.8690643310547,  g_loss: -23.561695098876953\n",
            "Training epoch 11778/1000000, d_loss: -106.32405853271484,  g_loss: 43.70747756958008\n",
            "Training epoch 11779/1000000, d_loss: -438.8883972167969,  g_loss: 6.093969821929932\n",
            "Training epoch 11780/1000000, d_loss: -48.54170227050781,  g_loss: 72.43148040771484\n",
            "Training epoch 11781/1000000, d_loss: -76.64094543457031,  g_loss: 38.42284393310547\n",
            "Training epoch 11782/1000000, d_loss: -103.94852447509766,  g_loss: 2.7522783279418945\n",
            "Training epoch 11783/1000000, d_loss: -108.20979309082031,  g_loss: 53.470218658447266\n",
            "Training epoch 11784/1000000, d_loss: -85.44793701171875,  g_loss: 17.819297790527344\n",
            "Training epoch 11785/1000000, d_loss: -189.99037170410156,  g_loss: 73.56538391113281\n",
            "Training epoch 11786/1000000, d_loss: -158.82730102539062,  g_loss: 281.9136657714844\n",
            "Training epoch 11787/1000000, d_loss: -116.14959716796875,  g_loss: 71.48097229003906\n",
            "Training epoch 11788/1000000, d_loss: -53.87748718261719,  g_loss: 59.423519134521484\n",
            "Training epoch 11789/1000000, d_loss: -145.11766052246094,  g_loss: 128.63043212890625\n",
            "Training epoch 11790/1000000, d_loss: -236.250244140625,  g_loss: 68.3543701171875\n",
            "Training epoch 11791/1000000, d_loss: -61.95530700683594,  g_loss: 100.89602661132812\n",
            "Training epoch 11792/1000000, d_loss: -53.364139556884766,  g_loss: 105.99551391601562\n",
            "Training epoch 11793/1000000, d_loss: -52.759117126464844,  g_loss: 69.42351531982422\n",
            "Training epoch 11794/1000000, d_loss: -71.75392150878906,  g_loss: 79.08224487304688\n",
            "Training epoch 11795/1000000, d_loss: -890.5294799804688,  g_loss: -152.80960083007812\n",
            "Training epoch 11796/1000000, d_loss: 369.3458251953125,  g_loss: -33.856597900390625\n",
            "Training epoch 11797/1000000, d_loss: -260.8087158203125,  g_loss: -79.41230773925781\n",
            "Training epoch 11798/1000000, d_loss: -180.6759490966797,  g_loss: 135.32049560546875\n",
            "Training epoch 11799/1000000, d_loss: -82.54182434082031,  g_loss: 232.81719970703125\n",
            "Training epoch 11800/1000000, d_loss: 225.99246215820312,  g_loss: 61.13761901855469\n",
            "Training epoch 11801/1000000, d_loss: -59.407691955566406,  g_loss: 27.255084991455078\n",
            "Training epoch 11802/1000000, d_loss: -156.25271606445312,  g_loss: -36.000389099121094\n",
            "Training epoch 11803/1000000, d_loss: -83.87338256835938,  g_loss: -17.033493041992188\n",
            "Training epoch 11804/1000000, d_loss: -117.69340515136719,  g_loss: 98.77809143066406\n",
            "Training epoch 11805/1000000, d_loss: -35.94822311401367,  g_loss: 93.0899658203125\n",
            "Training epoch 11806/1000000, d_loss: 27.539382934570312,  g_loss: 99.003173828125\n",
            "Training epoch 11807/1000000, d_loss: -29.924518585205078,  g_loss: 47.316795349121094\n",
            "Training epoch 11808/1000000, d_loss: -96.91555786132812,  g_loss: 87.66543579101562\n",
            "Training epoch 11809/1000000, d_loss: -177.88436889648438,  g_loss: 63.83295822143555\n",
            "Training epoch 11810/1000000, d_loss: -229.27635192871094,  g_loss: 412.370361328125\n",
            "Training epoch 11811/1000000, d_loss: -287.108642578125,  g_loss: 34.59920120239258\n",
            "Training epoch 11812/1000000, d_loss: -61.730796813964844,  g_loss: 41.099853515625\n",
            "Training epoch 11813/1000000, d_loss: -612.5401000976562,  g_loss: -102.30850982666016\n",
            "Training epoch 11814/1000000, d_loss: -61.87670135498047,  g_loss: 109.68988037109375\n",
            "Training epoch 11815/1000000, d_loss: 17.553245544433594,  g_loss: 86.58476257324219\n",
            "Training epoch 11816/1000000, d_loss: -53.96025085449219,  g_loss: 66.30805206298828\n",
            "Training epoch 11817/1000000, d_loss: -437.6337585449219,  g_loss: -63.88532257080078\n",
            "Training epoch 11818/1000000, d_loss: -120.35968017578125,  g_loss: 73.49246215820312\n",
            "Training epoch 11819/1000000, d_loss: -71.42617797851562,  g_loss: 41.04521942138672\n",
            "Training epoch 11820/1000000, d_loss: -89.8214111328125,  g_loss: 95.25593566894531\n",
            "Training epoch 11821/1000000, d_loss: -204.42063903808594,  g_loss: 264.58203125\n",
            "Training epoch 11822/1000000, d_loss: -125.84281921386719,  g_loss: 42.0421028137207\n",
            "Training epoch 11823/1000000, d_loss: -107.21456909179688,  g_loss: 66.1937026977539\n",
            "Training epoch 11824/1000000, d_loss: -563.6997680664062,  g_loss: -194.87435913085938\n",
            "Training epoch 11825/1000000, d_loss: -215.98416137695312,  g_loss: -99.86009216308594\n",
            "Training epoch 11826/1000000, d_loss: 14.745773315429688,  g_loss: -44.6782341003418\n",
            "Training epoch 11827/1000000, d_loss: -35.63596725463867,  g_loss: 125.6640625\n",
            "Training epoch 11828/1000000, d_loss: -196.5111541748047,  g_loss: 165.09780883789062\n",
            "Training epoch 11829/1000000, d_loss: -319.71435546875,  g_loss: 273.5999450683594\n",
            "Training epoch 11830/1000000, d_loss: -63.7015380859375,  g_loss: 44.253787994384766\n",
            "Training epoch 11831/1000000, d_loss: -39.470157623291016,  g_loss: 109.02560424804688\n",
            "Training epoch 11832/1000000, d_loss: 31.062637329101562,  g_loss: 41.858585357666016\n",
            "Training epoch 11833/1000000, d_loss: -151.17581176757812,  g_loss: 213.2501220703125\n",
            "Training epoch 11834/1000000, d_loss: -105.06825256347656,  g_loss: 156.2218780517578\n",
            "Training epoch 11835/1000000, d_loss: 1657.840087890625,  g_loss: 17.16797637939453\n",
            "Training epoch 11836/1000000, d_loss: -66.74362182617188,  g_loss: 55.44706344604492\n",
            "Training epoch 11837/1000000, d_loss: -72.23989868164062,  g_loss: 46.83768081665039\n",
            "Training epoch 11838/1000000, d_loss: -517.0169067382812,  g_loss: 14.285575866699219\n",
            "Training epoch 11839/1000000, d_loss: -133.7644500732422,  g_loss: 194.91683959960938\n",
            "Training epoch 11840/1000000, d_loss: -76.99226379394531,  g_loss: 65.93971252441406\n",
            "Training epoch 11841/1000000, d_loss: -322.2470703125,  g_loss: 72.0629653930664\n",
            "Training epoch 11842/1000000, d_loss: -1068.486328125,  g_loss: -95.5772476196289\n",
            "Training epoch 11843/1000000, d_loss: -2525.0546875,  g_loss: -863.290283203125\n",
            "Training epoch 11844/1000000, d_loss: 1503.9505615234375,  g_loss: -161.61257934570312\n",
            "Training epoch 11845/1000000, d_loss: 205.06002807617188,  g_loss: -47.06073760986328\n",
            "Training epoch 11846/1000000, d_loss: 325.18707275390625,  g_loss: -48.65752029418945\n",
            "Training epoch 11847/1000000, d_loss: 510.9883728027344,  g_loss: -14.52290153503418\n",
            "Training epoch 11848/1000000, d_loss: -92.98193359375,  g_loss: 31.245929718017578\n",
            "Training epoch 11849/1000000, d_loss: -273.1070556640625,  g_loss: 256.4757995605469\n",
            "Training epoch 11850/1000000, d_loss: -36.05125427246094,  g_loss: -91.01471710205078\n",
            "Training epoch 11851/1000000, d_loss: -86.44879150390625,  g_loss: 69.52702331542969\n",
            "Training epoch 11852/1000000, d_loss: -43.42486572265625,  g_loss: 66.77932739257812\n",
            "Training epoch 11853/1000000, d_loss: -15.67681884765625,  g_loss: -30.467144012451172\n",
            "Training epoch 11854/1000000, d_loss: -167.07275390625,  g_loss: -10.874797821044922\n",
            "Training epoch 11855/1000000, d_loss: -237.73545837402344,  g_loss: -118.64640045166016\n",
            "Training epoch 11856/1000000, d_loss: 33.18534851074219,  g_loss: -111.19013977050781\n",
            "Training epoch 11857/1000000, d_loss: -120.86416625976562,  g_loss: 2.2512130737304688\n",
            "Training epoch 11858/1000000, d_loss: -42.85344314575195,  g_loss: -233.04647827148438\n",
            "Training epoch 11859/1000000, d_loss: -233.58888244628906,  g_loss: -141.9918212890625\n",
            "Training epoch 11860/1000000, d_loss: -816.168212890625,  g_loss: -415.7579040527344\n",
            "Training epoch 11861/1000000, d_loss: 43.97602081298828,  g_loss: -51.27436065673828\n",
            "Training epoch 11862/1000000, d_loss: 536.4052124023438,  g_loss: -61.102508544921875\n",
            "Training epoch 11863/1000000, d_loss: -34.64237594604492,  g_loss: -26.462535858154297\n",
            "Training epoch 11864/1000000, d_loss: -31.66115951538086,  g_loss: 95.77369689941406\n",
            "Training epoch 11865/1000000, d_loss: -17.583105087280273,  g_loss: 22.422258377075195\n",
            "Training epoch 11866/1000000, d_loss: -48.48368835449219,  g_loss: 209.16046142578125\n",
            "Training epoch 11867/1000000, d_loss: -83.98680114746094,  g_loss: 91.19645690917969\n",
            "Training epoch 11868/1000000, d_loss: -160.42022705078125,  g_loss: 143.50636291503906\n",
            "Training epoch 11869/1000000, d_loss: -253.73387145996094,  g_loss: -63.68968963623047\n",
            "Training epoch 11870/1000000, d_loss: -96.52132415771484,  g_loss: 80.40369415283203\n",
            "Training epoch 11871/1000000, d_loss: -78.64187622070312,  g_loss: 89.77632141113281\n",
            "Training epoch 11872/1000000, d_loss: -190.88760375976562,  g_loss: 327.548095703125\n",
            "Training epoch 11873/1000000, d_loss: -103.36125183105469,  g_loss: 32.68854904174805\n",
            "Training epoch 11874/1000000, d_loss: -147.72940063476562,  g_loss: 299.7179260253906\n",
            "Training epoch 11875/1000000, d_loss: -192.79734802246094,  g_loss: 80.80885314941406\n",
            "Training epoch 11876/1000000, d_loss: -2.76690673828125,  g_loss: 108.50990295410156\n",
            "Training epoch 11877/1000000, d_loss: -51.218563079833984,  g_loss: 102.7227554321289\n",
            "Training epoch 11878/1000000, d_loss: -158.18728637695312,  g_loss: 261.9349365234375\n",
            "Training epoch 11879/1000000, d_loss: -48.10289001464844,  g_loss: 69.42101287841797\n",
            "Training epoch 11880/1000000, d_loss: -130.7741241455078,  g_loss: 121.32615661621094\n",
            "Training epoch 11881/1000000, d_loss: -171.21255493164062,  g_loss: 110.93760681152344\n",
            "Training epoch 11882/1000000, d_loss: -72.54804992675781,  g_loss: 158.67544555664062\n",
            "Training epoch 11883/1000000, d_loss: -84.76338195800781,  g_loss: 122.03105163574219\n",
            "Training epoch 11884/1000000, d_loss: -19.40218734741211,  g_loss: 66.9224853515625\n",
            "Training epoch 11885/1000000, d_loss: -62.31559753417969,  g_loss: 100.56283569335938\n",
            "Training epoch 11886/1000000, d_loss: -94.70445251464844,  g_loss: 64.78622436523438\n",
            "Training epoch 11887/1000000, d_loss: -127.25218200683594,  g_loss: 70.11239624023438\n",
            "Training epoch 11888/1000000, d_loss: -127.48033142089844,  g_loss: 10.333134651184082\n",
            "Training epoch 11889/1000000, d_loss: -223.25694274902344,  g_loss: -17.52979278564453\n",
            "Training epoch 11890/1000000, d_loss: 5808.62353515625,  g_loss: 75.80078125\n",
            "Training epoch 11891/1000000, d_loss: -77.80442810058594,  g_loss: 73.19000244140625\n",
            "Training epoch 11892/1000000, d_loss: -195.0608673095703,  g_loss: 86.95533752441406\n",
            "Training epoch 11893/1000000, d_loss: -121.15081024169922,  g_loss: 43.78142547607422\n",
            "Training epoch 11894/1000000, d_loss: -106.78092193603516,  g_loss: 70.91226196289062\n",
            "Training epoch 11895/1000000, d_loss: -79.12844848632812,  g_loss: 78.13007354736328\n",
            "Training epoch 11896/1000000, d_loss: -167.53213500976562,  g_loss: 102.7977523803711\n",
            "Training epoch 11897/1000000, d_loss: -227.6507568359375,  g_loss: 110.71070861816406\n",
            "Training epoch 11898/1000000, d_loss: -76.76299285888672,  g_loss: 62.418983459472656\n",
            "Training epoch 11899/1000000, d_loss: 43.40660095214844,  g_loss: 57.108985900878906\n",
            "Training epoch 11900/1000000, d_loss: -155.07192993164062,  g_loss: 65.08187866210938\n",
            "Training epoch 11901/1000000, d_loss: -643.4579467773438,  g_loss: -5.959718704223633\n",
            "Training epoch 11902/1000000, d_loss: 29.213253021240234,  g_loss: 59.41441345214844\n",
            "Training epoch 11903/1000000, d_loss: -82.76458740234375,  g_loss: 55.520790100097656\n",
            "Training epoch 11904/1000000, d_loss: 12.680328369140625,  g_loss: 92.32818603515625\n",
            "Training epoch 11905/1000000, d_loss: -138.83343505859375,  g_loss: 68.57162475585938\n",
            "Training epoch 11906/1000000, d_loss: -4.079582214355469,  g_loss: 62.1873893737793\n",
            "Training epoch 11907/1000000, d_loss: -105.7726058959961,  g_loss: 47.774688720703125\n",
            "Training epoch 11908/1000000, d_loss: -141.8484344482422,  g_loss: 23.72234344482422\n",
            "Training epoch 11909/1000000, d_loss: -131.541259765625,  g_loss: 190.12718200683594\n",
            "Training epoch 11910/1000000, d_loss: -472.70123291015625,  g_loss: -0.9985775947570801\n",
            "Training epoch 11911/1000000, d_loss: -66.0352783203125,  g_loss: 35.660072326660156\n",
            "Training epoch 11912/1000000, d_loss: -169.82589721679688,  g_loss: 31.546348571777344\n",
            "Training epoch 11913/1000000, d_loss: -410.1037292480469,  g_loss: 8.09463119506836\n",
            "Training epoch 11914/1000000, d_loss: -79.5220718383789,  g_loss: 62.97588348388672\n",
            "Training epoch 11915/1000000, d_loss: -89.65384674072266,  g_loss: 1.6688270568847656\n",
            "Training epoch 11916/1000000, d_loss: -7.1381988525390625,  g_loss: 54.323211669921875\n",
            "Training epoch 11917/1000000, d_loss: -443.62640380859375,  g_loss: 30.37339973449707\n",
            "Training epoch 11918/1000000, d_loss: -538.3292846679688,  g_loss: -64.57894897460938\n",
            "Training epoch 11919/1000000, d_loss: -2.8072214126586914,  g_loss: 15.814464569091797\n",
            "Training epoch 11920/1000000, d_loss: -31.96805191040039,  g_loss: 127.92425537109375\n",
            "Training epoch 11921/1000000, d_loss: -1587.5567626953125,  g_loss: -330.1756591796875\n",
            "Training epoch 11922/1000000, d_loss: 37.72856521606445,  g_loss: -34.461063385009766\n",
            "Training epoch 11923/1000000, d_loss: 11.472892761230469,  g_loss: 50.28594207763672\n",
            "Training epoch 11924/1000000, d_loss: -246.93252563476562,  g_loss: 472.63385009765625\n",
            "Training epoch 11925/1000000, d_loss: -79.96284484863281,  g_loss: 69.5723648071289\n",
            "Training epoch 11926/1000000, d_loss: -498.42431640625,  g_loss: 69.607421875\n",
            "Training epoch 11927/1000000, d_loss: -366.4195251464844,  g_loss: 18.6971435546875\n",
            "Training epoch 11928/1000000, d_loss: -60.55320739746094,  g_loss: 61.042015075683594\n",
            "Training epoch 11929/1000000, d_loss: -102.32976531982422,  g_loss: 77.2770767211914\n",
            "Training epoch 11930/1000000, d_loss: -95.4876937866211,  g_loss: 50.5366096496582\n",
            "Training epoch 11931/1000000, d_loss: -230.23968505859375,  g_loss: 44.5184211730957\n",
            "Training epoch 11932/1000000, d_loss: -152.62326049804688,  g_loss: 164.61285400390625\n",
            "Training epoch 11933/1000000, d_loss: -670.91162109375,  g_loss: -85.11734008789062\n",
            "Training epoch 11934/1000000, d_loss: -257.0318603515625,  g_loss: -54.00334167480469\n",
            "Training epoch 11935/1000000, d_loss: -99.80301666259766,  g_loss: -26.443042755126953\n",
            "Training epoch 11936/1000000, d_loss: -152.75125122070312,  g_loss: -123.16234588623047\n",
            "Training epoch 11937/1000000, d_loss: -105.55331420898438,  g_loss: -82.30683898925781\n",
            "Training epoch 11938/1000000, d_loss: -211.64895629882812,  g_loss: -139.22450256347656\n",
            "Training epoch 11939/1000000, d_loss: -36.00579071044922,  g_loss: 47.337432861328125\n",
            "Training epoch 11940/1000000, d_loss: -99.65876007080078,  g_loss: 28.477611541748047\n",
            "Training epoch 11941/1000000, d_loss: -491.035888671875,  g_loss: -224.86207580566406\n",
            "Training epoch 11942/1000000, d_loss: -116.04109191894531,  g_loss: 194.44422912597656\n",
            "Training epoch 11943/1000000, d_loss: -78.4695053100586,  g_loss: 156.90086364746094\n",
            "Training epoch 11944/1000000, d_loss: -170.81576538085938,  g_loss: 231.90933227539062\n",
            "Training epoch 11945/1000000, d_loss: -637.968505859375,  g_loss: 46.065025329589844\n",
            "Training epoch 11946/1000000, d_loss: -76.36817932128906,  g_loss: 94.00491333007812\n",
            "Training epoch 11947/1000000, d_loss: -159.94378662109375,  g_loss: 327.1202087402344\n",
            "Training epoch 11948/1000000, d_loss: -102.15031433105469,  g_loss: 87.33195495605469\n",
            "Training epoch 11949/1000000, d_loss: -5.604305267333984,  g_loss: 31.646631240844727\n",
            "Training epoch 11950/1000000, d_loss: -166.3367156982422,  g_loss: 138.1123046875\n",
            "Training epoch 11951/1000000, d_loss: -327.05767822265625,  g_loss: -11.005363464355469\n",
            "Training epoch 11952/1000000, d_loss: -712.080322265625,  g_loss: -301.6089172363281\n",
            "Training epoch 11953/1000000, d_loss: -58.221580505371094,  g_loss: -46.5278434753418\n",
            "Training epoch 11954/1000000, d_loss: -197.6746063232422,  g_loss: -46.10258483886719\n",
            "Training epoch 11955/1000000, d_loss: 17.57175064086914,  g_loss: 35.669708251953125\n",
            "Training epoch 11956/1000000, d_loss: -66.13597106933594,  g_loss: 115.45530700683594\n",
            "Training epoch 11957/1000000, d_loss: -179.394775390625,  g_loss: 216.653564453125\n",
            "Training epoch 11958/1000000, d_loss: -312.7541809082031,  g_loss: 433.2444152832031\n",
            "Training epoch 11959/1000000, d_loss: -12.87446403503418,  g_loss: 0.5630941390991211\n",
            "Training epoch 11960/1000000, d_loss: -85.8012466430664,  g_loss: 80.97727966308594\n",
            "Training epoch 11961/1000000, d_loss: -88.48020935058594,  g_loss: 0.2731001377105713\n",
            "Training epoch 11962/1000000, d_loss: -83.959716796875,  g_loss: 94.72087860107422\n",
            "Training epoch 11963/1000000, d_loss: 17.459686279296875,  g_loss: 37.215362548828125\n",
            "Training epoch 11964/1000000, d_loss: -32.462867736816406,  g_loss: 100.46907043457031\n",
            "Training epoch 11965/1000000, d_loss: -92.8394775390625,  g_loss: 103.86602020263672\n",
            "Training epoch 11966/1000000, d_loss: -63.12559509277344,  g_loss: 82.32008361816406\n",
            "Training epoch 11967/1000000, d_loss: -77.63130187988281,  g_loss: 103.58988952636719\n",
            "Training epoch 11968/1000000, d_loss: -282.29205322265625,  g_loss: 38.38698196411133\n",
            "Training epoch 11969/1000000, d_loss: -2249.427978515625,  g_loss: -361.462890625\n",
            "Training epoch 11970/1000000, d_loss: 513.8466186523438,  g_loss: -134.67691040039062\n",
            "Training epoch 11971/1000000, d_loss: 114.48069763183594,  g_loss: -10.002069473266602\n",
            "Training epoch 11972/1000000, d_loss: -494.67474365234375,  g_loss: -69.75651550292969\n",
            "Training epoch 11973/1000000, d_loss: -9.093315124511719,  g_loss: -63.261775970458984\n",
            "Training epoch 11974/1000000, d_loss: -0.9362030029296875,  g_loss: -187.52871704101562\n",
            "Training epoch 11975/1000000, d_loss: -6.129974365234375,  g_loss: -21.527416229248047\n",
            "Training epoch 11976/1000000, d_loss: 20.389198303222656,  g_loss: -15.217090606689453\n",
            "Training epoch 11977/1000000, d_loss: 142.9958038330078,  g_loss: -33.06840515136719\n",
            "Training epoch 11978/1000000, d_loss: -179.71954345703125,  g_loss: 56.054588317871094\n",
            "Training epoch 11979/1000000, d_loss: -43.10796356201172,  g_loss: -35.5152702331543\n",
            "Training epoch 11980/1000000, d_loss: -227.34837341308594,  g_loss: 115.83221435546875\n",
            "Training epoch 11981/1000000, d_loss: -140.8880615234375,  g_loss: -28.043655395507812\n",
            "Training epoch 11982/1000000, d_loss: -122.76828002929688,  g_loss: 15.92959976196289\n",
            "Training epoch 11983/1000000, d_loss: -163.14202880859375,  g_loss: -152.6579132080078\n",
            "Training epoch 11984/1000000, d_loss: -162.65628051757812,  g_loss: -31.570091247558594\n",
            "Training epoch 11985/1000000, d_loss: -151.19314575195312,  g_loss: -4.297773361206055\n",
            "Training epoch 11986/1000000, d_loss: -374.2406921386719,  g_loss: 386.9505310058594\n",
            "Training epoch 11987/1000000, d_loss: -113.73216247558594,  g_loss: 30.60939598083496\n",
            "Training epoch 11988/1000000, d_loss: 10129.9013671875,  g_loss: 9.927668571472168\n",
            "Training epoch 11989/1000000, d_loss: -33.40484619140625,  g_loss: 40.82244110107422\n",
            "Training epoch 11990/1000000, d_loss: -57.933616638183594,  g_loss: 32.70660400390625\n",
            "Training epoch 11991/1000000, d_loss: -108.47161865234375,  g_loss: 57.463722229003906\n",
            "Training epoch 11992/1000000, d_loss: -162.450927734375,  g_loss: 25.699359893798828\n",
            "Training epoch 11993/1000000, d_loss: -79.25243377685547,  g_loss: 54.2939338684082\n",
            "Training epoch 11994/1000000, d_loss: -64.75239562988281,  g_loss: 74.32281494140625\n",
            "Training epoch 11995/1000000, d_loss: -207.93515014648438,  g_loss: 44.323211669921875\n",
            "Training epoch 11996/1000000, d_loss: -163.98370361328125,  g_loss: 10.927119255065918\n",
            "Training epoch 11997/1000000, d_loss: -411.8656311035156,  g_loss: 36.57066345214844\n",
            "Training epoch 11998/1000000, d_loss: -107.30015563964844,  g_loss: 61.15321350097656\n",
            "Training epoch 11999/1000000, d_loss: 4.185966491699219,  g_loss: 42.20645523071289\n",
            "Training epoch 12000/1000000, d_loss: -72.56813049316406,  g_loss: 84.48900604248047\n",
            "Training epoch 12001/1000000, d_loss: -32.53563690185547,  g_loss: 96.45748138427734\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 62/62 [00:00<00:00, 77.48it/s]\n",
            "Meshing: 100%|██████████| 23940/23940 [00:08<00:00, 2783.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_12001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_12001/assets\n",
            "Training epoch 12002/1000000, d_loss: -150.17507934570312,  g_loss: 34.69377899169922\n",
            "Training epoch 12003/1000000, d_loss: -89.2031021118164,  g_loss: 122.32616424560547\n",
            "Training epoch 12004/1000000, d_loss: -233.0905303955078,  g_loss: 373.55755615234375\n",
            "Training epoch 12005/1000000, d_loss: -167.2073211669922,  g_loss: 51.211944580078125\n",
            "Training epoch 12006/1000000, d_loss: -110.88912963867188,  g_loss: -30.621734619140625\n",
            "Training epoch 12007/1000000, d_loss: -118.91706848144531,  g_loss: -3.9937171936035156\n",
            "Training epoch 12008/1000000, d_loss: -221.2474822998047,  g_loss: -38.674949645996094\n",
            "Training epoch 12009/1000000, d_loss: -456.19024658203125,  g_loss: 0.9542427062988281\n",
            "Training epoch 12010/1000000, d_loss: -88.46590423583984,  g_loss: 41.760948181152344\n",
            "Training epoch 12011/1000000, d_loss: -284.8278503417969,  g_loss: 38.65879440307617\n",
            "Training epoch 12012/1000000, d_loss: -1643.692138671875,  g_loss: -99.64075469970703\n",
            "Training epoch 12013/1000000, d_loss: 244.2054443359375,  g_loss: 85.89369201660156\n",
            "Training epoch 12014/1000000, d_loss: -21.197662353515625,  g_loss: 120.67479705810547\n",
            "Training epoch 12015/1000000, d_loss: -143.89678955078125,  g_loss: 46.716705322265625\n",
            "Training epoch 12016/1000000, d_loss: 12.002792358398438,  g_loss: 82.92302703857422\n",
            "Training epoch 12017/1000000, d_loss: -35.036598205566406,  g_loss: 68.35971069335938\n",
            "Training epoch 12018/1000000, d_loss: -87.67735290527344,  g_loss: 52.833229064941406\n",
            "Training epoch 12019/1000000, d_loss: -945.1844482421875,  g_loss: -96.74363708496094\n",
            "Training epoch 12020/1000000, d_loss: 3387.456787109375,  g_loss: 53.64918518066406\n",
            "Training epoch 12021/1000000, d_loss: -13.311727523803711,  g_loss: 93.30441284179688\n",
            "Training epoch 12022/1000000, d_loss: 3.9668731689453125,  g_loss: 92.23499298095703\n",
            "Training epoch 12023/1000000, d_loss: -171.3471221923828,  g_loss: 32.20839309692383\n",
            "Training epoch 12024/1000000, d_loss: -99.16773223876953,  g_loss: 34.86600875854492\n",
            "Training epoch 12025/1000000, d_loss: -13.66949462890625,  g_loss: 69.38629150390625\n",
            "Training epoch 12026/1000000, d_loss: -207.4571990966797,  g_loss: 41.75242614746094\n",
            "Training epoch 12027/1000000, d_loss: -77.91544342041016,  g_loss: 54.66637420654297\n",
            "Training epoch 12028/1000000, d_loss: -218.92550659179688,  g_loss: 39.25642395019531\n",
            "Training epoch 12029/1000000, d_loss: -204.52110290527344,  g_loss: -12.416851043701172\n",
            "Training epoch 12030/1000000, d_loss: -161.56167602539062,  g_loss: -188.47982788085938\n",
            "Training epoch 12031/1000000, d_loss: 123.48367309570312,  g_loss: 77.41665649414062\n",
            "Training epoch 12032/1000000, d_loss: -133.0152587890625,  g_loss: 79.00947570800781\n",
            "Training epoch 12033/1000000, d_loss: -13.39836311340332,  g_loss: 55.99958801269531\n",
            "Training epoch 12034/1000000, d_loss: -836.9448852539062,  g_loss: -83.15827178955078\n",
            "Training epoch 12035/1000000, d_loss: -27.462318420410156,  g_loss: 61.782920837402344\n",
            "Training epoch 12036/1000000, d_loss: -18.40308380126953,  g_loss: 160.59036254882812\n",
            "Training epoch 12037/1000000, d_loss: -96.46012115478516,  g_loss: 85.07537841796875\n",
            "Training epoch 12038/1000000, d_loss: -119.05807495117188,  g_loss: 145.04135131835938\n",
            "Training epoch 12039/1000000, d_loss: -85.93925476074219,  g_loss: 62.35247039794922\n",
            "Training epoch 12040/1000000, d_loss: -99.5573501586914,  g_loss: 191.44866943359375\n",
            "Training epoch 12041/1000000, d_loss: -80.4603500366211,  g_loss: 70.64772033691406\n",
            "Training epoch 12042/1000000, d_loss: 263.0907287597656,  g_loss: 123.54227447509766\n",
            "Training epoch 12043/1000000, d_loss: -81.45609283447266,  g_loss: 37.14921951293945\n",
            "Training epoch 12044/1000000, d_loss: -235.65060424804688,  g_loss: 73.6749496459961\n",
            "Training epoch 12045/1000000, d_loss: -263.0503845214844,  g_loss: 273.5718688964844\n",
            "Training epoch 12046/1000000, d_loss: -177.88787841796875,  g_loss: 550.0999145507812\n",
            "Training epoch 12047/1000000, d_loss: -43.332252502441406,  g_loss: 47.55169677734375\n",
            "Training epoch 12048/1000000, d_loss: -82.94351196289062,  g_loss: 76.0102767944336\n",
            "Training epoch 12049/1000000, d_loss: -120.31787109375,  g_loss: 25.875782012939453\n",
            "Training epoch 12050/1000000, d_loss: -137.07785034179688,  g_loss: 52.144691467285156\n",
            "Training epoch 12051/1000000, d_loss: -74.55084991455078,  g_loss: 143.25729370117188\n",
            "Training epoch 12052/1000000, d_loss: -48.97203826904297,  g_loss: 98.36734771728516\n",
            "Training epoch 12053/1000000, d_loss: -67.78730773925781,  g_loss: 190.53057861328125\n",
            "Training epoch 12054/1000000, d_loss: -75.31462860107422,  g_loss: 154.78854370117188\n",
            "Training epoch 12055/1000000, d_loss: -53.719520568847656,  g_loss: 75.640380859375\n",
            "Training epoch 12056/1000000, d_loss: -158.0592041015625,  g_loss: 107.78811645507812\n",
            "Training epoch 12057/1000000, d_loss: -110.86701965332031,  g_loss: 54.551185607910156\n",
            "Training epoch 12058/1000000, d_loss: -10.729682922363281,  g_loss: 155.85789489746094\n",
            "Training epoch 12059/1000000, d_loss: -155.4232177734375,  g_loss: 152.56361389160156\n",
            "Training epoch 12060/1000000, d_loss: -1426.0496826171875,  g_loss: -47.14119338989258\n",
            "Training epoch 12061/1000000, d_loss: 62.58687973022461,  g_loss: -38.691368103027344\n",
            "Training epoch 12062/1000000, d_loss: -166.01805114746094,  g_loss: -46.087860107421875\n",
            "Training epoch 12063/1000000, d_loss: 53.32066345214844,  g_loss: 77.0674057006836\n",
            "Training epoch 12064/1000000, d_loss: -184.8368377685547,  g_loss: 50.42131805419922\n",
            "Training epoch 12065/1000000, d_loss: -91.26026916503906,  g_loss: 84.06828308105469\n",
            "Training epoch 12066/1000000, d_loss: -417.6566467285156,  g_loss: -23.096458435058594\n",
            "Training epoch 12067/1000000, d_loss: -50.2268180847168,  g_loss: 44.03373336791992\n",
            "Training epoch 12068/1000000, d_loss: -115.09123992919922,  g_loss: 42.952056884765625\n",
            "Training epoch 12069/1000000, d_loss: -497.3199768066406,  g_loss: -78.58512878417969\n",
            "Training epoch 12070/1000000, d_loss: -233.8778533935547,  g_loss: -154.57810974121094\n",
            "Training epoch 12071/1000000, d_loss: -68.01670837402344,  g_loss: 43.17291259765625\n",
            "Training epoch 12072/1000000, d_loss: -71.80935668945312,  g_loss: 100.3306655883789\n",
            "Training epoch 12073/1000000, d_loss: -311.80145263671875,  g_loss: 47.92279815673828\n",
            "Training epoch 12074/1000000, d_loss: -177.49136352539062,  g_loss: 261.5093994140625\n",
            "Training epoch 12075/1000000, d_loss: -170.67733764648438,  g_loss: 78.28229522705078\n",
            "Training epoch 12076/1000000, d_loss: -55.59074783325195,  g_loss: 106.17436981201172\n",
            "Training epoch 12077/1000000, d_loss: -240.81167602539062,  g_loss: -32.78904342651367\n",
            "Training epoch 12078/1000000, d_loss: 26.686309814453125,  g_loss: 93.0709228515625\n",
            "Training epoch 12079/1000000, d_loss: -90.52714538574219,  g_loss: 86.41157531738281\n",
            "Training epoch 12080/1000000, d_loss: -98.2708511352539,  g_loss: 68.15587615966797\n",
            "Training epoch 12081/1000000, d_loss: -29.8505802154541,  g_loss: 44.64527130126953\n",
            "Training epoch 12082/1000000, d_loss: -72.18458557128906,  g_loss: 67.18992614746094\n",
            "Training epoch 12083/1000000, d_loss: -69.60713195800781,  g_loss: 88.500732421875\n",
            "Training epoch 12084/1000000, d_loss: -117.19334411621094,  g_loss: 128.81234741210938\n",
            "Training epoch 12085/1000000, d_loss: -138.20150756835938,  g_loss: 141.6600341796875\n",
            "Training epoch 12086/1000000, d_loss: -132.51878356933594,  g_loss: 79.27693176269531\n",
            "Training epoch 12087/1000000, d_loss: -8.520050048828125,  g_loss: 33.48484802246094\n",
            "Training epoch 12088/1000000, d_loss: -144.39768981933594,  g_loss: 32.689857482910156\n",
            "Training epoch 12089/1000000, d_loss: -247.3201446533203,  g_loss: 70.59870910644531\n",
            "Training epoch 12090/1000000, d_loss: -56.84757614135742,  g_loss: 46.347686767578125\n",
            "Training epoch 12091/1000000, d_loss: -255.51669311523438,  g_loss: 16.590068817138672\n",
            "Training epoch 12092/1000000, d_loss: -44.52409362792969,  g_loss: 45.39884948730469\n",
            "Training epoch 12093/1000000, d_loss: -238.18414306640625,  g_loss: 124.15644073486328\n",
            "Training epoch 12094/1000000, d_loss: 281.9466857910156,  g_loss: 13.567724227905273\n",
            "Training epoch 12095/1000000, d_loss: -86.45096588134766,  g_loss: 20.048410415649414\n",
            "Training epoch 12096/1000000, d_loss: -184.93687438964844,  g_loss: 259.62451171875\n",
            "Training epoch 12097/1000000, d_loss: 52.59649658203125,  g_loss: 56.045555114746094\n",
            "Training epoch 12098/1000000, d_loss: -100.26466369628906,  g_loss: 125.458740234375\n",
            "Training epoch 12099/1000000, d_loss: -135.16343688964844,  g_loss: 63.968021392822266\n",
            "Training epoch 12100/1000000, d_loss: -56.950775146484375,  g_loss: 60.4508056640625\n",
            "Training epoch 12101/1000000, d_loss: -303.7373962402344,  g_loss: 66.6644058227539\n",
            "Training epoch 12102/1000000, d_loss: -43.19858932495117,  g_loss: 73.64654541015625\n",
            "Training epoch 12103/1000000, d_loss: -324.41107177734375,  g_loss: 40.90428161621094\n",
            "Training epoch 12104/1000000, d_loss: -58.48223114013672,  g_loss: 102.0085678100586\n",
            "Training epoch 12105/1000000, d_loss: -147.9522247314453,  g_loss: 297.573486328125\n",
            "Training epoch 12106/1000000, d_loss: -1.6441879272460938,  g_loss: 115.88888549804688\n",
            "Training epoch 12107/1000000, d_loss: -133.78884887695312,  g_loss: 115.96327209472656\n",
            "Training epoch 12108/1000000, d_loss: -78.53829956054688,  g_loss: 110.07472229003906\n",
            "Training epoch 12109/1000000, d_loss: -465.4256286621094,  g_loss: -27.911808013916016\n",
            "Training epoch 12110/1000000, d_loss: 69.03357696533203,  g_loss: 129.5859375\n",
            "Training epoch 12111/1000000, d_loss: -46.58002853393555,  g_loss: 119.40098571777344\n",
            "Training epoch 12112/1000000, d_loss: -89.11444854736328,  g_loss: 109.86002349853516\n",
            "Training epoch 12113/1000000, d_loss: -99.37493896484375,  g_loss: 118.03182983398438\n",
            "Training epoch 12114/1000000, d_loss: -91.54693603515625,  g_loss: 83.32283020019531\n",
            "Training epoch 12115/1000000, d_loss: -245.0707550048828,  g_loss: 92.22273254394531\n",
            "Training epoch 12116/1000000, d_loss: -161.95333862304688,  g_loss: 66.83757019042969\n",
            "Training epoch 12117/1000000, d_loss: -78.64077758789062,  g_loss: 115.02311706542969\n",
            "Training epoch 12118/1000000, d_loss: -62.40465545654297,  g_loss: 105.63069152832031\n",
            "Training epoch 12119/1000000, d_loss: -233.22738647460938,  g_loss: 83.4852294921875\n",
            "Training epoch 12120/1000000, d_loss: 63.90361785888672,  g_loss: 102.28392791748047\n",
            "Training epoch 12121/1000000, d_loss: -717.1748046875,  g_loss: -196.6011962890625\n",
            "Training epoch 12122/1000000, d_loss: -19.015172958374023,  g_loss: 87.75315856933594\n",
            "Training epoch 12123/1000000, d_loss: -3.5986557006835938,  g_loss: 178.29592895507812\n",
            "Training epoch 12124/1000000, d_loss: -203.32492065429688,  g_loss: 50.41887664794922\n",
            "Training epoch 12125/1000000, d_loss: -186.14865112304688,  g_loss: 320.6802673339844\n",
            "Training epoch 12126/1000000, d_loss: -66.5904312133789,  g_loss: 165.590087890625\n",
            "Training epoch 12127/1000000, d_loss: -138.59226989746094,  g_loss: 79.9828109741211\n",
            "Training epoch 12128/1000000, d_loss: -316.5505676269531,  g_loss: 27.410306930541992\n",
            "Training epoch 12129/1000000, d_loss: -274.0256042480469,  g_loss: -8.113265991210938\n",
            "Training epoch 12130/1000000, d_loss: -81.43965911865234,  g_loss: 136.8573455810547\n",
            "Training epoch 12131/1000000, d_loss: -314.6076354980469,  g_loss: 5.8430986404418945\n",
            "Training epoch 12132/1000000, d_loss: -110.43766021728516,  g_loss: 105.61032104492188\n",
            "Training epoch 12133/1000000, d_loss: -89.2654800415039,  g_loss: 199.42599487304688\n",
            "Training epoch 12134/1000000, d_loss: -187.01124572753906,  g_loss: 43.91204071044922\n",
            "Training epoch 12135/1000000, d_loss: -211.24594116210938,  g_loss: 16.031475067138672\n",
            "Training epoch 12136/1000000, d_loss: -56.875640869140625,  g_loss: -55.80958557128906\n",
            "Training epoch 12137/1000000, d_loss: -148.20718383789062,  g_loss: -10.538164138793945\n",
            "Training epoch 12138/1000000, d_loss: -16.088157653808594,  g_loss: 1.2254676818847656\n",
            "Training epoch 12139/1000000, d_loss: -67.65406799316406,  g_loss: 60.27264404296875\n",
            "Training epoch 12140/1000000, d_loss: -273.2447204589844,  g_loss: 532.7711181640625\n",
            "Training epoch 12141/1000000, d_loss: -62.13701248168945,  g_loss: 40.13385009765625\n",
            "Training epoch 12142/1000000, d_loss: -58.32512664794922,  g_loss: 57.263301849365234\n",
            "Training epoch 12143/1000000, d_loss: -105.64971923828125,  g_loss: 17.677215576171875\n",
            "Training epoch 12144/1000000, d_loss: -65.7927017211914,  g_loss: 80.7593994140625\n",
            "Training epoch 12145/1000000, d_loss: -127.87400817871094,  g_loss: 78.30065155029297\n",
            "Training epoch 12146/1000000, d_loss: -142.3843994140625,  g_loss: 105.97624206542969\n",
            "Training epoch 12147/1000000, d_loss: -135.96127319335938,  g_loss: 81.46803283691406\n",
            "Training epoch 12148/1000000, d_loss: -100.40644836425781,  g_loss: 19.769981384277344\n",
            "Training epoch 12149/1000000, d_loss: -1440.9471435546875,  g_loss: -84.84447479248047\n",
            "Training epoch 12150/1000000, d_loss: -34.81603240966797,  g_loss: -59.695594787597656\n",
            "Training epoch 12151/1000000, d_loss: 6226.1865234375,  g_loss: 31.985740661621094\n",
            "Training epoch 12152/1000000, d_loss: -804.041748046875,  g_loss: 19.194217681884766\n",
            "Training epoch 12153/1000000, d_loss: -93.81351470947266,  g_loss: -15.872068405151367\n",
            "Training epoch 12154/1000000, d_loss: 115.33973693847656,  g_loss: -46.94231033325195\n",
            "Training epoch 12155/1000000, d_loss: -339.8638916015625,  g_loss: 49.87925720214844\n",
            "Training epoch 12156/1000000, d_loss: -118.34437561035156,  g_loss: 259.2267150878906\n",
            "Training epoch 12157/1000000, d_loss: -149.59921264648438,  g_loss: 39.88844299316406\n",
            "Training epoch 12158/1000000, d_loss: -114.95028686523438,  g_loss: 101.32710266113281\n",
            "Training epoch 12159/1000000, d_loss: -192.29412841796875,  g_loss: -2.1428098678588867\n",
            "Training epoch 12160/1000000, d_loss: -64.2420883178711,  g_loss: 206.04287719726562\n",
            "Training epoch 12161/1000000, d_loss: -233.7147216796875,  g_loss: -14.226502418518066\n",
            "Training epoch 12162/1000000, d_loss: 510.27008056640625,  g_loss: 79.86201477050781\n",
            "Training epoch 12163/1000000, d_loss: -71.05422973632812,  g_loss: 119.19905853271484\n",
            "Training epoch 12164/1000000, d_loss: -154.02105712890625,  g_loss: 66.59058380126953\n",
            "Training epoch 12165/1000000, d_loss: -99.21136474609375,  g_loss: 105.83377075195312\n",
            "Training epoch 12166/1000000, d_loss: -5.551246643066406,  g_loss: 119.194580078125\n",
            "Training epoch 12167/1000000, d_loss: -101.98237609863281,  g_loss: 110.57029724121094\n",
            "Training epoch 12168/1000000, d_loss: -109.07363891601562,  g_loss: 86.94793701171875\n",
            "Training epoch 12169/1000000, d_loss: -207.02493286132812,  g_loss: 88.34071350097656\n",
            "Training epoch 12170/1000000, d_loss: -290.96392822265625,  g_loss: -35.960182189941406\n",
            "Training epoch 12171/1000000, d_loss: -24.848791122436523,  g_loss: 89.71713256835938\n",
            "Training epoch 12172/1000000, d_loss: -89.45635223388672,  g_loss: 182.08197021484375\n",
            "Training epoch 12173/1000000, d_loss: -260.7957458496094,  g_loss: 559.9908447265625\n",
            "Training epoch 12174/1000000, d_loss: -50.912288665771484,  g_loss: 62.92715072631836\n",
            "Training epoch 12175/1000000, d_loss: -131.94810485839844,  g_loss: 97.3251724243164\n",
            "Training epoch 12176/1000000, d_loss: -64.7374038696289,  g_loss: 85.35626220703125\n",
            "Training epoch 12177/1000000, d_loss: -42.144447326660156,  g_loss: 36.51939010620117\n",
            "Training epoch 12178/1000000, d_loss: -33.163909912109375,  g_loss: 55.63032913208008\n",
            "Training epoch 12179/1000000, d_loss: -467.0611572265625,  g_loss: 7.758831977844238\n",
            "Training epoch 12180/1000000, d_loss: 859.25830078125,  g_loss: 39.8013916015625\n",
            "Training epoch 12181/1000000, d_loss: -26.373092651367188,  g_loss: 62.753379821777344\n",
            "Training epoch 12182/1000000, d_loss: -81.6852798461914,  g_loss: 50.78345489501953\n",
            "Training epoch 12183/1000000, d_loss: 77.56114196777344,  g_loss: 40.929927825927734\n",
            "Training epoch 12184/1000000, d_loss: -105.21748352050781,  g_loss: 91.06060791015625\n",
            "Training epoch 12185/1000000, d_loss: -155.22549438476562,  g_loss: 56.990684509277344\n",
            "Training epoch 12186/1000000, d_loss: -776.8837280273438,  g_loss: 24.318105697631836\n",
            "Training epoch 12187/1000000, d_loss: -217.5694122314453,  g_loss: 51.93956756591797\n",
            "Training epoch 12188/1000000, d_loss: -85.34048461914062,  g_loss: 18.34937286376953\n",
            "Training epoch 12189/1000000, d_loss: -230.1796112060547,  g_loss: -44.71280288696289\n",
            "Training epoch 12190/1000000, d_loss: -204.20826721191406,  g_loss: -84.50373840332031\n",
            "Training epoch 12191/1000000, d_loss: -23.98737335205078,  g_loss: 80.71014404296875\n",
            "Training epoch 12192/1000000, d_loss: -66.58355712890625,  g_loss: 157.39688110351562\n",
            "Training epoch 12193/1000000, d_loss: -93.47413635253906,  g_loss: 102.99816131591797\n",
            "Training epoch 12194/1000000, d_loss: -70.70012664794922,  g_loss: 70.47904205322266\n",
            "Training epoch 12195/1000000, d_loss: -218.49285888671875,  g_loss: 61.58638000488281\n",
            "Training epoch 12196/1000000, d_loss: -112.19537353515625,  g_loss: 103.47724914550781\n",
            "Training epoch 12197/1000000, d_loss: -68.70744323730469,  g_loss: 212.21092224121094\n",
            "Training epoch 12198/1000000, d_loss: -46.845947265625,  g_loss: 69.11711120605469\n",
            "Training epoch 12199/1000000, d_loss: -97.23651123046875,  g_loss: 276.27838134765625\n",
            "Training epoch 12200/1000000, d_loss: -152.4183807373047,  g_loss: 74.85587310791016\n",
            "Training epoch 12201/1000000, d_loss: -172.65359497070312,  g_loss: 84.7020263671875\n",
            "Training epoch 12202/1000000, d_loss: -42.26597213745117,  g_loss: 102.27420806884766\n",
            "Training epoch 12203/1000000, d_loss: -36.05297088623047,  g_loss: 83.06055450439453\n",
            "Training epoch 12204/1000000, d_loss: -68.77837371826172,  g_loss: 89.8348617553711\n",
            "Training epoch 12205/1000000, d_loss: -277.34576416015625,  g_loss: 52.16815185546875\n",
            "Training epoch 12206/1000000, d_loss: -271.19232177734375,  g_loss: -46.30171203613281\n",
            "Training epoch 12207/1000000, d_loss: 58.04315185546875,  g_loss: 145.66864013671875\n",
            "Training epoch 12208/1000000, d_loss: -74.14167785644531,  g_loss: 120.88670349121094\n",
            "Training epoch 12209/1000000, d_loss: -37.19438171386719,  g_loss: 104.43492126464844\n",
            "Training epoch 12210/1000000, d_loss: -38.24030685424805,  g_loss: 101.81595611572266\n",
            "Training epoch 12211/1000000, d_loss: -153.901123046875,  g_loss: 65.68556213378906\n",
            "Training epoch 12212/1000000, d_loss: -175.15943908691406,  g_loss: 40.145851135253906\n",
            "Training epoch 12213/1000000, d_loss: -76.5398941040039,  g_loss: 88.24039459228516\n",
            "Training epoch 12214/1000000, d_loss: -349.4984130859375,  g_loss: 44.86825180053711\n",
            "Training epoch 12215/1000000, d_loss: 44.316932678222656,  g_loss: 112.25334930419922\n",
            "Training epoch 12216/1000000, d_loss: -82.0924301147461,  g_loss: 79.30780029296875\n",
            "Training epoch 12217/1000000, d_loss: -48.84734344482422,  g_loss: 101.97876739501953\n",
            "Training epoch 12218/1000000, d_loss: -135.25546264648438,  g_loss: 114.02468872070312\n",
            "Training epoch 12219/1000000, d_loss: -20.268356323242188,  g_loss: 77.71205139160156\n",
            "Training epoch 12220/1000000, d_loss: -100.89622497558594,  g_loss: 83.8167495727539\n",
            "Training epoch 12221/1000000, d_loss: -71.62727355957031,  g_loss: 55.00334167480469\n",
            "Training epoch 12222/1000000, d_loss: -131.49871826171875,  g_loss: 49.61382293701172\n",
            "Training epoch 12223/1000000, d_loss: -57.75979995727539,  g_loss: 76.68484497070312\n",
            "Training epoch 12224/1000000, d_loss: -117.15826416015625,  g_loss: 61.07966613769531\n",
            "Training epoch 12225/1000000, d_loss: -323.71099853515625,  g_loss: 11.750466346740723\n",
            "Training epoch 12226/1000000, d_loss: -273.77520751953125,  g_loss: -0.07888031005859375\n",
            "Training epoch 12227/1000000, d_loss: -110.98941040039062,  g_loss: 105.14726257324219\n",
            "Training epoch 12228/1000000, d_loss: -64.84156036376953,  g_loss: 87.787109375\n",
            "Training epoch 12229/1000000, d_loss: 771.7490844726562,  g_loss: 93.52694702148438\n",
            "Training epoch 12230/1000000, d_loss: -56.56923294067383,  g_loss: 81.96076965332031\n",
            "Training epoch 12231/1000000, d_loss: 101.74838256835938,  g_loss: 105.12764739990234\n",
            "Training epoch 12232/1000000, d_loss: -45.46221160888672,  g_loss: 107.23745727539062\n",
            "Training epoch 12233/1000000, d_loss: -53.082237243652344,  g_loss: 104.82914733886719\n",
            "Training epoch 12234/1000000, d_loss: 1596.16015625,  g_loss: 152.9246826171875\n",
            "Training epoch 12235/1000000, d_loss: -50.01057434082031,  g_loss: 111.47980499267578\n",
            "Training epoch 12236/1000000, d_loss: -85.83539581298828,  g_loss: 118.13174438476562\n",
            "Training epoch 12237/1000000, d_loss: -44.00444030761719,  g_loss: 106.95130157470703\n",
            "Training epoch 12238/1000000, d_loss: -49.84254455566406,  g_loss: 113.2480239868164\n",
            "Training epoch 12239/1000000, d_loss: -179.51678466796875,  g_loss: 98.71247863769531\n",
            "Training epoch 12240/1000000, d_loss: -394.5355224609375,  g_loss: 62.224246978759766\n",
            "Training epoch 12241/1000000, d_loss: -44.05878448486328,  g_loss: 136.58262634277344\n",
            "Training epoch 12242/1000000, d_loss: -121.34420776367188,  g_loss: 74.8909912109375\n",
            "Training epoch 12243/1000000, d_loss: -119.34852600097656,  g_loss: 85.29902648925781\n",
            "Training epoch 12244/1000000, d_loss: -88.41490936279297,  g_loss: 91.92080688476562\n",
            "Training epoch 12245/1000000, d_loss: -85.21385192871094,  g_loss: 97.13841247558594\n",
            "Training epoch 12246/1000000, d_loss: -121.798095703125,  g_loss: 61.31351089477539\n",
            "Training epoch 12247/1000000, d_loss: -130.83091735839844,  g_loss: 77.50222778320312\n",
            "Training epoch 12248/1000000, d_loss: -131.88372802734375,  g_loss: 107.12461853027344\n",
            "Training epoch 12249/1000000, d_loss: -68.62590789794922,  g_loss: 126.78268432617188\n",
            "Training epoch 12250/1000000, d_loss: -68.63497924804688,  g_loss: 86.30587768554688\n",
            "Training epoch 12251/1000000, d_loss: -329.43572998046875,  g_loss: 316.37432861328125\n",
            "Training epoch 12252/1000000, d_loss: -98.26339721679688,  g_loss: -7.976511478424072\n",
            "Training epoch 12253/1000000, d_loss: -196.96083068847656,  g_loss: -7.874624729156494\n",
            "Training epoch 12254/1000000, d_loss: -109.32598876953125,  g_loss: 47.55242156982422\n",
            "Training epoch 12255/1000000, d_loss: -56.16093063354492,  g_loss: 20.47635269165039\n",
            "Training epoch 12256/1000000, d_loss: -58.342620849609375,  g_loss: 17.750446319580078\n",
            "Training epoch 12257/1000000, d_loss: -371.27447509765625,  g_loss: -16.341981887817383\n",
            "Training epoch 12258/1000000, d_loss: -17.845855712890625,  g_loss: 14.982026100158691\n",
            "Training epoch 12259/1000000, d_loss: -62.05621337890625,  g_loss: 11.98714828491211\n",
            "Training epoch 12260/1000000, d_loss: -28.385107040405273,  g_loss: 14.563932418823242\n",
            "Training epoch 12261/1000000, d_loss: -823.0415649414062,  g_loss: -90.18118286132812\n",
            "Training epoch 12262/1000000, d_loss: -123.94888305664062,  g_loss: -33.373531341552734\n",
            "Training epoch 12263/1000000, d_loss: -60.236839294433594,  g_loss: -8.765007972717285\n",
            "Training epoch 12264/1000000, d_loss: -48.983428955078125,  g_loss: 4.239269256591797\n",
            "Training epoch 12265/1000000, d_loss: -606.0466918945312,  g_loss: 17.94230079650879\n",
            "Training epoch 12266/1000000, d_loss: -1366.757568359375,  g_loss: -60.77715301513672\n",
            "Training epoch 12267/1000000, d_loss: -26.502456665039062,  g_loss: -122.03392791748047\n",
            "Training epoch 12268/1000000, d_loss: -39.37975311279297,  g_loss: -58.245330810546875\n",
            "Training epoch 12269/1000000, d_loss: -1235.2811279296875,  g_loss: -219.75396728515625\n",
            "Training epoch 12270/1000000, d_loss: 183.11190795898438,  g_loss: -127.2537841796875\n",
            "Training epoch 12271/1000000, d_loss: 23.72821044921875,  g_loss: 46.358612060546875\n",
            "Training epoch 12272/1000000, d_loss: -161.28396606445312,  g_loss: 79.81137084960938\n",
            "Training epoch 12273/1000000, d_loss: 836.7915649414062,  g_loss: -43.31333923339844\n",
            "Training epoch 12274/1000000, d_loss: -137.43026733398438,  g_loss: 85.02713012695312\n",
            "Training epoch 12275/1000000, d_loss: -134.29498291015625,  g_loss: -51.19744873046875\n",
            "Training epoch 12276/1000000, d_loss: 30.695083618164062,  g_loss: -15.022751808166504\n",
            "Training epoch 12277/1000000, d_loss: -171.00677490234375,  g_loss: -9.803537368774414\n",
            "Training epoch 12278/1000000, d_loss: 12.426361083984375,  g_loss: 29.850751876831055\n",
            "Training epoch 12279/1000000, d_loss: -113.97783660888672,  g_loss: 55.22704315185547\n",
            "Training epoch 12280/1000000, d_loss: -188.885009765625,  g_loss: 46.834686279296875\n",
            "Training epoch 12281/1000000, d_loss: -72.55590057373047,  g_loss: 48.26274108886719\n",
            "Training epoch 12282/1000000, d_loss: -42.69731521606445,  g_loss: 42.495643615722656\n",
            "Training epoch 12283/1000000, d_loss: -62.73955535888672,  g_loss: 71.39451599121094\n",
            "Training epoch 12284/1000000, d_loss: -282.7602844238281,  g_loss: 76.07907104492188\n",
            "Training epoch 12285/1000000, d_loss: -59.52699661254883,  g_loss: -73.8038558959961\n",
            "Training epoch 12286/1000000, d_loss: -128.60397338867188,  g_loss: -7.4770355224609375\n",
            "Training epoch 12287/1000000, d_loss: -67.56544494628906,  g_loss: -4.498202323913574\n",
            "Training epoch 12288/1000000, d_loss: -105.38993835449219,  g_loss: 10.896957397460938\n",
            "Training epoch 12289/1000000, d_loss: -82.93647766113281,  g_loss: -15.904388427734375\n",
            "Training epoch 12290/1000000, d_loss: 15.607810974121094,  g_loss: 78.48536682128906\n",
            "Training epoch 12291/1000000, d_loss: -66.17912292480469,  g_loss: 59.413795471191406\n",
            "Training epoch 12292/1000000, d_loss: -20.1348876953125,  g_loss: 142.87063598632812\n",
            "Training epoch 12293/1000000, d_loss: -117.73916625976562,  g_loss: 107.19650268554688\n",
            "Training epoch 12294/1000000, d_loss: -19.08245277404785,  g_loss: 65.25176239013672\n",
            "Training epoch 12295/1000000, d_loss: -350.3833312988281,  g_loss: -34.03144073486328\n",
            "Training epoch 12296/1000000, d_loss: -173.37786865234375,  g_loss: 4.678047180175781\n",
            "Training epoch 12297/1000000, d_loss: 249.73716735839844,  g_loss: 128.5287322998047\n",
            "Training epoch 12298/1000000, d_loss: -142.446044921875,  g_loss: 189.7406005859375\n",
            "Training epoch 12299/1000000, d_loss: -149.6156768798828,  g_loss: 111.4854965209961\n",
            "Training epoch 12300/1000000, d_loss: -92.66709899902344,  g_loss: 165.82540893554688\n",
            "Training epoch 12301/1000000, d_loss: -255.4036102294922,  g_loss: 135.2891845703125\n",
            "Training epoch 12302/1000000, d_loss: -209.2901611328125,  g_loss: 27.74340057373047\n",
            "Training epoch 12303/1000000, d_loss: -498.79913330078125,  g_loss: -350.669189453125\n",
            "Training epoch 12304/1000000, d_loss: -21.411712646484375,  g_loss: 96.59220886230469\n",
            "Training epoch 12305/1000000, d_loss: -49.974124908447266,  g_loss: 85.00358581542969\n",
            "Training epoch 12306/1000000, d_loss: -90.302001953125,  g_loss: 100.74101257324219\n",
            "Training epoch 12307/1000000, d_loss: -214.56442260742188,  g_loss: 353.0058288574219\n",
            "Training epoch 12308/1000000, d_loss: -154.03199768066406,  g_loss: 125.64266204833984\n",
            "Training epoch 12309/1000000, d_loss: -266.13067626953125,  g_loss: 3.6190414428710938\n",
            "Training epoch 12310/1000000, d_loss: -184.96368408203125,  g_loss: 546.0843505859375\n",
            "Training epoch 12311/1000000, d_loss: -23.611024856567383,  g_loss: 122.60346221923828\n",
            "Training epoch 12312/1000000, d_loss: -141.8812255859375,  g_loss: 165.83258056640625\n",
            "Training epoch 12313/1000000, d_loss: -104.06546783447266,  g_loss: 119.11575317382812\n",
            "Training epoch 12314/1000000, d_loss: -7.676017761230469,  g_loss: 129.77615356445312\n",
            "Training epoch 12315/1000000, d_loss: -16.14181137084961,  g_loss: 125.00560760498047\n",
            "Training epoch 12316/1000000, d_loss: -170.02427673339844,  g_loss: 180.70620727539062\n",
            "Training epoch 12317/1000000, d_loss: -97.8810043334961,  g_loss: 104.95392608642578\n",
            "Training epoch 12318/1000000, d_loss: -89.20744323730469,  g_loss: 87.7600326538086\n",
            "Training epoch 12319/1000000, d_loss: -224.6173553466797,  g_loss: 18.037443161010742\n",
            "Training epoch 12320/1000000, d_loss: -317.45831298828125,  g_loss: 454.28436279296875\n",
            "Training epoch 12321/1000000, d_loss: -65.22309875488281,  g_loss: 116.1744155883789\n",
            "Training epoch 12322/1000000, d_loss: -45.29528045654297,  g_loss: 147.8114471435547\n",
            "Training epoch 12323/1000000, d_loss: -26.25738525390625,  g_loss: 83.74301147460938\n",
            "Training epoch 12324/1000000, d_loss: -51.63462829589844,  g_loss: 114.24407958984375\n",
            "Training epoch 12325/1000000, d_loss: 85.01971435546875,  g_loss: 217.376953125\n",
            "Training epoch 12326/1000000, d_loss: -320.8935852050781,  g_loss: 113.99577331542969\n",
            "Training epoch 12327/1000000, d_loss: -58.276573181152344,  g_loss: 136.22344970703125\n",
            "Training epoch 12328/1000000, d_loss: -115.8125,  g_loss: 171.31387329101562\n",
            "Training epoch 12329/1000000, d_loss: -263.4768371582031,  g_loss: 94.92205810546875\n",
            "Training epoch 12330/1000000, d_loss: -39.01002502441406,  g_loss: 167.0265655517578\n",
            "Training epoch 12331/1000000, d_loss: -108.50741577148438,  g_loss: 164.48486328125\n",
            "Training epoch 12332/1000000, d_loss: -65.28619384765625,  g_loss: 124.45362854003906\n",
            "Training epoch 12333/1000000, d_loss: -58.4782829284668,  g_loss: 103.59840393066406\n",
            "Training epoch 12334/1000000, d_loss: -143.66067504882812,  g_loss: 115.83271026611328\n",
            "Training epoch 12335/1000000, d_loss: -79.86299133300781,  g_loss: 89.13807678222656\n",
            "Training epoch 12336/1000000, d_loss: -252.7108612060547,  g_loss: -30.052465438842773\n",
            "Training epoch 12337/1000000, d_loss: -284.35888671875,  g_loss: 19.52259063720703\n",
            "Training epoch 12338/1000000, d_loss: -855.1055908203125,  g_loss: -183.8004150390625\n",
            "Training epoch 12339/1000000, d_loss: 475.4085388183594,  g_loss: 96.9459457397461\n",
            "Training epoch 12340/1000000, d_loss: -167.39987182617188,  g_loss: -76.62117004394531\n",
            "Training epoch 12341/1000000, d_loss: 105.3578109741211,  g_loss: 58.60581970214844\n",
            "Training epoch 12342/1000000, d_loss: -18.507659912109375,  g_loss: 96.70964813232422\n",
            "Training epoch 12343/1000000, d_loss: -57.39949035644531,  g_loss: 105.37937927246094\n",
            "Training epoch 12344/1000000, d_loss: -43.50914764404297,  g_loss: 61.0698356628418\n",
            "Training epoch 12345/1000000, d_loss: -98.99854278564453,  g_loss: 202.11997985839844\n",
            "Training epoch 12346/1000000, d_loss: -40.600006103515625,  g_loss: 130.24911499023438\n",
            "Training epoch 12347/1000000, d_loss: -46.3354377746582,  g_loss: 113.14990997314453\n",
            "Training epoch 12348/1000000, d_loss: -121.660888671875,  g_loss: 127.40554809570312\n",
            "Training epoch 12349/1000000, d_loss: 3.2176284790039062,  g_loss: 53.38219451904297\n",
            "Training epoch 12350/1000000, d_loss: -54.965614318847656,  g_loss: 41.393802642822266\n",
            "Training epoch 12351/1000000, d_loss: -323.306884765625,  g_loss: -17.843446731567383\n",
            "Training epoch 12352/1000000, d_loss: -125.93606567382812,  g_loss: 36.036590576171875\n",
            "Training epoch 12353/1000000, d_loss: -21.52179718017578,  g_loss: -6.449169158935547\n",
            "Training epoch 12354/1000000, d_loss: -222.51608276367188,  g_loss: 43.91443634033203\n",
            "Training epoch 12355/1000000, d_loss: 926.7437744140625,  g_loss: 84.20807647705078\n",
            "Training epoch 12356/1000000, d_loss: -55.233131408691406,  g_loss: 29.868865966796875\n",
            "Training epoch 12357/1000000, d_loss: -101.07708740234375,  g_loss: 83.58235931396484\n",
            "Training epoch 12358/1000000, d_loss: -84.06432342529297,  g_loss: 66.75316619873047\n",
            "Training epoch 12359/1000000, d_loss: -78.30613708496094,  g_loss: 144.40516662597656\n",
            "Training epoch 12360/1000000, d_loss: -190.83633422851562,  g_loss: 71.15040588378906\n",
            "Training epoch 12361/1000000, d_loss: -180.14169311523438,  g_loss: 49.666561126708984\n",
            "Training epoch 12362/1000000, d_loss: -1268.2705078125,  g_loss: -83.28678894042969\n",
            "Training epoch 12363/1000000, d_loss: -72.78826141357422,  g_loss: -23.054006576538086\n",
            "Training epoch 12364/1000000, d_loss: -351.48358154296875,  g_loss: -101.28459167480469\n",
            "Training epoch 12365/1000000, d_loss: 7.078258514404297,  g_loss: 16.635360717773438\n",
            "Training epoch 12366/1000000, d_loss: -70.68070220947266,  g_loss: 43.945457458496094\n",
            "Training epoch 12367/1000000, d_loss: -187.23097229003906,  g_loss: -5.583148956298828\n",
            "Training epoch 12368/1000000, d_loss: -260.9640197753906,  g_loss: -121.76788330078125\n",
            "Training epoch 12369/1000000, d_loss: -45.22520065307617,  g_loss: 82.70630645751953\n",
            "Training epoch 12370/1000000, d_loss: -267.7265930175781,  g_loss: 74.17289733886719\n",
            "Training epoch 12371/1000000, d_loss: -107.40997314453125,  g_loss: 133.63720703125\n",
            "Training epoch 12372/1000000, d_loss: -94.96907043457031,  g_loss: 231.80308532714844\n",
            "Training epoch 12373/1000000, d_loss: -286.00567626953125,  g_loss: 335.90863037109375\n",
            "Training epoch 12374/1000000, d_loss: -212.98687744140625,  g_loss: -81.77247619628906\n",
            "Training epoch 12375/1000000, d_loss: -412.1483154296875,  g_loss: -219.54620361328125\n",
            "Training epoch 12376/1000000, d_loss: -191.20855712890625,  g_loss: 18.671457290649414\n",
            "Training epoch 12377/1000000, d_loss: -334.2101135253906,  g_loss: -125.21778869628906\n",
            "Training epoch 12378/1000000, d_loss: -170.90924072265625,  g_loss: 274.1965026855469\n",
            "Training epoch 12379/1000000, d_loss: -664.0360107421875,  g_loss: -175.10345458984375\n",
            "Training epoch 12380/1000000, d_loss: 184.13290405273438,  g_loss: 116.4161376953125\n",
            "Training epoch 12381/1000000, d_loss: -79.73479461669922,  g_loss: 74.65897369384766\n",
            "Training epoch 12382/1000000, d_loss: -145.5795135498047,  g_loss: 117.23683166503906\n",
            "Training epoch 12383/1000000, d_loss: -64.55635070800781,  g_loss: 52.168548583984375\n",
            "Training epoch 12384/1000000, d_loss: -27.192039489746094,  g_loss: 59.22539138793945\n",
            "Training epoch 12385/1000000, d_loss: 38.457969665527344,  g_loss: 104.16326141357422\n",
            "Training epoch 12386/1000000, d_loss: -478.926025390625,  g_loss: 119.3902359008789\n",
            "Training epoch 12387/1000000, d_loss: -436.10162353515625,  g_loss: 381.2481689453125\n",
            "Training epoch 12388/1000000, d_loss: -137.7900390625,  g_loss: 106.3748779296875\n",
            "Training epoch 12389/1000000, d_loss: 12.929351806640625,  g_loss: 174.51535034179688\n",
            "Training epoch 12390/1000000, d_loss: -66.01345825195312,  g_loss: 141.28411865234375\n",
            "Training epoch 12391/1000000, d_loss: -141.93344116210938,  g_loss: 126.23635864257812\n",
            "Training epoch 12392/1000000, d_loss: -775.5960083007812,  g_loss: 86.95954895019531\n",
            "Training epoch 12393/1000000, d_loss: 354.32470703125,  g_loss: 35.595703125\n",
            "Training epoch 12394/1000000, d_loss: 155.41078186035156,  g_loss: 79.51962280273438\n",
            "Training epoch 12395/1000000, d_loss: -74.48220825195312,  g_loss: 64.78157043457031\n",
            "Training epoch 12396/1000000, d_loss: -32.27665328979492,  g_loss: 113.98802185058594\n",
            "Training epoch 12397/1000000, d_loss: -332.83758544921875,  g_loss: 17.457422256469727\n",
            "Training epoch 12398/1000000, d_loss: -64.05177307128906,  g_loss: 151.07510375976562\n",
            "Training epoch 12399/1000000, d_loss: 49.99595642089844,  g_loss: 30.434062957763672\n",
            "Training epoch 12400/1000000, d_loss: -70.80560302734375,  g_loss: -5.297571182250977\n",
            "Training epoch 12401/1000000, d_loss: -894.306640625,  g_loss: -437.2032470703125\n",
            "Training epoch 12402/1000000, d_loss: 47.84784698486328,  g_loss: 3.8261585235595703\n",
            "Training epoch 12403/1000000, d_loss: -674.14599609375,  g_loss: -19.658437728881836\n",
            "Training epoch 12404/1000000, d_loss: -58.21553039550781,  g_loss: 78.73030090332031\n",
            "Training epoch 12405/1000000, d_loss: -67.0155029296875,  g_loss: 91.03424072265625\n",
            "Training epoch 12406/1000000, d_loss: -167.94407653808594,  g_loss: 242.16018676757812\n",
            "Training epoch 12407/1000000, d_loss: -81.78201293945312,  g_loss: 339.9006042480469\n",
            "Training epoch 12408/1000000, d_loss: 353.5581970214844,  g_loss: 181.43626403808594\n",
            "Training epoch 12409/1000000, d_loss: -59.20440673828125,  g_loss: 96.23252868652344\n",
            "Training epoch 12410/1000000, d_loss: -113.14464569091797,  g_loss: 82.9888916015625\n",
            "Training epoch 12411/1000000, d_loss: -609.6871948242188,  g_loss: 37.43989562988281\n",
            "Training epoch 12412/1000000, d_loss: -143.05841064453125,  g_loss: 3.142209529876709\n",
            "Training epoch 12413/1000000, d_loss: -977.7926025390625,  g_loss: -307.1408996582031\n",
            "Training epoch 12414/1000000, d_loss: 70.23116302490234,  g_loss: 206.539794921875\n",
            "Training epoch 12415/1000000, d_loss: -299.18817138671875,  g_loss: -86.43496704101562\n",
            "Training epoch 12416/1000000, d_loss: -23.50353240966797,  g_loss: 5.379464149475098\n",
            "Training epoch 12417/1000000, d_loss: -206.32220458984375,  g_loss: 107.08964538574219\n",
            "Training epoch 12418/1000000, d_loss: -358.11083984375,  g_loss: 5.525486469268799\n",
            "Training epoch 12419/1000000, d_loss: -1288.840576171875,  g_loss: -349.9239501953125\n",
            "Training epoch 12420/1000000, d_loss: -270.9072265625,  g_loss: -277.21337890625\n",
            "Training epoch 12421/1000000, d_loss: 57.87261962890625,  g_loss: 89.15461730957031\n",
            "Training epoch 12422/1000000, d_loss: -202.1549835205078,  g_loss: 429.67413330078125\n",
            "Training epoch 12423/1000000, d_loss: -82.71766662597656,  g_loss: 93.61163330078125\n",
            "Training epoch 12424/1000000, d_loss: -367.95751953125,  g_loss: 475.865234375\n",
            "Training epoch 12425/1000000, d_loss: -387.8679504394531,  g_loss: 640.265380859375\n",
            "Training epoch 12426/1000000, d_loss: 15251.3828125,  g_loss: 81.46206665039062\n",
            "Training epoch 12427/1000000, d_loss: 87.98622131347656,  g_loss: -12.226175308227539\n",
            "Training epoch 12428/1000000, d_loss: 145.63848876953125,  g_loss: 45.11606979370117\n",
            "Training epoch 12429/1000000, d_loss: -104.20258331298828,  g_loss: 59.92517852783203\n",
            "Training epoch 12430/1000000, d_loss: -88.80618286132812,  g_loss: -21.226591110229492\n",
            "Training epoch 12431/1000000, d_loss: -76.90200805664062,  g_loss: 9.465848922729492\n",
            "Training epoch 12432/1000000, d_loss: 77.04542541503906,  g_loss: 0.5101203918457031\n",
            "Training epoch 12433/1000000, d_loss: -105.55810546875,  g_loss: 7.7565741539001465\n",
            "Training epoch 12434/1000000, d_loss: 19.684112548828125,  g_loss: 31.58531951904297\n",
            "Training epoch 12435/1000000, d_loss: -231.97042846679688,  g_loss: -14.39559555053711\n",
            "Training epoch 12436/1000000, d_loss: -69.01898193359375,  g_loss: 50.01908874511719\n",
            "Training epoch 12437/1000000, d_loss: -132.2532958984375,  g_loss: 97.14688110351562\n",
            "Training epoch 12438/1000000, d_loss: -174.719482421875,  g_loss: 118.73057556152344\n",
            "Training epoch 12439/1000000, d_loss: -24.90276336669922,  g_loss: 58.873207092285156\n",
            "Training epoch 12440/1000000, d_loss: -67.8558120727539,  g_loss: 35.33024978637695\n",
            "Training epoch 12441/1000000, d_loss: -227.64425659179688,  g_loss: 75.79822540283203\n",
            "Training epoch 12442/1000000, d_loss: -251.6402587890625,  g_loss: 49.997459411621094\n",
            "Training epoch 12443/1000000, d_loss: 41.5110969543457,  g_loss: 61.389671325683594\n",
            "Training epoch 12444/1000000, d_loss: -49.11878204345703,  g_loss: 34.13022232055664\n",
            "Training epoch 12445/1000000, d_loss: -115.60520935058594,  g_loss: 121.35739135742188\n",
            "Training epoch 12446/1000000, d_loss: -79.304931640625,  g_loss: 117.5574951171875\n",
            "Training epoch 12447/1000000, d_loss: -160.24310302734375,  g_loss: 81.32937622070312\n",
            "Training epoch 12448/1000000, d_loss: -27.464147567749023,  g_loss: 93.99517059326172\n",
            "Training epoch 12449/1000000, d_loss: -112.39979553222656,  g_loss: 162.79013061523438\n",
            "Training epoch 12450/1000000, d_loss: -203.12359619140625,  g_loss: 82.70561981201172\n",
            "Training epoch 12451/1000000, d_loss: -113.08214569091797,  g_loss: 97.60002899169922\n",
            "Training epoch 12452/1000000, d_loss: -77.75341796875,  g_loss: 72.38797760009766\n",
            "Training epoch 12453/1000000, d_loss: -239.5672149658203,  g_loss: 82.01724243164062\n",
            "Training epoch 12454/1000000, d_loss: -52.20449447631836,  g_loss: 98.58099365234375\n",
            "Training epoch 12455/1000000, d_loss: -37.28240966796875,  g_loss: 59.644718170166016\n",
            "Training epoch 12456/1000000, d_loss: -59.764373779296875,  g_loss: 66.05863952636719\n",
            "Training epoch 12457/1000000, d_loss: -217.49696350097656,  g_loss: -49.822452545166016\n",
            "Training epoch 12458/1000000, d_loss: -295.5945739746094,  g_loss: 25.225482940673828\n",
            "Training epoch 12459/1000000, d_loss: -483.01995849609375,  g_loss: -43.178993225097656\n",
            "Training epoch 12460/1000000, d_loss: -270.02154541015625,  g_loss: 335.78350830078125\n",
            "Training epoch 12461/1000000, d_loss: -444.3508605957031,  g_loss: -52.196044921875\n",
            "Training epoch 12462/1000000, d_loss: 361.3154296875,  g_loss: 118.04713439941406\n",
            "Training epoch 12463/1000000, d_loss: -339.69549560546875,  g_loss: 73.93909454345703\n",
            "Training epoch 12464/1000000, d_loss: -146.13888549804688,  g_loss: 51.78419494628906\n",
            "Training epoch 12465/1000000, d_loss: -515.1788940429688,  g_loss: -104.63742065429688\n",
            "Training epoch 12466/1000000, d_loss: -15.005607604980469,  g_loss: 25.339313507080078\n",
            "Training epoch 12467/1000000, d_loss: -277.2008361816406,  g_loss: -55.454933166503906\n",
            "Training epoch 12468/1000000, d_loss: -879.7293701171875,  g_loss: -231.8997344970703\n",
            "Training epoch 12469/1000000, d_loss: 1.5798492431640625,  g_loss: 3.1001806259155273\n",
            "Training epoch 12470/1000000, d_loss: -274.52410888671875,  g_loss: 324.312255859375\n",
            "Training epoch 12471/1000000, d_loss: -133.2230682373047,  g_loss: 41.14221954345703\n",
            "Training epoch 12472/1000000, d_loss: -96.32305908203125,  g_loss: -26.318025588989258\n",
            "Training epoch 12473/1000000, d_loss: -110.46394348144531,  g_loss: 92.75826263427734\n",
            "Training epoch 12474/1000000, d_loss: -119.96775817871094,  g_loss: 25.754606246948242\n",
            "Training epoch 12475/1000000, d_loss: -439.49578857421875,  g_loss: -202.7515869140625\n",
            "Training epoch 12476/1000000, d_loss: -115.61988830566406,  g_loss: -74.2896499633789\n",
            "Training epoch 12477/1000000, d_loss: -439.00836181640625,  g_loss: 517.4840087890625\n",
            "Training epoch 12478/1000000, d_loss: -345.4319152832031,  g_loss: 378.0220031738281\n",
            "Training epoch 12479/1000000, d_loss: -170.68362426757812,  g_loss: 135.002197265625\n",
            "Training epoch 12480/1000000, d_loss: -220.67523193359375,  g_loss: -13.58971118927002\n",
            "Training epoch 12481/1000000, d_loss: -279.24932861328125,  g_loss: -95.20861053466797\n",
            "Training epoch 12482/1000000, d_loss: 5.024513244628906,  g_loss: 87.77204895019531\n",
            "Training epoch 12483/1000000, d_loss: -194.49636840820312,  g_loss: 236.33233642578125\n",
            "Training epoch 12484/1000000, d_loss: -98.16810607910156,  g_loss: 6.980929851531982\n",
            "Training epoch 12485/1000000, d_loss: 1.6901931762695312,  g_loss: 22.92156982421875\n",
            "Training epoch 12486/1000000, d_loss: -598.4926147460938,  g_loss: -85.38008880615234\n",
            "Training epoch 12487/1000000, d_loss: -266.4808654785156,  g_loss: -56.23944091796875\n",
            "Training epoch 12488/1000000, d_loss: -704.7053833007812,  g_loss: -202.274658203125\n",
            "Training epoch 12489/1000000, d_loss: -37.57756423950195,  g_loss: 21.229736328125\n",
            "Training epoch 12490/1000000, d_loss: -48.52318572998047,  g_loss: 34.6765022277832\n",
            "Training epoch 12491/1000000, d_loss: -384.36492919921875,  g_loss: 62.6147346496582\n",
            "Training epoch 12492/1000000, d_loss: -124.67939758300781,  g_loss: 21.57838249206543\n",
            "Training epoch 12493/1000000, d_loss: -60.755802154541016,  g_loss: 76.3999252319336\n",
            "Training epoch 12494/1000000, d_loss: -44.6541748046875,  g_loss: 115.15689086914062\n",
            "Training epoch 12495/1000000, d_loss: 650.1200561523438,  g_loss: 98.51899719238281\n",
            "Training epoch 12496/1000000, d_loss: -65.46607971191406,  g_loss: 103.49717712402344\n",
            "Training epoch 12497/1000000, d_loss: -22.899381637573242,  g_loss: 71.45697021484375\n",
            "Training epoch 12498/1000000, d_loss: -85.69013214111328,  g_loss: 73.03976440429688\n",
            "Training epoch 12499/1000000, d_loss: -218.95606994628906,  g_loss: 67.86518859863281\n",
            "Training epoch 12500/1000000, d_loss: -197.9703369140625,  g_loss: 49.22605895996094\n",
            "Training epoch 12501/1000000, d_loss: -192.37310791015625,  g_loss: 81.90242004394531\n",
            "Training epoch 12502/1000000, d_loss: -66.35333251953125,  g_loss: 108.07804870605469\n",
            "Training epoch 12503/1000000, d_loss: -282.76739501953125,  g_loss: -34.76081085205078\n",
            "Training epoch 12504/1000000, d_loss: -56.746253967285156,  g_loss: 34.802066802978516\n",
            "Training epoch 12505/1000000, d_loss: -0.7283096313476562,  g_loss: -14.226356506347656\n",
            "Training epoch 12506/1000000, d_loss: -122.62288665771484,  g_loss: -43.544578552246094\n",
            "Training epoch 12507/1000000, d_loss: -170.6442413330078,  g_loss: 17.647918701171875\n",
            "Training epoch 12508/1000000, d_loss: -156.4956512451172,  g_loss: -3.2419633865356445\n",
            "Training epoch 12509/1000000, d_loss: -220.52557373046875,  g_loss: 32.82470703125\n",
            "Training epoch 12510/1000000, d_loss: -86.43523406982422,  g_loss: 74.0149917602539\n",
            "Training epoch 12511/1000000, d_loss: -396.6701354980469,  g_loss: 12.28994083404541\n",
            "Training epoch 12512/1000000, d_loss: -48.26659393310547,  g_loss: 36.549503326416016\n",
            "Training epoch 12513/1000000, d_loss: 23.69916534423828,  g_loss: -23.489030838012695\n",
            "Training epoch 12514/1000000, d_loss: -132.6829833984375,  g_loss: 101.096435546875\n",
            "Training epoch 12515/1000000, d_loss: -152.93182373046875,  g_loss: 37.409934997558594\n",
            "Training epoch 12516/1000000, d_loss: -200.89089965820312,  g_loss: -24.806316375732422\n",
            "Training epoch 12517/1000000, d_loss: -65.23800659179688,  g_loss: 48.98133850097656\n",
            "Training epoch 12518/1000000, d_loss: -248.627197265625,  g_loss: 104.2078628540039\n",
            "Training epoch 12519/1000000, d_loss: -150.86398315429688,  g_loss: 42.69609832763672\n",
            "Training epoch 12520/1000000, d_loss: -127.18890380859375,  g_loss: 1.3706274032592773\n",
            "Training epoch 12521/1000000, d_loss: -278.1208801269531,  g_loss: 51.92090606689453\n",
            "Training epoch 12522/1000000, d_loss: -75.51878356933594,  g_loss: 96.72791290283203\n",
            "Training epoch 12523/1000000, d_loss: -138.54290771484375,  g_loss: 84.39794921875\n",
            "Training epoch 12524/1000000, d_loss: -77.57884216308594,  g_loss: 101.12894439697266\n",
            "Training epoch 12525/1000000, d_loss: -120.6439437866211,  g_loss: 68.90544891357422\n",
            "Training epoch 12526/1000000, d_loss: -103.78456115722656,  g_loss: 188.89308166503906\n",
            "Training epoch 12527/1000000, d_loss: -27.842771530151367,  g_loss: 46.32902908325195\n",
            "Training epoch 12528/1000000, d_loss: -287.1974182128906,  g_loss: 112.07156372070312\n",
            "Training epoch 12529/1000000, d_loss: -75.05169677734375,  g_loss: 11.724477767944336\n",
            "Training epoch 12530/1000000, d_loss: -150.0478973388672,  g_loss: 11.480676651000977\n",
            "Training epoch 12531/1000000, d_loss: -82.07833862304688,  g_loss: 44.87062454223633\n",
            "Training epoch 12532/1000000, d_loss: -80.85404968261719,  g_loss: 65.19432830810547\n",
            "Training epoch 12533/1000000, d_loss: -27.77309799194336,  g_loss: 48.96924591064453\n",
            "Training epoch 12534/1000000, d_loss: 608.0260009765625,  g_loss: 67.15367126464844\n",
            "Training epoch 12535/1000000, d_loss: -57.90785598754883,  g_loss: 83.11024475097656\n",
            "Training epoch 12536/1000000, d_loss: -219.27796936035156,  g_loss: 4.950405120849609\n",
            "Training epoch 12537/1000000, d_loss: -43.74468994140625,  g_loss: 64.23876190185547\n",
            "Training epoch 12538/1000000, d_loss: -206.97833251953125,  g_loss: 3.0641889572143555\n",
            "Training epoch 12539/1000000, d_loss: -53.83785629272461,  g_loss: 17.516685485839844\n",
            "Training epoch 12540/1000000, d_loss: -61.97492599487305,  g_loss: 6.8739471435546875\n",
            "Training epoch 12541/1000000, d_loss: -217.96856689453125,  g_loss: -20.647350311279297\n",
            "Training epoch 12542/1000000, d_loss: -84.01631164550781,  g_loss: 91.87276458740234\n",
            "Training epoch 12543/1000000, d_loss: -265.6041259765625,  g_loss: -19.070362091064453\n",
            "Training epoch 12544/1000000, d_loss: -322.42926025390625,  g_loss: -26.716623306274414\n",
            "Training epoch 12545/1000000, d_loss: -1181.417724609375,  g_loss: -186.85308837890625\n",
            "Training epoch 12546/1000000, d_loss: -153.41250610351562,  g_loss: -23.9964542388916\n",
            "Training epoch 12547/1000000, d_loss: 42.00952911376953,  g_loss: -40.479705810546875\n",
            "Training epoch 12548/1000000, d_loss: 60.34710693359375,  g_loss: -9.648681640625\n",
            "Training epoch 12549/1000000, d_loss: -29.662860870361328,  g_loss: -66.81092071533203\n",
            "Training epoch 12550/1000000, d_loss: -30.657989501953125,  g_loss: -43.43233108520508\n",
            "Training epoch 12551/1000000, d_loss: -128.3790740966797,  g_loss: -50.5078239440918\n",
            "Training epoch 12552/1000000, d_loss: -156.99697875976562,  g_loss: -40.01822280883789\n",
            "Training epoch 12553/1000000, d_loss: -204.97415161132812,  g_loss: -108.573974609375\n",
            "Training epoch 12554/1000000, d_loss: -108.8697509765625,  g_loss: -9.021564483642578\n",
            "Training epoch 12555/1000000, d_loss: 8.36972427368164,  g_loss: -8.761722564697266\n",
            "Training epoch 12556/1000000, d_loss: -136.98866271972656,  g_loss: 23.296438217163086\n",
            "Training epoch 12557/1000000, d_loss: -55.0536994934082,  g_loss: 77.33789825439453\n",
            "Training epoch 12558/1000000, d_loss: -272.3099060058594,  g_loss: -118.86280822753906\n",
            "Training epoch 12559/1000000, d_loss: -81.72395324707031,  g_loss: -5.691566467285156\n",
            "Training epoch 12560/1000000, d_loss: -74.57005310058594,  g_loss: 82.04146575927734\n",
            "Training epoch 12561/1000000, d_loss: -125.88819122314453,  g_loss: 189.23135375976562\n",
            "Training epoch 12562/1000000, d_loss: -48.25928497314453,  g_loss: 157.35516357421875\n",
            "Training epoch 12563/1000000, d_loss: -34.20027160644531,  g_loss: 89.74592590332031\n",
            "Training epoch 12564/1000000, d_loss: -252.00389099121094,  g_loss: 152.96603393554688\n",
            "Training epoch 12565/1000000, d_loss: -551.271728515625,  g_loss: -68.61942291259766\n",
            "Training epoch 12566/1000000, d_loss: -112.55416870117188,  g_loss: 65.87039184570312\n",
            "Training epoch 12567/1000000, d_loss: 22.660720825195312,  g_loss: 27.66183090209961\n",
            "Training epoch 12568/1000000, d_loss: -50.75682067871094,  g_loss: 58.01615905761719\n",
            "Training epoch 12569/1000000, d_loss: -40.42412567138672,  g_loss: 46.7308349609375\n",
            "Training epoch 12570/1000000, d_loss: 269.6478271484375,  g_loss: 125.95863342285156\n",
            "Training epoch 12571/1000000, d_loss: -119.23490905761719,  g_loss: 111.89990997314453\n",
            "Training epoch 12572/1000000, d_loss: -236.3153839111328,  g_loss: 89.7246322631836\n",
            "Training epoch 12573/1000000, d_loss: -174.90557861328125,  g_loss: 196.29833984375\n",
            "Training epoch 12574/1000000, d_loss: -54.833343505859375,  g_loss: 104.74691772460938\n",
            "Training epoch 12575/1000000, d_loss: -434.566650390625,  g_loss: 33.559967041015625\n",
            "Training epoch 12576/1000000, d_loss: 36.190162658691406,  g_loss: 78.55359649658203\n",
            "Training epoch 12577/1000000, d_loss: -57.89491271972656,  g_loss: 94.39522552490234\n",
            "Training epoch 12578/1000000, d_loss: -518.5587158203125,  g_loss: -8.756454467773438\n",
            "Training epoch 12579/1000000, d_loss: -995.6190795898438,  g_loss: -334.193603515625\n",
            "Training epoch 12580/1000000, d_loss: 101.99666595458984,  g_loss: -80.5689468383789\n",
            "Training epoch 12581/1000000, d_loss: -164.8641815185547,  g_loss: 104.25286102294922\n",
            "Training epoch 12582/1000000, d_loss: -126.7310791015625,  g_loss: 85.0676498413086\n",
            "Training epoch 12583/1000000, d_loss: -134.0834197998047,  g_loss: 183.51837158203125\n",
            "Training epoch 12584/1000000, d_loss: -176.58750915527344,  g_loss: 159.93270874023438\n",
            "Training epoch 12585/1000000, d_loss: -356.37554931640625,  g_loss: 640.4873657226562\n",
            "Training epoch 12586/1000000, d_loss: -28.448883056640625,  g_loss: 75.55653381347656\n",
            "Training epoch 12587/1000000, d_loss: -308.38665771484375,  g_loss: 373.2492370605469\n",
            "Training epoch 12588/1000000, d_loss: -72.96400451660156,  g_loss: 72.56126403808594\n",
            "Training epoch 12589/1000000, d_loss: -27.709823608398438,  g_loss: 133.3653564453125\n",
            "Training epoch 12590/1000000, d_loss: -120.7879867553711,  g_loss: 128.93797302246094\n",
            "Training epoch 12591/1000000, d_loss: -204.99313354492188,  g_loss: 350.2684326171875\n",
            "Training epoch 12592/1000000, d_loss: -36.751163482666016,  g_loss: 61.413291931152344\n",
            "Training epoch 12593/1000000, d_loss: -92.30638122558594,  g_loss: 113.98887634277344\n",
            "Training epoch 12594/1000000, d_loss: 28.12152862548828,  g_loss: 48.011444091796875\n",
            "Training epoch 12595/1000000, d_loss: -123.38330078125,  g_loss: 106.51333618164062\n",
            "Training epoch 12596/1000000, d_loss: -110.05412292480469,  g_loss: 239.93527221679688\n",
            "Training epoch 12597/1000000, d_loss: -79.58586883544922,  g_loss: 151.98165893554688\n",
            "Training epoch 12598/1000000, d_loss: -6.740592956542969,  g_loss: 121.68388366699219\n",
            "Training epoch 12599/1000000, d_loss: -54.985130310058594,  g_loss: 34.612735748291016\n",
            "Training epoch 12600/1000000, d_loss: -211.513671875,  g_loss: 25.89540672302246\n",
            "Training epoch 12601/1000000, d_loss: 729.1547241210938,  g_loss: 86.75715637207031\n",
            "Training epoch 12602/1000000, d_loss: -121.55223083496094,  g_loss: 89.82182312011719\n",
            "Training epoch 12603/1000000, d_loss: -6.178682327270508,  g_loss: 76.02641296386719\n",
            "Training epoch 12604/1000000, d_loss: -117.96208190917969,  g_loss: 81.18289947509766\n",
            "Training epoch 12605/1000000, d_loss: -96.54141235351562,  g_loss: 89.42196655273438\n",
            "Training epoch 12606/1000000, d_loss: -62.5785026550293,  g_loss: 64.73025512695312\n",
            "Training epoch 12607/1000000, d_loss: -43.692142486572266,  g_loss: 85.75215911865234\n",
            "Training epoch 12608/1000000, d_loss: -709.2991943359375,  g_loss: 22.983064651489258\n",
            "Training epoch 12609/1000000, d_loss: -301.22442626953125,  g_loss: -23.048458099365234\n",
            "Training epoch 12610/1000000, d_loss: -21.56850814819336,  g_loss: 67.10336303710938\n",
            "Training epoch 12611/1000000, d_loss: -38.083065032958984,  g_loss: 77.07594299316406\n",
            "Training epoch 12612/1000000, d_loss: -97.24444580078125,  g_loss: 69.88792419433594\n",
            "Training epoch 12613/1000000, d_loss: -267.8854064941406,  g_loss: 294.8888244628906\n",
            "Training epoch 12614/1000000, d_loss: -61.70227813720703,  g_loss: 140.98138427734375\n",
            "Training epoch 12615/1000000, d_loss: -58.93071365356445,  g_loss: 84.96560668945312\n",
            "Training epoch 12616/1000000, d_loss: -76.51918029785156,  g_loss: 71.4317398071289\n",
            "Training epoch 12617/1000000, d_loss: -51.04481887817383,  g_loss: 81.95063781738281\n",
            "Training epoch 12618/1000000, d_loss: -595.7620239257812,  g_loss: -2.779183864593506\n",
            "Training epoch 12619/1000000, d_loss: -166.68455505371094,  g_loss: 54.96467590332031\n",
            "Training epoch 12620/1000000, d_loss: 54.767066955566406,  g_loss: 76.29689025878906\n",
            "Training epoch 12621/1000000, d_loss: -128.77175903320312,  g_loss: 55.455116271972656\n",
            "Training epoch 12622/1000000, d_loss: -86.15644073486328,  g_loss: 55.936317443847656\n",
            "Training epoch 12623/1000000, d_loss: -15.169364929199219,  g_loss: 83.87508392333984\n",
            "Training epoch 12624/1000000, d_loss: 630.2611083984375,  g_loss: 114.18421936035156\n",
            "Training epoch 12625/1000000, d_loss: -88.34696960449219,  g_loss: 118.53926086425781\n",
            "Training epoch 12626/1000000, d_loss: -151.9595947265625,  g_loss: 96.38278198242188\n",
            "Training epoch 12627/1000000, d_loss: -111.51327514648438,  g_loss: 76.39933013916016\n",
            "Training epoch 12628/1000000, d_loss: -427.421875,  g_loss: 0.1488790512084961\n",
            "Training epoch 12629/1000000, d_loss: 30.479995727539062,  g_loss: 47.22039031982422\n",
            "Training epoch 12630/1000000, d_loss: -9.194412231445312,  g_loss: 52.970069885253906\n",
            "Training epoch 12631/1000000, d_loss: -57.40546417236328,  g_loss: 56.57453155517578\n",
            "Training epoch 12632/1000000, d_loss: -47.00446701049805,  g_loss: 39.410377502441406\n",
            "Training epoch 12633/1000000, d_loss: -216.4044189453125,  g_loss: 1.4918746948242188\n",
            "Training epoch 12634/1000000, d_loss: -89.61348724365234,  g_loss: 33.581153869628906\n",
            "Training epoch 12635/1000000, d_loss: -109.38823699951172,  g_loss: 147.93215942382812\n",
            "Training epoch 12636/1000000, d_loss: -22.844024658203125,  g_loss: 51.99607849121094\n",
            "Training epoch 12637/1000000, d_loss: -88.64044189453125,  g_loss: 96.27409362792969\n",
            "Training epoch 12638/1000000, d_loss: -120.88040924072266,  g_loss: 32.808082580566406\n",
            "Training epoch 12639/1000000, d_loss: -243.11581420898438,  g_loss: 76.29113006591797\n",
            "Training epoch 12640/1000000, d_loss: -588.701416015625,  g_loss: 19.29401397705078\n",
            "Training epoch 12641/1000000, d_loss: -310.21478271484375,  g_loss: 36.1574821472168\n",
            "Training epoch 12642/1000000, d_loss: 59.180442810058594,  g_loss: 63.37483215332031\n",
            "Training epoch 12643/1000000, d_loss: -67.9615478515625,  g_loss: 76.43385314941406\n",
            "Training epoch 12644/1000000, d_loss: -58.80612564086914,  g_loss: 61.09318161010742\n",
            "Training epoch 12645/1000000, d_loss: -322.820556640625,  g_loss: 101.45185852050781\n",
            "Training epoch 12646/1000000, d_loss: 0.5625991821289062,  g_loss: 73.87197875976562\n",
            "Training epoch 12647/1000000, d_loss: -55.071495056152344,  g_loss: 25.79132843017578\n",
            "Training epoch 12648/1000000, d_loss: -33.85150909423828,  g_loss: 21.13692855834961\n",
            "Training epoch 12649/1000000, d_loss: -136.60556030273438,  g_loss: 16.648651123046875\n",
            "Training epoch 12650/1000000, d_loss: -220.29183959960938,  g_loss: 48.555686950683594\n",
            "Training epoch 12651/1000000, d_loss: -94.8853759765625,  g_loss: 105.5596923828125\n",
            "Training epoch 12652/1000000, d_loss: -259.45880126953125,  g_loss: 53.695274353027344\n",
            "Training epoch 12653/1000000, d_loss: -740.404541015625,  g_loss: -164.29762268066406\n",
            "Training epoch 12654/1000000, d_loss: -43.664161682128906,  g_loss: -90.83210754394531\n",
            "Training epoch 12655/1000000, d_loss: -136.06216430664062,  g_loss: 13.645411491394043\n",
            "Training epoch 12656/1000000, d_loss: -161.3240509033203,  g_loss: 207.5953369140625\n",
            "Training epoch 12657/1000000, d_loss: -113.41848754882812,  g_loss: 114.87345886230469\n",
            "Training epoch 12658/1000000, d_loss: 786.087890625,  g_loss: 35.38880920410156\n",
            "Training epoch 12659/1000000, d_loss: -167.1576385498047,  g_loss: 59.29899215698242\n",
            "Training epoch 12660/1000000, d_loss: -59.59624481201172,  g_loss: 30.268644332885742\n",
            "Training epoch 12661/1000000, d_loss: -104.56145477294922,  g_loss: 77.82247924804688\n",
            "Training epoch 12662/1000000, d_loss: -93.51385498046875,  g_loss: 55.09733200073242\n",
            "Training epoch 12663/1000000, d_loss: -173.5576934814453,  g_loss: 55.10791778564453\n",
            "Training epoch 12664/1000000, d_loss: -93.8942642211914,  g_loss: 64.40249633789062\n",
            "Training epoch 12665/1000000, d_loss: -70.980224609375,  g_loss: 46.55923843383789\n",
            "Training epoch 12666/1000000, d_loss: -131.7234344482422,  g_loss: 21.126983642578125\n",
            "Training epoch 12667/1000000, d_loss: -65.99772644042969,  g_loss: 47.22217559814453\n",
            "Training epoch 12668/1000000, d_loss: -165.34902954101562,  g_loss: 54.114166259765625\n",
            "Training epoch 12669/1000000, d_loss: -123.67304992675781,  g_loss: 103.16016387939453\n",
            "Training epoch 12670/1000000, d_loss: -121.12716674804688,  g_loss: 89.15975189208984\n",
            "Training epoch 12671/1000000, d_loss: -36.96712875366211,  g_loss: 94.36537170410156\n",
            "Training epoch 12672/1000000, d_loss: -77.036865234375,  g_loss: 99.63172912597656\n",
            "Training epoch 12673/1000000, d_loss: -39.821468353271484,  g_loss: 66.74797058105469\n",
            "Training epoch 12674/1000000, d_loss: -84.93960571289062,  g_loss: 90.89599609375\n",
            "Training epoch 12675/1000000, d_loss: -53.77996063232422,  g_loss: 59.901649475097656\n",
            "Training epoch 12676/1000000, d_loss: -310.7857971191406,  g_loss: -54.844566345214844\n",
            "Training epoch 12677/1000000, d_loss: -57.93964385986328,  g_loss: 69.38165283203125\n",
            "Training epoch 12678/1000000, d_loss: -40.25626754760742,  g_loss: 48.097816467285156\n",
            "Training epoch 12679/1000000, d_loss: -168.95709228515625,  g_loss: 51.2737922668457\n",
            "Training epoch 12680/1000000, d_loss: -31.400707244873047,  g_loss: 54.819854736328125\n",
            "Training epoch 12681/1000000, d_loss: -128.24417114257812,  g_loss: 49.68055725097656\n",
            "Training epoch 12682/1000000, d_loss: -474.42431640625,  g_loss: -102.26095581054688\n",
            "Training epoch 12683/1000000, d_loss: -138.48167419433594,  g_loss: -14.689046859741211\n",
            "Training epoch 12684/1000000, d_loss: -117.85881805419922,  g_loss: 6.003849029541016\n",
            "Training epoch 12685/1000000, d_loss: -397.6690979003906,  g_loss: 31.45811653137207\n",
            "Training epoch 12686/1000000, d_loss: -43.79230499267578,  g_loss: -38.7080078125\n",
            "Training epoch 12687/1000000, d_loss: -73.95095825195312,  g_loss: 54.2525749206543\n",
            "Training epoch 12688/1000000, d_loss: -159.82550048828125,  g_loss: 26.896934509277344\n",
            "Training epoch 12689/1000000, d_loss: -400.0344543457031,  g_loss: -11.032430648803711\n",
            "Training epoch 12690/1000000, d_loss: 78.95977783203125,  g_loss: 61.8116455078125\n",
            "Training epoch 12691/1000000, d_loss: -141.59432983398438,  g_loss: 99.98326110839844\n",
            "Training epoch 12692/1000000, d_loss: -101.40129089355469,  g_loss: 103.28765869140625\n",
            "Training epoch 12693/1000000, d_loss: -142.0748291015625,  g_loss: 155.156005859375\n",
            "Training epoch 12694/1000000, d_loss: -188.67919921875,  g_loss: 210.68980407714844\n",
            "Training epoch 12695/1000000, d_loss: -11.879066467285156,  g_loss: 22.268516540527344\n",
            "Training epoch 12696/1000000, d_loss: -133.95440673828125,  g_loss: 131.78378295898438\n",
            "Training epoch 12697/1000000, d_loss: -180.41775512695312,  g_loss: 62.51146697998047\n",
            "Training epoch 12698/1000000, d_loss: -177.46604919433594,  g_loss: 82.36133575439453\n",
            "Training epoch 12699/1000000, d_loss: -72.65022277832031,  g_loss: 117.26327514648438\n",
            "Training epoch 12700/1000000, d_loss: -37.964420318603516,  g_loss: 139.05172729492188\n",
            "Training epoch 12701/1000000, d_loss: -142.64512634277344,  g_loss: 81.0681381225586\n",
            "Training epoch 12702/1000000, d_loss: -90.26078796386719,  g_loss: 118.5814437866211\n",
            "Training epoch 12703/1000000, d_loss: -139.82118225097656,  g_loss: 79.9306640625\n",
            "Training epoch 12704/1000000, d_loss: -105.79652404785156,  g_loss: 75.14932250976562\n",
            "Training epoch 12705/1000000, d_loss: -46.31880187988281,  g_loss: 76.934814453125\n",
            "Training epoch 12706/1000000, d_loss: -375.3907470703125,  g_loss: -66.63616943359375\n",
            "Training epoch 12707/1000000, d_loss: -48.028785705566406,  g_loss: 55.90175247192383\n",
            "Training epoch 12708/1000000, d_loss: -133.44525146484375,  g_loss: 38.927608489990234\n",
            "Training epoch 12709/1000000, d_loss: -64.62670135498047,  g_loss: 201.7171173095703\n",
            "Training epoch 12710/1000000, d_loss: -161.47315979003906,  g_loss: 12.265690803527832\n",
            "Training epoch 12711/1000000, d_loss: 28.970169067382812,  g_loss: 44.514549255371094\n",
            "Training epoch 12712/1000000, d_loss: -31.154987335205078,  g_loss: 61.81779479980469\n",
            "Training epoch 12713/1000000, d_loss: -96.78395080566406,  g_loss: 60.96533966064453\n",
            "Training epoch 12714/1000000, d_loss: -191.01988220214844,  g_loss: -12.496633529663086\n",
            "Training epoch 12715/1000000, d_loss: -219.91275024414062,  g_loss: -47.63148880004883\n",
            "Training epoch 12716/1000000, d_loss: -112.68348693847656,  g_loss: 64.11856842041016\n",
            "Training epoch 12717/1000000, d_loss: -37.436519622802734,  g_loss: 119.82833862304688\n",
            "Training epoch 12718/1000000, d_loss: -149.51773071289062,  g_loss: 87.86598205566406\n",
            "Training epoch 12719/1000000, d_loss: -202.19085693359375,  g_loss: -0.9238295555114746\n",
            "Training epoch 12720/1000000, d_loss: -638.5917358398438,  g_loss: -11.948071479797363\n",
            "Training epoch 12721/1000000, d_loss: -602.9685668945312,  g_loss: -144.02745056152344\n",
            "Training epoch 12722/1000000, d_loss: 233.62664794921875,  g_loss: -97.64866638183594\n",
            "Training epoch 12723/1000000, d_loss: -73.35433959960938,  g_loss: -29.733442306518555\n",
            "Training epoch 12724/1000000, d_loss: 2.6857757568359375,  g_loss: 9.621713638305664\n",
            "Training epoch 12725/1000000, d_loss: -276.6722412109375,  g_loss: 38.11090850830078\n",
            "Training epoch 12726/1000000, d_loss: 10.157081604003906,  g_loss: -6.626648902893066\n",
            "Training epoch 12727/1000000, d_loss: -112.82136535644531,  g_loss: 55.969444274902344\n",
            "Training epoch 12728/1000000, d_loss: -187.68299865722656,  g_loss: 277.5491027832031\n",
            "Training epoch 12729/1000000, d_loss: -113.23623657226562,  g_loss: 130.48318481445312\n",
            "Training epoch 12730/1000000, d_loss: -267.093994140625,  g_loss: -19.467655181884766\n",
            "Training epoch 12731/1000000, d_loss: -219.20213317871094,  g_loss: -27.135345458984375\n",
            "Training epoch 12732/1000000, d_loss: -132.9237060546875,  g_loss: -1.825026512145996\n",
            "Training epoch 12733/1000000, d_loss: -104.92657470703125,  g_loss: 99.27798461914062\n",
            "Training epoch 12734/1000000, d_loss: -42.991355895996094,  g_loss: -24.17684555053711\n",
            "Training epoch 12735/1000000, d_loss: -90.8174819946289,  g_loss: -19.887636184692383\n",
            "Training epoch 12736/1000000, d_loss: -94.9052963256836,  g_loss: -10.825965881347656\n",
            "Training epoch 12737/1000000, d_loss: -104.3755874633789,  g_loss: -42.99998474121094\n",
            "Training epoch 12738/1000000, d_loss: -66.2967529296875,  g_loss: 27.387718200683594\n",
            "Training epoch 12739/1000000, d_loss: -55.81185531616211,  g_loss: 22.685636520385742\n",
            "Training epoch 12740/1000000, d_loss: -51.76726531982422,  g_loss: 22.591461181640625\n",
            "Training epoch 12741/1000000, d_loss: -314.3673095703125,  g_loss: -44.38517379760742\n",
            "Training epoch 12742/1000000, d_loss: -72.59497833251953,  g_loss: 24.463991165161133\n",
            "Training epoch 12743/1000000, d_loss: -101.82332611083984,  g_loss: 55.32844924926758\n",
            "Training epoch 12744/1000000, d_loss: -187.311767578125,  g_loss: -44.799171447753906\n",
            "Training epoch 12745/1000000, d_loss: -95.2676773071289,  g_loss: -7.993579864501953\n",
            "Training epoch 12746/1000000, d_loss: -57.49418640136719,  g_loss: 36.15031433105469\n",
            "Training epoch 12747/1000000, d_loss: -38.278099060058594,  g_loss: 54.66827392578125\n",
            "Training epoch 12748/1000000, d_loss: -18.057422637939453,  g_loss: 8.801993370056152\n",
            "Training epoch 12749/1000000, d_loss: -69.6061019897461,  g_loss: 7.7247700691223145\n",
            "Training epoch 12750/1000000, d_loss: 97.61805725097656,  g_loss: 127.23541259765625\n",
            "Training epoch 12751/1000000, d_loss: -97.33119201660156,  g_loss: 78.4937744140625\n",
            "Training epoch 12752/1000000, d_loss: -682.4326171875,  g_loss: 4.791507720947266\n",
            "Training epoch 12753/1000000, d_loss: -269.0224609375,  g_loss: -12.703487396240234\n",
            "Training epoch 12754/1000000, d_loss: -16.44476318359375,  g_loss: 63.883365631103516\n",
            "Training epoch 12755/1000000, d_loss: -55.329219818115234,  g_loss: 102.08702850341797\n",
            "Training epoch 12756/1000000, d_loss: -209.1759033203125,  g_loss: 83.66398620605469\n",
            "Training epoch 12757/1000000, d_loss: -637.0814819335938,  g_loss: -54.98292541503906\n",
            "Training epoch 12758/1000000, d_loss: 345.5011291503906,  g_loss: 132.7840576171875\n",
            "Training epoch 12759/1000000, d_loss: -136.47964477539062,  g_loss: 125.0979995727539\n",
            "Training epoch 12760/1000000, d_loss: -193.30496215820312,  g_loss: 85.14717864990234\n",
            "Training epoch 12761/1000000, d_loss: -98.77620697021484,  g_loss: 41.81525421142578\n",
            "Training epoch 12762/1000000, d_loss: 23.55950927734375,  g_loss: 94.42327880859375\n",
            "Training epoch 12763/1000000, d_loss: -208.44932556152344,  g_loss: 64.52227783203125\n",
            "Training epoch 12764/1000000, d_loss: 82.63433837890625,  g_loss: 90.91786193847656\n",
            "Training epoch 12765/1000000, d_loss: -88.6209487915039,  g_loss: 53.17179870605469\n",
            "Training epoch 12766/1000000, d_loss: -832.2665405273438,  g_loss: -97.69387817382812\n",
            "Training epoch 12767/1000000, d_loss: -196.20237731933594,  g_loss: 9.31941032409668\n",
            "Training epoch 12768/1000000, d_loss: -48.07362365722656,  g_loss: 71.30528259277344\n",
            "Training epoch 12769/1000000, d_loss: -76.95946502685547,  g_loss: 91.46710205078125\n",
            "Training epoch 12770/1000000, d_loss: -155.625732421875,  g_loss: 74.7093276977539\n",
            "Training epoch 12771/1000000, d_loss: -398.0148620605469,  g_loss: -27.293987274169922\n",
            "Training epoch 12772/1000000, d_loss: -180.97637939453125,  g_loss: 103.41018676757812\n",
            "Training epoch 12773/1000000, d_loss: 135.14508056640625,  g_loss: 86.6483154296875\n",
            "Training epoch 12774/1000000, d_loss: -49.603416442871094,  g_loss: 139.1774139404297\n",
            "Training epoch 12775/1000000, d_loss: 91.04571533203125,  g_loss: 57.962646484375\n",
            "Training epoch 12776/1000000, d_loss: -248.04530334472656,  g_loss: 6.182595252990723\n",
            "Training epoch 12777/1000000, d_loss: -95.07366180419922,  g_loss: 36.900047302246094\n",
            "Training epoch 12778/1000000, d_loss: -78.27811431884766,  g_loss: 70.05427551269531\n",
            "Training epoch 12779/1000000, d_loss: -221.98863220214844,  g_loss: 125.83172607421875\n",
            "Training epoch 12780/1000000, d_loss: -138.0372314453125,  g_loss: 39.982826232910156\n",
            "Training epoch 12781/1000000, d_loss: -109.10321044921875,  g_loss: 155.44589233398438\n",
            "Training epoch 12782/1000000, d_loss: -492.4189758300781,  g_loss: 381.16552734375\n",
            "Training epoch 12783/1000000, d_loss: -726.3824462890625,  g_loss: -246.26959228515625\n",
            "Training epoch 12784/1000000, d_loss: -48.96491241455078,  g_loss: -4.579677581787109\n",
            "Training epoch 12785/1000000, d_loss: 14.127685546875,  g_loss: 13.139759063720703\n",
            "Training epoch 12786/1000000, d_loss: -155.1833953857422,  g_loss: 122.12399291992188\n",
            "Training epoch 12787/1000000, d_loss: -148.48277282714844,  g_loss: 97.70065307617188\n",
            "Training epoch 12788/1000000, d_loss: -61.82685089111328,  g_loss: 99.70561981201172\n",
            "Training epoch 12789/1000000, d_loss: -3.943450927734375,  g_loss: 206.1824951171875\n",
            "Training epoch 12790/1000000, d_loss: -225.96112060546875,  g_loss: 239.41639709472656\n",
            "Training epoch 12791/1000000, d_loss: -129.35748291015625,  g_loss: 247.10812377929688\n",
            "Training epoch 12792/1000000, d_loss: -86.47364044189453,  g_loss: 96.86538696289062\n",
            "Training epoch 12793/1000000, d_loss: -162.76101684570312,  g_loss: 168.24127197265625\n",
            "Training epoch 12794/1000000, d_loss: -119.67842864990234,  g_loss: 123.62493896484375\n",
            "Training epoch 12795/1000000, d_loss: -69.21070861816406,  g_loss: 73.74710083007812\n",
            "Training epoch 12796/1000000, d_loss: -159.7396697998047,  g_loss: 184.66738891601562\n",
            "Training epoch 12797/1000000, d_loss: -73.53840637207031,  g_loss: 61.580318450927734\n",
            "Training epoch 12798/1000000, d_loss: 17.466293334960938,  g_loss: 12.396562576293945\n",
            "Training epoch 12799/1000000, d_loss: -125.60586547851562,  g_loss: 37.547794342041016\n",
            "Training epoch 12800/1000000, d_loss: 53.94081115722656,  g_loss: 91.15757751464844\n",
            "Training epoch 12801/1000000, d_loss: -315.0317687988281,  g_loss: 25.697885513305664\n",
            "Training epoch 12802/1000000, d_loss: -226.92526245117188,  g_loss: 82.538818359375\n",
            "Training epoch 12803/1000000, d_loss: -98.97222900390625,  g_loss: 120.78948974609375\n",
            "Training epoch 12804/1000000, d_loss: -180.41580200195312,  g_loss: 144.04559326171875\n",
            "Training epoch 12805/1000000, d_loss: -220.28298950195312,  g_loss: 134.51889038085938\n",
            "Training epoch 12806/1000000, d_loss: -197.98403930664062,  g_loss: 62.58731460571289\n",
            "Training epoch 12807/1000000, d_loss: -500.381103515625,  g_loss: -60.885047912597656\n",
            "Training epoch 12808/1000000, d_loss: -24.68014144897461,  g_loss: 39.339820861816406\n",
            "Training epoch 12809/1000000, d_loss: -241.15676879882812,  g_loss: -52.04254150390625\n",
            "Training epoch 12810/1000000, d_loss: 57.90399932861328,  g_loss: 51.731815338134766\n",
            "Training epoch 12811/1000000, d_loss: -73.28898620605469,  g_loss: 52.97154998779297\n",
            "Training epoch 12812/1000000, d_loss: -76.13584899902344,  g_loss: 120.02952575683594\n",
            "Training epoch 12813/1000000, d_loss: -334.5444030761719,  g_loss: 33.11802673339844\n",
            "Training epoch 12814/1000000, d_loss: -36.56109619140625,  g_loss: 84.85888671875\n",
            "Training epoch 12815/1000000, d_loss: -151.66201782226562,  g_loss: 75.80923461914062\n",
            "Training epoch 12816/1000000, d_loss: -408.55426025390625,  g_loss: -14.60342788696289\n",
            "Training epoch 12817/1000000, d_loss: -199.6035919189453,  g_loss: 45.74689865112305\n",
            "Training epoch 12818/1000000, d_loss: -185.6651611328125,  g_loss: 20.316600799560547\n",
            "Training epoch 12819/1000000, d_loss: -85.170166015625,  g_loss: -71.40428161621094\n",
            "Training epoch 12820/1000000, d_loss: -75.35111236572266,  g_loss: -44.129188537597656\n",
            "Training epoch 12821/1000000, d_loss: -76.47587585449219,  g_loss: 7.599448204040527\n",
            "Training epoch 12822/1000000, d_loss: -2.5327529907226562,  g_loss: 203.67822265625\n",
            "Training epoch 12823/1000000, d_loss: -139.16615295410156,  g_loss: 22.011945724487305\n",
            "Training epoch 12824/1000000, d_loss: -32.053993225097656,  g_loss: 65.73196411132812\n",
            "Training epoch 12825/1000000, d_loss: -24.901695251464844,  g_loss: 81.74435424804688\n",
            "Training epoch 12826/1000000, d_loss: -489.3777770996094,  g_loss: -33.011985778808594\n",
            "Training epoch 12827/1000000, d_loss: -72.75628662109375,  g_loss: 43.8271598815918\n",
            "Training epoch 12828/1000000, d_loss: -51.965110778808594,  g_loss: 45.689605712890625\n",
            "Training epoch 12829/1000000, d_loss: -689.3867797851562,  g_loss: -80.55810546875\n",
            "Training epoch 12830/1000000, d_loss: -414.51324462890625,  g_loss: -114.53295135498047\n",
            "Training epoch 12831/1000000, d_loss: -23.820951461791992,  g_loss: -30.75703239440918\n",
            "Training epoch 12832/1000000, d_loss: -61.56068801879883,  g_loss: 144.0994873046875\n",
            "Training epoch 12833/1000000, d_loss: -129.11734008789062,  g_loss: 197.17054748535156\n",
            "Training epoch 12834/1000000, d_loss: -119.40340423583984,  g_loss: 46.362449645996094\n",
            "Training epoch 12835/1000000, d_loss: -96.6175537109375,  g_loss: 16.963098526000977\n",
            "Training epoch 12836/1000000, d_loss: -433.1806640625,  g_loss: 16.965394973754883\n",
            "Training epoch 12837/1000000, d_loss: -1427.5894775390625,  g_loss: -1037.20263671875\n",
            "Training epoch 12838/1000000, d_loss: 650.8482666015625,  g_loss: -671.7589111328125\n",
            "Training epoch 12839/1000000, d_loss: 175.94041442871094,  g_loss: -59.9238166809082\n",
            "Training epoch 12840/1000000, d_loss: -43.31089401245117,  g_loss: -5.399914741516113\n",
            "Training epoch 12841/1000000, d_loss: -63.36329650878906,  g_loss: 73.13287353515625\n",
            "Training epoch 12842/1000000, d_loss: -112.99842834472656,  g_loss: -11.824518203735352\n",
            "Training epoch 12843/1000000, d_loss: -561.508056640625,  g_loss: 50.650142669677734\n",
            "Training epoch 12844/1000000, d_loss: -10.385757446289062,  g_loss: 32.94356918334961\n",
            "Training epoch 12845/1000000, d_loss: -594.9790649414062,  g_loss: -29.336124420166016\n",
            "Training epoch 12846/1000000, d_loss: 89.88754272460938,  g_loss: 133.65008544921875\n",
            "Training epoch 12847/1000000, d_loss: 95.15425109863281,  g_loss: 51.52317810058594\n",
            "Training epoch 12848/1000000, d_loss: -277.9686584472656,  g_loss: 347.5298767089844\n",
            "Training epoch 12849/1000000, d_loss: -217.0718994140625,  g_loss: 167.98703002929688\n",
            "Training epoch 12850/1000000, d_loss: -9.396438598632812,  g_loss: 180.37991333007812\n",
            "Training epoch 12851/1000000, d_loss: -45.96520233154297,  g_loss: 159.8947296142578\n",
            "Training epoch 12852/1000000, d_loss: -87.87328338623047,  g_loss: 116.0496826171875\n",
            "Training epoch 12853/1000000, d_loss: -288.6189880371094,  g_loss: 179.1141357421875\n",
            "Training epoch 12854/1000000, d_loss: -120.04214477539062,  g_loss: 25.798133850097656\n",
            "Training epoch 12855/1000000, d_loss: -181.8498077392578,  g_loss: 272.43988037109375\n",
            "Training epoch 12856/1000000, d_loss: -120.12568664550781,  g_loss: 324.0694580078125\n",
            "Training epoch 12857/1000000, d_loss: -171.5706787109375,  g_loss: 139.58700561523438\n",
            "Training epoch 12858/1000000, d_loss: -43.458404541015625,  g_loss: -70.99005126953125\n",
            "Training epoch 12859/1000000, d_loss: -51.395050048828125,  g_loss: -15.919464111328125\n",
            "Training epoch 12860/1000000, d_loss: -138.32327270507812,  g_loss: 5.973822593688965\n",
            "Training epoch 12861/1000000, d_loss: -59.287872314453125,  g_loss: -69.86112213134766\n",
            "Training epoch 12862/1000000, d_loss: -128.77894592285156,  g_loss: -88.7243423461914\n",
            "Training epoch 12863/1000000, d_loss: -121.48513793945312,  g_loss: -50.92316818237305\n",
            "Training epoch 12864/1000000, d_loss: -24.047143936157227,  g_loss: 29.67601776123047\n",
            "Training epoch 12865/1000000, d_loss: -126.29755401611328,  g_loss: 106.63700103759766\n",
            "Training epoch 12866/1000000, d_loss: -95.15813446044922,  g_loss: -55.130348205566406\n",
            "Training epoch 12867/1000000, d_loss: -42.47867965698242,  g_loss: 9.076102256774902\n",
            "Training epoch 12868/1000000, d_loss: -67.51648712158203,  g_loss: 65.73637390136719\n",
            "Training epoch 12869/1000000, d_loss: -600.8540649414062,  g_loss: -7.138480186462402\n",
            "Training epoch 12870/1000000, d_loss: -386.29425048828125,  g_loss: -3.760268211364746\n",
            "Training epoch 12871/1000000, d_loss: -101.40962982177734,  g_loss: 40.46321105957031\n",
            "Training epoch 12872/1000000, d_loss: -79.45901489257812,  g_loss: 33.67683029174805\n",
            "Training epoch 12873/1000000, d_loss: 141.27679443359375,  g_loss: 65.31352233886719\n",
            "Training epoch 12874/1000000, d_loss: -66.1207275390625,  g_loss: 72.88143920898438\n",
            "Training epoch 12875/1000000, d_loss: -63.45843505859375,  g_loss: 93.31887817382812\n",
            "Training epoch 12876/1000000, d_loss: -164.2992706298828,  g_loss: 9.273561477661133\n",
            "Training epoch 12877/1000000, d_loss: -30.617267608642578,  g_loss: 28.740354537963867\n",
            "Training epoch 12878/1000000, d_loss: -45.51578140258789,  g_loss: 29.130115509033203\n",
            "Training epoch 12879/1000000, d_loss: -34.970558166503906,  g_loss: 28.693910598754883\n",
            "Training epoch 12880/1000000, d_loss: -82.37872314453125,  g_loss: 59.11067581176758\n",
            "Training epoch 12881/1000000, d_loss: -277.31024169921875,  g_loss: -11.276667594909668\n",
            "Training epoch 12882/1000000, d_loss: -71.49276733398438,  g_loss: 41.286373138427734\n",
            "Training epoch 12883/1000000, d_loss: -137.033935546875,  g_loss: 88.31095886230469\n",
            "Training epoch 12884/1000000, d_loss: -240.5768280029297,  g_loss: -87.00506591796875\n",
            "Training epoch 12885/1000000, d_loss: -419.6717224121094,  g_loss: 44.629844665527344\n",
            "Training epoch 12886/1000000, d_loss: -1191.315673828125,  g_loss: -1337.010009765625\n",
            "Training epoch 12887/1000000, d_loss: 8088.396484375,  g_loss: 81.55713653564453\n",
            "Training epoch 12888/1000000, d_loss: 347.34979248046875,  g_loss: -1.2570133209228516\n",
            "Training epoch 12889/1000000, d_loss: -8.9632568359375,  g_loss: -90.28147888183594\n",
            "Training epoch 12890/1000000, d_loss: 347.2890625,  g_loss: -364.3519287109375\n",
            "Training epoch 12891/1000000, d_loss: 2069.606689453125,  g_loss: -391.91717529296875\n",
            "Training epoch 12892/1000000, d_loss: 222.18113708496094,  g_loss: 34.19474411010742\n",
            "Training epoch 12893/1000000, d_loss: 1271.436767578125,  g_loss: 130.89605712890625\n",
            "Training epoch 12894/1000000, d_loss: -65.53373718261719,  g_loss: 295.49566650390625\n",
            "Training epoch 12895/1000000, d_loss: -148.66094970703125,  g_loss: 313.963623046875\n",
            "Training epoch 12896/1000000, d_loss: -321.9097900390625,  g_loss: 501.5151672363281\n",
            "Training epoch 12897/1000000, d_loss: -631.3259887695312,  g_loss: 1061.219482421875\n",
            "Training epoch 12898/1000000, d_loss: -153.45428466796875,  g_loss: 294.02020263671875\n",
            "Training epoch 12899/1000000, d_loss: -385.05145263671875,  g_loss: 326.000732421875\n",
            "Training epoch 12900/1000000, d_loss: -89.26624298095703,  g_loss: 173.3115234375\n",
            "Training epoch 12901/1000000, d_loss: -213.77728271484375,  g_loss: 170.27955627441406\n",
            "Training epoch 12902/1000000, d_loss: -456.0856018066406,  g_loss: -225.64239501953125\n",
            "Training epoch 12903/1000000, d_loss: 62.22352600097656,  g_loss: -31.801719665527344\n",
            "Training epoch 12904/1000000, d_loss: -115.5960922241211,  g_loss: 49.69313049316406\n",
            "Training epoch 12905/1000000, d_loss: -154.58470153808594,  g_loss: 90.17875671386719\n",
            "Training epoch 12906/1000000, d_loss: -173.6387176513672,  g_loss: -41.619110107421875\n",
            "Training epoch 12907/1000000, d_loss: -307.29083251953125,  g_loss: -168.5158233642578\n",
            "Training epoch 12908/1000000, d_loss: -226.95005798339844,  g_loss: -73.99613189697266\n",
            "Training epoch 12909/1000000, d_loss: -15.623138427734375,  g_loss: 16.685306549072266\n",
            "Training epoch 12910/1000000, d_loss: -35.29432678222656,  g_loss: 14.171010971069336\n",
            "Training epoch 12911/1000000, d_loss: -58.74309539794922,  g_loss: 42.18326950073242\n",
            "Training epoch 12912/1000000, d_loss: -74.51773071289062,  g_loss: 95.76579284667969\n",
            "Training epoch 12913/1000000, d_loss: -135.3429718017578,  g_loss: -5.066371917724609\n",
            "Training epoch 12914/1000000, d_loss: -82.13288879394531,  g_loss: 87.19330596923828\n",
            "Training epoch 12915/1000000, d_loss: -265.0998229980469,  g_loss: 358.16259765625\n",
            "Training epoch 12916/1000000, d_loss: -114.17366790771484,  g_loss: 104.89469909667969\n",
            "Training epoch 12917/1000000, d_loss: -283.44281005859375,  g_loss: 281.1854248046875\n",
            "Training epoch 12918/1000000, d_loss: -36.411582946777344,  g_loss: 30.406448364257812\n",
            "Training epoch 12919/1000000, d_loss: -89.62632751464844,  g_loss: 97.00094604492188\n",
            "Training epoch 12920/1000000, d_loss: -74.97559356689453,  g_loss: 37.218894958496094\n",
            "Training epoch 12921/1000000, d_loss: -123.20984649658203,  g_loss: 128.04501342773438\n",
            "Training epoch 12922/1000000, d_loss: -147.24325561523438,  g_loss: 7.578405380249023\n",
            "Training epoch 12923/1000000, d_loss: -299.2551574707031,  g_loss: -7.769164085388184\n",
            "Training epoch 12924/1000000, d_loss: -382.507080078125,  g_loss: -7.676614761352539\n",
            "Training epoch 12925/1000000, d_loss: -187.6788330078125,  g_loss: 12.364752769470215\n",
            "Training epoch 12926/1000000, d_loss: -66.45413208007812,  g_loss: 22.653667449951172\n",
            "Training epoch 12927/1000000, d_loss: -92.21685028076172,  g_loss: 97.12016296386719\n",
            "Training epoch 12928/1000000, d_loss: -64.44623565673828,  g_loss: 57.199440002441406\n",
            "Training epoch 12929/1000000, d_loss: -46.11211395263672,  g_loss: 58.73056411743164\n",
            "Training epoch 12930/1000000, d_loss: -942.317138671875,  g_loss: -401.0849304199219\n",
            "Training epoch 12931/1000000, d_loss: -7.237358093261719,  g_loss: -6.030518054962158\n",
            "Training epoch 12932/1000000, d_loss: -169.67755126953125,  g_loss: 30.900291442871094\n",
            "Training epoch 12933/1000000, d_loss: 74.6290283203125,  g_loss: 117.22315216064453\n",
            "Training epoch 12934/1000000, d_loss: -209.6031494140625,  g_loss: 78.129638671875\n",
            "Training epoch 12935/1000000, d_loss: 31.481422424316406,  g_loss: 89.45188903808594\n",
            "Training epoch 12936/1000000, d_loss: -71.55610656738281,  g_loss: 120.1396484375\n",
            "Training epoch 12937/1000000, d_loss: -19.531333923339844,  g_loss: 143.40341186523438\n",
            "Training epoch 12938/1000000, d_loss: -66.7784423828125,  g_loss: 141.78883361816406\n",
            "Training epoch 12939/1000000, d_loss: 22.597946166992188,  g_loss: 130.05010986328125\n",
            "Training epoch 12940/1000000, d_loss: -244.09603881835938,  g_loss: 134.03428649902344\n",
            "Training epoch 12941/1000000, d_loss: -147.70188903808594,  g_loss: 113.37872314453125\n",
            "Training epoch 12942/1000000, d_loss: -129.95144653320312,  g_loss: 81.76998901367188\n",
            "Training epoch 12943/1000000, d_loss: -277.7810363769531,  g_loss: 18.5814208984375\n",
            "Training epoch 12944/1000000, d_loss: -61.543006896972656,  g_loss: 106.17904663085938\n",
            "Training epoch 12945/1000000, d_loss: -40.699623107910156,  g_loss: 226.41241455078125\n",
            "Training epoch 12946/1000000, d_loss: -138.52157592773438,  g_loss: 137.5538330078125\n",
            "Training epoch 12947/1000000, d_loss: -90.00254821777344,  g_loss: 88.6414566040039\n",
            "Training epoch 12948/1000000, d_loss: -221.42039489746094,  g_loss: 62.29905700683594\n",
            "Training epoch 12949/1000000, d_loss: -52.44031524658203,  g_loss: 32.09506607055664\n",
            "Training epoch 12950/1000000, d_loss: -314.35723876953125,  g_loss: -7.143002986907959\n",
            "Training epoch 12951/1000000, d_loss: -184.4024658203125,  g_loss: 78.54434204101562\n",
            "Training epoch 12952/1000000, d_loss: -201.02293395996094,  g_loss: 30.791561126708984\n",
            "Training epoch 12953/1000000, d_loss: -100.86285400390625,  g_loss: 120.80774688720703\n",
            "Training epoch 12954/1000000, d_loss: 76.53500366210938,  g_loss: 87.08119201660156\n",
            "Training epoch 12955/1000000, d_loss: -9.933418273925781,  g_loss: 56.34088134765625\n",
            "Training epoch 12956/1000000, d_loss: -23.0010986328125,  g_loss: 80.26260375976562\n",
            "Training epoch 12957/1000000, d_loss: -94.22535705566406,  g_loss: 87.40396118164062\n",
            "Training epoch 12958/1000000, d_loss: -62.40743637084961,  g_loss: 121.3775863647461\n",
            "Training epoch 12959/1000000, d_loss: -60.22433090209961,  g_loss: 73.64781188964844\n",
            "Training epoch 12960/1000000, d_loss: -139.5489044189453,  g_loss: 39.22618103027344\n",
            "Training epoch 12961/1000000, d_loss: -66.86884307861328,  g_loss: 43.060638427734375\n",
            "Training epoch 12962/1000000, d_loss: -69.21861267089844,  g_loss: 40.98324203491211\n",
            "Training epoch 12963/1000000, d_loss: -501.1812438964844,  g_loss: -1.8731060028076172\n",
            "Training epoch 12964/1000000, d_loss: -2.3441810607910156,  g_loss: 32.714111328125\n",
            "Training epoch 12965/1000000, d_loss: -111.60258483886719,  g_loss: 23.53885269165039\n",
            "Training epoch 12966/1000000, d_loss: -139.4138641357422,  g_loss: 23.844467163085938\n",
            "Training epoch 12967/1000000, d_loss: -150.880859375,  g_loss: 41.46371078491211\n",
            "Training epoch 12968/1000000, d_loss: -61.166778564453125,  g_loss: 68.7106704711914\n",
            "Training epoch 12969/1000000, d_loss: -61.70777130126953,  g_loss: 124.52827453613281\n",
            "Training epoch 12970/1000000, d_loss: -139.97329711914062,  g_loss: 18.997249603271484\n",
            "Training epoch 12971/1000000, d_loss: -29.361188888549805,  g_loss: 83.44200134277344\n",
            "Training epoch 12972/1000000, d_loss: -357.2633056640625,  g_loss: 24.06871795654297\n",
            "Training epoch 12973/1000000, d_loss: -56.322532653808594,  g_loss: 74.76461029052734\n",
            "Training epoch 12974/1000000, d_loss: -174.60052490234375,  g_loss: 112.01932525634766\n",
            "Training epoch 12975/1000000, d_loss: 53.148681640625,  g_loss: 51.04965591430664\n",
            "Training epoch 12976/1000000, d_loss: -159.56887817382812,  g_loss: 72.99261474609375\n",
            "Training epoch 12977/1000000, d_loss: -39.389137268066406,  g_loss: 61.79585266113281\n",
            "Training epoch 12978/1000000, d_loss: -62.94118881225586,  g_loss: 96.04996490478516\n",
            "Training epoch 12979/1000000, d_loss: -181.5897674560547,  g_loss: 10.533232688903809\n",
            "Training epoch 12980/1000000, d_loss: -142.673828125,  g_loss: 15.647662162780762\n",
            "Training epoch 12981/1000000, d_loss: -67.27993774414062,  g_loss: 72.12333679199219\n",
            "Training epoch 12982/1000000, d_loss: -70.11952209472656,  g_loss: 25.35511016845703\n",
            "Training epoch 12983/1000000, d_loss: 2.625518798828125,  g_loss: 88.63481140136719\n",
            "Training epoch 12984/1000000, d_loss: -359.96514892578125,  g_loss: 103.71974182128906\n",
            "Training epoch 12985/1000000, d_loss: -103.99075317382812,  g_loss: 37.726837158203125\n",
            "Training epoch 12986/1000000, d_loss: -397.1011962890625,  g_loss: 27.999507904052734\n",
            "Training epoch 12987/1000000, d_loss: 38.57281494140625,  g_loss: 85.92938232421875\n",
            "Training epoch 12988/1000000, d_loss: -84.28852081298828,  g_loss: 45.6453857421875\n",
            "Training epoch 12989/1000000, d_loss: -154.46005249023438,  g_loss: 50.87495422363281\n",
            "Training epoch 12990/1000000, d_loss: -130.34091186523438,  g_loss: 82.89421844482422\n",
            "Training epoch 12991/1000000, d_loss: 76.94747924804688,  g_loss: 83.53425598144531\n",
            "Training epoch 12992/1000000, d_loss: -292.2771911621094,  g_loss: 45.79216003417969\n",
            "Training epoch 12993/1000000, d_loss: -44.042625427246094,  g_loss: 64.90203857421875\n",
            "Training epoch 12994/1000000, d_loss: -48.72442626953125,  g_loss: 36.75624084472656\n",
            "Training epoch 12995/1000000, d_loss: -203.7682342529297,  g_loss: 40.7782096862793\n",
            "Training epoch 12996/1000000, d_loss: -54.891510009765625,  g_loss: 67.79170227050781\n",
            "Training epoch 12997/1000000, d_loss: -131.3544921875,  g_loss: 11.44777774810791\n",
            "Training epoch 12998/1000000, d_loss: -723.3912353515625,  g_loss: -157.12530517578125\n",
            "Training epoch 12999/1000000, d_loss: -151.62283325195312,  g_loss: -13.08736801147461\n",
            "Training epoch 13000/1000000, d_loss: -1.224822998046875,  g_loss: -32.206382751464844\n",
            "Training epoch 13001/1000000, d_loss: -31.933780670166016,  g_loss: -48.19285583496094\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 51/51 [00:00<00:00, 620.02it/s]\n",
            "Meshing: 100%|██████████| 3132/3132 [00:00<00:00, 3353.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_13001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_13001/assets\n",
            "Training epoch 13002/1000000, d_loss: -57.586395263671875,  g_loss: -7.470985412597656\n",
            "Training epoch 13003/1000000, d_loss: -204.4593048095703,  g_loss: -50.50092697143555\n",
            "Training epoch 13004/1000000, d_loss: -98.22026062011719,  g_loss: -59.82988357543945\n",
            "Training epoch 13005/1000000, d_loss: -111.9023666381836,  g_loss: -20.741741180419922\n",
            "Training epoch 13006/1000000, d_loss: -26.40045166015625,  g_loss: 5.557755470275879\n",
            "Training epoch 13007/1000000, d_loss: -54.555599212646484,  g_loss: 17.00322723388672\n",
            "Training epoch 13008/1000000, d_loss: -120.18904113769531,  g_loss: 0.8962235450744629\n",
            "Training epoch 13009/1000000, d_loss: 2721.896484375,  g_loss: 74.71611022949219\n",
            "Training epoch 13010/1000000, d_loss: -64.39677429199219,  g_loss: 24.051942825317383\n",
            "Training epoch 13011/1000000, d_loss: -144.3623046875,  g_loss: -19.754833221435547\n",
            "Training epoch 13012/1000000, d_loss: 23.0850830078125,  g_loss: 44.44508361816406\n",
            "Training epoch 13013/1000000, d_loss: -85.02153015136719,  g_loss: 68.298828125\n",
            "Training epoch 13014/1000000, d_loss: -22.5645751953125,  g_loss: 76.358642578125\n",
            "Training epoch 13015/1000000, d_loss: -45.53248596191406,  g_loss: 68.99710083007812\n",
            "Training epoch 13016/1000000, d_loss: -315.85101318359375,  g_loss: 85.60130310058594\n",
            "Training epoch 13017/1000000, d_loss: -359.4595031738281,  g_loss: 20.82221031188965\n",
            "Training epoch 13018/1000000, d_loss: -108.46023559570312,  g_loss: 47.0749397277832\n",
            "Training epoch 13019/1000000, d_loss: -355.43499755859375,  g_loss: 1.8543062210083008\n",
            "Training epoch 13020/1000000, d_loss: -315.36627197265625,  g_loss: 19.039527893066406\n",
            "Training epoch 13021/1000000, d_loss: -27.901308059692383,  g_loss: 25.063072204589844\n",
            "Training epoch 13022/1000000, d_loss: -520.6734619140625,  g_loss: -71.40149688720703\n",
            "Training epoch 13023/1000000, d_loss: -34.0009765625,  g_loss: 13.10791015625\n",
            "Training epoch 13024/1000000, d_loss: -410.803466796875,  g_loss: 20.39165496826172\n",
            "Training epoch 13025/1000000, d_loss: -53.868186950683594,  g_loss: 51.48175048828125\n",
            "Training epoch 13026/1000000, d_loss: -109.5735855102539,  g_loss: -115.77200317382812\n",
            "Training epoch 13027/1000000, d_loss: -124.18277740478516,  g_loss: 57.526371002197266\n",
            "Training epoch 13028/1000000, d_loss: -68.25387573242188,  g_loss: 18.715545654296875\n",
            "Training epoch 13029/1000000, d_loss: -205.89373779296875,  g_loss: 93.46298217773438\n",
            "Training epoch 13030/1000000, d_loss: -228.45753479003906,  g_loss: 199.60580444335938\n",
            "Training epoch 13031/1000000, d_loss: -289.1726379394531,  g_loss: -51.26237869262695\n",
            "Training epoch 13032/1000000, d_loss: -107.94979858398438,  g_loss: -43.87482452392578\n",
            "Training epoch 13033/1000000, d_loss: -202.8049774169922,  g_loss: -94.79786682128906\n",
            "Training epoch 13034/1000000, d_loss: -2452.488037109375,  g_loss: -410.88372802734375\n",
            "Training epoch 13035/1000000, d_loss: -1211.293701171875,  g_loss: -142.02359008789062\n",
            "Training epoch 13036/1000000, d_loss: 229.69241333007812,  g_loss: -382.3648681640625\n",
            "Training epoch 13037/1000000, d_loss: 797.7767333984375,  g_loss: -776.0621948242188\n",
            "Training epoch 13038/1000000, d_loss: 1044.0860595703125,  g_loss: -287.4679870605469\n",
            "Training epoch 13039/1000000, d_loss: 30.748313903808594,  g_loss: -377.58587646484375\n",
            "Training epoch 13040/1000000, d_loss: 121.92378234863281,  g_loss: 53.58033752441406\n",
            "Training epoch 13041/1000000, d_loss: 21.91320037841797,  g_loss: -26.131107330322266\n",
            "Training epoch 13042/1000000, d_loss: -223.56533813476562,  g_loss: 374.8599853515625\n",
            "Training epoch 13043/1000000, d_loss: -677.0096435546875,  g_loss: 1265.281982421875\n",
            "Training epoch 13044/1000000, d_loss: -927.9154663085938,  g_loss: 1716.544189453125\n",
            "Training epoch 13045/1000000, d_loss: 67.64425659179688,  g_loss: -102.68514251708984\n",
            "Training epoch 13046/1000000, d_loss: -5.4369964599609375,  g_loss: -251.9980926513672\n",
            "Training epoch 13047/1000000, d_loss: -49.71887969970703,  g_loss: -179.25527954101562\n",
            "Training epoch 13048/1000000, d_loss: -114.87274169921875,  g_loss: -166.7913055419922\n",
            "Training epoch 13049/1000000, d_loss: -33.394046783447266,  g_loss: -217.1445770263672\n",
            "Training epoch 13050/1000000, d_loss: -62.99418640136719,  g_loss: -209.7720489501953\n",
            "Training epoch 13051/1000000, d_loss: -86.06919860839844,  g_loss: -162.079833984375\n",
            "Training epoch 13052/1000000, d_loss: -272.6536865234375,  g_loss: -74.57084655761719\n",
            "Training epoch 13053/1000000, d_loss: -23.333267211914062,  g_loss: -44.03419494628906\n",
            "Training epoch 13054/1000000, d_loss: 80.36810302734375,  g_loss: -49.055564880371094\n",
            "Training epoch 13055/1000000, d_loss: -252.81031799316406,  g_loss: -11.401615142822266\n",
            "Training epoch 13056/1000000, d_loss: -67.5028076171875,  g_loss: 14.538461685180664\n",
            "Training epoch 13057/1000000, d_loss: -160.2174835205078,  g_loss: 88.6163558959961\n",
            "Training epoch 13058/1000000, d_loss: -787.352294921875,  g_loss: -203.6043243408203\n",
            "Training epoch 13059/1000000, d_loss: 2251.37744140625,  g_loss: 102.76397705078125\n",
            "Training epoch 13060/1000000, d_loss: -141.60952758789062,  g_loss: 80.9464111328125\n",
            "Training epoch 13061/1000000, d_loss: -93.68143463134766,  g_loss: 354.3870849609375\n",
            "Training epoch 13062/1000000, d_loss: -56.712745666503906,  g_loss: 95.00439453125\n",
            "Training epoch 13063/1000000, d_loss: -423.3232421875,  g_loss: 779.7237548828125\n",
            "Training epoch 13064/1000000, d_loss: -2.7593116760253906,  g_loss: 70.7721939086914\n",
            "Training epoch 13065/1000000, d_loss: -78.11048889160156,  g_loss: 85.34136199951172\n",
            "Training epoch 13066/1000000, d_loss: 4.06109619140625,  g_loss: 134.35064697265625\n",
            "Training epoch 13067/1000000, d_loss: -120.00248718261719,  g_loss: 183.8697509765625\n",
            "Training epoch 13068/1000000, d_loss: -200.13392639160156,  g_loss: 188.6138916015625\n",
            "Training epoch 13069/1000000, d_loss: -131.7454833984375,  g_loss: 249.4717559814453\n",
            "Training epoch 13070/1000000, d_loss: -290.1014099121094,  g_loss: 355.6081848144531\n",
            "Training epoch 13071/1000000, d_loss: -117.76114654541016,  g_loss: 50.072174072265625\n",
            "Training epoch 13072/1000000, d_loss: -107.5949478149414,  g_loss: 23.18386459350586\n",
            "Training epoch 13073/1000000, d_loss: -122.72874450683594,  g_loss: 101.12469482421875\n",
            "Training epoch 13074/1000000, d_loss: -262.8523864746094,  g_loss: 55.91196060180664\n",
            "Training epoch 13075/1000000, d_loss: 26.164169311523438,  g_loss: 124.31114959716797\n",
            "Training epoch 13076/1000000, d_loss: -102.90052795410156,  g_loss: 109.25575256347656\n",
            "Training epoch 13077/1000000, d_loss: -31.94646453857422,  g_loss: 117.89955139160156\n",
            "Training epoch 13078/1000000, d_loss: -91.74674987792969,  g_loss: 188.23284912109375\n",
            "Training epoch 13079/1000000, d_loss: -394.4794006347656,  g_loss: 101.11849975585938\n",
            "Training epoch 13080/1000000, d_loss: -75.86180114746094,  g_loss: 44.82028579711914\n",
            "Training epoch 13081/1000000, d_loss: -290.642333984375,  g_loss: -17.13424301147461\n",
            "Training epoch 13082/1000000, d_loss: 1.06695556640625,  g_loss: 71.1031723022461\n",
            "Training epoch 13083/1000000, d_loss: -61.07520294189453,  g_loss: 61.64579772949219\n",
            "Training epoch 13084/1000000, d_loss: -158.52792358398438,  g_loss: 69.22459411621094\n",
            "Training epoch 13085/1000000, d_loss: -29.975887298583984,  g_loss: 100.27238464355469\n",
            "Training epoch 13086/1000000, d_loss: -213.9912109375,  g_loss: 376.23223876953125\n",
            "Training epoch 13087/1000000, d_loss: -243.47837829589844,  g_loss: 230.60214233398438\n",
            "Training epoch 13088/1000000, d_loss: -107.4527816772461,  g_loss: 195.88479614257812\n",
            "Training epoch 13089/1000000, d_loss: -140.74725341796875,  g_loss: 37.85725402832031\n",
            "Training epoch 13090/1000000, d_loss: -59.267417907714844,  g_loss: 13.003107070922852\n",
            "Training epoch 13091/1000000, d_loss: -919.1466064453125,  g_loss: -24.74175453186035\n",
            "Training epoch 13092/1000000, d_loss: -89.33621978759766,  g_loss: 30.16665267944336\n",
            "Training epoch 13093/1000000, d_loss: -90.5990982055664,  g_loss: -23.96906280517578\n",
            "Training epoch 13094/1000000, d_loss: -334.6733093261719,  g_loss: -15.041704177856445\n",
            "Training epoch 13095/1000000, d_loss: 100.09186553955078,  g_loss: -16.528060913085938\n",
            "Training epoch 13096/1000000, d_loss: -11.831756591796875,  g_loss: 20.20313835144043\n",
            "Training epoch 13097/1000000, d_loss: -64.4513931274414,  g_loss: 31.437244415283203\n",
            "Training epoch 13098/1000000, d_loss: -122.14185333251953,  g_loss: -8.7999267578125\n",
            "Training epoch 13099/1000000, d_loss: 218.56997680664062,  g_loss: 80.2331771850586\n",
            "Training epoch 13100/1000000, d_loss: -112.08843231201172,  g_loss: 77.14414978027344\n",
            "Training epoch 13101/1000000, d_loss: -654.9835815429688,  g_loss: -37.51249694824219\n",
            "Training epoch 13102/1000000, d_loss: 159.29530334472656,  g_loss: 85.92257690429688\n",
            "Training epoch 13103/1000000, d_loss: -115.90676879882812,  g_loss: 164.01312255859375\n",
            "Training epoch 13104/1000000, d_loss: -213.77423095703125,  g_loss: 256.47607421875\n",
            "Training epoch 13105/1000000, d_loss: -91.68862915039062,  g_loss: 65.44056701660156\n",
            "Training epoch 13106/1000000, d_loss: -79.37401580810547,  g_loss: 82.75751495361328\n",
            "Training epoch 13107/1000000, d_loss: -109.40180969238281,  g_loss: 51.580841064453125\n",
            "Training epoch 13108/1000000, d_loss: 11.815139770507812,  g_loss: 77.51189422607422\n",
            "Training epoch 13109/1000000, d_loss: -77.22120666503906,  g_loss: 70.35780334472656\n",
            "Training epoch 13110/1000000, d_loss: -145.30191040039062,  g_loss: 70.12498474121094\n",
            "Training epoch 13111/1000000, d_loss: -174.69570922851562,  g_loss: 32.91873550415039\n",
            "Training epoch 13112/1000000, d_loss: -83.6025619506836,  g_loss: 35.221351623535156\n",
            "Training epoch 13113/1000000, d_loss: 306.0432434082031,  g_loss: 161.82354736328125\n",
            "Training epoch 13114/1000000, d_loss: -254.81109619140625,  g_loss: 135.06173706054688\n",
            "Training epoch 13115/1000000, d_loss: -79.58329772949219,  g_loss: 103.71185302734375\n",
            "Training epoch 13116/1000000, d_loss: -61.308876037597656,  g_loss: 72.89840698242188\n",
            "Training epoch 13117/1000000, d_loss: -62.24394607543945,  g_loss: 139.95875549316406\n",
            "Training epoch 13118/1000000, d_loss: -40.091033935546875,  g_loss: 131.55099487304688\n",
            "Training epoch 13119/1000000, d_loss: -48.54383850097656,  g_loss: 124.25531005859375\n",
            "Training epoch 13120/1000000, d_loss: -47.21709442138672,  g_loss: 188.41114807128906\n",
            "Training epoch 13121/1000000, d_loss: 24.849746704101562,  g_loss: 167.01083374023438\n",
            "Training epoch 13122/1000000, d_loss: -87.99274444580078,  g_loss: 170.15151977539062\n",
            "Training epoch 13123/1000000, d_loss: -57.874732971191406,  g_loss: 147.67098999023438\n",
            "Training epoch 13124/1000000, d_loss: -90.39067077636719,  g_loss: 122.83475494384766\n",
            "Training epoch 13125/1000000, d_loss: -21.022972106933594,  g_loss: 130.33346557617188\n",
            "Training epoch 13126/1000000, d_loss: -84.78477478027344,  g_loss: 89.56637573242188\n",
            "Training epoch 13127/1000000, d_loss: -79.22290802001953,  g_loss: 105.42454528808594\n",
            "Training epoch 13128/1000000, d_loss: -122.47590637207031,  g_loss: 66.23738098144531\n",
            "Training epoch 13129/1000000, d_loss: -5.872840881347656,  g_loss: 92.60568237304688\n",
            "Training epoch 13130/1000000, d_loss: -85.62458038330078,  g_loss: 148.0258026123047\n",
            "Training epoch 13131/1000000, d_loss: -241.24925231933594,  g_loss: 140.21966552734375\n",
            "Training epoch 13132/1000000, d_loss: -45.532440185546875,  g_loss: 156.68955993652344\n",
            "Training epoch 13133/1000000, d_loss: -188.8435516357422,  g_loss: 93.88729858398438\n",
            "Training epoch 13134/1000000, d_loss: -92.05565643310547,  g_loss: 195.09222412109375\n",
            "Training epoch 13135/1000000, d_loss: -247.5504150390625,  g_loss: 57.41080856323242\n",
            "Training epoch 13136/1000000, d_loss: -309.7685852050781,  g_loss: 80.13572692871094\n",
            "Training epoch 13137/1000000, d_loss: -228.78512573242188,  g_loss: 109.65299224853516\n",
            "Training epoch 13138/1000000, d_loss: -94.95642852783203,  g_loss: -8.424324035644531\n",
            "Training epoch 13139/1000000, d_loss: -12.094053268432617,  g_loss: -54.131553649902344\n",
            "Training epoch 13140/1000000, d_loss: -41.49013137817383,  g_loss: 110.75155639648438\n",
            "Training epoch 13141/1000000, d_loss: -323.967041015625,  g_loss: 105.72172546386719\n",
            "Training epoch 13142/1000000, d_loss: -88.44747924804688,  g_loss: 49.153385162353516\n",
            "Training epoch 13143/1000000, d_loss: -303.6449890136719,  g_loss: 466.94659423828125\n",
            "Training epoch 13144/1000000, d_loss: -146.92098999023438,  g_loss: 321.50335693359375\n",
            "Training epoch 13145/1000000, d_loss: -505.7727966308594,  g_loss: 1043.383056640625\n",
            "Training epoch 13146/1000000, d_loss: -222.57467651367188,  g_loss: 44.24413299560547\n",
            "Training epoch 13147/1000000, d_loss: -76.5029296875,  g_loss: 18.497554779052734\n",
            "Training epoch 13148/1000000, d_loss: -48.66722106933594,  g_loss: 90.25333404541016\n",
            "Training epoch 13149/1000000, d_loss: -299.8206787109375,  g_loss: 47.7488899230957\n",
            "Training epoch 13150/1000000, d_loss: -176.32144165039062,  g_loss: 34.986602783203125\n",
            "Training epoch 13151/1000000, d_loss: -110.65827941894531,  g_loss: 57.82084274291992\n",
            "Training epoch 13152/1000000, d_loss: -108.66850280761719,  g_loss: 125.20468139648438\n",
            "Training epoch 13153/1000000, d_loss: -173.50680541992188,  g_loss: 99.57872009277344\n",
            "Training epoch 13154/1000000, d_loss: -476.7660827636719,  g_loss: 47.900901794433594\n",
            "Training epoch 13155/1000000, d_loss: -224.1375732421875,  g_loss: -4.903478622436523\n",
            "Training epoch 13156/1000000, d_loss: -73.1191177368164,  g_loss: 44.22415542602539\n",
            "Training epoch 13157/1000000, d_loss: -18.52655029296875,  g_loss: 116.96034240722656\n",
            "Training epoch 13158/1000000, d_loss: -164.5547637939453,  g_loss: 42.906707763671875\n",
            "Training epoch 13159/1000000, d_loss: -128.7978515625,  g_loss: 142.12303161621094\n",
            "Training epoch 13160/1000000, d_loss: -117.33039093017578,  g_loss: 127.32453918457031\n",
            "Training epoch 13161/1000000, d_loss: -61.31578826904297,  g_loss: 196.619384765625\n",
            "Training epoch 13162/1000000, d_loss: -60.725032806396484,  g_loss: 150.6787109375\n",
            "Training epoch 13163/1000000, d_loss: -354.2641906738281,  g_loss: -21.98141098022461\n",
            "Training epoch 13164/1000000, d_loss: -1462.787841796875,  g_loss: -39.37136459350586\n",
            "Training epoch 13165/1000000, d_loss: 88.84945678710938,  g_loss: 1.1467294692993164\n",
            "Training epoch 13166/1000000, d_loss: -119.49047088623047,  g_loss: 3.5043888092041016\n",
            "Training epoch 13167/1000000, d_loss: -133.83877563476562,  g_loss: 52.680809020996094\n",
            "Training epoch 13168/1000000, d_loss: -113.65886688232422,  g_loss: 61.84238815307617\n",
            "Training epoch 13169/1000000, d_loss: -87.03551483154297,  g_loss: 111.87074279785156\n",
            "Training epoch 13170/1000000, d_loss: -160.24871826171875,  g_loss: 83.40093231201172\n",
            "Training epoch 13171/1000000, d_loss: -177.00083923339844,  g_loss: 127.62460327148438\n",
            "Training epoch 13172/1000000, d_loss: -81.06434631347656,  g_loss: 82.12248229980469\n",
            "Training epoch 13173/1000000, d_loss: -313.9316101074219,  g_loss: 7.078367710113525\n",
            "Training epoch 13174/1000000, d_loss: -64.91313171386719,  g_loss: -41.312232971191406\n",
            "Training epoch 13175/1000000, d_loss: -87.43179321289062,  g_loss: -14.622400283813477\n",
            "Training epoch 13176/1000000, d_loss: -222.88922119140625,  g_loss: -40.34686279296875\n",
            "Training epoch 13177/1000000, d_loss: 15.614509582519531,  g_loss: 72.81489562988281\n",
            "Training epoch 13178/1000000, d_loss: -36.644622802734375,  g_loss: 51.891883850097656\n",
            "Training epoch 13179/1000000, d_loss: -82.56120300292969,  g_loss: 82.57186126708984\n",
            "Training epoch 13180/1000000, d_loss: -80.86551666259766,  g_loss: 122.10076904296875\n",
            "Training epoch 13181/1000000, d_loss: -221.0630340576172,  g_loss: 232.45590209960938\n",
            "Training epoch 13182/1000000, d_loss: 1.0359649658203125,  g_loss: 59.48081970214844\n",
            "Training epoch 13183/1000000, d_loss: -59.094215393066406,  g_loss: 31.617591857910156\n",
            "Training epoch 13184/1000000, d_loss: -133.64102172851562,  g_loss: 9.413009643554688\n",
            "Training epoch 13185/1000000, d_loss: -263.6979675292969,  g_loss: -115.83529663085938\n",
            "Training epoch 13186/1000000, d_loss: -564.593994140625,  g_loss: -95.37686157226562\n",
            "Training epoch 13187/1000000, d_loss: -23.729415893554688,  g_loss: 20.254039764404297\n",
            "Training epoch 13188/1000000, d_loss: -530.1787719726562,  g_loss: -91.4930419921875\n",
            "Training epoch 13189/1000000, d_loss: -155.94781494140625,  g_loss: -75.92416381835938\n",
            "Training epoch 13190/1000000, d_loss: 2.3347930908203125,  g_loss: 25.91226577758789\n",
            "Training epoch 13191/1000000, d_loss: 21.00933837890625,  g_loss: 81.12693786621094\n",
            "Training epoch 13192/1000000, d_loss: -84.33868408203125,  g_loss: 123.67499542236328\n",
            "Training epoch 13193/1000000, d_loss: -46.563865661621094,  g_loss: 95.97138977050781\n",
            "Training epoch 13194/1000000, d_loss: -131.93667602539062,  g_loss: 98.28243255615234\n",
            "Training epoch 13195/1000000, d_loss: -69.71661376953125,  g_loss: 161.70301818847656\n",
            "Training epoch 13196/1000000, d_loss: -173.32940673828125,  g_loss: 52.244911193847656\n",
            "Training epoch 13197/1000000, d_loss: -33.592002868652344,  g_loss: 82.57784271240234\n",
            "Training epoch 13198/1000000, d_loss: -822.6329345703125,  g_loss: 8.022668838500977\n",
            "Training epoch 13199/1000000, d_loss: 11.956117630004883,  g_loss: 11.900564193725586\n",
            "Training epoch 13200/1000000, d_loss: -0.8675994873046875,  g_loss: 29.022829055786133\n",
            "Training epoch 13201/1000000, d_loss: -40.437007904052734,  g_loss: 105.0791015625\n",
            "Training epoch 13202/1000000, d_loss: 34.127197265625,  g_loss: 62.91596221923828\n",
            "Training epoch 13203/1000000, d_loss: -11.657958984375,  g_loss: 90.73056030273438\n",
            "Training epoch 13204/1000000, d_loss: -49.50461196899414,  g_loss: 70.09308624267578\n",
            "Training epoch 13205/1000000, d_loss: -91.24565124511719,  g_loss: 65.63937377929688\n",
            "Training epoch 13206/1000000, d_loss: -61.44993591308594,  g_loss: 113.74897766113281\n",
            "Training epoch 13207/1000000, d_loss: -121.58363342285156,  g_loss: -14.166176795959473\n",
            "Training epoch 13208/1000000, d_loss: -642.0540771484375,  g_loss: -199.81765747070312\n",
            "Training epoch 13209/1000000, d_loss: 110.44938659667969,  g_loss: 56.848655700683594\n",
            "Training epoch 13210/1000000, d_loss: -22.593515396118164,  g_loss: 18.704864501953125\n",
            "Training epoch 13211/1000000, d_loss: -83.94825744628906,  g_loss: 62.591651916503906\n",
            "Training epoch 13212/1000000, d_loss: -102.87989044189453,  g_loss: 52.935401916503906\n",
            "Training epoch 13213/1000000, d_loss: -96.99134826660156,  g_loss: 144.08106994628906\n",
            "Training epoch 13214/1000000, d_loss: -105.05764770507812,  g_loss: 167.14340209960938\n",
            "Training epoch 13215/1000000, d_loss: -158.4832000732422,  g_loss: 108.7659683227539\n",
            "Training epoch 13216/1000000, d_loss: -169.30906677246094,  g_loss: 224.3075714111328\n",
            "Training epoch 13217/1000000, d_loss: -49.69872283935547,  g_loss: 95.20263671875\n",
            "Training epoch 13218/1000000, d_loss: -275.8565979003906,  g_loss: 75.7191162109375\n",
            "Training epoch 13219/1000000, d_loss: -754.8880615234375,  g_loss: -186.9100799560547\n",
            "Training epoch 13220/1000000, d_loss: -484.5001220703125,  g_loss: -229.44956970214844\n",
            "Training epoch 13221/1000000, d_loss: -197.5168914794922,  g_loss: -68.79377746582031\n",
            "Training epoch 13222/1000000, d_loss: -24.45452117919922,  g_loss: -1.3657283782958984\n",
            "Training epoch 13223/1000000, d_loss: -133.3651885986328,  g_loss: 393.9832763671875\n",
            "Training epoch 13224/1000000, d_loss: -28.69384765625,  g_loss: 108.73649597167969\n",
            "Training epoch 13225/1000000, d_loss: -176.45120239257812,  g_loss: 133.36703491210938\n",
            "Training epoch 13226/1000000, d_loss: -90.9383773803711,  g_loss: 164.58255004882812\n",
            "Training epoch 13227/1000000, d_loss: -12.286293029785156,  g_loss: 68.080322265625\n",
            "Training epoch 13228/1000000, d_loss: -220.66165161132812,  g_loss: 24.75564956665039\n",
            "Training epoch 13229/1000000, d_loss: -62.22386169433594,  g_loss: 90.74024200439453\n",
            "Training epoch 13230/1000000, d_loss: -376.60284423828125,  g_loss: 155.10189819335938\n",
            "Training epoch 13231/1000000, d_loss: -42.764015197753906,  g_loss: 113.95565032958984\n",
            "Training epoch 13232/1000000, d_loss: -233.31333923339844,  g_loss: 45.62479019165039\n",
            "Training epoch 13233/1000000, d_loss: -126.4557113647461,  g_loss: 176.52911376953125\n",
            "Training epoch 13234/1000000, d_loss: -149.75277709960938,  g_loss: 176.75927734375\n",
            "Training epoch 13235/1000000, d_loss: -37.16932678222656,  g_loss: 133.92037963867188\n",
            "Training epoch 13236/1000000, d_loss: -289.0306396484375,  g_loss: 451.9108581542969\n",
            "Training epoch 13237/1000000, d_loss: -100.18875885009766,  g_loss: 60.10580825805664\n",
            "Training epoch 13238/1000000, d_loss: -308.78857421875,  g_loss: -1.5384521484375\n",
            "Training epoch 13239/1000000, d_loss: -87.07958221435547,  g_loss: 43.61762237548828\n",
            "Training epoch 13240/1000000, d_loss: -27.820632934570312,  g_loss: 32.71522903442383\n",
            "Training epoch 13241/1000000, d_loss: -79.12905883789062,  g_loss: 42.44911575317383\n",
            "Training epoch 13242/1000000, d_loss: -233.7231903076172,  g_loss: -3.1437578201293945\n",
            "Training epoch 13243/1000000, d_loss: -103.99787902832031,  g_loss: 19.275066375732422\n",
            "Training epoch 13244/1000000, d_loss: -1461.169189453125,  g_loss: -223.87466430664062\n",
            "Training epoch 13245/1000000, d_loss: -336.8772888183594,  g_loss: -47.886356353759766\n",
            "Training epoch 13246/1000000, d_loss: 145.28582763671875,  g_loss: -60.09846496582031\n",
            "Training epoch 13247/1000000, d_loss: -833.740234375,  g_loss: -321.1455993652344\n",
            "Training epoch 13248/1000000, d_loss: -126.51809692382812,  g_loss: 56.13261032104492\n",
            "Training epoch 13249/1000000, d_loss: -67.65065002441406,  g_loss: 64.62995910644531\n",
            "Training epoch 13250/1000000, d_loss: -81.71863555908203,  g_loss: 30.715961456298828\n",
            "Training epoch 13251/1000000, d_loss: -63.336669921875,  g_loss: 161.76083374023438\n",
            "Training epoch 13252/1000000, d_loss: -189.5006866455078,  g_loss: 84.66429901123047\n",
            "Training epoch 13253/1000000, d_loss: 17.332237243652344,  g_loss: 49.03343963623047\n",
            "Training epoch 13254/1000000, d_loss: -66.76206970214844,  g_loss: 52.53429412841797\n",
            "Training epoch 13255/1000000, d_loss: -72.24785614013672,  g_loss: -2.483999252319336\n",
            "Training epoch 13256/1000000, d_loss: -33.483741760253906,  g_loss: -0.7883663177490234\n",
            "Training epoch 13257/1000000, d_loss: -97.87886047363281,  g_loss: -11.9693021774292\n",
            "Training epoch 13258/1000000, d_loss: -156.1875,  g_loss: -24.58582305908203\n",
            "Training epoch 13259/1000000, d_loss: -67.9473648071289,  g_loss: 30.057615280151367\n",
            "Training epoch 13260/1000000, d_loss: -85.0072021484375,  g_loss: 71.26268005371094\n",
            "Training epoch 13261/1000000, d_loss: -361.0729675292969,  g_loss: -7.932655334472656\n",
            "Training epoch 13262/1000000, d_loss: -90.40181732177734,  g_loss: 31.435245513916016\n",
            "Training epoch 13263/1000000, d_loss: -549.0596923828125,  g_loss: 74.99435424804688\n",
            "Training epoch 13264/1000000, d_loss: -22.201824188232422,  g_loss: 88.95039367675781\n",
            "Training epoch 13265/1000000, d_loss: -6.76617431640625,  g_loss: 143.0171356201172\n",
            "Training epoch 13266/1000000, d_loss: -77.74623107910156,  g_loss: 99.46571350097656\n",
            "Training epoch 13267/1000000, d_loss: -33.348541259765625,  g_loss: 32.10945510864258\n",
            "Training epoch 13268/1000000, d_loss: -362.0330810546875,  g_loss: -237.0015106201172\n",
            "Training epoch 13269/1000000, d_loss: -36.89409637451172,  g_loss: 92.80485534667969\n",
            "Training epoch 13270/1000000, d_loss: -159.22930908203125,  g_loss: 79.24027252197266\n",
            "Training epoch 13271/1000000, d_loss: -187.5792999267578,  g_loss: 110.29531860351562\n",
            "Training epoch 13272/1000000, d_loss: -149.13189697265625,  g_loss: 45.90786361694336\n",
            "Training epoch 13273/1000000, d_loss: -254.81317138671875,  g_loss: 10.116002082824707\n",
            "Training epoch 13274/1000000, d_loss: -76.36921691894531,  g_loss: 160.00042724609375\n",
            "Training epoch 13275/1000000, d_loss: -131.13555908203125,  g_loss: 174.40640258789062\n",
            "Training epoch 13276/1000000, d_loss: -60.893516540527344,  g_loss: 129.54371643066406\n",
            "Training epoch 13277/1000000, d_loss: -305.267333984375,  g_loss: -76.94435119628906\n",
            "Training epoch 13278/1000000, d_loss: -145.96046447753906,  g_loss: -0.5654945373535156\n",
            "Training epoch 13279/1000000, d_loss: -104.043212890625,  g_loss: 20.783580780029297\n",
            "Training epoch 13280/1000000, d_loss: -93.03804016113281,  g_loss: 5.385427474975586\n",
            "Training epoch 13281/1000000, d_loss: -71.33971405029297,  g_loss: 23.005718231201172\n",
            "Training epoch 13282/1000000, d_loss: -85.22801208496094,  g_loss: 146.37109375\n",
            "Training epoch 13283/1000000, d_loss: -74.26292419433594,  g_loss: 110.58372497558594\n",
            "Training epoch 13284/1000000, d_loss: -133.04190063476562,  g_loss: 42.786651611328125\n",
            "Training epoch 13285/1000000, d_loss: -74.27186584472656,  g_loss: 11.828832626342773\n",
            "Training epoch 13286/1000000, d_loss: -796.5613403320312,  g_loss: -23.763450622558594\n",
            "Training epoch 13287/1000000, d_loss: -230.8611297607422,  g_loss: -106.78047180175781\n",
            "Training epoch 13288/1000000, d_loss: 745.789794921875,  g_loss: 39.65891647338867\n",
            "Training epoch 13289/1000000, d_loss: -256.0888671875,  g_loss: -186.3502960205078\n",
            "Training epoch 13290/1000000, d_loss: -93.16133117675781,  g_loss: 30.77822494506836\n",
            "Training epoch 13291/1000000, d_loss: -278.1445007324219,  g_loss: -37.92991638183594\n",
            "Training epoch 13292/1000000, d_loss: -204.16659545898438,  g_loss: -93.43663787841797\n",
            "Training epoch 13293/1000000, d_loss: -335.1591796875,  g_loss: 235.5478973388672\n",
            "Training epoch 13294/1000000, d_loss: 54.87394714355469,  g_loss: 53.48103713989258\n",
            "Training epoch 13295/1000000, d_loss: -195.80276489257812,  g_loss: 366.65484619140625\n",
            "Training epoch 13296/1000000, d_loss: 3071.2451171875,  g_loss: 136.44760131835938\n",
            "Training epoch 13297/1000000, d_loss: 12.140045166015625,  g_loss: 100.45053100585938\n",
            "Training epoch 13298/1000000, d_loss: -190.85838317871094,  g_loss: 148.3738555908203\n",
            "Training epoch 13299/1000000, d_loss: 33.424171447753906,  g_loss: -1.6066455841064453\n",
            "Training epoch 13300/1000000, d_loss: -74.51348876953125,  g_loss: 48.07056427001953\n",
            "Training epoch 13301/1000000, d_loss: -107.28067016601562,  g_loss: 37.42156219482422\n",
            "Training epoch 13302/1000000, d_loss: -96.973876953125,  g_loss: -0.6283526420593262\n",
            "Training epoch 13303/1000000, d_loss: -113.01685333251953,  g_loss: 46.258182525634766\n",
            "Training epoch 13304/1000000, d_loss: -30.205459594726562,  g_loss: 51.0351448059082\n",
            "Training epoch 13305/1000000, d_loss: -245.92140197753906,  g_loss: 34.70854949951172\n",
            "Training epoch 13306/1000000, d_loss: -116.64507293701172,  g_loss: 27.945167541503906\n",
            "Training epoch 13307/1000000, d_loss: -29.319374084472656,  g_loss: 4.812900543212891\n",
            "Training epoch 13308/1000000, d_loss: -154.73814392089844,  g_loss: 3.449766159057617\n",
            "Training epoch 13309/1000000, d_loss: -157.61888122558594,  g_loss: 23.325313568115234\n",
            "Training epoch 13310/1000000, d_loss: -257.44122314453125,  g_loss: 85.39854431152344\n",
            "Training epoch 13311/1000000, d_loss: -89.11143493652344,  g_loss: -30.917396545410156\n",
            "Training epoch 13312/1000000, d_loss: -100.5031509399414,  g_loss: 35.08558654785156\n",
            "Training epoch 13313/1000000, d_loss: -93.4830322265625,  g_loss: 67.02104187011719\n",
            "Training epoch 13314/1000000, d_loss: -104.21453094482422,  g_loss: 106.5664291381836\n",
            "Training epoch 13315/1000000, d_loss: -85.59504699707031,  g_loss: 55.25850296020508\n",
            "Training epoch 13316/1000000, d_loss: -382.68951416015625,  g_loss: -134.8323516845703\n",
            "Training epoch 13317/1000000, d_loss: -117.23724365234375,  g_loss: -63.543785095214844\n",
            "Training epoch 13318/1000000, d_loss: -818.3917236328125,  g_loss: -82.06187438964844\n",
            "Training epoch 13319/1000000, d_loss: 32.4324951171875,  g_loss: -28.133583068847656\n",
            "Training epoch 13320/1000000, d_loss: -22.52390480041504,  g_loss: -24.732295989990234\n",
            "Training epoch 13321/1000000, d_loss: -110.90574645996094,  g_loss: -64.13241577148438\n",
            "Training epoch 13322/1000000, d_loss: -45.19523239135742,  g_loss: 46.312557220458984\n",
            "Training epoch 13323/1000000, d_loss: 942.2634887695312,  g_loss: 200.04287719726562\n",
            "Training epoch 13324/1000000, d_loss: -84.1456069946289,  g_loss: 131.64041137695312\n",
            "Training epoch 13325/1000000, d_loss: -189.0531768798828,  g_loss: 105.5490951538086\n",
            "Training epoch 13326/1000000, d_loss: -57.355812072753906,  g_loss: 52.34695816040039\n",
            "Training epoch 13327/1000000, d_loss: -149.20799255371094,  g_loss: -26.712615966796875\n",
            "Training epoch 13328/1000000, d_loss: -19.866912841796875,  g_loss: 16.13754653930664\n",
            "Training epoch 13329/1000000, d_loss: -404.4482116699219,  g_loss: -51.51802444458008\n",
            "Training epoch 13330/1000000, d_loss: -35.81455993652344,  g_loss: 35.098854064941406\n",
            "Training epoch 13331/1000000, d_loss: -119.15391540527344,  g_loss: 37.59025573730469\n",
            "Training epoch 13332/1000000, d_loss: -211.57723999023438,  g_loss: 187.2168731689453\n",
            "Training epoch 13333/1000000, d_loss: -307.7602844238281,  g_loss: 120.38247680664062\n",
            "Training epoch 13334/1000000, d_loss: -74.28390502929688,  g_loss: -2.9972898960113525\n",
            "Training epoch 13335/1000000, d_loss: -82.28699493408203,  g_loss: 30.631927490234375\n",
            "Training epoch 13336/1000000, d_loss: -86.23519134521484,  g_loss: 35.58283996582031\n",
            "Training epoch 13337/1000000, d_loss: -151.61647033691406,  g_loss: -17.52155876159668\n",
            "Training epoch 13338/1000000, d_loss: -86.5679931640625,  g_loss: 3.3551244735717773\n",
            "Training epoch 13339/1000000, d_loss: -14.503486633300781,  g_loss: -4.298757553100586\n",
            "Training epoch 13340/1000000, d_loss: -96.32323455810547,  g_loss: 2.9000275135040283\n",
            "Training epoch 13341/1000000, d_loss: -264.9472961425781,  g_loss: -21.054161071777344\n",
            "Training epoch 13342/1000000, d_loss: -180.77268981933594,  g_loss: -27.605588912963867\n",
            "Training epoch 13343/1000000, d_loss: -125.3480224609375,  g_loss: -1.330803394317627\n",
            "Training epoch 13344/1000000, d_loss: -66.68392944335938,  g_loss: 144.83160400390625\n",
            "Training epoch 13345/1000000, d_loss: -34.38954162597656,  g_loss: 36.88435745239258\n",
            "Training epoch 13346/1000000, d_loss: -432.30023193359375,  g_loss: -74.311279296875\n",
            "Training epoch 13347/1000000, d_loss: 30147.54296875,  g_loss: -27.34305191040039\n",
            "Training epoch 13348/1000000, d_loss: -9.598403930664062,  g_loss: -55.03675842285156\n",
            "Training epoch 13349/1000000, d_loss: -712.4432373046875,  g_loss: -21.020442962646484\n",
            "Training epoch 13350/1000000, d_loss: -534.07421875,  g_loss: -140.99171447753906\n",
            "Training epoch 13351/1000000, d_loss: 171.16110229492188,  g_loss: -76.15471649169922\n",
            "Training epoch 13352/1000000, d_loss: -346.67327880859375,  g_loss: -79.98766326904297\n",
            "Training epoch 13353/1000000, d_loss: 64.57035064697266,  g_loss: -151.1962890625\n",
            "Training epoch 13354/1000000, d_loss: 309.8182373046875,  g_loss: 235.36009216308594\n",
            "Training epoch 13355/1000000, d_loss: -332.5958251953125,  g_loss: 237.43797302246094\n",
            "Training epoch 13356/1000000, d_loss: 54.977752685546875,  g_loss: 222.2129364013672\n",
            "Training epoch 13357/1000000, d_loss: 210.09609985351562,  g_loss: 11.069480895996094\n",
            "Training epoch 13358/1000000, d_loss: -10.96589469909668,  g_loss: -3.7069129943847656\n",
            "Training epoch 13359/1000000, d_loss: -134.1766357421875,  g_loss: 2.5897483825683594\n",
            "Training epoch 13360/1000000, d_loss: -124.89630126953125,  g_loss: -26.681888580322266\n",
            "Training epoch 13361/1000000, d_loss: -123.61824035644531,  g_loss: 20.60199546813965\n",
            "Training epoch 13362/1000000, d_loss: 187.34124755859375,  g_loss: 53.746551513671875\n",
            "Training epoch 13363/1000000, d_loss: -125.30116271972656,  g_loss: 137.41522216796875\n",
            "Training epoch 13364/1000000, d_loss: -15.719108581542969,  g_loss: 50.34479522705078\n",
            "Training epoch 13365/1000000, d_loss: -46.63397216796875,  g_loss: 99.14942932128906\n",
            "Training epoch 13366/1000000, d_loss: -391.694580078125,  g_loss: 14.350337982177734\n",
            "Training epoch 13367/1000000, d_loss: -65.57759857177734,  g_loss: 61.69064712524414\n",
            "Training epoch 13368/1000000, d_loss: -597.9786376953125,  g_loss: -35.522621154785156\n",
            "Training epoch 13369/1000000, d_loss: -39.52179718017578,  g_loss: -67.8781967163086\n",
            "Training epoch 13370/1000000, d_loss: 13.370750427246094,  g_loss: 64.33137512207031\n",
            "Training epoch 13371/1000000, d_loss: -272.3276062011719,  g_loss: 217.02178955078125\n",
            "Training epoch 13372/1000000, d_loss: -69.09031677246094,  g_loss: 21.094451904296875\n",
            "Training epoch 13373/1000000, d_loss: -43.92036437988281,  g_loss: 24.221668243408203\n",
            "Training epoch 13374/1000000, d_loss: -108.64543914794922,  g_loss: 134.8117218017578\n",
            "Training epoch 13375/1000000, d_loss: -68.9432373046875,  g_loss: 64.78251647949219\n",
            "Training epoch 13376/1000000, d_loss: -401.1205139160156,  g_loss: -155.79298400878906\n",
            "Training epoch 13377/1000000, d_loss: -22.902664184570312,  g_loss: -13.466029167175293\n",
            "Training epoch 13378/1000000, d_loss: -166.2880401611328,  g_loss: 122.24337768554688\n",
            "Training epoch 13379/1000000, d_loss: -28.148468017578125,  g_loss: -85.0303726196289\n",
            "Training epoch 13380/1000000, d_loss: -31.993682861328125,  g_loss: 11.447999954223633\n",
            "Training epoch 13381/1000000, d_loss: -166.0670928955078,  g_loss: 53.757022857666016\n",
            "Training epoch 13382/1000000, d_loss: -318.9171142578125,  g_loss: 35.60169219970703\n",
            "Training epoch 13383/1000000, d_loss: -195.3021697998047,  g_loss: -2.017871856689453\n",
            "Training epoch 13384/1000000, d_loss: -308.36505126953125,  g_loss: 28.164196014404297\n",
            "Training epoch 13385/1000000, d_loss: -52.837215423583984,  g_loss: 7.973947525024414\n",
            "Training epoch 13386/1000000, d_loss: -114.88170623779297,  g_loss: 217.45831298828125\n",
            "Training epoch 13387/1000000, d_loss: -3.6951370239257812,  g_loss: 7.071995735168457\n",
            "Training epoch 13388/1000000, d_loss: -108.68846130371094,  g_loss: 83.7510986328125\n",
            "Training epoch 13389/1000000, d_loss: -115.67570495605469,  g_loss: 133.45394897460938\n",
            "Training epoch 13390/1000000, d_loss: -63.06536865234375,  g_loss: 71.45964813232422\n",
            "Training epoch 13391/1000000, d_loss: -83.63739776611328,  g_loss: 135.0613250732422\n",
            "Training epoch 13392/1000000, d_loss: -121.55412292480469,  g_loss: 144.9634246826172\n",
            "Training epoch 13393/1000000, d_loss: -211.8899383544922,  g_loss: -9.468361854553223\n",
            "Training epoch 13394/1000000, d_loss: -74.2918930053711,  g_loss: 62.41294479370117\n",
            "Training epoch 13395/1000000, d_loss: -109.8616943359375,  g_loss: 116.33257293701172\n",
            "Training epoch 13396/1000000, d_loss: -81.40494537353516,  g_loss: 64.06475830078125\n",
            "Training epoch 13397/1000000, d_loss: -129.49978637695312,  g_loss: 54.88063049316406\n",
            "Training epoch 13398/1000000, d_loss: -76.96775817871094,  g_loss: 41.04576110839844\n",
            "Training epoch 13399/1000000, d_loss: -131.0528564453125,  g_loss: -4.825085639953613\n",
            "Training epoch 13400/1000000, d_loss: -57.38096618652344,  g_loss: 62.94124221801758\n",
            "Training epoch 13401/1000000, d_loss: -33.86980438232422,  g_loss: 63.30967330932617\n",
            "Training epoch 13402/1000000, d_loss: -29.336109161376953,  g_loss: 65.22817993164062\n",
            "Training epoch 13403/1000000, d_loss: -604.4879760742188,  g_loss: -113.57086181640625\n",
            "Training epoch 13404/1000000, d_loss: -39.76722717285156,  g_loss: 51.08955383300781\n",
            "Training epoch 13405/1000000, d_loss: -134.27447509765625,  g_loss: 111.42074584960938\n",
            "Training epoch 13406/1000000, d_loss: -132.50054931640625,  g_loss: 54.23090362548828\n",
            "Training epoch 13407/1000000, d_loss: -74.07967376708984,  g_loss: 13.130929946899414\n",
            "Training epoch 13408/1000000, d_loss: -105.53491973876953,  g_loss: 154.9624481201172\n",
            "Training epoch 13409/1000000, d_loss: -107.83106994628906,  g_loss: 202.2389678955078\n",
            "Training epoch 13410/1000000, d_loss: -43.68199920654297,  g_loss: -15.36522102355957\n",
            "Training epoch 13411/1000000, d_loss: -119.57301330566406,  g_loss: -21.793807983398438\n",
            "Training epoch 13412/1000000, d_loss: -66.87391662597656,  g_loss: 3.036651372909546\n",
            "Training epoch 13413/1000000, d_loss: -37.89480209350586,  g_loss: 19.747024536132812\n",
            "Training epoch 13414/1000000, d_loss: -57.541526794433594,  g_loss: 22.644287109375\n",
            "Training epoch 13415/1000000, d_loss: -2197.4912109375,  g_loss: -141.98028564453125\n",
            "Training epoch 13416/1000000, d_loss: -63.989654541015625,  g_loss: -18.61663818359375\n",
            "Training epoch 13417/1000000, d_loss: -93.58612060546875,  g_loss: -8.691520690917969\n",
            "Training epoch 13418/1000000, d_loss: 4.098554611206055,  g_loss: -36.26055908203125\n",
            "Training epoch 13419/1000000, d_loss: -49.35908126831055,  g_loss: -29.960363388061523\n",
            "Training epoch 13420/1000000, d_loss: -39.718597412109375,  g_loss: 11.692813873291016\n",
            "Training epoch 13421/1000000, d_loss: -752.6292724609375,  g_loss: 37.456199645996094\n",
            "Training epoch 13422/1000000, d_loss: -229.47422790527344,  g_loss: 86.06340026855469\n",
            "Training epoch 13423/1000000, d_loss: -51.583003997802734,  g_loss: -0.5945594310760498\n",
            "Training epoch 13424/1000000, d_loss: -651.2723388671875,  g_loss: -50.07110595703125\n",
            "Training epoch 13425/1000000, d_loss: -174.61932373046875,  g_loss: -48.99007797241211\n",
            "Training epoch 13426/1000000, d_loss: -9.881828308105469,  g_loss: -2.280463218688965\n",
            "Training epoch 13427/1000000, d_loss: -105.60649871826172,  g_loss: 160.61959838867188\n",
            "Training epoch 13428/1000000, d_loss: -547.3839111328125,  g_loss: 676.740478515625\n",
            "Training epoch 13429/1000000, d_loss: -103.65206909179688,  g_loss: 199.68077087402344\n",
            "Training epoch 13430/1000000, d_loss: 8.3226318359375,  g_loss: -23.194427490234375\n",
            "Training epoch 13431/1000000, d_loss: -87.60555267333984,  g_loss: -20.791343688964844\n",
            "Training epoch 13432/1000000, d_loss: -62.776222229003906,  g_loss: 44.94744873046875\n",
            "Training epoch 13433/1000000, d_loss: -40.67106628417969,  g_loss: 18.15387725830078\n",
            "Training epoch 13434/1000000, d_loss: -134.30902099609375,  g_loss: -16.742277145385742\n",
            "Training epoch 13435/1000000, d_loss: 253.78359985351562,  g_loss: -104.20531463623047\n",
            "Training epoch 13436/1000000, d_loss: -73.60939025878906,  g_loss: -62.034385681152344\n",
            "Training epoch 13437/1000000, d_loss: -114.38955688476562,  g_loss: -45.77772521972656\n",
            "Training epoch 13438/1000000, d_loss: -588.5614013671875,  g_loss: -224.11651611328125\n",
            "Training epoch 13439/1000000, d_loss: -42.12590026855469,  g_loss: 14.503219604492188\n",
            "Training epoch 13440/1000000, d_loss: -50.26490783691406,  g_loss: -33.70783996582031\n",
            "Training epoch 13441/1000000, d_loss: -40.83763885498047,  g_loss: -26.746641159057617\n",
            "Training epoch 13442/1000000, d_loss: -28.114288330078125,  g_loss: 2.4560108184814453\n",
            "Training epoch 13443/1000000, d_loss: -423.8818664550781,  g_loss: -45.047794342041016\n",
            "Training epoch 13444/1000000, d_loss: -54.96516418457031,  g_loss: -44.862876892089844\n",
            "Training epoch 13445/1000000, d_loss: -110.10552215576172,  g_loss: -27.207080841064453\n",
            "Training epoch 13446/1000000, d_loss: -43.278507232666016,  g_loss: 19.714296340942383\n",
            "Training epoch 13447/1000000, d_loss: -82.38703918457031,  g_loss: -12.94871711730957\n",
            "Training epoch 13448/1000000, d_loss: -660.9600830078125,  g_loss: -40.73951721191406\n",
            "Training epoch 13449/1000000, d_loss: -38.712703704833984,  g_loss: -54.480628967285156\n",
            "Training epoch 13450/1000000, d_loss: -26.402637481689453,  g_loss: -38.90576934814453\n",
            "Training epoch 13451/1000000, d_loss: -126.77392578125,  g_loss: -48.10923767089844\n",
            "Training epoch 13452/1000000, d_loss: -57.40953826904297,  g_loss: -58.72459411621094\n",
            "Training epoch 13453/1000000, d_loss: -327.76806640625,  g_loss: -52.01762390136719\n",
            "Training epoch 13454/1000000, d_loss: -110.00390625,  g_loss: -11.439350128173828\n",
            "Training epoch 13455/1000000, d_loss: -371.9140625,  g_loss: -131.3960418701172\n",
            "Training epoch 13456/1000000, d_loss: -127.941650390625,  g_loss: -72.33563232421875\n",
            "Training epoch 13457/1000000, d_loss: -22.840599060058594,  g_loss: -29.135116577148438\n",
            "Training epoch 13458/1000000, d_loss: -111.36661529541016,  g_loss: -9.141407012939453\n",
            "Training epoch 13459/1000000, d_loss: -931.5691528320312,  g_loss: -372.17913818359375\n",
            "Training epoch 13460/1000000, d_loss: -39.94429397583008,  g_loss: 11.656250953674316\n",
            "Training epoch 13461/1000000, d_loss: 175.55679321289062,  g_loss: 118.66726684570312\n",
            "Training epoch 13462/1000000, d_loss: -19.50283432006836,  g_loss: -28.088836669921875\n",
            "Training epoch 13463/1000000, d_loss: -56.873382568359375,  g_loss: 23.9377498626709\n",
            "Training epoch 13464/1000000, d_loss: -168.41531372070312,  g_loss: -24.452239990234375\n",
            "Training epoch 13465/1000000, d_loss: -177.36822509765625,  g_loss: 336.6890563964844\n",
            "Training epoch 13466/1000000, d_loss: -61.56916809082031,  g_loss: 182.16305541992188\n",
            "Training epoch 13467/1000000, d_loss: 55.77655029296875,  g_loss: 53.28277587890625\n",
            "Training epoch 13468/1000000, d_loss: -305.0201721191406,  g_loss: 1.2151637077331543\n",
            "Training epoch 13469/1000000, d_loss: -118.05716705322266,  g_loss: -2.7562804222106934\n",
            "Training epoch 13470/1000000, d_loss: -89.80776977539062,  g_loss: 3.12465763092041\n",
            "Training epoch 13471/1000000, d_loss: -158.38645935058594,  g_loss: 112.30557250976562\n",
            "Training epoch 13472/1000000, d_loss: -159.74447631835938,  g_loss: 81.27144622802734\n",
            "Training epoch 13473/1000000, d_loss: -176.01641845703125,  g_loss: 32.545928955078125\n",
            "Training epoch 13474/1000000, d_loss: -275.9255676269531,  g_loss: -7.212638854980469\n",
            "Training epoch 13475/1000000, d_loss: -300.30914306640625,  g_loss: -171.9302978515625\n",
            "Training epoch 13476/1000000, d_loss: 16.60224151611328,  g_loss: -39.87067413330078\n",
            "Training epoch 13477/1000000, d_loss: -66.18815612792969,  g_loss: 43.409454345703125\n",
            "Training epoch 13478/1000000, d_loss: -276.1051330566406,  g_loss: 2.609219551086426\n",
            "Training epoch 13479/1000000, d_loss: -646.4564208984375,  g_loss: -336.12603759765625\n",
            "Training epoch 13480/1000000, d_loss: -167.39291381835938,  g_loss: -42.283363342285156\n",
            "Training epoch 13481/1000000, d_loss: -6.265830993652344,  g_loss: 52.70585632324219\n",
            "Training epoch 13482/1000000, d_loss: 77.23335266113281,  g_loss: -34.83836364746094\n",
            "Training epoch 13483/1000000, d_loss: -12.846778869628906,  g_loss: -49.36725997924805\n",
            "Training epoch 13484/1000000, d_loss: -215.09848022460938,  g_loss: 48.02814865112305\n",
            "Training epoch 13485/1000000, d_loss: -286.1911926269531,  g_loss: -74.11566162109375\n",
            "Training epoch 13486/1000000, d_loss: -28.331756591796875,  g_loss: -26.934688568115234\n",
            "Training epoch 13487/1000000, d_loss: -122.98637390136719,  g_loss: 97.59934997558594\n",
            "Training epoch 13488/1000000, d_loss: -37.45306396484375,  g_loss: 7.0344953536987305\n",
            "Training epoch 13489/1000000, d_loss: -89.00717163085938,  g_loss: 128.5137939453125\n",
            "Training epoch 13490/1000000, d_loss: -345.91583251953125,  g_loss: 4.220561504364014\n",
            "Training epoch 13491/1000000, d_loss: -251.56024169921875,  g_loss: 705.9659423828125\n",
            "Training epoch 13492/1000000, d_loss: -141.60861206054688,  g_loss: 80.49823760986328\n",
            "Training epoch 13493/1000000, d_loss: -165.0158233642578,  g_loss: 117.35060119628906\n",
            "Training epoch 13494/1000000, d_loss: 13.965873718261719,  g_loss: -7.985867500305176\n",
            "Training epoch 13495/1000000, d_loss: -186.5471954345703,  g_loss: 83.04647064208984\n",
            "Training epoch 13496/1000000, d_loss: -247.08114624023438,  g_loss: 252.681396484375\n",
            "Training epoch 13497/1000000, d_loss: -41.19899368286133,  g_loss: -26.596187591552734\n",
            "Training epoch 13498/1000000, d_loss: -190.5565185546875,  g_loss: 71.01146697998047\n",
            "Training epoch 13499/1000000, d_loss: -59.00684356689453,  g_loss: 31.261798858642578\n",
            "Training epoch 13500/1000000, d_loss: -353.6163330078125,  g_loss: -6.147616386413574\n",
            "Training epoch 13501/1000000, d_loss: -149.14434814453125,  g_loss: -40.50801086425781\n",
            "Training epoch 13502/1000000, d_loss: -144.13021850585938,  g_loss: -8.318304061889648\n",
            "Training epoch 13503/1000000, d_loss: -63.38239288330078,  g_loss: -23.559062957763672\n",
            "Training epoch 13504/1000000, d_loss: -97.31521606445312,  g_loss: -55.7884521484375\n",
            "Training epoch 13505/1000000, d_loss: -95.926025390625,  g_loss: -38.32795715332031\n",
            "Training epoch 13506/1000000, d_loss: -154.4792022705078,  g_loss: 32.67359161376953\n",
            "Training epoch 13507/1000000, d_loss: -679.693115234375,  g_loss: -101.78706359863281\n",
            "Training epoch 13508/1000000, d_loss: -83.52938079833984,  g_loss: -57.54347610473633\n",
            "Training epoch 13509/1000000, d_loss: -64.36695861816406,  g_loss: -18.231151580810547\n",
            "Training epoch 13510/1000000, d_loss: -350.833984375,  g_loss: -82.32172393798828\n",
            "Training epoch 13511/1000000, d_loss: -1470.8323974609375,  g_loss: -1164.470703125\n",
            "Training epoch 13512/1000000, d_loss: 213.1678924560547,  g_loss: -36.67424774169922\n",
            "Training epoch 13513/1000000, d_loss: 726.958984375,  g_loss: -141.06216430664062\n",
            "Training epoch 13514/1000000, d_loss: -72.50625610351562,  g_loss: -45.526588439941406\n",
            "Training epoch 13515/1000000, d_loss: -703.8639526367188,  g_loss: -234.0127716064453\n",
            "Training epoch 13516/1000000, d_loss: 163.59130859375,  g_loss: -354.535400390625\n",
            "Training epoch 13517/1000000, d_loss: 1.2928924560546875,  g_loss: 38.84551239013672\n",
            "Training epoch 13518/1000000, d_loss: 2131.657958984375,  g_loss: -273.5559387207031\n",
            "Training epoch 13519/1000000, d_loss: -69.17493438720703,  g_loss: -82.6815185546875\n",
            "Training epoch 13520/1000000, d_loss: -51.63966369628906,  g_loss: -79.15411376953125\n",
            "Training epoch 13521/1000000, d_loss: -73.00361633300781,  g_loss: -91.16236114501953\n",
            "Training epoch 13522/1000000, d_loss: -166.94625854492188,  g_loss: -31.866161346435547\n",
            "Training epoch 13523/1000000, d_loss: -144.6936492919922,  g_loss: 45.571617126464844\n",
            "Training epoch 13524/1000000, d_loss: -46.32770538330078,  g_loss: -12.927596092224121\n",
            "Training epoch 13525/1000000, d_loss: -39.26679992675781,  g_loss: 28.215612411499023\n",
            "Training epoch 13526/1000000, d_loss: -219.98118591308594,  g_loss: -28.756616592407227\n",
            "Training epoch 13527/1000000, d_loss: -43.84504699707031,  g_loss: -1.4946956634521484\n",
            "Training epoch 13528/1000000, d_loss: -295.6930236816406,  g_loss: -276.0576171875\n",
            "Training epoch 13529/1000000, d_loss: -158.78697204589844,  g_loss: -70.52914428710938\n",
            "Training epoch 13530/1000000, d_loss: -96.61677551269531,  g_loss: 1.8552932739257812\n",
            "Training epoch 13531/1000000, d_loss: -25.909658432006836,  g_loss: 3.875560760498047\n",
            "Training epoch 13532/1000000, d_loss: -267.1839294433594,  g_loss: -69.06234741210938\n",
            "Training epoch 13533/1000000, d_loss: -129.90480041503906,  g_loss: -56.67225646972656\n",
            "Training epoch 13534/1000000, d_loss: 41.603126525878906,  g_loss: 66.06578826904297\n",
            "Training epoch 13535/1000000, d_loss: -263.55035400390625,  g_loss: 264.4635925292969\n",
            "Training epoch 13536/1000000, d_loss: -81.14798736572266,  g_loss: 62.27051544189453\n",
            "Training epoch 13537/1000000, d_loss: -21.010066986083984,  g_loss: -3.403923511505127\n",
            "Training epoch 13538/1000000, d_loss: -60.50904083251953,  g_loss: 41.7581672668457\n",
            "Training epoch 13539/1000000, d_loss: -149.79075622558594,  g_loss: -24.419677734375\n",
            "Training epoch 13540/1000000, d_loss: 30.598480224609375,  g_loss: 63.077640533447266\n",
            "Training epoch 13541/1000000, d_loss: -128.55873107910156,  g_loss: 52.937767028808594\n",
            "Training epoch 13542/1000000, d_loss: -53.02265167236328,  g_loss: 84.95587158203125\n",
            "Training epoch 13543/1000000, d_loss: -48.878318786621094,  g_loss: 21.40293312072754\n",
            "Training epoch 13544/1000000, d_loss: -43.545249938964844,  g_loss: 19.700666427612305\n",
            "Training epoch 13545/1000000, d_loss: -127.66439056396484,  g_loss: 7.836127281188965\n",
            "Training epoch 13546/1000000, d_loss: -292.71942138671875,  g_loss: 7.578596115112305\n",
            "Training epoch 13547/1000000, d_loss: -81.66444396972656,  g_loss: 3.102947235107422\n",
            "Training epoch 13548/1000000, d_loss: -91.79702758789062,  g_loss: 117.08103942871094\n",
            "Training epoch 13549/1000000, d_loss: -341.7747802734375,  g_loss: -111.37641906738281\n",
            "Training epoch 13550/1000000, d_loss: -78.14451599121094,  g_loss: 90.33214569091797\n",
            "Training epoch 13551/1000000, d_loss: -240.173583984375,  g_loss: 74.29827117919922\n",
            "Training epoch 13552/1000000, d_loss: -131.9680938720703,  g_loss: 94.5092544555664\n",
            "Training epoch 13553/1000000, d_loss: -53.428619384765625,  g_loss: 258.5452575683594\n",
            "Training epoch 13554/1000000, d_loss: -69.30745697021484,  g_loss: 166.14794921875\n",
            "Training epoch 13555/1000000, d_loss: -40.868133544921875,  g_loss: 42.620826721191406\n",
            "Training epoch 13556/1000000, d_loss: -51.410072326660156,  g_loss: 31.45587730407715\n",
            "Training epoch 13557/1000000, d_loss: -116.27770233154297,  g_loss: 32.205787658691406\n",
            "Training epoch 13558/1000000, d_loss: -82.40738677978516,  g_loss: 8.913472175598145\n",
            "Training epoch 13559/1000000, d_loss: -30.576045989990234,  g_loss: 48.24237823486328\n",
            "Training epoch 13560/1000000, d_loss: -252.44677734375,  g_loss: 27.331384658813477\n",
            "Training epoch 13561/1000000, d_loss: -69.79963684082031,  g_loss: -9.097795486450195\n",
            "Training epoch 13562/1000000, d_loss: -90.31745910644531,  g_loss: 48.41408157348633\n",
            "Training epoch 13563/1000000, d_loss: -170.0626220703125,  g_loss: 194.41278076171875\n",
            "Training epoch 13564/1000000, d_loss: -108.35993194580078,  g_loss: 182.5215301513672\n",
            "Training epoch 13565/1000000, d_loss: -145.68313598632812,  g_loss: 204.666748046875\n",
            "Training epoch 13566/1000000, d_loss: -123.1299057006836,  g_loss: -12.513432502746582\n",
            "Training epoch 13567/1000000, d_loss: -263.97589111328125,  g_loss: -182.11724853515625\n",
            "Training epoch 13568/1000000, d_loss: 4.209911346435547,  g_loss: 5.390336990356445\n",
            "Training epoch 13569/1000000, d_loss: -395.11346435546875,  g_loss: -23.649717330932617\n",
            "Training epoch 13570/1000000, d_loss: 44.96000671386719,  g_loss: -12.650667190551758\n",
            "Training epoch 13571/1000000, d_loss: -956.2022705078125,  g_loss: -110.30342102050781\n",
            "Training epoch 13572/1000000, d_loss: -83.32213592529297,  g_loss: -20.349815368652344\n",
            "Training epoch 13573/1000000, d_loss: -345.2951965332031,  g_loss: -23.13188934326172\n",
            "Training epoch 13574/1000000, d_loss: -58.48497009277344,  g_loss: 37.98438262939453\n",
            "Training epoch 13575/1000000, d_loss: -492.6590576171875,  g_loss: -0.7015657424926758\n",
            "Training epoch 13576/1000000, d_loss: -386.7816162109375,  g_loss: -125.37222290039062\n",
            "Training epoch 13577/1000000, d_loss: -77.32322692871094,  g_loss: -104.40533447265625\n",
            "Training epoch 13578/1000000, d_loss: -224.98617553710938,  g_loss: 129.07183837890625\n",
            "Training epoch 13579/1000000, d_loss: -104.56487274169922,  g_loss: 39.9502067565918\n",
            "Training epoch 13580/1000000, d_loss: -921.2094116210938,  g_loss: -160.57406616210938\n",
            "Training epoch 13581/1000000, d_loss: -92.33828735351562,  g_loss: 84.24879455566406\n",
            "Training epoch 13582/1000000, d_loss: -404.2513427734375,  g_loss: 233.1368408203125\n",
            "Training epoch 13583/1000000, d_loss: -193.24571228027344,  g_loss: 197.33563232421875\n",
            "Training epoch 13584/1000000, d_loss: 56.18521499633789,  g_loss: 110.51863098144531\n",
            "Training epoch 13585/1000000, d_loss: -126.4526596069336,  g_loss: 22.390666961669922\n",
            "Training epoch 13586/1000000, d_loss: -56.961402893066406,  g_loss: 35.0510139465332\n",
            "Training epoch 13587/1000000, d_loss: -184.48109436035156,  g_loss: -22.895008087158203\n",
            "Training epoch 13588/1000000, d_loss: -338.513427734375,  g_loss: -138.17599487304688\n",
            "Training epoch 13589/1000000, d_loss: -14.931182861328125,  g_loss: -7.252464294433594\n",
            "Training epoch 13590/1000000, d_loss: -139.262451171875,  g_loss: 87.81222534179688\n",
            "Training epoch 13591/1000000, d_loss: -87.33295440673828,  g_loss: 127.09272766113281\n",
            "Training epoch 13592/1000000, d_loss: -116.38822174072266,  g_loss: 65.40878295898438\n",
            "Training epoch 13593/1000000, d_loss: -213.61224365234375,  g_loss: 7.22275447845459\n",
            "Training epoch 13594/1000000, d_loss: -222.4139404296875,  g_loss: 13.421215057373047\n",
            "Training epoch 13595/1000000, d_loss: -181.37142944335938,  g_loss: 20.002653121948242\n",
            "Training epoch 13596/1000000, d_loss: -120.22528076171875,  g_loss: 45.741092681884766\n",
            "Training epoch 13597/1000000, d_loss: -85.01483154296875,  g_loss: 60.04078674316406\n",
            "Training epoch 13598/1000000, d_loss: 38.94415283203125,  g_loss: 18.403099060058594\n",
            "Training epoch 13599/1000000, d_loss: -170.61529541015625,  g_loss: 33.268741607666016\n",
            "Training epoch 13600/1000000, d_loss: -212.63819885253906,  g_loss: -70.42753601074219\n",
            "Training epoch 13601/1000000, d_loss: -42.93463134765625,  g_loss: 21.391529083251953\n",
            "Training epoch 13602/1000000, d_loss: -224.48635864257812,  g_loss: 248.3558349609375\n",
            "Training epoch 13603/1000000, d_loss: -224.94573974609375,  g_loss: -109.48920440673828\n",
            "Training epoch 13604/1000000, d_loss: -172.81231689453125,  g_loss: 41.88990783691406\n",
            "Training epoch 13605/1000000, d_loss: -67.98145294189453,  g_loss: 5.822903633117676\n",
            "Training epoch 13606/1000000, d_loss: -201.67315673828125,  g_loss: -26.106237411499023\n",
            "Training epoch 13607/1000000, d_loss: -90.87930297851562,  g_loss: 51.292171478271484\n",
            "Training epoch 13608/1000000, d_loss: -162.25770568847656,  g_loss: -67.87887573242188\n",
            "Training epoch 13609/1000000, d_loss: -35.21659851074219,  g_loss: -35.866920471191406\n",
            "Training epoch 13610/1000000, d_loss: 29.988876342773438,  g_loss: 72.43382263183594\n",
            "Training epoch 13611/1000000, d_loss: -124.12064361572266,  g_loss: 62.92692565917969\n",
            "Training epoch 13612/1000000, d_loss: -158.8802947998047,  g_loss: 195.3162384033203\n",
            "Training epoch 13613/1000000, d_loss: -25.849628448486328,  g_loss: 102.59809112548828\n",
            "Training epoch 13614/1000000, d_loss: -21.087467193603516,  g_loss: 50.070343017578125\n",
            "Training epoch 13615/1000000, d_loss: -122.53333282470703,  g_loss: 28.301658630371094\n",
            "Training epoch 13616/1000000, d_loss: -116.67662811279297,  g_loss: -2.429957866668701\n",
            "Training epoch 13617/1000000, d_loss: -83.50318908691406,  g_loss: -0.14938831329345703\n",
            "Training epoch 13618/1000000, d_loss: -248.490966796875,  g_loss: -9.388136863708496\n",
            "Training epoch 13619/1000000, d_loss: -898.8326416015625,  g_loss: -132.06509399414062\n",
            "Training epoch 13620/1000000, d_loss: -4.028575897216797,  g_loss: -58.72813415527344\n",
            "Training epoch 13621/1000000, d_loss: -38.45745086669922,  g_loss: -78.44729614257812\n",
            "Training epoch 13622/1000000, d_loss: -986.747802734375,  g_loss: -229.38885498046875\n",
            "Training epoch 13623/1000000, d_loss: -45.97496032714844,  g_loss: -115.90309143066406\n",
            "Training epoch 13624/1000000, d_loss: 98.701416015625,  g_loss: -109.78433990478516\n",
            "Training epoch 13625/1000000, d_loss: -19.567941665649414,  g_loss: -58.89403533935547\n",
            "Training epoch 13626/1000000, d_loss: -0.94879150390625,  g_loss: -36.440650939941406\n",
            "Training epoch 13627/1000000, d_loss: -258.96942138671875,  g_loss: 2.8647918701171875\n",
            "Training epoch 13628/1000000, d_loss: -847.8749389648438,  g_loss: -173.76019287109375\n",
            "Training epoch 13629/1000000, d_loss: 18.320404052734375,  g_loss: -118.26670837402344\n",
            "Training epoch 13630/1000000, d_loss: -138.37913513183594,  g_loss: -104.69114685058594\n",
            "Training epoch 13631/1000000, d_loss: -76.22734069824219,  g_loss: 46.13969039916992\n",
            "Training epoch 13632/1000000, d_loss: -130.8531494140625,  g_loss: 136.41860961914062\n",
            "Training epoch 13633/1000000, d_loss: -169.8095703125,  g_loss: 82.1905517578125\n",
            "Training epoch 13634/1000000, d_loss: 9.341644287109375,  g_loss: 38.693931579589844\n",
            "Training epoch 13635/1000000, d_loss: -138.91537475585938,  g_loss: 50.22740936279297\n",
            "Training epoch 13636/1000000, d_loss: -175.5595245361328,  g_loss: -49.628395080566406\n",
            "Training epoch 13637/1000000, d_loss: -144.38279724121094,  g_loss: 3.957275390625\n",
            "Training epoch 13638/1000000, d_loss: -66.62702941894531,  g_loss: -11.071802139282227\n",
            "Training epoch 13639/1000000, d_loss: -372.028076171875,  g_loss: -43.111473083496094\n",
            "Training epoch 13640/1000000, d_loss: -203.74354553222656,  g_loss: -54.277008056640625\n",
            "Training epoch 13641/1000000, d_loss: -48.75942611694336,  g_loss: 19.077844619750977\n",
            "Training epoch 13642/1000000, d_loss: -89.15048217773438,  g_loss: -10.565564155578613\n",
            "Training epoch 13643/1000000, d_loss: -0.32646942138671875,  g_loss: 17.649959564208984\n",
            "Training epoch 13644/1000000, d_loss: -76.05415344238281,  g_loss: 82.85330963134766\n",
            "Training epoch 13645/1000000, d_loss: -126.83204650878906,  g_loss: 10.395484924316406\n",
            "Training epoch 13646/1000000, d_loss: -176.93173217773438,  g_loss: 14.610067367553711\n",
            "Training epoch 13647/1000000, d_loss: 251.2919921875,  g_loss: -65.39749145507812\n",
            "Training epoch 13648/1000000, d_loss: -96.44923400878906,  g_loss: -78.79076385498047\n",
            "Training epoch 13649/1000000, d_loss: -56.82195281982422,  g_loss: -9.447240829467773\n",
            "Training epoch 13650/1000000, d_loss: -167.385009765625,  g_loss: 284.91094970703125\n",
            "Training epoch 13651/1000000, d_loss: -121.61561584472656,  g_loss: 133.822265625\n",
            "Training epoch 13652/1000000, d_loss: -66.38806915283203,  g_loss: 78.90925598144531\n",
            "Training epoch 13653/1000000, d_loss: -153.35577392578125,  g_loss: 100.35475158691406\n",
            "Training epoch 13654/1000000, d_loss: -75.28546142578125,  g_loss: -13.111084938049316\n",
            "Training epoch 13655/1000000, d_loss: -226.1111297607422,  g_loss: -7.824200630187988\n",
            "Training epoch 13656/1000000, d_loss: -24.204574584960938,  g_loss: -4.135267734527588\n",
            "Training epoch 13657/1000000, d_loss: -836.9596557617188,  g_loss: -249.62515258789062\n",
            "Training epoch 13658/1000000, d_loss: -40.06763458251953,  g_loss: -16.6167049407959\n",
            "Training epoch 13659/1000000, d_loss: -529.1102294921875,  g_loss: -146.76756286621094\n",
            "Training epoch 13660/1000000, d_loss: -65.4788818359375,  g_loss: 126.69583129882812\n",
            "Training epoch 13661/1000000, d_loss: -73.07594299316406,  g_loss: 33.80525588989258\n",
            "Training epoch 13662/1000000, d_loss: -66.17308807373047,  g_loss: 127.72634887695312\n",
            "Training epoch 13663/1000000, d_loss: -162.27532958984375,  g_loss: 232.50482177734375\n",
            "Training epoch 13664/1000000, d_loss: -23.433502197265625,  g_loss: 21.828319549560547\n",
            "Training epoch 13665/1000000, d_loss: -46.52649688720703,  g_loss: 13.211870193481445\n",
            "Training epoch 13666/1000000, d_loss: -213.139892578125,  g_loss: -55.33673095703125\n",
            "Training epoch 13667/1000000, d_loss: -12.960853576660156,  g_loss: -30.553707122802734\n",
            "Training epoch 13668/1000000, d_loss: -129.45628356933594,  g_loss: -6.9912309646606445\n",
            "Training epoch 13669/1000000, d_loss: -92.83338928222656,  g_loss: 14.990313529968262\n",
            "Training epoch 13670/1000000, d_loss: -135.04794311523438,  g_loss: 17.85533905029297\n",
            "Training epoch 13671/1000000, d_loss: -110.28488159179688,  g_loss: 67.38751983642578\n",
            "Training epoch 13672/1000000, d_loss: -185.6734619140625,  g_loss: -58.722633361816406\n",
            "Training epoch 13673/1000000, d_loss: -43.342811584472656,  g_loss: -39.000831604003906\n",
            "Training epoch 13674/1000000, d_loss: 1459.628173828125,  g_loss: 6.217034816741943\n",
            "Training epoch 13675/1000000, d_loss: 11.450546264648438,  g_loss: 40.402915954589844\n",
            "Training epoch 13676/1000000, d_loss: -101.60263061523438,  g_loss: 49.28697967529297\n",
            "Training epoch 13677/1000000, d_loss: -661.5035400390625,  g_loss: -38.76527786254883\n",
            "Training epoch 13678/1000000, d_loss: -24.35350799560547,  g_loss: -18.442790985107422\n",
            "Training epoch 13679/1000000, d_loss: -120.9118881225586,  g_loss: 11.76636028289795\n",
            "Training epoch 13680/1000000, d_loss: -220.5601043701172,  g_loss: 10.39320182800293\n",
            "Training epoch 13681/1000000, d_loss: 2540.851806640625,  g_loss: 120.46653747558594\n",
            "Training epoch 13682/1000000, d_loss: -112.80909729003906,  g_loss: 115.15432739257812\n",
            "Training epoch 13683/1000000, d_loss: -38.187461853027344,  g_loss: 114.72610473632812\n",
            "Training epoch 13684/1000000, d_loss: -179.8875732421875,  g_loss: 164.0810546875\n",
            "Training epoch 13685/1000000, d_loss: 105.81527709960938,  g_loss: -37.890098571777344\n",
            "Training epoch 13686/1000000, d_loss: -74.57430267333984,  g_loss: 23.735658645629883\n",
            "Training epoch 13687/1000000, d_loss: -117.8462142944336,  g_loss: -45.10259246826172\n",
            "Training epoch 13688/1000000, d_loss: -201.9396514892578,  g_loss: -38.4897575378418\n",
            "Training epoch 13689/1000000, d_loss: -132.6217041015625,  g_loss: -0.7594766616821289\n",
            "Training epoch 13690/1000000, d_loss: -358.5667724609375,  g_loss: -87.5118408203125\n",
            "Training epoch 13691/1000000, d_loss: -118.27336120605469,  g_loss: 153.8353271484375\n",
            "Training epoch 13692/1000000, d_loss: -97.64469909667969,  g_loss: 118.27705383300781\n",
            "Training epoch 13693/1000000, d_loss: -246.71533203125,  g_loss: -24.989734649658203\n",
            "Training epoch 13694/1000000, d_loss: -126.07991790771484,  g_loss: 40.670623779296875\n",
            "Training epoch 13695/1000000, d_loss: -75.63165283203125,  g_loss: 61.647586822509766\n",
            "Training epoch 13696/1000000, d_loss: -549.7403564453125,  g_loss: -139.7174072265625\n",
            "Training epoch 13697/1000000, d_loss: 753.8685302734375,  g_loss: 74.60614776611328\n",
            "Training epoch 13698/1000000, d_loss: -59.77873992919922,  g_loss: 52.36152648925781\n",
            "Training epoch 13699/1000000, d_loss: -82.36558532714844,  g_loss: 62.423126220703125\n",
            "Training epoch 13700/1000000, d_loss: -79.29974365234375,  g_loss: 33.33872604370117\n",
            "Training epoch 13701/1000000, d_loss: -47.938743591308594,  g_loss: 55.98637008666992\n",
            "Training epoch 13702/1000000, d_loss: -59.194358825683594,  g_loss: 74.05574035644531\n",
            "Training epoch 13703/1000000, d_loss: -72.06071472167969,  g_loss: 56.707664489746094\n",
            "Training epoch 13704/1000000, d_loss: -147.37425231933594,  g_loss: 21.162534713745117\n",
            "Training epoch 13705/1000000, d_loss: 398.3778076171875,  g_loss: 108.10350036621094\n",
            "Training epoch 13706/1000000, d_loss: 10.465831756591797,  g_loss: 85.30599212646484\n",
            "Training epoch 13707/1000000, d_loss: -87.64979553222656,  g_loss: 110.78260803222656\n",
            "Training epoch 13708/1000000, d_loss: -50.08318328857422,  g_loss: 93.13499450683594\n",
            "Training epoch 13709/1000000, d_loss: -50.575233459472656,  g_loss: 87.4453125\n",
            "Training epoch 13710/1000000, d_loss: -120.21654510498047,  g_loss: 74.81100463867188\n",
            "Training epoch 13711/1000000, d_loss: -227.81826782226562,  g_loss: 24.10369110107422\n",
            "Training epoch 13712/1000000, d_loss: -175.48989868164062,  g_loss: 118.80436706542969\n",
            "Training epoch 13713/1000000, d_loss: -250.66348266601562,  g_loss: 102.98818969726562\n",
            "Training epoch 13714/1000000, d_loss: -124.58528137207031,  g_loss: 71.53040313720703\n",
            "Training epoch 13715/1000000, d_loss: -45.982086181640625,  g_loss: 68.74353790283203\n",
            "Training epoch 13716/1000000, d_loss: -132.13485717773438,  g_loss: 53.21855163574219\n",
            "Training epoch 13717/1000000, d_loss: 5.108955383300781,  g_loss: 59.33925247192383\n",
            "Training epoch 13718/1000000, d_loss: -38.738807678222656,  g_loss: 54.28605270385742\n",
            "Training epoch 13719/1000000, d_loss: -57.71715545654297,  g_loss: 48.60511779785156\n",
            "Training epoch 13720/1000000, d_loss: -224.65245056152344,  g_loss: 18.891998291015625\n",
            "Training epoch 13721/1000000, d_loss: -638.0059814453125,  g_loss: -172.6124725341797\n",
            "Training epoch 13722/1000000, d_loss: -37.765541076660156,  g_loss: -67.09855651855469\n",
            "Training epoch 13723/1000000, d_loss: -20.070491790771484,  g_loss: 14.431228637695312\n",
            "Training epoch 13724/1000000, d_loss: -64.62690734863281,  g_loss: 78.40914916992188\n",
            "Training epoch 13725/1000000, d_loss: -76.3976058959961,  g_loss: 182.95428466796875\n",
            "Training epoch 13726/1000000, d_loss: -72.95932006835938,  g_loss: 134.2538299560547\n",
            "Training epoch 13727/1000000, d_loss: -104.32830047607422,  g_loss: 38.23497772216797\n",
            "Training epoch 13728/1000000, d_loss: -119.83234405517578,  g_loss: 48.3865852355957\n",
            "Training epoch 13729/1000000, d_loss: -294.7584533691406,  g_loss: 3.54478120803833\n",
            "Training epoch 13730/1000000, d_loss: -30.859798431396484,  g_loss: 72.396484375\n",
            "Training epoch 13731/1000000, d_loss: -79.8371810913086,  g_loss: 79.55693054199219\n",
            "Training epoch 13732/1000000, d_loss: -44.573875427246094,  g_loss: 56.53715515136719\n",
            "Training epoch 13733/1000000, d_loss: -229.5813446044922,  g_loss: 23.743698120117188\n",
            "Training epoch 13734/1000000, d_loss: -78.37220764160156,  g_loss: 32.784698486328125\n",
            "Training epoch 13735/1000000, d_loss: -142.0923309326172,  g_loss: 39.82202911376953\n",
            "Training epoch 13736/1000000, d_loss: -140.04998779296875,  g_loss: 21.887189865112305\n",
            "Training epoch 13737/1000000, d_loss: -108.09911346435547,  g_loss: 19.614822387695312\n",
            "Training epoch 13738/1000000, d_loss: -104.42764282226562,  g_loss: -12.29443359375\n",
            "Training epoch 13739/1000000, d_loss: -93.18757629394531,  g_loss: 47.67182922363281\n",
            "Training epoch 13740/1000000, d_loss: -411.98590087890625,  g_loss: -12.882558822631836\n",
            "Training epoch 13741/1000000, d_loss: -6.596668243408203,  g_loss: -28.598350524902344\n",
            "Training epoch 13742/1000000, d_loss: -139.30923461914062,  g_loss: -86.36087036132812\n",
            "Training epoch 13743/1000000, d_loss: -157.70358276367188,  g_loss: -248.67788696289062\n",
            "Training epoch 13744/1000000, d_loss: -418.271728515625,  g_loss: -522.0471801757812\n",
            "Training epoch 13745/1000000, d_loss: -327.1659240722656,  g_loss: -104.96620178222656\n",
            "Training epoch 13746/1000000, d_loss: 66.96893310546875,  g_loss: 86.83231353759766\n",
            "Training epoch 13747/1000000, d_loss: 71.5547103881836,  g_loss: 155.58120727539062\n",
            "Training epoch 13748/1000000, d_loss: -174.54806518554688,  g_loss: 401.91534423828125\n",
            "Training epoch 13749/1000000, d_loss: -140.67098999023438,  g_loss: 173.98922729492188\n",
            "Training epoch 13750/1000000, d_loss: -11.175369262695312,  g_loss: 29.82733726501465\n",
            "Training epoch 13751/1000000, d_loss: -175.2146453857422,  g_loss: 19.888010025024414\n",
            "Training epoch 13752/1000000, d_loss: -79.63645935058594,  g_loss: -28.423954010009766\n",
            "Training epoch 13753/1000000, d_loss: -161.76295471191406,  g_loss: 9.577442169189453\n",
            "Training epoch 13754/1000000, d_loss: -216.43753051757812,  g_loss: 205.16522216796875\n",
            "Training epoch 13755/1000000, d_loss: -29.692569732666016,  g_loss: -37.554115295410156\n",
            "Training epoch 13756/1000000, d_loss: -99.49465942382812,  g_loss: -8.989531517028809\n",
            "Training epoch 13757/1000000, d_loss: -280.2778625488281,  g_loss: -116.68655395507812\n",
            "Training epoch 13758/1000000, d_loss: -66.17533874511719,  g_loss: 5.590137958526611\n",
            "Training epoch 13759/1000000, d_loss: -95.76104736328125,  g_loss: 67.86640930175781\n",
            "Training epoch 13760/1000000, d_loss: -200.23233032226562,  g_loss: 163.92080688476562\n",
            "Training epoch 13761/1000000, d_loss: -89.10594177246094,  g_loss: 115.52171325683594\n",
            "Training epoch 13762/1000000, d_loss: -191.11827087402344,  g_loss: 91.71748352050781\n",
            "Training epoch 13763/1000000, d_loss: -50.20364761352539,  g_loss: 67.582763671875\n",
            "Training epoch 13764/1000000, d_loss: -112.96207427978516,  g_loss: 46.92456817626953\n",
            "Training epoch 13765/1000000, d_loss: -77.53495788574219,  g_loss: 15.346548080444336\n",
            "Training epoch 13766/1000000, d_loss: -1170.9688720703125,  g_loss: -35.39265060424805\n",
            "Training epoch 13767/1000000, d_loss: -93.99939727783203,  g_loss: -178.05873107910156\n",
            "Training epoch 13768/1000000, d_loss: 60.26980209350586,  g_loss: -61.529144287109375\n",
            "Training epoch 13769/1000000, d_loss: -25.535415649414062,  g_loss: 56.8856315612793\n",
            "Training epoch 13770/1000000, d_loss: -60.5059814453125,  g_loss: 67.54859924316406\n",
            "Training epoch 13771/1000000, d_loss: -1453.36376953125,  g_loss: -415.934326171875\n",
            "Training epoch 13772/1000000, d_loss: 87.952392578125,  g_loss: 76.92166900634766\n",
            "Training epoch 13773/1000000, d_loss: -627.4932861328125,  g_loss: -278.3916931152344\n",
            "Training epoch 13774/1000000, d_loss: -46.71030807495117,  g_loss: -221.74986267089844\n",
            "Training epoch 13775/1000000, d_loss: -265.46856689453125,  g_loss: 286.7353515625\n",
            "Training epoch 13776/1000000, d_loss: -268.8391418457031,  g_loss: 466.6223449707031\n",
            "Training epoch 13777/1000000, d_loss: -211.774169921875,  g_loss: 301.84661865234375\n",
            "Training epoch 13778/1000000, d_loss: -170.6956329345703,  g_loss: -19.736125946044922\n",
            "Training epoch 13779/1000000, d_loss: -153.49237060546875,  g_loss: 4.2999701499938965\n",
            "Training epoch 13780/1000000, d_loss: 244.5531005859375,  g_loss: 83.58001708984375\n",
            "Training epoch 13781/1000000, d_loss: -367.7716369628906,  g_loss: 491.046875\n",
            "Training epoch 13782/1000000, d_loss: -240.94664001464844,  g_loss: 277.4850769042969\n",
            "Training epoch 13783/1000000, d_loss: 442.0769348144531,  g_loss: 36.58124542236328\n",
            "Training epoch 13784/1000000, d_loss: -127.74496459960938,  g_loss: 200.95111083984375\n",
            "Training epoch 13785/1000000, d_loss: -122.85157775878906,  g_loss: 230.581787109375\n",
            "Training epoch 13786/1000000, d_loss: -96.79237365722656,  g_loss: 137.62535095214844\n",
            "Training epoch 13787/1000000, d_loss: -199.65802001953125,  g_loss: 210.21157836914062\n",
            "Training epoch 13788/1000000, d_loss: 315.86602783203125,  g_loss: 59.29171371459961\n",
            "Training epoch 13789/1000000, d_loss: 4.818603515625,  g_loss: 39.858551025390625\n",
            "Training epoch 13790/1000000, d_loss: -7.771484375,  g_loss: 85.26922607421875\n",
            "Training epoch 13791/1000000, d_loss: -164.399658203125,  g_loss: 36.85333251953125\n",
            "Training epoch 13792/1000000, d_loss: -76.16659545898438,  g_loss: 121.15827941894531\n",
            "Training epoch 13793/1000000, d_loss: -125.96705627441406,  g_loss: 176.7998046875\n",
            "Training epoch 13794/1000000, d_loss: -201.39491271972656,  g_loss: 249.4345245361328\n",
            "Training epoch 13795/1000000, d_loss: -452.99456787109375,  g_loss: 505.81243896484375\n",
            "Training epoch 13796/1000000, d_loss: -49.878509521484375,  g_loss: 18.224960327148438\n",
            "Training epoch 13797/1000000, d_loss: -69.37234497070312,  g_loss: 90.948486328125\n",
            "Training epoch 13798/1000000, d_loss: -215.20565795898438,  g_loss: -47.26523971557617\n",
            "Training epoch 13799/1000000, d_loss: 583.863037109375,  g_loss: 62.48567199707031\n",
            "Training epoch 13800/1000000, d_loss: -121.51834106445312,  g_loss: 53.0246696472168\n",
            "Training epoch 13801/1000000, d_loss: -39.07749938964844,  g_loss: 43.67550277709961\n",
            "Training epoch 13802/1000000, d_loss: -59.09579086303711,  g_loss: 47.94088363647461\n",
            "Training epoch 13803/1000000, d_loss: -378.9292297363281,  g_loss: -37.00847244262695\n",
            "Training epoch 13804/1000000, d_loss: -65.82061767578125,  g_loss: 21.75046157836914\n",
            "Training epoch 13805/1000000, d_loss: -70.58441925048828,  g_loss: 87.48394012451172\n",
            "Training epoch 13806/1000000, d_loss: -7.104082107543945,  g_loss: 63.417781829833984\n",
            "Training epoch 13807/1000000, d_loss: -153.58966064453125,  g_loss: 73.12930297851562\n",
            "Training epoch 13808/1000000, d_loss: -138.88519287109375,  g_loss: 52.79081726074219\n",
            "Training epoch 13809/1000000, d_loss: -43.658905029296875,  g_loss: 64.60505676269531\n",
            "Training epoch 13810/1000000, d_loss: -180.5254364013672,  g_loss: 44.39054870605469\n",
            "Training epoch 13811/1000000, d_loss: -109.98269653320312,  g_loss: 129.9993133544922\n",
            "Training epoch 13812/1000000, d_loss: -43.62149429321289,  g_loss: 47.14537811279297\n",
            "Training epoch 13813/1000000, d_loss: -72.58084106445312,  g_loss: 48.2562255859375\n",
            "Training epoch 13814/1000000, d_loss: -159.90081787109375,  g_loss: 2.811343193054199\n",
            "Training epoch 13815/1000000, d_loss: -269.145751953125,  g_loss: -118.35375213623047\n",
            "Training epoch 13816/1000000, d_loss: -658.9649658203125,  g_loss: -357.1033935546875\n",
            "Training epoch 13817/1000000, d_loss: 73.49652099609375,  g_loss: -69.93081665039062\n",
            "Training epoch 13818/1000000, d_loss: -96.85710144042969,  g_loss: -66.13616943359375\n",
            "Training epoch 13819/1000000, d_loss: -100.8505859375,  g_loss: -17.956356048583984\n",
            "Training epoch 13820/1000000, d_loss: -532.6561279296875,  g_loss: -197.82843017578125\n",
            "Training epoch 13821/1000000, d_loss: -96.00352478027344,  g_loss: -36.804039001464844\n",
            "Training epoch 13822/1000000, d_loss: -124.15200805664062,  g_loss: -13.67952823638916\n",
            "Training epoch 13823/1000000, d_loss: -49.783958435058594,  g_loss: 23.722375869750977\n",
            "Training epoch 13824/1000000, d_loss: -69.92110443115234,  g_loss: 59.826515197753906\n",
            "Training epoch 13825/1000000, d_loss: -106.61549377441406,  g_loss: 96.9656982421875\n",
            "Training epoch 13826/1000000, d_loss: -6.800262451171875,  g_loss: 73.27565002441406\n",
            "Training epoch 13827/1000000, d_loss: 100.42523193359375,  g_loss: 68.32030487060547\n",
            "Training epoch 13828/1000000, d_loss: -99.19993591308594,  g_loss: 23.596216201782227\n",
            "Training epoch 13829/1000000, d_loss: -210.88241577148438,  g_loss: 65.04954528808594\n",
            "Training epoch 13830/1000000, d_loss: 1.2553863525390625,  g_loss: 38.6331901550293\n",
            "Training epoch 13831/1000000, d_loss: -1403.100830078125,  g_loss: -866.049560546875\n",
            "Training epoch 13832/1000000, d_loss: -36.083770751953125,  g_loss: -1.271723747253418\n",
            "Training epoch 13833/1000000, d_loss: -92.77115631103516,  g_loss: 102.18592834472656\n",
            "Training epoch 13834/1000000, d_loss: 1053.9208984375,  g_loss: 61.20398712158203\n",
            "Training epoch 13835/1000000, d_loss: -130.21267700195312,  g_loss: 23.217636108398438\n",
            "Training epoch 13836/1000000, d_loss: -139.89907836914062,  g_loss: 135.03758239746094\n",
            "Training epoch 13837/1000000, d_loss: -187.00689697265625,  g_loss: 89.5594482421875\n",
            "Training epoch 13838/1000000, d_loss: -66.43023681640625,  g_loss: 153.86228942871094\n",
            "Training epoch 13839/1000000, d_loss: -226.13565063476562,  g_loss: 99.40687561035156\n",
            "Training epoch 13840/1000000, d_loss: -315.8379211425781,  g_loss: 680.6983032226562\n",
            "Training epoch 13841/1000000, d_loss: -27.08011817932129,  g_loss: 54.17042541503906\n",
            "Training epoch 13842/1000000, d_loss: 76.51667022705078,  g_loss: 58.90611267089844\n",
            "Training epoch 13843/1000000, d_loss: -142.21182250976562,  g_loss: 52.4787712097168\n",
            "Training epoch 13844/1000000, d_loss: -292.5240478515625,  g_loss: -74.92599487304688\n",
            "Training epoch 13845/1000000, d_loss: -84.44406127929688,  g_loss: 97.57069396972656\n",
            "Training epoch 13846/1000000, d_loss: -206.3439483642578,  g_loss: 118.27273559570312\n",
            "Training epoch 13847/1000000, d_loss: -14.30120849609375,  g_loss: 37.24489212036133\n",
            "Training epoch 13848/1000000, d_loss: -104.48645782470703,  g_loss: 83.64460754394531\n",
            "Training epoch 13849/1000000, d_loss: -1162.8328857421875,  g_loss: -289.1643371582031\n",
            "Training epoch 13850/1000000, d_loss: 141.07362365722656,  g_loss: 6.975519180297852\n",
            "Training epoch 13851/1000000, d_loss: -181.63685607910156,  g_loss: 3.0556671619415283\n",
            "Training epoch 13852/1000000, d_loss: -285.91607666015625,  g_loss: 277.9600830078125\n",
            "Training epoch 13853/1000000, d_loss: -90.8368911743164,  g_loss: 138.8220977783203\n",
            "Training epoch 13854/1000000, d_loss: 28.986663818359375,  g_loss: 10.967899322509766\n",
            "Training epoch 13855/1000000, d_loss: -149.56251525878906,  g_loss: 41.370208740234375\n",
            "Training epoch 13856/1000000, d_loss: -141.737548828125,  g_loss: 314.56195068359375\n",
            "Training epoch 13857/1000000, d_loss: -33.13134002685547,  g_loss: 7.878235816955566\n",
            "Training epoch 13858/1000000, d_loss: -54.827266693115234,  g_loss: 24.837528228759766\n",
            "Training epoch 13859/1000000, d_loss: -112.94131469726562,  g_loss: 65.0032958984375\n",
            "Training epoch 13860/1000000, d_loss: -62.590606689453125,  g_loss: 52.346920013427734\n",
            "Training epoch 13861/1000000, d_loss: -55.37232208251953,  g_loss: 75.03224182128906\n",
            "Training epoch 13862/1000000, d_loss: -138.09408569335938,  g_loss: 45.709739685058594\n",
            "Training epoch 13863/1000000, d_loss: -56.22461700439453,  g_loss: 93.58123779296875\n",
            "Training epoch 13864/1000000, d_loss: -121.02128601074219,  g_loss: 14.35119915008545\n",
            "Training epoch 13865/1000000, d_loss: -1124.530029296875,  g_loss: -168.520751953125\n",
            "Training epoch 13866/1000000, d_loss: -28.846981048583984,  g_loss: -105.62423706054688\n",
            "Training epoch 13867/1000000, d_loss: -70.33184814453125,  g_loss: -131.56263732910156\n",
            "Training epoch 13868/1000000, d_loss: -30.308692932128906,  g_loss: 66.21806335449219\n",
            "Training epoch 13869/1000000, d_loss: -182.3380889892578,  g_loss: 22.2320613861084\n",
            "Training epoch 13870/1000000, d_loss: -913.36181640625,  g_loss: -379.1860656738281\n",
            "Training epoch 13871/1000000, d_loss: 23.069164276123047,  g_loss: 140.505615234375\n",
            "Training epoch 13872/1000000, d_loss: -197.83229064941406,  g_loss: 242.6483154296875\n",
            "Training epoch 13873/1000000, d_loss: -91.95477294921875,  g_loss: 127.55612182617188\n",
            "Training epoch 13874/1000000, d_loss: -348.275146484375,  g_loss: 238.580322265625\n",
            "Training epoch 13875/1000000, d_loss: -214.3677978515625,  g_loss: 265.4414978027344\n",
            "Training epoch 13876/1000000, d_loss: -177.45098876953125,  g_loss: 105.84083557128906\n",
            "Training epoch 13877/1000000, d_loss: -10.640762329101562,  g_loss: -49.2305793762207\n",
            "Training epoch 13878/1000000, d_loss: -15.163330078125,  g_loss: -14.288667678833008\n",
            "Training epoch 13879/1000000, d_loss: -65.42644500732422,  g_loss: -23.992511749267578\n",
            "Training epoch 13880/1000000, d_loss: -106.00320434570312,  g_loss: 66.1209487915039\n",
            "Training epoch 13881/1000000, d_loss: -60.730247497558594,  g_loss: 103.04476928710938\n",
            "Training epoch 13882/1000000, d_loss: -173.6370849609375,  g_loss: 36.93281555175781\n",
            "Training epoch 13883/1000000, d_loss: -71.39523315429688,  g_loss: 38.82672882080078\n",
            "Training epoch 13884/1000000, d_loss: -681.0947875976562,  g_loss: -400.26385498046875\n",
            "Training epoch 13885/1000000, d_loss: -103.9961166381836,  g_loss: 9.553546905517578\n",
            "Training epoch 13886/1000000, d_loss: -412.168212890625,  g_loss: 644.0690307617188\n",
            "Training epoch 13887/1000000, d_loss: -59.84446334838867,  g_loss: -17.3850040435791\n",
            "Training epoch 13888/1000000, d_loss: -366.3524475097656,  g_loss: -74.04898834228516\n",
            "Training epoch 13889/1000000, d_loss: -117.22129821777344,  g_loss: -71.07859802246094\n",
            "Training epoch 13890/1000000, d_loss: -32.946563720703125,  g_loss: 69.04023742675781\n",
            "Training epoch 13891/1000000, d_loss: -174.70840454101562,  g_loss: 3.8954877853393555\n",
            "Training epoch 13892/1000000, d_loss: -91.56956481933594,  g_loss: 3.190079689025879\n",
            "Training epoch 13893/1000000, d_loss: -57.769081115722656,  g_loss: 98.09245300292969\n",
            "Training epoch 13894/1000000, d_loss: -319.91253662109375,  g_loss: 25.170373916625977\n",
            "Training epoch 13895/1000000, d_loss: -152.15647888183594,  g_loss: 244.54806518554688\n",
            "Training epoch 13896/1000000, d_loss: -29.306121826171875,  g_loss: 13.335184097290039\n",
            "Training epoch 13897/1000000, d_loss: 274.4283752441406,  g_loss: 43.62041091918945\n",
            "Training epoch 13898/1000000, d_loss: -90.30381774902344,  g_loss: 66.17205810546875\n",
            "Training epoch 13899/1000000, d_loss: -58.429779052734375,  g_loss: 45.289276123046875\n",
            "Training epoch 13900/1000000, d_loss: -205.62220764160156,  g_loss: 208.1103515625\n",
            "Training epoch 13901/1000000, d_loss: -81.07827758789062,  g_loss: 9.087285041809082\n",
            "Training epoch 13902/1000000, d_loss: -106.46659851074219,  g_loss: 16.84918975830078\n",
            "Training epoch 13903/1000000, d_loss: -305.81939697265625,  g_loss: -56.652530670166016\n",
            "Training epoch 13904/1000000, d_loss: -192.5994110107422,  g_loss: 0.08778762817382812\n",
            "Training epoch 13905/1000000, d_loss: -112.77310180664062,  g_loss: -19.0656795501709\n",
            "Training epoch 13906/1000000, d_loss: -80.83736419677734,  g_loss: 8.860475540161133\n",
            "Training epoch 13907/1000000, d_loss: 36.07135009765625,  g_loss: 43.40840530395508\n",
            "Training epoch 13908/1000000, d_loss: -89.31610107421875,  g_loss: 28.483488082885742\n",
            "Training epoch 13909/1000000, d_loss: -168.59593200683594,  g_loss: -6.050257682800293\n",
            "Training epoch 13910/1000000, d_loss: -316.712158203125,  g_loss: -117.3828353881836\n",
            "Training epoch 13911/1000000, d_loss: -190.1700439453125,  g_loss: 164.37765502929688\n",
            "Training epoch 13912/1000000, d_loss: -519.2537841796875,  g_loss: -115.48512268066406\n",
            "Training epoch 13913/1000000, d_loss: -56.917030334472656,  g_loss: -21.129253387451172\n",
            "Training epoch 13914/1000000, d_loss: -34.803916931152344,  g_loss: -28.033077239990234\n",
            "Training epoch 13915/1000000, d_loss: -209.7164306640625,  g_loss: -50.953880310058594\n",
            "Training epoch 13916/1000000, d_loss: -38.92932891845703,  g_loss: -17.330089569091797\n",
            "Training epoch 13917/1000000, d_loss: -377.68157958984375,  g_loss: -175.42825317382812\n",
            "Training epoch 13918/1000000, d_loss: -61.035675048828125,  g_loss: -43.15817642211914\n",
            "Training epoch 13919/1000000, d_loss: -91.45072937011719,  g_loss: 11.159039497375488\n",
            "Training epoch 13920/1000000, d_loss: -124.85389709472656,  g_loss: -43.464744567871094\n",
            "Training epoch 13921/1000000, d_loss: -163.73704528808594,  g_loss: 37.82667922973633\n",
            "Training epoch 13922/1000000, d_loss: -567.8009033203125,  g_loss: -290.53466796875\n",
            "Training epoch 13923/1000000, d_loss: -42.30535125732422,  g_loss: -48.71793746948242\n",
            "Training epoch 13924/1000000, d_loss: -421.2872314453125,  g_loss: 218.02195739746094\n",
            "Training epoch 13925/1000000, d_loss: -108.04560089111328,  g_loss: -58.43096923828125\n",
            "Training epoch 13926/1000000, d_loss: 154.89007568359375,  g_loss: 15.361456871032715\n",
            "Training epoch 13927/1000000, d_loss: -55.01872253417969,  g_loss: 17.093175888061523\n",
            "Training epoch 13928/1000000, d_loss: -194.096435546875,  g_loss: -74.72163391113281\n",
            "Training epoch 13929/1000000, d_loss: -198.25112915039062,  g_loss: -19.284814834594727\n",
            "Training epoch 13930/1000000, d_loss: -137.48321533203125,  g_loss: 30.08385467529297\n",
            "Training epoch 13931/1000000, d_loss: -460.0942077636719,  g_loss: -190.78945922851562\n",
            "Training epoch 13932/1000000, d_loss: 98.67526245117188,  g_loss: 21.432872772216797\n",
            "Training epoch 13933/1000000, d_loss: -25.67870330810547,  g_loss: 18.069730758666992\n",
            "Training epoch 13934/1000000, d_loss: -33.86420822143555,  g_loss: 18.883346557617188\n",
            "Training epoch 13935/1000000, d_loss: -148.5246124267578,  g_loss: 45.063255310058594\n",
            "Training epoch 13936/1000000, d_loss: -147.34112548828125,  g_loss: 64.20106506347656\n",
            "Training epoch 13937/1000000, d_loss: -152.09835815429688,  g_loss: 13.49874496459961\n",
            "Training epoch 13938/1000000, d_loss: -184.37713623046875,  g_loss: 52.817806243896484\n",
            "Training epoch 13939/1000000, d_loss: -115.31749725341797,  g_loss: 103.62416076660156\n",
            "Training epoch 13940/1000000, d_loss: -237.63983154296875,  g_loss: 14.863250732421875\n",
            "Training epoch 13941/1000000, d_loss: -9.011116027832031,  g_loss: 45.86528778076172\n",
            "Training epoch 13942/1000000, d_loss: -135.70277404785156,  g_loss: -57.83348846435547\n",
            "Training epoch 13943/1000000, d_loss: -1812.635498046875,  g_loss: -376.3537902832031\n",
            "Training epoch 13944/1000000, d_loss: -167.4078369140625,  g_loss: -220.63922119140625\n",
            "Training epoch 13945/1000000, d_loss: 17.0517578125,  g_loss: -226.6846160888672\n",
            "Training epoch 13946/1000000, d_loss: 56.00196838378906,  g_loss: -9.054443359375\n",
            "Training epoch 13947/1000000, d_loss: -17.68041229248047,  g_loss: -83.09407806396484\n",
            "Training epoch 13948/1000000, d_loss: -318.68572998046875,  g_loss: -54.549957275390625\n",
            "Training epoch 13949/1000000, d_loss: -388.2168273925781,  g_loss: -43.36566162109375\n",
            "Training epoch 13950/1000000, d_loss: -398.29449462890625,  g_loss: -301.13671875\n",
            "Training epoch 13951/1000000, d_loss: 27.203075408935547,  g_loss: -145.0706787109375\n",
            "Training epoch 13952/1000000, d_loss: -282.500732421875,  g_loss: -81.74076080322266\n",
            "Training epoch 13953/1000000, d_loss: -306.069091796875,  g_loss: -78.87874603271484\n",
            "Training epoch 13954/1000000, d_loss: -103.13664245605469,  g_loss: 169.20115661621094\n",
            "Training epoch 13955/1000000, d_loss: -63.40925598144531,  g_loss: 125.75372314453125\n",
            "Training epoch 13956/1000000, d_loss: -174.9750518798828,  g_loss: 203.37496948242188\n",
            "Training epoch 13957/1000000, d_loss: -185.0869140625,  g_loss: 19.02280044555664\n",
            "Training epoch 13958/1000000, d_loss: -142.81753540039062,  g_loss: 317.07354736328125\n",
            "Training epoch 13959/1000000, d_loss: -197.50323486328125,  g_loss: 293.98468017578125\n",
            "Training epoch 13960/1000000, d_loss: -98.27393341064453,  g_loss: -23.38589859008789\n",
            "Training epoch 13961/1000000, d_loss: 6.1789398193359375,  g_loss: -6.3976287841796875\n",
            "Training epoch 13962/1000000, d_loss: -229.09310913085938,  g_loss: 369.878662109375\n",
            "Training epoch 13963/1000000, d_loss: -165.58885192871094,  g_loss: -9.416061401367188\n",
            "Training epoch 13964/1000000, d_loss: -224.889404296875,  g_loss: -2.254484176635742\n",
            "Training epoch 13965/1000000, d_loss: 39.941253662109375,  g_loss: -20.568050384521484\n",
            "Training epoch 13966/1000000, d_loss: -124.16732025146484,  g_loss: 52.65692901611328\n",
            "Training epoch 13967/1000000, d_loss: -92.48123931884766,  g_loss: 8.727721214294434\n",
            "Training epoch 13968/1000000, d_loss: -152.5010986328125,  g_loss: -16.23741912841797\n",
            "Training epoch 13969/1000000, d_loss: -17.78850555419922,  g_loss: 42.83103942871094\n",
            "Training epoch 13970/1000000, d_loss: -190.3457489013672,  g_loss: 224.73660278320312\n",
            "Training epoch 13971/1000000, d_loss: -83.29602813720703,  g_loss: 41.164634704589844\n",
            "Training epoch 13972/1000000, d_loss: -315.3692932128906,  g_loss: 6.236263275146484\n",
            "Training epoch 13973/1000000, d_loss: -281.8072204589844,  g_loss: -25.985008239746094\n",
            "Training epoch 13974/1000000, d_loss: -153.8875274658203,  g_loss: 188.16036987304688\n",
            "Training epoch 13975/1000000, d_loss: -139.68946838378906,  g_loss: -72.65033721923828\n",
            "Training epoch 13976/1000000, d_loss: -98.42391967773438,  g_loss: -1.9491603374481201\n",
            "Training epoch 13977/1000000, d_loss: -41.77978515625,  g_loss: 28.194684982299805\n",
            "Training epoch 13978/1000000, d_loss: -135.76889038085938,  g_loss: -45.425010681152344\n",
            "Training epoch 13979/1000000, d_loss: -175.93048095703125,  g_loss: -86.1800308227539\n",
            "Training epoch 13980/1000000, d_loss: 264.926513671875,  g_loss: -61.32249069213867\n",
            "Training epoch 13981/1000000, d_loss: 41.80860900878906,  g_loss: -56.599151611328125\n",
            "Training epoch 13982/1000000, d_loss: -71.3479232788086,  g_loss: -41.553062438964844\n",
            "Training epoch 13983/1000000, d_loss: -141.89151000976562,  g_loss: -57.87892532348633\n",
            "Training epoch 13984/1000000, d_loss: -93.0687255859375,  g_loss: 38.94729232788086\n",
            "Training epoch 13985/1000000, d_loss: -384.98272705078125,  g_loss: -31.810794830322266\n",
            "Training epoch 13986/1000000, d_loss: -23.333660125732422,  g_loss: 52.526222229003906\n",
            "Training epoch 13987/1000000, d_loss: -58.07481384277344,  g_loss: 104.50205993652344\n",
            "Training epoch 13988/1000000, d_loss: -97.30494689941406,  g_loss: 47.859771728515625\n",
            "Training epoch 13989/1000000, d_loss: -145.16061401367188,  g_loss: 26.13629913330078\n",
            "Training epoch 13990/1000000, d_loss: -129.27442932128906,  g_loss: 30.50679588317871\n",
            "Training epoch 13991/1000000, d_loss: -69.86492919921875,  g_loss: 3.847829818725586\n",
            "Training epoch 13992/1000000, d_loss: -94.76396179199219,  g_loss: 83.18326568603516\n",
            "Training epoch 13993/1000000, d_loss: -179.210693359375,  g_loss: 233.8264923095703\n",
            "Training epoch 13994/1000000, d_loss: -59.396453857421875,  g_loss: 32.66210174560547\n",
            "Training epoch 13995/1000000, d_loss: -194.55682373046875,  g_loss: -39.69750213623047\n",
            "Training epoch 13996/1000000, d_loss: -13.107307434082031,  g_loss: -3.136934757232666\n",
            "Training epoch 13997/1000000, d_loss: -103.6534652709961,  g_loss: 24.812482833862305\n",
            "Training epoch 13998/1000000, d_loss: -120.54219818115234,  g_loss: 3.7104148864746094\n",
            "Training epoch 13999/1000000, d_loss: -45.65556716918945,  g_loss: 12.51547622680664\n",
            "Training epoch 14000/1000000, d_loss: -113.49182891845703,  g_loss: 7.347348690032959\n",
            "Training epoch 14001/1000000, d_loss: -109.47817993164062,  g_loss: 18.674129486083984\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 26ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 64/64 [00:00<00:00, 78.38it/s]\n",
            "Meshing: 100%|██████████| 16554/16554 [00:06<00:00, 2704.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_14001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_14001/assets\n",
            "Training epoch 14002/1000000, d_loss: -138.50543212890625,  g_loss: 85.9273452758789\n",
            "Training epoch 14003/1000000, d_loss: -104.19377136230469,  g_loss: 25.05099868774414\n",
            "Training epoch 14004/1000000, d_loss: 796.5786743164062,  g_loss: 39.01545715332031\n",
            "Training epoch 14005/1000000, d_loss: -260.7554016113281,  g_loss: 8.173070907592773\n",
            "Training epoch 14006/1000000, d_loss: -28.250656127929688,  g_loss: 33.071285247802734\n",
            "Training epoch 14007/1000000, d_loss: -145.54434204101562,  g_loss: 17.487348556518555\n",
            "Training epoch 14008/1000000, d_loss: -56.9873046875,  g_loss: 45.13849639892578\n",
            "Training epoch 14009/1000000, d_loss: -187.8902130126953,  g_loss: 95.95840454101562\n",
            "Training epoch 14010/1000000, d_loss: -331.5921325683594,  g_loss: -63.60878372192383\n",
            "Training epoch 14011/1000000, d_loss: -196.62362670898438,  g_loss: 19.972806930541992\n",
            "Training epoch 14012/1000000, d_loss: -138.5070037841797,  g_loss: 9.524288177490234\n",
            "Training epoch 14013/1000000, d_loss: -89.45880889892578,  g_loss: 22.9814453125\n",
            "Training epoch 14014/1000000, d_loss: -55.65782928466797,  g_loss: 8.592260360717773\n",
            "Training epoch 14015/1000000, d_loss: 19.483901977539062,  g_loss: -18.94765853881836\n",
            "Training epoch 14016/1000000, d_loss: -193.72308349609375,  g_loss: -57.36517333984375\n",
            "Training epoch 14017/1000000, d_loss: -100.73410034179688,  g_loss: 61.908111572265625\n",
            "Training epoch 14018/1000000, d_loss: -108.73625946044922,  g_loss: 220.98190307617188\n",
            "Training epoch 14019/1000000, d_loss: -121.88862609863281,  g_loss: -3.005643844604492\n",
            "Training epoch 14020/1000000, d_loss: -163.94955444335938,  g_loss: -55.54696273803711\n",
            "Training epoch 14021/1000000, d_loss: -69.01837158203125,  g_loss: 14.9109525680542\n",
            "Training epoch 14022/1000000, d_loss: -121.44627380371094,  g_loss: -11.879446029663086\n",
            "Training epoch 14023/1000000, d_loss: -653.0548706054688,  g_loss: -45.59333801269531\n",
            "Training epoch 14024/1000000, d_loss: -868.832763671875,  g_loss: -243.26678466796875\n",
            "Training epoch 14025/1000000, d_loss: -272.0581359863281,  g_loss: -238.39511108398438\n",
            "Training epoch 14026/1000000, d_loss: -519.9617919921875,  g_loss: -213.2256317138672\n",
            "Training epoch 14027/1000000, d_loss: -279.01458740234375,  g_loss: -35.09699630737305\n",
            "Training epoch 14028/1000000, d_loss: -96.07817077636719,  g_loss: -158.14584350585938\n",
            "Training epoch 14029/1000000, d_loss: -152.12518310546875,  g_loss: -78.98625183105469\n",
            "Training epoch 14030/1000000, d_loss: -184.9398193359375,  g_loss: 242.5392303466797\n",
            "Training epoch 14031/1000000, d_loss: -332.8770751953125,  g_loss: 5.037517547607422\n",
            "Training epoch 14032/1000000, d_loss: -326.0083923339844,  g_loss: 313.40673828125\n",
            "Training epoch 14033/1000000, d_loss: -137.8729248046875,  g_loss: 14.552499771118164\n",
            "Training epoch 14034/1000000, d_loss: -419.28045654296875,  g_loss: 612.6849975585938\n",
            "Training epoch 14035/1000000, d_loss: -13.86492919921875,  g_loss: 13.109131813049316\n",
            "Training epoch 14036/1000000, d_loss: -73.58262634277344,  g_loss: -0.8785390853881836\n",
            "Training epoch 14037/1000000, d_loss: -163.42225646972656,  g_loss: 152.27281188964844\n",
            "Training epoch 14038/1000000, d_loss: -190.75543212890625,  g_loss: 137.16932678222656\n",
            "Training epoch 14039/1000000, d_loss: -90.45874786376953,  g_loss: 6.124234199523926\n",
            "Training epoch 14040/1000000, d_loss: -263.2261657714844,  g_loss: 201.67715454101562\n",
            "Training epoch 14041/1000000, d_loss: -82.59991455078125,  g_loss: -15.872739791870117\n",
            "Training epoch 14042/1000000, d_loss: 8.583274841308594,  g_loss: 6.048492431640625\n",
            "Training epoch 14043/1000000, d_loss: -98.90089416503906,  g_loss: 20.948406219482422\n",
            "Training epoch 14044/1000000, d_loss: -641.5728759765625,  g_loss: -280.8327941894531\n",
            "Training epoch 14045/1000000, d_loss: -41.29280090332031,  g_loss: 10.017595291137695\n",
            "Training epoch 14046/1000000, d_loss: 23.53638458251953,  g_loss: -2.1401853561401367\n",
            "Training epoch 14047/1000000, d_loss: -146.80548095703125,  g_loss: 38.975624084472656\n",
            "Training epoch 14048/1000000, d_loss: -163.3708953857422,  g_loss: 41.42222213745117\n",
            "Training epoch 14049/1000000, d_loss: -1257.523193359375,  g_loss: -611.4746704101562\n",
            "Training epoch 14050/1000000, d_loss: 7.109697341918945,  g_loss: -54.95790100097656\n",
            "Training epoch 14051/1000000, d_loss: -232.3749237060547,  g_loss: -412.38470458984375\n",
            "Training epoch 14052/1000000, d_loss: -46.04510498046875,  g_loss: -36.152099609375\n",
            "Training epoch 14053/1000000, d_loss: -58.79295349121094,  g_loss: 30.388626098632812\n",
            "Training epoch 14054/1000000, d_loss: -136.0624542236328,  g_loss: 308.1343994140625\n",
            "Training epoch 14055/1000000, d_loss: -291.0047607421875,  g_loss: 451.0784912109375\n",
            "Training epoch 14056/1000000, d_loss: -336.2173767089844,  g_loss: 399.43780517578125\n",
            "Training epoch 14057/1000000, d_loss: -77.36959838867188,  g_loss: 19.236783981323242\n",
            "Training epoch 14058/1000000, d_loss: -106.09626007080078,  g_loss: -25.7169189453125\n",
            "Training epoch 14059/1000000, d_loss: -119.75949096679688,  g_loss: 39.32096481323242\n",
            "Training epoch 14060/1000000, d_loss: -159.90077209472656,  g_loss: -12.397911071777344\n",
            "Training epoch 14061/1000000, d_loss: -42.068885803222656,  g_loss: 35.32296371459961\n",
            "Training epoch 14062/1000000, d_loss: -47.96681594848633,  g_loss: 70.2325210571289\n",
            "Training epoch 14063/1000000, d_loss: -87.05525207519531,  g_loss: 170.65943908691406\n",
            "Training epoch 14064/1000000, d_loss: 195.45530700683594,  g_loss: 9.768312454223633\n",
            "Training epoch 14065/1000000, d_loss: -36.087039947509766,  g_loss: 26.29476547241211\n",
            "Training epoch 14066/1000000, d_loss: -278.85028076171875,  g_loss: -17.2835693359375\n",
            "Training epoch 14067/1000000, d_loss: -41.550201416015625,  g_loss: 16.662389755249023\n",
            "Training epoch 14068/1000000, d_loss: -284.6184387207031,  g_loss: -4.121145248413086\n",
            "Training epoch 14069/1000000, d_loss: -652.805908203125,  g_loss: -36.55284881591797\n",
            "Training epoch 14070/1000000, d_loss: 25.126962661743164,  g_loss: -6.907050132751465\n",
            "Training epoch 14071/1000000, d_loss: -69.32759094238281,  g_loss: -24.875839233398438\n",
            "Training epoch 14072/1000000, d_loss: -116.52933502197266,  g_loss: -19.933530807495117\n",
            "Training epoch 14073/1000000, d_loss: -76.51023864746094,  g_loss: 27.06671905517578\n",
            "Training epoch 14074/1000000, d_loss: -98.88555145263672,  g_loss: 53.32074737548828\n",
            "Training epoch 14075/1000000, d_loss: -678.6636352539062,  g_loss: -101.42927551269531\n",
            "Training epoch 14076/1000000, d_loss: -408.4168395996094,  g_loss: -241.51220703125\n",
            "Training epoch 14077/1000000, d_loss: -7.508462905883789,  g_loss: -9.600626945495605\n",
            "Training epoch 14078/1000000, d_loss: -996.713623046875,  g_loss: -192.73867797851562\n",
            "Training epoch 14079/1000000, d_loss: 24.526641845703125,  g_loss: -46.30276870727539\n",
            "Training epoch 14080/1000000, d_loss: -43.181007385253906,  g_loss: 64.41613006591797\n",
            "Training epoch 14081/1000000, d_loss: -369.8908996582031,  g_loss: 513.016357421875\n",
            "Training epoch 14082/1000000, d_loss: -22.109527587890625,  g_loss: -67.09212493896484\n",
            "Training epoch 14083/1000000, d_loss: -93.02142333984375,  g_loss: -85.33831787109375\n",
            "Training epoch 14084/1000000, d_loss: -103.51424407958984,  g_loss: 44.86743927001953\n",
            "Training epoch 14085/1000000, d_loss: -205.2421417236328,  g_loss: -3.2562952041625977\n",
            "Training epoch 14086/1000000, d_loss: -184.97998046875,  g_loss: 56.22169494628906\n",
            "Training epoch 14087/1000000, d_loss: -269.32684326171875,  g_loss: 117.39103698730469\n",
            "Training epoch 14088/1000000, d_loss: -434.98333740234375,  g_loss: 616.926025390625\n",
            "Training epoch 14089/1000000, d_loss: -124.78284454345703,  g_loss: -51.447296142578125\n",
            "Training epoch 14090/1000000, d_loss: -162.68038940429688,  g_loss: -59.558807373046875\n",
            "Training epoch 14091/1000000, d_loss: -8.34515380859375,  g_loss: -49.54484558105469\n",
            "Training epoch 14092/1000000, d_loss: -12.019359588623047,  g_loss: -29.42856216430664\n",
            "Training epoch 14093/1000000, d_loss: -48.111175537109375,  g_loss: -11.057153701782227\n",
            "Training epoch 14094/1000000, d_loss: -50.42289352416992,  g_loss: -17.562976837158203\n",
            "Training epoch 14095/1000000, d_loss: -43.68442153930664,  g_loss: 9.744568824768066\n",
            "Training epoch 14096/1000000, d_loss: -133.69317626953125,  g_loss: -29.65635871887207\n",
            "Training epoch 14097/1000000, d_loss: -18.392623901367188,  g_loss: -40.75245666503906\n",
            "Training epoch 14098/1000000, d_loss: -86.81913757324219,  g_loss: -29.594371795654297\n",
            "Training epoch 14099/1000000, d_loss: -61.295799255371094,  g_loss: -13.365283012390137\n",
            "Training epoch 14100/1000000, d_loss: -145.09332275390625,  g_loss: 12.450932502746582\n",
            "Training epoch 14101/1000000, d_loss: -410.2966613769531,  g_loss: -83.64334106445312\n",
            "Training epoch 14102/1000000, d_loss: -89.38028717041016,  g_loss: 7.052384376525879\n",
            "Training epoch 14103/1000000, d_loss: -96.30351257324219,  g_loss: 19.037527084350586\n",
            "Training epoch 14104/1000000, d_loss: -99.20109558105469,  g_loss: 38.71366882324219\n",
            "Training epoch 14105/1000000, d_loss: -1158.821533203125,  g_loss: -182.95330810546875\n",
            "Training epoch 14106/1000000, d_loss: 64.20319366455078,  g_loss: -27.006122589111328\n",
            "Training epoch 14107/1000000, d_loss: -762.43017578125,  g_loss: -31.322431564331055\n",
            "Training epoch 14108/1000000, d_loss: -105.7046890258789,  g_loss: -23.38956642150879\n",
            "Training epoch 14109/1000000, d_loss: -223.43287658691406,  g_loss: -311.2311096191406\n",
            "Training epoch 14110/1000000, d_loss: -194.13441467285156,  g_loss: -144.75350952148438\n",
            "Training epoch 14111/1000000, d_loss: -144.42434692382812,  g_loss: 77.60952758789062\n",
            "Training epoch 14112/1000000, d_loss: 818.2896728515625,  g_loss: 14.7504301071167\n",
            "Training epoch 14113/1000000, d_loss: -116.24563598632812,  g_loss: 131.7924041748047\n",
            "Training epoch 14114/1000000, d_loss: -158.52940368652344,  g_loss: 43.588523864746094\n",
            "Training epoch 14115/1000000, d_loss: -170.68838500976562,  g_loss: -9.05379867553711\n",
            "Training epoch 14116/1000000, d_loss: -15.454944610595703,  g_loss: 6.686333656311035\n",
            "Training epoch 14117/1000000, d_loss: -111.59910583496094,  g_loss: 71.40077209472656\n",
            "Training epoch 14118/1000000, d_loss: -59.505252838134766,  g_loss: -28.871837615966797\n",
            "Training epoch 14119/1000000, d_loss: -219.20416259765625,  g_loss: -103.37345886230469\n",
            "Training epoch 14120/1000000, d_loss: -163.9431610107422,  g_loss: -7.08641242980957\n",
            "Training epoch 14121/1000000, d_loss: -95.64125061035156,  g_loss: -36.54082107543945\n",
            "Training epoch 14122/1000000, d_loss: -346.32965087890625,  g_loss: -126.7750244140625\n",
            "Training epoch 14123/1000000, d_loss: -77.24667358398438,  g_loss: -22.67762565612793\n",
            "Training epoch 14124/1000000, d_loss: -205.87904357910156,  g_loss: -16.630615234375\n",
            "Training epoch 14125/1000000, d_loss: -161.0487518310547,  g_loss: 53.51945114135742\n",
            "Training epoch 14126/1000000, d_loss: -41.653995513916016,  g_loss: 7.142340183258057\n",
            "Training epoch 14127/1000000, d_loss: -184.531494140625,  g_loss: 18.36598014831543\n",
            "Training epoch 14128/1000000, d_loss: -97.10283660888672,  g_loss: -3.3415088653564453\n",
            "Training epoch 14129/1000000, d_loss: -197.6444854736328,  g_loss: -5.709227561950684\n",
            "Training epoch 14130/1000000, d_loss: -83.3222427368164,  g_loss: 26.28110122680664\n",
            "Training epoch 14131/1000000, d_loss: -171.33770751953125,  g_loss: 189.95816040039062\n",
            "Training epoch 14132/1000000, d_loss: -135.0584716796875,  g_loss: 5.739026069641113\n",
            "Training epoch 14133/1000000, d_loss: -99.18097686767578,  g_loss: 92.49748229980469\n",
            "Training epoch 14134/1000000, d_loss: -84.25387573242188,  g_loss: 3.6928653717041016\n",
            "Training epoch 14135/1000000, d_loss: -96.88015747070312,  g_loss: 51.306602478027344\n",
            "Training epoch 14136/1000000, d_loss: -372.2991638183594,  g_loss: -95.88563537597656\n",
            "Training epoch 14137/1000000, d_loss: -30.456466674804688,  g_loss: -20.38945770263672\n",
            "Training epoch 14138/1000000, d_loss: -162.35018920898438,  g_loss: -32.86836242675781\n",
            "Training epoch 14139/1000000, d_loss: -155.51669311523438,  g_loss: -31.806711196899414\n",
            "Training epoch 14140/1000000, d_loss: -351.8379211425781,  g_loss: -32.12858581542969\n",
            "Training epoch 14141/1000000, d_loss: -64.25186157226562,  g_loss: 30.237518310546875\n",
            "Training epoch 14142/1000000, d_loss: -134.65322875976562,  g_loss: 61.45018768310547\n",
            "Training epoch 14143/1000000, d_loss: -79.92424011230469,  g_loss: 115.87777709960938\n",
            "Training epoch 14144/1000000, d_loss: -63.1544189453125,  g_loss: 69.60292053222656\n",
            "Training epoch 14145/1000000, d_loss: -336.7392578125,  g_loss: -17.119298934936523\n",
            "Training epoch 14146/1000000, d_loss: -972.847900390625,  g_loss: -144.4171142578125\n",
            "Training epoch 14147/1000000, d_loss: -104.4658203125,  g_loss: -54.08808135986328\n",
            "Training epoch 14148/1000000, d_loss: -180.91268920898438,  g_loss: -86.19622802734375\n",
            "Training epoch 14149/1000000, d_loss: -178.15530395507812,  g_loss: 20.066051483154297\n",
            "Training epoch 14150/1000000, d_loss: -1196.325927734375,  g_loss: -264.25128173828125\n",
            "Training epoch 14151/1000000, d_loss: -71.20780944824219,  g_loss: -104.45401763916016\n",
            "Training epoch 14152/1000000, d_loss: 240.25753784179688,  g_loss: 41.819374084472656\n",
            "Training epoch 14153/1000000, d_loss: -161.1648712158203,  g_loss: 141.54556274414062\n",
            "Training epoch 14154/1000000, d_loss: -244.96189880371094,  g_loss: 84.57156372070312\n",
            "Training epoch 14155/1000000, d_loss: -1561.1624755859375,  g_loss: -56.163570404052734\n",
            "Training epoch 14156/1000000, d_loss: -32.19581604003906,  g_loss: 144.06358337402344\n",
            "Training epoch 14157/1000000, d_loss: -15.309814453125,  g_loss: 38.62111282348633\n",
            "Training epoch 14158/1000000, d_loss: 0.3289146423339844,  g_loss: -93.1402359008789\n",
            "Training epoch 14159/1000000, d_loss: -15.122371673583984,  g_loss: 15.408315658569336\n",
            "Training epoch 14160/1000000, d_loss: -13.175224304199219,  g_loss: -37.97336959838867\n",
            "Training epoch 14161/1000000, d_loss: -98.77513122558594,  g_loss: -29.421194076538086\n",
            "Training epoch 14162/1000000, d_loss: -193.9521026611328,  g_loss: -44.27676010131836\n",
            "Training epoch 14163/1000000, d_loss: -168.41636657714844,  g_loss: -117.2408218383789\n",
            "Training epoch 14164/1000000, d_loss: -147.38438415527344,  g_loss: 19.063396453857422\n",
            "Training epoch 14165/1000000, d_loss: -241.83924865722656,  g_loss: 110.48965454101562\n",
            "Training epoch 14166/1000000, d_loss: -286.61865234375,  g_loss: 297.4169921875\n",
            "Training epoch 14167/1000000, d_loss: -31.494186401367188,  g_loss: 1.2197532653808594\n",
            "Training epoch 14168/1000000, d_loss: -1170.7418212890625,  g_loss: -95.97232055664062\n",
            "Training epoch 14169/1000000, d_loss: 19.85529327392578,  g_loss: -251.40866088867188\n",
            "Training epoch 14170/1000000, d_loss: -43.808963775634766,  g_loss: -38.0347900390625\n",
            "Training epoch 14171/1000000, d_loss: 139.19546508789062,  g_loss: 78.65963745117188\n",
            "Training epoch 14172/1000000, d_loss: -134.45394897460938,  g_loss: 173.50819396972656\n",
            "Training epoch 14173/1000000, d_loss: -205.4061737060547,  g_loss: 187.58499145507812\n",
            "Training epoch 14174/1000000, d_loss: 1476.1275634765625,  g_loss: -87.64871215820312\n",
            "Training epoch 14175/1000000, d_loss: -657.90576171875,  g_loss: -185.461181640625\n",
            "Training epoch 14176/1000000, d_loss: 93.58526611328125,  g_loss: -61.11153030395508\n",
            "Training epoch 14177/1000000, d_loss: -600.2838134765625,  g_loss: -179.31192016601562\n",
            "Training epoch 14178/1000000, d_loss: 8.873580932617188,  g_loss: -171.61843872070312\n",
            "Training epoch 14179/1000000, d_loss: 37.57679748535156,  g_loss: -14.47391128540039\n",
            "Training epoch 14180/1000000, d_loss: -290.2632751464844,  g_loss: -66.63655090332031\n",
            "Training epoch 14181/1000000, d_loss: -17.140823364257812,  g_loss: 4.739350318908691\n",
            "Training epoch 14182/1000000, d_loss: -58.91067886352539,  g_loss: 28.68426513671875\n",
            "Training epoch 14183/1000000, d_loss: -174.50143432617188,  g_loss: 98.98490905761719\n",
            "Training epoch 14184/1000000, d_loss: -37.02888107299805,  g_loss: 54.06532287597656\n",
            "Training epoch 14185/1000000, d_loss: -103.5199966430664,  g_loss: 65.28388214111328\n",
            "Training epoch 14186/1000000, d_loss: -84.98161315917969,  g_loss: 38.994773864746094\n",
            "Training epoch 14187/1000000, d_loss: -217.01113891601562,  g_loss: 53.251468658447266\n",
            "Training epoch 14188/1000000, d_loss: -160.87493896484375,  g_loss: 286.42962646484375\n",
            "Training epoch 14189/1000000, d_loss: -99.08061218261719,  g_loss: 22.27060890197754\n",
            "Training epoch 14190/1000000, d_loss: -94.62235260009766,  g_loss: 68.076171875\n",
            "Training epoch 14191/1000000, d_loss: -91.21981048583984,  g_loss: 52.656166076660156\n",
            "Training epoch 14192/1000000, d_loss: 9.995468139648438,  g_loss: -23.635265350341797\n",
            "Training epoch 14193/1000000, d_loss: -86.69413757324219,  g_loss: -0.744257926940918\n",
            "Training epoch 14194/1000000, d_loss: -622.5490112304688,  g_loss: -174.8865203857422\n",
            "Training epoch 14195/1000000, d_loss: -79.83838653564453,  g_loss: -60.27410888671875\n",
            "Training epoch 14196/1000000, d_loss: -29.47165298461914,  g_loss: 7.1637420654296875\n",
            "Training epoch 14197/1000000, d_loss: -59.36277770996094,  g_loss: 30.412235260009766\n",
            "Training epoch 14198/1000000, d_loss: -424.13336181640625,  g_loss: -144.6693115234375\n",
            "Training epoch 14199/1000000, d_loss: -288.8607482910156,  g_loss: -35.31282043457031\n",
            "Training epoch 14200/1000000, d_loss: -91.91825866699219,  g_loss: 0.11072444915771484\n",
            "Training epoch 14201/1000000, d_loss: -248.81182861328125,  g_loss: -39.06019592285156\n",
            "Training epoch 14202/1000000, d_loss: -61.687171936035156,  g_loss: -20.1744327545166\n",
            "Training epoch 14203/1000000, d_loss: -119.48844146728516,  g_loss: -27.5773983001709\n",
            "Training epoch 14204/1000000, d_loss: -380.22772216796875,  g_loss: -212.6403350830078\n",
            "Training epoch 14205/1000000, d_loss: -24.04523468017578,  g_loss: 45.762786865234375\n",
            "Training epoch 14206/1000000, d_loss: 30.874435424804688,  g_loss: 100.14615631103516\n",
            "Training epoch 14207/1000000, d_loss: -52.53402328491211,  g_loss: 35.170196533203125\n",
            "Training epoch 14208/1000000, d_loss: -230.13108825683594,  g_loss: 135.51144409179688\n",
            "Training epoch 14209/1000000, d_loss: -118.1627426147461,  g_loss: -6.33900260925293\n",
            "Training epoch 14210/1000000, d_loss: -97.51790618896484,  g_loss: 27.018512725830078\n",
            "Training epoch 14211/1000000, d_loss: -312.3881530761719,  g_loss: 69.92405700683594\n",
            "Training epoch 14212/1000000, d_loss: -113.02418518066406,  g_loss: 118.35865783691406\n",
            "Training epoch 14213/1000000, d_loss: -187.51419067382812,  g_loss: -28.484474182128906\n",
            "Training epoch 14214/1000000, d_loss: -130.82672119140625,  g_loss: -38.49616241455078\n",
            "Training epoch 14215/1000000, d_loss: -5.703857421875,  g_loss: -44.37184524536133\n",
            "Training epoch 14216/1000000, d_loss: -283.2431640625,  g_loss: -63.10678482055664\n",
            "Training epoch 14217/1000000, d_loss: -97.1251449584961,  g_loss: -43.816139221191406\n",
            "Training epoch 14218/1000000, d_loss: -25.626819610595703,  g_loss: -42.41265869140625\n",
            "Training epoch 14219/1000000, d_loss: -55.67222595214844,  g_loss: -31.455869674682617\n",
            "Training epoch 14220/1000000, d_loss: -161.67962646484375,  g_loss: -55.12838363647461\n",
            "Training epoch 14221/1000000, d_loss: -24.712814331054688,  g_loss: -23.95284652709961\n",
            "Training epoch 14222/1000000, d_loss: -95.82787322998047,  g_loss: -3.385610580444336\n",
            "Training epoch 14223/1000000, d_loss: -46.392860412597656,  g_loss: -6.356860160827637\n",
            "Training epoch 14224/1000000, d_loss: -200.47830200195312,  g_loss: -37.16859436035156\n",
            "Training epoch 14225/1000000, d_loss: -115.61319732666016,  g_loss: -96.13187408447266\n",
            "Training epoch 14226/1000000, d_loss: -283.2197265625,  g_loss: -27.20368003845215\n",
            "Training epoch 14227/1000000, d_loss: -88.56170654296875,  g_loss: -136.31390380859375\n",
            "Training epoch 14228/1000000, d_loss: -251.8846435546875,  g_loss: -213.45364379882812\n",
            "Training epoch 14229/1000000, d_loss: -317.87786865234375,  g_loss: -63.55254364013672\n",
            "Training epoch 14230/1000000, d_loss: -114.58843994140625,  g_loss: 19.440296173095703\n",
            "Training epoch 14231/1000000, d_loss: -91.69183349609375,  g_loss: 54.180416107177734\n",
            "Training epoch 14232/1000000, d_loss: -105.68943786621094,  g_loss: 60.901371002197266\n",
            "Training epoch 14233/1000000, d_loss: -87.00901794433594,  g_loss: 74.39986419677734\n",
            "Training epoch 14234/1000000, d_loss: -104.21832275390625,  g_loss: -5.491873264312744\n",
            "Training epoch 14235/1000000, d_loss: -372.787353515625,  g_loss: -100.97039031982422\n",
            "Training epoch 14236/1000000, d_loss: -121.51771545410156,  g_loss: -45.13251495361328\n",
            "Training epoch 14237/1000000, d_loss: -865.2613525390625,  g_loss: -186.88833618164062\n",
            "Training epoch 14238/1000000, d_loss: 132.5841064453125,  g_loss: -333.95782470703125\n",
            "Training epoch 14239/1000000, d_loss: -18.650192260742188,  g_loss: -9.64806842803955\n",
            "Training epoch 14240/1000000, d_loss: -36.54057312011719,  g_loss: -34.174617767333984\n",
            "Training epoch 14241/1000000, d_loss: -179.09573364257812,  g_loss: 32.3568115234375\n",
            "Training epoch 14242/1000000, d_loss: -54.144798278808594,  g_loss: 30.299686431884766\n",
            "Training epoch 14243/1000000, d_loss: 205.11355590820312,  g_loss: -17.80877113342285\n",
            "Training epoch 14244/1000000, d_loss: -34.376121520996094,  g_loss: 14.516860961914062\n",
            "Training epoch 14245/1000000, d_loss: 77.05606079101562,  g_loss: 1.3471102714538574\n",
            "Training epoch 14246/1000000, d_loss: -75.35416412353516,  g_loss: 67.8914794921875\n",
            "Training epoch 14247/1000000, d_loss: -195.03277587890625,  g_loss: 47.354225158691406\n",
            "Training epoch 14248/1000000, d_loss: -69.44119262695312,  g_loss: 13.068023681640625\n",
            "Training epoch 14249/1000000, d_loss: -629.1116943359375,  g_loss: -46.68623352050781\n",
            "Training epoch 14250/1000000, d_loss: 260.20880126953125,  g_loss: -44.61824417114258\n",
            "Training epoch 14251/1000000, d_loss: -122.71562194824219,  g_loss: 155.82257080078125\n",
            "Training epoch 14252/1000000, d_loss: -139.1532440185547,  g_loss: 170.3163604736328\n",
            "Training epoch 14253/1000000, d_loss: -35.12194061279297,  g_loss: -86.69755554199219\n",
            "Training epoch 14254/1000000, d_loss: -204.81849670410156,  g_loss: -118.47866821289062\n",
            "Training epoch 14255/1000000, d_loss: -144.06076049804688,  g_loss: -91.89138793945312\n",
            "Training epoch 14256/1000000, d_loss: -1436.3863525390625,  g_loss: -464.807861328125\n",
            "Training epoch 14257/1000000, d_loss: 5.983589172363281,  g_loss: -145.92626953125\n",
            "Training epoch 14258/1000000, d_loss: -1.9287109375,  g_loss: -193.1000213623047\n",
            "Training epoch 14259/1000000, d_loss: -163.2150115966797,  g_loss: -51.0743408203125\n",
            "Training epoch 14260/1000000, d_loss: -84.66765594482422,  g_loss: 152.85955810546875\n",
            "Training epoch 14261/1000000, d_loss: -35.983489990234375,  g_loss: 15.132858276367188\n",
            "Training epoch 14262/1000000, d_loss: -183.74903869628906,  g_loss: 162.66778564453125\n",
            "Training epoch 14263/1000000, d_loss: -27.851165771484375,  g_loss: -34.32221984863281\n",
            "Training epoch 14264/1000000, d_loss: -156.42982482910156,  g_loss: 107.85491943359375\n",
            "Training epoch 14265/1000000, d_loss: -440.2686767578125,  g_loss: -29.64242935180664\n",
            "Training epoch 14266/1000000, d_loss: -74.28202819824219,  g_loss: 35.109283447265625\n",
            "Training epoch 14267/1000000, d_loss: -146.5818634033203,  g_loss: 163.06787109375\n",
            "Training epoch 14268/1000000, d_loss: -129.4945526123047,  g_loss: 0.4504413604736328\n",
            "Training epoch 14269/1000000, d_loss: -109.82215881347656,  g_loss: 10.514542579650879\n",
            "Training epoch 14270/1000000, d_loss: -82.8629379272461,  g_loss: 39.66946792602539\n",
            "Training epoch 14271/1000000, d_loss: -62.09648895263672,  g_loss: 125.89965057373047\n",
            "Training epoch 14272/1000000, d_loss: -96.96416473388672,  g_loss: 66.5733413696289\n",
            "Training epoch 14273/1000000, d_loss: -48.1019287109375,  g_loss: 3.527886152267456\n",
            "Training epoch 14274/1000000, d_loss: -91.56512451171875,  g_loss: 87.82740783691406\n",
            "Training epoch 14275/1000000, d_loss: -223.64303588867188,  g_loss: 90.55268096923828\n",
            "Training epoch 14276/1000000, d_loss: -57.72588348388672,  g_loss: -9.799412727355957\n",
            "Training epoch 14277/1000000, d_loss: -63.804222106933594,  g_loss: 7.284783363342285\n",
            "Training epoch 14278/1000000, d_loss: -272.66876220703125,  g_loss: -87.6424331665039\n",
            "Training epoch 14279/1000000, d_loss: -53.10969543457031,  g_loss: -40.46855926513672\n",
            "Training epoch 14280/1000000, d_loss: -49.173240661621094,  g_loss: 17.249618530273438\n",
            "Training epoch 14281/1000000, d_loss: -149.57327270507812,  g_loss: -30.122129440307617\n",
            "Training epoch 14282/1000000, d_loss: -1957.435302734375,  g_loss: -123.52841186523438\n",
            "Training epoch 14283/1000000, d_loss: -146.5792236328125,  g_loss: -87.52556610107422\n",
            "Training epoch 14284/1000000, d_loss: -1594.591796875,  g_loss: -241.225341796875\n",
            "Training epoch 14285/1000000, d_loss: 86.4592056274414,  g_loss: -92.56468200683594\n",
            "Training epoch 14286/1000000, d_loss: -46.8388671875,  g_loss: -77.13143157958984\n",
            "Training epoch 14287/1000000, d_loss: -13.686408996582031,  g_loss: -131.334228515625\n",
            "Training epoch 14288/1000000, d_loss: -312.3172912597656,  g_loss: -104.22409057617188\n",
            "Training epoch 14289/1000000, d_loss: -162.51263427734375,  g_loss: -155.58489990234375\n",
            "Training epoch 14290/1000000, d_loss: -195.23153686523438,  g_loss: -97.90921783447266\n",
            "Training epoch 14291/1000000, d_loss: 315.5806579589844,  g_loss: 170.47547912597656\n",
            "Training epoch 14292/1000000, d_loss: -31.337966918945312,  g_loss: -41.49794006347656\n",
            "Training epoch 14293/1000000, d_loss: -414.50274658203125,  g_loss: 662.9689331054688\n",
            "Training epoch 14294/1000000, d_loss: -14.36480712890625,  g_loss: -37.24150848388672\n",
            "Training epoch 14295/1000000, d_loss: -129.0684814453125,  g_loss: -93.41395568847656\n",
            "Training epoch 14296/1000000, d_loss: -88.24751281738281,  g_loss: -95.98990631103516\n",
            "Training epoch 14297/1000000, d_loss: -280.3608093261719,  g_loss: -81.04324340820312\n",
            "Training epoch 14298/1000000, d_loss: -29.896392822265625,  g_loss: -30.987361907958984\n",
            "Training epoch 14299/1000000, d_loss: -95.43014526367188,  g_loss: 5.943183898925781\n",
            "Training epoch 14300/1000000, d_loss: -44.106815338134766,  g_loss: 31.219417572021484\n",
            "Training epoch 14301/1000000, d_loss: -171.66624450683594,  g_loss: 27.81850814819336\n",
            "Training epoch 14302/1000000, d_loss: -148.4197998046875,  g_loss: 74.6196060180664\n",
            "Training epoch 14303/1000000, d_loss: -81.17228698730469,  g_loss: 23.660476684570312\n",
            "Training epoch 14304/1000000, d_loss: -112.48021697998047,  g_loss: 68.06460571289062\n",
            "Training epoch 14305/1000000, d_loss: -72.7330322265625,  g_loss: -35.33258056640625\n",
            "Training epoch 14306/1000000, d_loss: -400.0067138671875,  g_loss: -52.15069580078125\n",
            "Training epoch 14307/1000000, d_loss: -50.20823669433594,  g_loss: -13.30108642578125\n",
            "Training epoch 14308/1000000, d_loss: -123.14218139648438,  g_loss: -9.163156509399414\n",
            "Training epoch 14309/1000000, d_loss: -177.68582153320312,  g_loss: 123.72251892089844\n",
            "Training epoch 14310/1000000, d_loss: -62.093536376953125,  g_loss: -11.506555557250977\n",
            "Training epoch 14311/1000000, d_loss: 24.897079467773438,  g_loss: -18.33060073852539\n",
            "Training epoch 14312/1000000, d_loss: -65.18586730957031,  g_loss: 5.762888431549072\n",
            "Training epoch 14313/1000000, d_loss: -181.97201538085938,  g_loss: 2.1550755500793457\n",
            "Training epoch 14314/1000000, d_loss: -876.009521484375,  g_loss: -185.431640625\n",
            "Training epoch 14315/1000000, d_loss: -189.97694396972656,  g_loss: -103.2966079711914\n",
            "Training epoch 14316/1000000, d_loss: -272.54876708984375,  g_loss: -68.30584716796875\n",
            "Training epoch 14317/1000000, d_loss: -76.1558837890625,  g_loss: 1.8131351470947266\n",
            "Training epoch 14318/1000000, d_loss: -21.608001708984375,  g_loss: -34.364601135253906\n",
            "Training epoch 14319/1000000, d_loss: -407.7421875,  g_loss: -25.980472564697266\n",
            "Training epoch 14320/1000000, d_loss: 42.76723861694336,  g_loss: -47.13060760498047\n",
            "Training epoch 14321/1000000, d_loss: -61.575897216796875,  g_loss: -56.98040771484375\n",
            "Training epoch 14322/1000000, d_loss: -121.42979431152344,  g_loss: 105.44345092773438\n",
            "Training epoch 14323/1000000, d_loss: -131.59039306640625,  g_loss: -8.961938858032227\n",
            "Training epoch 14324/1000000, d_loss: -38.58491134643555,  g_loss: -38.54376220703125\n",
            "Training epoch 14325/1000000, d_loss: -308.6025390625,  g_loss: -72.22994995117188\n",
            "Training epoch 14326/1000000, d_loss: -323.8562316894531,  g_loss: -101.84833526611328\n",
            "Training epoch 14327/1000000, d_loss: 25.783733367919922,  g_loss: -66.63386535644531\n",
            "Training epoch 14328/1000000, d_loss: 20.5748291015625,  g_loss: 41.644256591796875\n",
            "Training epoch 14329/1000000, d_loss: -312.2276916503906,  g_loss: -99.24356079101562\n",
            "Training epoch 14330/1000000, d_loss: -202.18853759765625,  g_loss: 407.504150390625\n",
            "Training epoch 14331/1000000, d_loss: -162.7501678466797,  g_loss: 89.59979248046875\n",
            "Training epoch 14332/1000000, d_loss: -217.56106567382812,  g_loss: 293.411865234375\n",
            "Training epoch 14333/1000000, d_loss: -82.32717895507812,  g_loss: 837.3727416992188\n",
            "Training epoch 14334/1000000, d_loss: 83.60101318359375,  g_loss: -3.059271812438965\n",
            "Training epoch 14335/1000000, d_loss: -57.8970947265625,  g_loss: -12.061655044555664\n",
            "Training epoch 14336/1000000, d_loss: -159.03199768066406,  g_loss: 71.90567016601562\n",
            "Training epoch 14337/1000000, d_loss: -193.7218475341797,  g_loss: 158.8859100341797\n",
            "Training epoch 14338/1000000, d_loss: -76.0268325805664,  g_loss: 20.08565902709961\n",
            "Training epoch 14339/1000000, d_loss: -157.8017578125,  g_loss: -6.436319351196289\n",
            "Training epoch 14340/1000000, d_loss: -145.97634887695312,  g_loss: 7.179448127746582\n",
            "Training epoch 14341/1000000, d_loss: -21.656137466430664,  g_loss: 17.024900436401367\n",
            "Training epoch 14342/1000000, d_loss: -66.67921447753906,  g_loss: 60.732601165771484\n",
            "Training epoch 14343/1000000, d_loss: -273.25732421875,  g_loss: -46.1361198425293\n",
            "Training epoch 14344/1000000, d_loss: -806.6351318359375,  g_loss: -38.27626037597656\n",
            "Training epoch 14345/1000000, d_loss: 5.447029113769531,  g_loss: 26.165435791015625\n",
            "Training epoch 14346/1000000, d_loss: -115.32441711425781,  g_loss: 73.66203308105469\n",
            "Training epoch 14347/1000000, d_loss: -142.4191131591797,  g_loss: -7.371710777282715\n",
            "Training epoch 14348/1000000, d_loss: -1014.9426879882812,  g_loss: -71.56652069091797\n",
            "Training epoch 14349/1000000, d_loss: -133.35638427734375,  g_loss: -152.3969268798828\n",
            "Training epoch 14350/1000000, d_loss: 94.1827392578125,  g_loss: 59.430381774902344\n",
            "Training epoch 14351/1000000, d_loss: -94.04496002197266,  g_loss: 37.741790771484375\n",
            "Training epoch 14352/1000000, d_loss: -123.5701675415039,  g_loss: 55.811038970947266\n",
            "Training epoch 14353/1000000, d_loss: -109.74105834960938,  g_loss: 6.405082702636719\n",
            "Training epoch 14354/1000000, d_loss: -94.77362823486328,  g_loss: 32.07057189941406\n",
            "Training epoch 14355/1000000, d_loss: -201.85911560058594,  g_loss: 320.3773498535156\n",
            "Training epoch 14356/1000000, d_loss: -156.53115844726562,  g_loss: 50.82332992553711\n",
            "Training epoch 14357/1000000, d_loss: -32.226097106933594,  g_loss: 10.63821792602539\n",
            "Training epoch 14358/1000000, d_loss: -78.13200378417969,  g_loss: 33.10104751586914\n",
            "Training epoch 14359/1000000, d_loss: -79.11372375488281,  g_loss: -7.426428318023682\n",
            "Training epoch 14360/1000000, d_loss: -217.53114318847656,  g_loss: -14.06602668762207\n",
            "Training epoch 14361/1000000, d_loss: -51.62557601928711,  g_loss: 5.854666709899902\n",
            "Training epoch 14362/1000000, d_loss: -364.7174377441406,  g_loss: 33.32866668701172\n",
            "Training epoch 14363/1000000, d_loss: -151.40228271484375,  g_loss: -57.924644470214844\n",
            "Training epoch 14364/1000000, d_loss: -39.33768844604492,  g_loss: 38.58258819580078\n",
            "Training epoch 14365/1000000, d_loss: -19.328659057617188,  g_loss: 34.24369430541992\n",
            "Training epoch 14366/1000000, d_loss: -167.2569122314453,  g_loss: 101.24066162109375\n",
            "Training epoch 14367/1000000, d_loss: -203.2091827392578,  g_loss: 22.26176643371582\n",
            "Training epoch 14368/1000000, d_loss: -28.07408332824707,  g_loss: -60.7109489440918\n",
            "Training epoch 14369/1000000, d_loss: -341.8746032714844,  g_loss: -108.92201232910156\n",
            "Training epoch 14370/1000000, d_loss: -531.370361328125,  g_loss: -163.90798950195312\n",
            "Training epoch 14371/1000000, d_loss: 37.105674743652344,  g_loss: -21.124921798706055\n",
            "Training epoch 14372/1000000, d_loss: -78.90072631835938,  g_loss: -65.99273681640625\n",
            "Training epoch 14373/1000000, d_loss: -198.20919799804688,  g_loss: 48.323970794677734\n",
            "Training epoch 14374/1000000, d_loss: -320.6628723144531,  g_loss: 376.58868408203125\n",
            "Training epoch 14375/1000000, d_loss: -91.899658203125,  g_loss: -79.97697448730469\n",
            "Training epoch 14376/1000000, d_loss: -317.34686279296875,  g_loss: -167.58743286132812\n",
            "Training epoch 14377/1000000, d_loss: -51.856842041015625,  g_loss: -110.31072998046875\n",
            "Training epoch 14378/1000000, d_loss: -14.946701049804688,  g_loss: 13.767023086547852\n",
            "Training epoch 14379/1000000, d_loss: -352.8361511230469,  g_loss: -19.77788543701172\n",
            "Training epoch 14380/1000000, d_loss: -268.3137512207031,  g_loss: -47.342594146728516\n",
            "Training epoch 14381/1000000, d_loss: -3.543926239013672,  g_loss: -28.322208404541016\n",
            "Training epoch 14382/1000000, d_loss: -108.066162109375,  g_loss: 35.32263946533203\n",
            "Training epoch 14383/1000000, d_loss: -148.30154418945312,  g_loss: 84.53123474121094\n",
            "Training epoch 14384/1000000, d_loss: -193.00875854492188,  g_loss: 30.759815216064453\n",
            "Training epoch 14385/1000000, d_loss: -182.2738494873047,  g_loss: 51.001739501953125\n",
            "Training epoch 14386/1000000, d_loss: -573.023681640625,  g_loss: 866.7998657226562\n",
            "Training epoch 14387/1000000, d_loss: 634.3396606445312,  g_loss: -29.122709274291992\n",
            "Training epoch 14388/1000000, d_loss: -82.28849792480469,  g_loss: -133.62481689453125\n",
            "Training epoch 14389/1000000, d_loss: -38.41557312011719,  g_loss: -123.86353302001953\n",
            "Training epoch 14390/1000000, d_loss: -59.4588623046875,  g_loss: -104.0823745727539\n",
            "Training epoch 14391/1000000, d_loss: -43.78807830810547,  g_loss: -82.70724487304688\n",
            "Training epoch 14392/1000000, d_loss: -50.819740295410156,  g_loss: -54.91798782348633\n",
            "Training epoch 14393/1000000, d_loss: -445.4425964355469,  g_loss: -212.36988830566406\n",
            "Training epoch 14394/1000000, d_loss: 293.15576171875,  g_loss: -57.31834030151367\n",
            "Training epoch 14395/1000000, d_loss: -232.8225860595703,  g_loss: -93.0490493774414\n",
            "Training epoch 14396/1000000, d_loss: -90.8322982788086,  g_loss: -11.239092826843262\n",
            "Training epoch 14397/1000000, d_loss: -151.94985961914062,  g_loss: -60.39739990234375\n",
            "Training epoch 14398/1000000, d_loss: -521.303466796875,  g_loss: -237.55870056152344\n",
            "Training epoch 14399/1000000, d_loss: -137.91893005371094,  g_loss: 7.6940202713012695\n",
            "Training epoch 14400/1000000, d_loss: -27.2125186920166,  g_loss: 120.74427795410156\n",
            "Training epoch 14401/1000000, d_loss: -115.45345306396484,  g_loss: 48.66437530517578\n",
            "Training epoch 14402/1000000, d_loss: -93.20306396484375,  g_loss: 109.84191131591797\n",
            "Training epoch 14403/1000000, d_loss: -76.4952392578125,  g_loss: 32.63941192626953\n",
            "Training epoch 14404/1000000, d_loss: -360.6237487792969,  g_loss: 233.06887817382812\n",
            "Training epoch 14405/1000000, d_loss: -13.986320495605469,  g_loss: -44.19167709350586\n",
            "Training epoch 14406/1000000, d_loss: -44.93814468383789,  g_loss: 4.6618523597717285\n",
            "Training epoch 14407/1000000, d_loss: -158.8002166748047,  g_loss: -56.92552185058594\n",
            "Training epoch 14408/1000000, d_loss: -80.6839599609375,  g_loss: 3.889801502227783\n",
            "Training epoch 14409/1000000, d_loss: -262.24127197265625,  g_loss: -52.861328125\n",
            "Training epoch 14410/1000000, d_loss: -17.453258514404297,  g_loss: -4.9124555587768555\n",
            "Training epoch 14411/1000000, d_loss: -78.70982360839844,  g_loss: 29.94908905029297\n",
            "Training epoch 14412/1000000, d_loss: -409.2979431152344,  g_loss: -14.655356407165527\n",
            "Training epoch 14413/1000000, d_loss: -6.741268157958984,  g_loss: 57.294010162353516\n",
            "Training epoch 14414/1000000, d_loss: -188.09962463378906,  g_loss: 15.024866104125977\n",
            "Training epoch 14415/1000000, d_loss: 7.540214538574219,  g_loss: 49.946903228759766\n",
            "Training epoch 14416/1000000, d_loss: -99.41545104980469,  g_loss: 58.33308029174805\n",
            "Training epoch 14417/1000000, d_loss: -311.5461730957031,  g_loss: -37.63308334350586\n",
            "Training epoch 14418/1000000, d_loss: -413.20989990234375,  g_loss: -124.41192626953125\n",
            "Training epoch 14419/1000000, d_loss: -30.62374496459961,  g_loss: -65.70899963378906\n",
            "Training epoch 14420/1000000, d_loss: -24.537797927856445,  g_loss: -32.713844299316406\n",
            "Training epoch 14421/1000000, d_loss: -106.31189727783203,  g_loss: -54.845054626464844\n",
            "Training epoch 14422/1000000, d_loss: -64.8433837890625,  g_loss: -32.576107025146484\n",
            "Training epoch 14423/1000000, d_loss: -227.7948455810547,  g_loss: -33.08216857910156\n",
            "Training epoch 14424/1000000, d_loss: -126.110107421875,  g_loss: -92.70421600341797\n",
            "Training epoch 14425/1000000, d_loss: -184.09376525878906,  g_loss: -161.9796142578125\n",
            "Training epoch 14426/1000000, d_loss: 302.43389892578125,  g_loss: -37.348548889160156\n",
            "Training epoch 14427/1000000, d_loss: -92.85774230957031,  g_loss: -81.38591766357422\n",
            "Training epoch 14428/1000000, d_loss: 467.58917236328125,  g_loss: -18.997882843017578\n",
            "Training epoch 14429/1000000, d_loss: -103.6289291381836,  g_loss: 0.007533907890319824\n",
            "Training epoch 14430/1000000, d_loss: -412.32550048828125,  g_loss: -47.94919204711914\n",
            "Training epoch 14431/1000000, d_loss: -14.320072174072266,  g_loss: -24.586227416992188\n",
            "Training epoch 14432/1000000, d_loss: -169.99078369140625,  g_loss: 110.9786605834961\n",
            "Training epoch 14433/1000000, d_loss: -29.676719665527344,  g_loss: -10.979208946228027\n",
            "Training epoch 14434/1000000, d_loss: -261.7225646972656,  g_loss: -7.607229232788086\n",
            "Training epoch 14435/1000000, d_loss: -40.48750686645508,  g_loss: -8.4979829788208\n",
            "Training epoch 14436/1000000, d_loss: -433.2758483886719,  g_loss: -214.62579345703125\n",
            "Training epoch 14437/1000000, d_loss: -259.83447265625,  g_loss: -41.62775802612305\n",
            "Training epoch 14438/1000000, d_loss: -233.4183349609375,  g_loss: -68.97530364990234\n",
            "Training epoch 14439/1000000, d_loss: 28.656587600708008,  g_loss: -44.39252471923828\n",
            "Training epoch 14440/1000000, d_loss: -96.3161392211914,  g_loss: -37.94255065917969\n",
            "Training epoch 14441/1000000, d_loss: -168.82730102539062,  g_loss: 46.38588333129883\n",
            "Training epoch 14442/1000000, d_loss: -273.4148864746094,  g_loss: 411.5827331542969\n",
            "Training epoch 14443/1000000, d_loss: -240.4478302001953,  g_loss: 168.7325897216797\n",
            "Training epoch 14444/1000000, d_loss: -112.9117431640625,  g_loss: -14.10405445098877\n",
            "Training epoch 14445/1000000, d_loss: 69.02568054199219,  g_loss: 17.915307998657227\n",
            "Training epoch 14446/1000000, d_loss: -171.55714416503906,  g_loss: 170.38494873046875\n",
            "Training epoch 14447/1000000, d_loss: -66.28782653808594,  g_loss: 24.409957885742188\n",
            "Training epoch 14448/1000000, d_loss: -148.19532775878906,  g_loss: 40.07391357421875\n",
            "Training epoch 14449/1000000, d_loss: -67.12191772460938,  g_loss: 77.99786376953125\n",
            "Training epoch 14450/1000000, d_loss: 69.88986206054688,  g_loss: 3.139158248901367\n",
            "Training epoch 14451/1000000, d_loss: -633.7311401367188,  g_loss: -102.4593505859375\n",
            "Training epoch 14452/1000000, d_loss: -28.123676300048828,  g_loss: 34.857234954833984\n",
            "Training epoch 14453/1000000, d_loss: -129.21267700195312,  g_loss: 21.227981567382812\n",
            "Training epoch 14454/1000000, d_loss: -37.286903381347656,  g_loss: 31.50554847717285\n",
            "Training epoch 14455/1000000, d_loss: -14.0933837890625,  g_loss: 45.359657287597656\n",
            "Training epoch 14456/1000000, d_loss: -122.61042022705078,  g_loss: 38.360389709472656\n",
            "Training epoch 14457/1000000, d_loss: 0.6691970825195312,  g_loss: 14.108098983764648\n",
            "Training epoch 14458/1000000, d_loss: -69.56407165527344,  g_loss: 12.169629096984863\n",
            "Training epoch 14459/1000000, d_loss: -164.84632873535156,  g_loss: -28.501895904541016\n",
            "Training epoch 14460/1000000, d_loss: -130.8974151611328,  g_loss: 15.512348175048828\n",
            "Training epoch 14461/1000000, d_loss: -132.62100219726562,  g_loss: 38.71052932739258\n",
            "Training epoch 14462/1000000, d_loss: -130.97523498535156,  g_loss: 48.08580017089844\n",
            "Training epoch 14463/1000000, d_loss: -90.40461730957031,  g_loss: 117.62399291992188\n",
            "Training epoch 14464/1000000, d_loss: -140.3873291015625,  g_loss: 135.46075439453125\n",
            "Training epoch 14465/1000000, d_loss: -81.65226745605469,  g_loss: 180.62185668945312\n",
            "Training epoch 14466/1000000, d_loss: -90.69334411621094,  g_loss: 103.66722106933594\n",
            "Training epoch 14467/1000000, d_loss: -27.94037628173828,  g_loss: 0.9498178958892822\n",
            "Training epoch 14468/1000000, d_loss: -208.73489379882812,  g_loss: 171.36241149902344\n",
            "Training epoch 14469/1000000, d_loss: -414.28973388671875,  g_loss: -13.93934440612793\n",
            "Training epoch 14470/1000000, d_loss: -52.22843933105469,  g_loss: -45.071067810058594\n",
            "Training epoch 14471/1000000, d_loss: -52.08421325683594,  g_loss: -27.608837127685547\n",
            "Training epoch 14472/1000000, d_loss: -100.39584350585938,  g_loss: -79.5556640625\n",
            "Training epoch 14473/1000000, d_loss: -113.26044464111328,  g_loss: -12.672822952270508\n",
            "Training epoch 14474/1000000, d_loss: -254.04623413085938,  g_loss: -72.10218048095703\n",
            "Training epoch 14475/1000000, d_loss: -19.012313842773438,  g_loss: 6.090892791748047\n",
            "Training epoch 14476/1000000, d_loss: -79.80394744873047,  g_loss: -38.96989059448242\n",
            "Training epoch 14477/1000000, d_loss: -2.2620620727539062,  g_loss: 5.45033073425293\n",
            "Training epoch 14478/1000000, d_loss: -53.0546760559082,  g_loss: 10.134939193725586\n",
            "Training epoch 14479/1000000, d_loss: 26.828765869140625,  g_loss: 50.37735366821289\n",
            "Training epoch 14480/1000000, d_loss: -47.02678298950195,  g_loss: 31.023622512817383\n",
            "Training epoch 14481/1000000, d_loss: -721.3115234375,  g_loss: -63.99386978149414\n",
            "Training epoch 14482/1000000, d_loss: -71.3082275390625,  g_loss: 33.074737548828125\n",
            "Training epoch 14483/1000000, d_loss: -7.034336090087891,  g_loss: 1.8904895782470703\n",
            "Training epoch 14484/1000000, d_loss: -49.047149658203125,  g_loss: -1.6467726230621338\n",
            "Training epoch 14485/1000000, d_loss: -32.55682373046875,  g_loss: 4.368823051452637\n",
            "Training epoch 14486/1000000, d_loss: -62.12628173828125,  g_loss: 14.360596656799316\n",
            "Training epoch 14487/1000000, d_loss: -62.30552291870117,  g_loss: 21.427474975585938\n",
            "Training epoch 14488/1000000, d_loss: -128.35186767578125,  g_loss: 8.347372055053711\n",
            "Training epoch 14489/1000000, d_loss: -50.61024856567383,  g_loss: -6.562439441680908\n",
            "Training epoch 14490/1000000, d_loss: -113.2604751586914,  g_loss: -5.141750335693359\n",
            "Training epoch 14491/1000000, d_loss: -115.89205932617188,  g_loss: 24.673969268798828\n",
            "Training epoch 14492/1000000, d_loss: -614.995361328125,  g_loss: -54.600799560546875\n",
            "Training epoch 14493/1000000, d_loss: -215.43016052246094,  g_loss: -49.86487579345703\n",
            "Training epoch 14494/1000000, d_loss: -281.6371154785156,  g_loss: -117.73080444335938\n",
            "Training epoch 14495/1000000, d_loss: -105.14274597167969,  g_loss: -38.70027542114258\n",
            "Training epoch 14496/1000000, d_loss: -719.0296630859375,  g_loss: -131.79489135742188\n",
            "Training epoch 14497/1000000, d_loss: 2.9576339721679688,  g_loss: -13.823278427124023\n",
            "Training epoch 14498/1000000, d_loss: -31.8673095703125,  g_loss: 4.447650909423828\n",
            "Training epoch 14499/1000000, d_loss: -178.3655242919922,  g_loss: 107.19466400146484\n",
            "Training epoch 14500/1000000, d_loss: -234.32980346679688,  g_loss: -12.927030563354492\n",
            "Training epoch 14501/1000000, d_loss: -365.3009033203125,  g_loss: -216.07821655273438\n",
            "Training epoch 14502/1000000, d_loss: -240.0283966064453,  g_loss: -87.80853271484375\n",
            "Training epoch 14503/1000000, d_loss: -162.51055908203125,  g_loss: 142.41256713867188\n",
            "Training epoch 14504/1000000, d_loss: -80.05915832519531,  g_loss: 52.083473205566406\n",
            "Training epoch 14505/1000000, d_loss: -158.08609008789062,  g_loss: 44.17718505859375\n",
            "Training epoch 14506/1000000, d_loss: -318.95184326171875,  g_loss: 3.0868866443634033\n",
            "Training epoch 14507/1000000, d_loss: -225.01844787597656,  g_loss: -60.948604583740234\n",
            "Training epoch 14508/1000000, d_loss: -116.34310913085938,  g_loss: 60.86153030395508\n",
            "Training epoch 14509/1000000, d_loss: -751.2482299804688,  g_loss: -203.78756713867188\n",
            "Training epoch 14510/1000000, d_loss: -6.256614685058594,  g_loss: 88.986328125\n",
            "Training epoch 14511/1000000, d_loss: -104.1222152709961,  g_loss: -27.605138778686523\n",
            "Training epoch 14512/1000000, d_loss: -433.0614929199219,  g_loss: -113.82388305664062\n",
            "Training epoch 14513/1000000, d_loss: -145.29864501953125,  g_loss: -80.0081787109375\n",
            "Training epoch 14514/1000000, d_loss: -107.03436279296875,  g_loss: -62.717342376708984\n",
            "Training epoch 14515/1000000, d_loss: -608.38330078125,  g_loss: -398.41046142578125\n",
            "Training epoch 14516/1000000, d_loss: 3.49267578125,  g_loss: -145.40786743164062\n",
            "Training epoch 14517/1000000, d_loss: -374.0970764160156,  g_loss: 53.96992492675781\n",
            "Training epoch 14518/1000000, d_loss: -58.20254898071289,  g_loss: 114.53105163574219\n",
            "Training epoch 14519/1000000, d_loss: -248.10108947753906,  g_loss: 313.7897033691406\n",
            "Training epoch 14520/1000000, d_loss: -152.8115692138672,  g_loss: 56.306270599365234\n",
            "Training epoch 14521/1000000, d_loss: -293.8434143066406,  g_loss: 35.65042495727539\n",
            "Training epoch 14522/1000000, d_loss: 93.69612121582031,  g_loss: -50.08197021484375\n",
            "Training epoch 14523/1000000, d_loss: -149.4215087890625,  g_loss: 4.7654805183410645\n",
            "Training epoch 14524/1000000, d_loss: -380.58905029296875,  g_loss: 163.3466033935547\n",
            "Training epoch 14525/1000000, d_loss: -212.9505615234375,  g_loss: 45.28956985473633\n",
            "Training epoch 14526/1000000, d_loss: -163.81988525390625,  g_loss: 147.6306915283203\n",
            "Training epoch 14527/1000000, d_loss: -166.4468231201172,  g_loss: -180.04595947265625\n",
            "Training epoch 14528/1000000, d_loss: -75.12588500976562,  g_loss: -75.62737274169922\n",
            "Training epoch 14529/1000000, d_loss: -141.3613739013672,  g_loss: -86.0386734008789\n",
            "Training epoch 14530/1000000, d_loss: -28.559890747070312,  g_loss: -182.65933227539062\n",
            "Training epoch 14531/1000000, d_loss: -432.7253723144531,  g_loss: -100.28929138183594\n",
            "Training epoch 14532/1000000, d_loss: -196.54005432128906,  g_loss: -224.27047729492188\n",
            "Training epoch 14533/1000000, d_loss: -46.028648376464844,  g_loss: -29.195858001708984\n",
            "Training epoch 14534/1000000, d_loss: -269.2569885253906,  g_loss: -56.933677673339844\n",
            "Training epoch 14535/1000000, d_loss: -569.79150390625,  g_loss: -56.21221923828125\n",
            "Training epoch 14536/1000000, d_loss: 65.4559326171875,  g_loss: -138.34271240234375\n",
            "Training epoch 14537/1000000, d_loss: -90.69522857666016,  g_loss: 16.30893325805664\n",
            "Training epoch 14538/1000000, d_loss: -233.11793518066406,  g_loss: 417.87884521484375\n",
            "Training epoch 14539/1000000, d_loss: -481.3284912109375,  g_loss: 531.9780883789062\n",
            "Training epoch 14540/1000000, d_loss: -80.42174530029297,  g_loss: 148.85276794433594\n",
            "Training epoch 14541/1000000, d_loss: -260.6754455566406,  g_loss: 15.83760929107666\n",
            "Training epoch 14542/1000000, d_loss: 150.08950805664062,  g_loss: -19.406356811523438\n",
            "Training epoch 14543/1000000, d_loss: -16.261558532714844,  g_loss: 53.98005294799805\n",
            "Training epoch 14544/1000000, d_loss: -88.62467193603516,  g_loss: -7.259004592895508\n",
            "Training epoch 14545/1000000, d_loss: -357.9924011230469,  g_loss: -135.04432678222656\n",
            "Training epoch 14546/1000000, d_loss: -383.09039306640625,  g_loss: -209.02700805664062\n",
            "Training epoch 14547/1000000, d_loss: -14.806943893432617,  g_loss: -68.7654037475586\n",
            "Training epoch 14548/1000000, d_loss: -116.80767059326172,  g_loss: 1.1200008392333984\n",
            "Training epoch 14549/1000000, d_loss: -122.12487030029297,  g_loss: 36.584007263183594\n",
            "Training epoch 14550/1000000, d_loss: -100.89297485351562,  g_loss: 39.24630355834961\n",
            "Training epoch 14551/1000000, d_loss: -228.90005493164062,  g_loss: 57.535614013671875\n",
            "Training epoch 14552/1000000, d_loss: -266.1261291503906,  g_loss: 192.8087158203125\n",
            "Training epoch 14553/1000000, d_loss: -75.8958511352539,  g_loss: 14.339666366577148\n",
            "Training epoch 14554/1000000, d_loss: -115.45987701416016,  g_loss: -4.242119789123535\n",
            "Training epoch 14555/1000000, d_loss: -145.24769592285156,  g_loss: 46.636207580566406\n",
            "Training epoch 14556/1000000, d_loss: -76.90969848632812,  g_loss: 64.39069366455078\n",
            "Training epoch 14557/1000000, d_loss: -68.47441101074219,  g_loss: 5.710969924926758\n",
            "Training epoch 14558/1000000, d_loss: -215.38699340820312,  g_loss: -47.7579460144043\n",
            "Training epoch 14559/1000000, d_loss: 1785.208984375,  g_loss: 40.95167922973633\n",
            "Training epoch 14560/1000000, d_loss: -38.50906753540039,  g_loss: 58.92784881591797\n",
            "Training epoch 14561/1000000, d_loss: -68.39411926269531,  g_loss: 61.01605987548828\n",
            "Training epoch 14562/1000000, d_loss: -114.46217346191406,  g_loss: 56.59539031982422\n",
            "Training epoch 14563/1000000, d_loss: -109.37722778320312,  g_loss: 47.43640899658203\n",
            "Training epoch 14564/1000000, d_loss: -114.513671875,  g_loss: 78.48945617675781\n",
            "Training epoch 14565/1000000, d_loss: -433.2825622558594,  g_loss: -29.626384735107422\n",
            "Training epoch 14566/1000000, d_loss: -190.5357666015625,  g_loss: 17.446884155273438\n",
            "Training epoch 14567/1000000, d_loss: 3.8565292358398438,  g_loss: 17.065017700195312\n",
            "Training epoch 14568/1000000, d_loss: -103.72969818115234,  g_loss: 57.615570068359375\n",
            "Training epoch 14569/1000000, d_loss: -206.33888244628906,  g_loss: 7.827573299407959\n",
            "Training epoch 14570/1000000, d_loss: -71.58854675292969,  g_loss: 66.45526123046875\n",
            "Training epoch 14571/1000000, d_loss: -74.4305191040039,  g_loss: 21.992639541625977\n",
            "Training epoch 14572/1000000, d_loss: -120.34722900390625,  g_loss: -44.94799041748047\n",
            "Training epoch 14573/1000000, d_loss: -66.30755615234375,  g_loss: 10.749408721923828\n",
            "Training epoch 14574/1000000, d_loss: -4.693683624267578,  g_loss: -1.2688803672790527\n",
            "Training epoch 14575/1000000, d_loss: -62.398983001708984,  g_loss: -30.519437789916992\n",
            "Training epoch 14576/1000000, d_loss: -54.99989318847656,  g_loss: 61.33351135253906\n",
            "Training epoch 14577/1000000, d_loss: -42.55350875854492,  g_loss: -4.789838790893555\n",
            "Training epoch 14578/1000000, d_loss: -151.63250732421875,  g_loss: -29.558269500732422\n",
            "Training epoch 14579/1000000, d_loss: -51.12990188598633,  g_loss: 42.610595703125\n",
            "Training epoch 14580/1000000, d_loss: -84.16111755371094,  g_loss: 11.202556610107422\n",
            "Training epoch 14581/1000000, d_loss: 35.27312469482422,  g_loss: 48.488128662109375\n",
            "Training epoch 14582/1000000, d_loss: -565.3001098632812,  g_loss: -33.20196533203125\n",
            "Training epoch 14583/1000000, d_loss: -50.94182205200195,  g_loss: 1.303192138671875\n",
            "Training epoch 14584/1000000, d_loss: -67.04426574707031,  g_loss: 53.18396759033203\n",
            "Training epoch 14585/1000000, d_loss: -126.96212005615234,  g_loss: 14.672541618347168\n",
            "Training epoch 14586/1000000, d_loss: -864.6613159179688,  g_loss: -90.60281372070312\n",
            "Training epoch 14587/1000000, d_loss: 64.50888061523438,  g_loss: -15.957307815551758\n",
            "Training epoch 14588/1000000, d_loss: -891.7964477539062,  g_loss: -110.95054626464844\n",
            "Training epoch 14589/1000000, d_loss: 176.79937744140625,  g_loss: -206.34027099609375\n",
            "Training epoch 14590/1000000, d_loss: -76.15052795410156,  g_loss: 7.996699333190918\n",
            "Training epoch 14591/1000000, d_loss: -77.59307861328125,  g_loss: 87.7081069946289\n",
            "Training epoch 14592/1000000, d_loss: -119.74694061279297,  g_loss: 7.125670433044434\n",
            "Training epoch 14593/1000000, d_loss: -93.96831512451172,  g_loss: 50.631736755371094\n",
            "Training epoch 14594/1000000, d_loss: -45.129981994628906,  g_loss: 16.66061019897461\n",
            "Training epoch 14595/1000000, d_loss: -514.7838745117188,  g_loss: -145.1119384765625\n",
            "Training epoch 14596/1000000, d_loss: -63.566715240478516,  g_loss: 125.18437194824219\n",
            "Training epoch 14597/1000000, d_loss: -362.3632507324219,  g_loss: 401.4831848144531\n",
            "Training epoch 14598/1000000, d_loss: 413.36785888671875,  g_loss: 399.2972412109375\n",
            "Training epoch 14599/1000000, d_loss: -186.25164794921875,  g_loss: -1.4575614929199219\n",
            "Training epoch 14600/1000000, d_loss: -24.842803955078125,  g_loss: 10.616260528564453\n",
            "Training epoch 14601/1000000, d_loss: -150.46261596679688,  g_loss: 123.94922637939453\n",
            "Training epoch 14602/1000000, d_loss: -75.1712646484375,  g_loss: 22.36886978149414\n",
            "Training epoch 14603/1000000, d_loss: -117.19232177734375,  g_loss: 1.6920089721679688\n",
            "Training epoch 14604/1000000, d_loss: -241.69302368164062,  g_loss: 16.489625930786133\n",
            "Training epoch 14605/1000000, d_loss: -49.98640823364258,  g_loss: 95.09088134765625\n",
            "Training epoch 14606/1000000, d_loss: -120.21432495117188,  g_loss: 106.95130920410156\n",
            "Training epoch 14607/1000000, d_loss: -132.267333984375,  g_loss: 319.13671875\n",
            "Training epoch 14608/1000000, d_loss: -57.50983428955078,  g_loss: 70.24224853515625\n",
            "Training epoch 14609/1000000, d_loss: -114.13423156738281,  g_loss: 18.717756271362305\n",
            "Training epoch 14610/1000000, d_loss: -90.63582611083984,  g_loss: 13.000919342041016\n",
            "Training epoch 14611/1000000, d_loss: -540.4174194335938,  g_loss: -133.910888671875\n",
            "Training epoch 14612/1000000, d_loss: -26.15185546875,  g_loss: 41.673824310302734\n",
            "Training epoch 14613/1000000, d_loss: -152.78192138671875,  g_loss: 8.688478469848633\n",
            "Training epoch 14614/1000000, d_loss: -74.96904754638672,  g_loss: 46.17493438720703\n",
            "Training epoch 14615/1000000, d_loss: -337.3375244140625,  g_loss: 56.863773345947266\n",
            "Training epoch 14616/1000000, d_loss: -394.49462890625,  g_loss: -44.69861602783203\n",
            "Training epoch 14617/1000000, d_loss: -40.62307357788086,  g_loss: -10.93617057800293\n",
            "Training epoch 14618/1000000, d_loss: -35.004207611083984,  g_loss: 34.704071044921875\n",
            "Training epoch 14619/1000000, d_loss: -82.84718322753906,  g_loss: 18.395217895507812\n",
            "Training epoch 14620/1000000, d_loss: -94.18372344970703,  g_loss: 72.87173461914062\n",
            "Training epoch 14621/1000000, d_loss: -149.9876251220703,  g_loss: -15.040011405944824\n",
            "Training epoch 14622/1000000, d_loss: -111.7711181640625,  g_loss: 11.170187950134277\n",
            "Training epoch 14623/1000000, d_loss: -377.3752136230469,  g_loss: -67.26956176757812\n",
            "Training epoch 14624/1000000, d_loss: -653.9103393554688,  g_loss: -296.7232971191406\n",
            "Training epoch 14625/1000000, d_loss: -247.81675720214844,  g_loss: 9.109296798706055\n",
            "Training epoch 14626/1000000, d_loss: 61.439231872558594,  g_loss: 29.461406707763672\n",
            "Training epoch 14627/1000000, d_loss: -78.91706848144531,  g_loss: 101.76835632324219\n",
            "Training epoch 14628/1000000, d_loss: -145.134521484375,  g_loss: 88.65380096435547\n",
            "Training epoch 14629/1000000, d_loss: -73.16875457763672,  g_loss: 36.627464294433594\n",
            "Training epoch 14630/1000000, d_loss: -106.00122833251953,  g_loss: 75.3779525756836\n",
            "Training epoch 14631/1000000, d_loss: -68.53959655761719,  g_loss: 33.5966796875\n",
            "Training epoch 14632/1000000, d_loss: 11.453384399414062,  g_loss: 68.17488098144531\n",
            "Training epoch 14633/1000000, d_loss: -25.38764190673828,  g_loss: 43.66103744506836\n",
            "Training epoch 14634/1000000, d_loss: -72.29234313964844,  g_loss: 89.33917236328125\n",
            "Training epoch 14635/1000000, d_loss: -193.14810180664062,  g_loss: 49.45233917236328\n",
            "Training epoch 14636/1000000, d_loss: -19.090147018432617,  g_loss: 18.163442611694336\n",
            "Training epoch 14637/1000000, d_loss: -80.22834014892578,  g_loss: -1.4332693815231323\n",
            "Training epoch 14638/1000000, d_loss: -150.0554656982422,  g_loss: 0.6430397033691406\n",
            "Training epoch 14639/1000000, d_loss: -105.47062683105469,  g_loss: 25.1658878326416\n",
            "Training epoch 14640/1000000, d_loss: -124.204345703125,  g_loss: -11.478496551513672\n",
            "Training epoch 14641/1000000, d_loss: -67.71304321289062,  g_loss: 2.2727644443511963\n",
            "Training epoch 14642/1000000, d_loss: -45.006072998046875,  g_loss: 29.424312591552734\n",
            "Training epoch 14643/1000000, d_loss: 7.10723876953125,  g_loss: -9.119865417480469\n",
            "Training epoch 14644/1000000, d_loss: -147.9501953125,  g_loss: -10.133406639099121\n",
            "Training epoch 14645/1000000, d_loss: -78.10586547851562,  g_loss: 23.76988983154297\n",
            "Training epoch 14646/1000000, d_loss: -155.3462677001953,  g_loss: -19.722261428833008\n",
            "Training epoch 14647/1000000, d_loss: -103.94379425048828,  g_loss: -26.370819091796875\n",
            "Training epoch 14648/1000000, d_loss: -367.0780944824219,  g_loss: -64.01287078857422\n",
            "Training epoch 14649/1000000, d_loss: -65.1605224609375,  g_loss: -68.00847625732422\n",
            "Training epoch 14650/1000000, d_loss: -343.0411376953125,  g_loss: -26.25259780883789\n",
            "Training epoch 14651/1000000, d_loss: -142.15518188476562,  g_loss: -28.540924072265625\n",
            "Training epoch 14652/1000000, d_loss: 7.641033172607422,  g_loss: -83.17491149902344\n",
            "Training epoch 14653/1000000, d_loss: 48.759586334228516,  g_loss: -54.325687408447266\n",
            "Training epoch 14654/1000000, d_loss: -51.45721435546875,  g_loss: -32.28276443481445\n",
            "Training epoch 14655/1000000, d_loss: -197.389404296875,  g_loss: -67.71235656738281\n",
            "Training epoch 14656/1000000, d_loss: -79.0063247680664,  g_loss: -5.304709434509277\n",
            "Training epoch 14657/1000000, d_loss: -312.90338134765625,  g_loss: -99.87765502929688\n",
            "Training epoch 14658/1000000, d_loss: -42.40660858154297,  g_loss: 20.375938415527344\n",
            "Training epoch 14659/1000000, d_loss: -158.55886840820312,  g_loss: 38.84709167480469\n",
            "Training epoch 14660/1000000, d_loss: -84.24049377441406,  g_loss: 19.31259536743164\n",
            "Training epoch 14661/1000000, d_loss: -41.634033203125,  g_loss: 78.27517700195312\n",
            "Training epoch 14662/1000000, d_loss: -80.1590576171875,  g_loss: 75.35884094238281\n",
            "Training epoch 14663/1000000, d_loss: -145.6428985595703,  g_loss: 226.80520629882812\n",
            "Training epoch 14664/1000000, d_loss: -7.780918121337891,  g_loss: -24.578739166259766\n",
            "Training epoch 14665/1000000, d_loss: -65.66542053222656,  g_loss: 22.491764068603516\n",
            "Training epoch 14666/1000000, d_loss: -51.37310791015625,  g_loss: 13.457026481628418\n",
            "Training epoch 14667/1000000, d_loss: -335.2828369140625,  g_loss: -66.7107925415039\n",
            "Training epoch 14668/1000000, d_loss: -191.336181640625,  g_loss: -73.78802490234375\n",
            "Training epoch 14669/1000000, d_loss: -139.1172637939453,  g_loss: -62.61247253417969\n",
            "Training epoch 14670/1000000, d_loss: -166.16770935058594,  g_loss: 80.19621276855469\n",
            "Training epoch 14671/1000000, d_loss: -164.38621520996094,  g_loss: 189.17791748046875\n",
            "Training epoch 14672/1000000, d_loss: 36.04905700683594,  g_loss: 186.50318908691406\n",
            "Training epoch 14673/1000000, d_loss: -594.0626220703125,  g_loss: -23.832752227783203\n",
            "Training epoch 14674/1000000, d_loss: -474.3883972167969,  g_loss: -108.97083282470703\n",
            "Training epoch 14675/1000000, d_loss: -8.511344909667969,  g_loss: -5.313844203948975\n",
            "Training epoch 14676/1000000, d_loss: -128.27102661132812,  g_loss: -33.41590118408203\n",
            "Training epoch 14677/1000000, d_loss: -121.55439758300781,  g_loss: 87.42554473876953\n",
            "Training epoch 14678/1000000, d_loss: -580.3463134765625,  g_loss: -198.35903930664062\n",
            "Training epoch 14679/1000000, d_loss: 302.17437744140625,  g_loss: 30.440950393676758\n",
            "Training epoch 14680/1000000, d_loss: -89.8808364868164,  g_loss: 79.99027252197266\n",
            "Training epoch 14681/1000000, d_loss: -101.70008850097656,  g_loss: -3.4472365379333496\n",
            "Training epoch 14682/1000000, d_loss: -103.37684631347656,  g_loss: 65.73271942138672\n",
            "Training epoch 14683/1000000, d_loss: -177.8047637939453,  g_loss: -41.74718475341797\n",
            "Training epoch 14684/1000000, d_loss: -68.6646728515625,  g_loss: 38.76235580444336\n",
            "Training epoch 14685/1000000, d_loss: -34.07005310058594,  g_loss: 24.502408981323242\n",
            "Training epoch 14686/1000000, d_loss: -109.02177429199219,  g_loss: 167.05201721191406\n",
            "Training epoch 14687/1000000, d_loss: -74.88296508789062,  g_loss: 170.8573455810547\n",
            "Training epoch 14688/1000000, d_loss: -497.76568603515625,  g_loss: -17.268896102905273\n",
            "Training epoch 14689/1000000, d_loss: -172.3419189453125,  g_loss: -3.5266857147216797\n",
            "Training epoch 14690/1000000, d_loss: -64.87798309326172,  g_loss: -16.11493682861328\n",
            "Training epoch 14691/1000000, d_loss: -431.53668212890625,  g_loss: -138.0936279296875\n",
            "Training epoch 14692/1000000, d_loss: -56.55296325683594,  g_loss: -48.017669677734375\n",
            "Training epoch 14693/1000000, d_loss: -293.13714599609375,  g_loss: 23.052364349365234\n",
            "Training epoch 14694/1000000, d_loss: -264.95196533203125,  g_loss: 439.7206115722656\n",
            "Training epoch 14695/1000000, d_loss: -87.46328735351562,  g_loss: 110.06063842773438\n",
            "Training epoch 14696/1000000, d_loss: 88.66505432128906,  g_loss: 44.22471618652344\n",
            "Training epoch 14697/1000000, d_loss: -111.59715270996094,  g_loss: 72.26899719238281\n",
            "Training epoch 14698/1000000, d_loss: -22.416702270507812,  g_loss: 7.966245174407959\n",
            "Training epoch 14699/1000000, d_loss: -176.8985595703125,  g_loss: -49.27983856201172\n",
            "Training epoch 14700/1000000, d_loss: -115.07569885253906,  g_loss: 26.005674362182617\n",
            "Training epoch 14701/1000000, d_loss: -168.6100311279297,  g_loss: -0.8561429977416992\n",
            "Training epoch 14702/1000000, d_loss: -92.43180084228516,  g_loss: 75.91197967529297\n",
            "Training epoch 14703/1000000, d_loss: -103.52323150634766,  g_loss: 18.75598907470703\n",
            "Training epoch 14704/1000000, d_loss: -580.923095703125,  g_loss: -252.1009063720703\n",
            "Training epoch 14705/1000000, d_loss: -359.573486328125,  g_loss: 186.97576904296875\n",
            "Training epoch 14706/1000000, d_loss: -102.89189147949219,  g_loss: -55.15171813964844\n",
            "Training epoch 14707/1000000, d_loss: -83.94578552246094,  g_loss: 37.33087158203125\n",
            "Training epoch 14708/1000000, d_loss: -192.12429809570312,  g_loss: 71.10580444335938\n",
            "Training epoch 14709/1000000, d_loss: -22.294212341308594,  g_loss: 49.60308074951172\n",
            "Training epoch 14710/1000000, d_loss: 56.77613067626953,  g_loss: -3.388166904449463\n",
            "Training epoch 14711/1000000, d_loss: -135.31166076660156,  g_loss: -15.854137420654297\n",
            "Training epoch 14712/1000000, d_loss: -109.42544555664062,  g_loss: 23.860803604125977\n",
            "Training epoch 14713/1000000, d_loss: -141.98623657226562,  g_loss: 20.44158172607422\n",
            "Training epoch 14714/1000000, d_loss: -85.98534393310547,  g_loss: 54.364036560058594\n",
            "Training epoch 14715/1000000, d_loss: -374.6882019042969,  g_loss: 130.01995849609375\n",
            "Training epoch 14716/1000000, d_loss: -112.3970718383789,  g_loss: -43.49276351928711\n",
            "Training epoch 14717/1000000, d_loss: -48.90357971191406,  g_loss: 55.0735969543457\n",
            "Training epoch 14718/1000000, d_loss: -1569.3690185546875,  g_loss: -391.9788513183594\n",
            "Training epoch 14719/1000000, d_loss: 428.44342041015625,  g_loss: -145.75563049316406\n",
            "Training epoch 14720/1000000, d_loss: 135.24378967285156,  g_loss: -41.09123229980469\n",
            "Training epoch 14721/1000000, d_loss: 116.0379638671875,  g_loss: -68.52640533447266\n",
            "Training epoch 14722/1000000, d_loss: -57.32231903076172,  g_loss: -59.658935546875\n",
            "Training epoch 14723/1000000, d_loss: -26.387462615966797,  g_loss: 178.62887573242188\n",
            "Training epoch 14724/1000000, d_loss: -112.87507629394531,  g_loss: 238.92098999023438\n",
            "Training epoch 14725/1000000, d_loss: -143.76397705078125,  g_loss: 57.424530029296875\n",
            "Training epoch 14726/1000000, d_loss: -53.03864288330078,  g_loss: -7.91480827331543\n",
            "Training epoch 14727/1000000, d_loss: -56.91143798828125,  g_loss: 10.534022331237793\n",
            "Training epoch 14728/1000000, d_loss: -224.51116943359375,  g_loss: -1.9945244789123535\n",
            "Training epoch 14729/1000000, d_loss: -397.5086975097656,  g_loss: -105.62350463867188\n",
            "Training epoch 14730/1000000, d_loss: -104.56953430175781,  g_loss: 23.531593322753906\n",
            "Training epoch 14731/1000000, d_loss: -157.22023010253906,  g_loss: 91.46434783935547\n",
            "Training epoch 14732/1000000, d_loss: -345.2468566894531,  g_loss: 53.38544845581055\n",
            "Training epoch 14733/1000000, d_loss: -130.90975952148438,  g_loss: 64.13237762451172\n",
            "Training epoch 14734/1000000, d_loss: -117.473388671875,  g_loss: 177.20794677734375\n",
            "Training epoch 14735/1000000, d_loss: -271.3020324707031,  g_loss: 14.381734848022461\n",
            "Training epoch 14736/1000000, d_loss: -195.1209716796875,  g_loss: 23.261981964111328\n",
            "Training epoch 14737/1000000, d_loss: -268.391357421875,  g_loss: 178.88510131835938\n",
            "Training epoch 14738/1000000, d_loss: -180.57717895507812,  g_loss: 120.48533630371094\n",
            "Training epoch 14739/1000000, d_loss: -98.23297119140625,  g_loss: 137.4993133544922\n",
            "Training epoch 14740/1000000, d_loss: -360.76055908203125,  g_loss: 230.98817443847656\n",
            "Training epoch 14741/1000000, d_loss: -112.9073257446289,  g_loss: 29.995304107666016\n",
            "Training epoch 14742/1000000, d_loss: -338.48406982421875,  g_loss: -40.67328643798828\n",
            "Training epoch 14743/1000000, d_loss: -120.43728637695312,  g_loss: -42.214141845703125\n",
            "Training epoch 14744/1000000, d_loss: 89.81431579589844,  g_loss: -39.19765853881836\n",
            "Training epoch 14745/1000000, d_loss: -111.54444122314453,  g_loss: -27.720455169677734\n",
            "Training epoch 14746/1000000, d_loss: -95.80939483642578,  g_loss: 19.33422088623047\n",
            "Training epoch 14747/1000000, d_loss: -40.921180725097656,  g_loss: 43.004356384277344\n",
            "Training epoch 14748/1000000, d_loss: -109.52198791503906,  g_loss: 18.954097747802734\n",
            "Training epoch 14749/1000000, d_loss: -223.3250732421875,  g_loss: 0.7747917175292969\n",
            "Training epoch 14750/1000000, d_loss: -364.23077392578125,  g_loss: -86.5180435180664\n",
            "Training epoch 14751/1000000, d_loss: 29.988161087036133,  g_loss: -59.46595001220703\n",
            "Training epoch 14752/1000000, d_loss: -380.65838623046875,  g_loss: -51.702796936035156\n",
            "Training epoch 14753/1000000, d_loss: -1.6783199310302734,  g_loss: -66.44286346435547\n",
            "Training epoch 14754/1000000, d_loss: -113.97997283935547,  g_loss: -109.40331268310547\n",
            "Training epoch 14755/1000000, d_loss: -137.80958557128906,  g_loss: -84.79776763916016\n",
            "Training epoch 14756/1000000, d_loss: -133.48208618164062,  g_loss: 57.96023941040039\n",
            "Training epoch 14757/1000000, d_loss: -136.66098022460938,  g_loss: -15.629484176635742\n",
            "Training epoch 14758/1000000, d_loss: -173.1929931640625,  g_loss: 12.877744674682617\n",
            "Training epoch 14759/1000000, d_loss: -63.30552291870117,  g_loss: -36.25517654418945\n",
            "Training epoch 14760/1000000, d_loss: -169.07559204101562,  g_loss: -66.96649169921875\n",
            "Training epoch 14761/1000000, d_loss: -484.04254150390625,  g_loss: -155.86764526367188\n",
            "Training epoch 14762/1000000, d_loss: -40.298885345458984,  g_loss: -58.51263427734375\n",
            "Training epoch 14763/1000000, d_loss: -72.99034881591797,  g_loss: -67.30538940429688\n",
            "Training epoch 14764/1000000, d_loss: -211.7812957763672,  g_loss: -84.03192138671875\n",
            "Training epoch 14765/1000000, d_loss: 16.42346954345703,  g_loss: -5.07243013381958\n",
            "Training epoch 14766/1000000, d_loss: -1490.8951416015625,  g_loss: -148.09417724609375\n",
            "Training epoch 14767/1000000, d_loss: -308.77685546875,  g_loss: -89.69862365722656\n",
            "Training epoch 14768/1000000, d_loss: -119.73235321044922,  g_loss: -17.269731521606445\n",
            "Training epoch 14769/1000000, d_loss: -604.7988891601562,  g_loss: -54.810359954833984\n",
            "Training epoch 14770/1000000, d_loss: 103.16044616699219,  g_loss: 7.84921932220459\n",
            "Training epoch 14771/1000000, d_loss: -61.625465393066406,  g_loss: -4.065956115722656\n",
            "Training epoch 14772/1000000, d_loss: 14.100662231445312,  g_loss: -25.98133087158203\n",
            "Training epoch 14773/1000000, d_loss: 52.10475540161133,  g_loss: -22.970306396484375\n",
            "Training epoch 14774/1000000, d_loss: -134.70291137695312,  g_loss: 5.778520584106445\n",
            "Training epoch 14775/1000000, d_loss: -152.17971801757812,  g_loss: -20.029417037963867\n",
            "Training epoch 14776/1000000, d_loss: -105.99958038330078,  g_loss: -54.778968811035156\n",
            "Training epoch 14777/1000000, d_loss: -113.37599182128906,  g_loss: 35.72669219970703\n",
            "Training epoch 14778/1000000, d_loss: -116.41264343261719,  g_loss: -48.05048370361328\n",
            "Training epoch 14779/1000000, d_loss: -154.945068359375,  g_loss: -11.280473709106445\n",
            "Training epoch 14780/1000000, d_loss: -186.67959594726562,  g_loss: -8.260292053222656\n",
            "Training epoch 14781/1000000, d_loss: -280.85986328125,  g_loss: -112.49987030029297\n",
            "Training epoch 14782/1000000, d_loss: -522.26025390625,  g_loss: -74.40576171875\n",
            "Training epoch 14783/1000000, d_loss: -250.19500732421875,  g_loss: -262.3664245605469\n",
            "Training epoch 14784/1000000, d_loss: -87.99942016601562,  g_loss: -25.503541946411133\n",
            "Training epoch 14785/1000000, d_loss: 10.859161376953125,  g_loss: 109.605224609375\n",
            "Training epoch 14786/1000000, d_loss: -140.84539794921875,  g_loss: 14.897144317626953\n",
            "Training epoch 14787/1000000, d_loss: 123.97125244140625,  g_loss: 83.6298828125\n",
            "Training epoch 14788/1000000, d_loss: -78.99413299560547,  g_loss: 105.38644409179688\n",
            "Training epoch 14789/1000000, d_loss: 391.08001708984375,  g_loss: 93.08413696289062\n",
            "Training epoch 14790/1000000, d_loss: -170.71768188476562,  g_loss: -9.297222137451172\n",
            "Training epoch 14791/1000000, d_loss: -175.44488525390625,  g_loss: 38.102943420410156\n",
            "Training epoch 14792/1000000, d_loss: -246.2281951904297,  g_loss: -28.9958438873291\n",
            "Training epoch 14793/1000000, d_loss: -20.56366729736328,  g_loss: 42.22132873535156\n",
            "Training epoch 14794/1000000, d_loss: -49.884613037109375,  g_loss: -6.721445560455322\n",
            "Training epoch 14795/1000000, d_loss: -187.8253173828125,  g_loss: 73.23001861572266\n",
            "Training epoch 14796/1000000, d_loss: -49.12712860107422,  g_loss: -35.47899627685547\n",
            "Training epoch 14797/1000000, d_loss: -120.83758544921875,  g_loss: 27.785919189453125\n",
            "Training epoch 14798/1000000, d_loss: -495.4280700683594,  g_loss: -67.53932189941406\n",
            "Training epoch 14799/1000000, d_loss: -17.15511703491211,  g_loss: 45.549407958984375\n",
            "Training epoch 14800/1000000, d_loss: 46.71946716308594,  g_loss: 57.271522521972656\n",
            "Training epoch 14801/1000000, d_loss: -143.86843872070312,  g_loss: 118.96949768066406\n",
            "Training epoch 14802/1000000, d_loss: -83.9400405883789,  g_loss: -16.98682403564453\n",
            "Training epoch 14803/1000000, d_loss: -86.36526489257812,  g_loss: 16.010168075561523\n",
            "Training epoch 14804/1000000, d_loss: -84.092041015625,  g_loss: 16.252775192260742\n",
            "Training epoch 14805/1000000, d_loss: -178.786865234375,  g_loss: -17.054370880126953\n",
            "Training epoch 14806/1000000, d_loss: -134.4766845703125,  g_loss: -30.46596336364746\n",
            "Training epoch 14807/1000000, d_loss: -614.159912109375,  g_loss: -221.7292938232422\n",
            "Training epoch 14808/1000000, d_loss: 483.11480712890625,  g_loss: -10.362006187438965\n",
            "Training epoch 14809/1000000, d_loss: -374.4537353515625,  g_loss: -37.2746467590332\n",
            "Training epoch 14810/1000000, d_loss: -11.713687896728516,  g_loss: -17.568395614624023\n",
            "Training epoch 14811/1000000, d_loss: -127.03836822509766,  g_loss: -20.457447052001953\n",
            "Training epoch 14812/1000000, d_loss: 49.848663330078125,  g_loss: -2.418382167816162\n",
            "Training epoch 14813/1000000, d_loss: -540.0487670898438,  g_loss: 5.733560562133789\n",
            "Training epoch 14814/1000000, d_loss: -162.5409393310547,  g_loss: -2.081265449523926\n",
            "Training epoch 14815/1000000, d_loss: -79.3448257446289,  g_loss: 4.253467559814453\n",
            "Training epoch 14816/1000000, d_loss: -75.04823303222656,  g_loss: 12.629738807678223\n",
            "Training epoch 14817/1000000, d_loss: -182.08636474609375,  g_loss: 63.862430572509766\n",
            "Training epoch 14818/1000000, d_loss: -128.1918487548828,  g_loss: -74.29869842529297\n",
            "Training epoch 14819/1000000, d_loss: -83.79766845703125,  g_loss: 102.66197204589844\n",
            "Training epoch 14820/1000000, d_loss: -117.19599151611328,  g_loss: 299.40570068359375\n",
            "Training epoch 14821/1000000, d_loss: -90.88729858398438,  g_loss: -55.62739944458008\n",
            "Training epoch 14822/1000000, d_loss: -58.67863845825195,  g_loss: -0.49568653106689453\n",
            "Training epoch 14823/1000000, d_loss: -95.89572143554688,  g_loss: -15.801806449890137\n",
            "Training epoch 14824/1000000, d_loss: -181.47129821777344,  g_loss: -88.16590118408203\n",
            "Training epoch 14825/1000000, d_loss: -157.8905487060547,  g_loss: 9.654346466064453\n",
            "Training epoch 14826/1000000, d_loss: -194.02313232421875,  g_loss: -18.461021423339844\n",
            "Training epoch 14827/1000000, d_loss: -354.951416015625,  g_loss: 38.392364501953125\n",
            "Training epoch 14828/1000000, d_loss: -73.01174926757812,  g_loss: -20.691919326782227\n",
            "Training epoch 14829/1000000, d_loss: -64.53271484375,  g_loss: -2.4091687202453613\n",
            "Training epoch 14830/1000000, d_loss: 4.278221130371094,  g_loss: 6.203724384307861\n",
            "Training epoch 14831/1000000, d_loss: 25.80739974975586,  g_loss: 31.465316772460938\n",
            "Training epoch 14832/1000000, d_loss: -58.01447296142578,  g_loss: -10.803638458251953\n",
            "Training epoch 14833/1000000, d_loss: -96.80857849121094,  g_loss: -13.79848861694336\n",
            "Training epoch 14834/1000000, d_loss: -223.31472778320312,  g_loss: -36.41389465332031\n",
            "Training epoch 14835/1000000, d_loss: -129.55723571777344,  g_loss: -44.389434814453125\n",
            "Training epoch 14836/1000000, d_loss: -102.59159088134766,  g_loss: 80.43944549560547\n",
            "Training epoch 14837/1000000, d_loss: -410.301025390625,  g_loss: -145.18008422851562\n",
            "Training epoch 14838/1000000, d_loss: 87.58491516113281,  g_loss: -370.3345031738281\n",
            "Training epoch 14839/1000000, d_loss: -16.071121215820312,  g_loss: -40.86264419555664\n",
            "Training epoch 14840/1000000, d_loss: -130.32278442382812,  g_loss: 8.67501163482666\n",
            "Training epoch 14841/1000000, d_loss: -86.08268737792969,  g_loss: 51.68526840209961\n",
            "Training epoch 14842/1000000, d_loss: -282.36474609375,  g_loss: -25.92247772216797\n",
            "Training epoch 14843/1000000, d_loss: -261.2825927734375,  g_loss: -109.20758056640625\n",
            "Training epoch 14844/1000000, d_loss: -224.34674072265625,  g_loss: 280.8561706542969\n",
            "Training epoch 14845/1000000, d_loss: -294.5079650878906,  g_loss: 560.9819946289062\n",
            "Training epoch 14846/1000000, d_loss: -116.94148254394531,  g_loss: 142.2355194091797\n",
            "Training epoch 14847/1000000, d_loss: -88.1235580444336,  g_loss: 163.9686737060547\n",
            "Training epoch 14848/1000000, d_loss: -61.870086669921875,  g_loss: 137.91265869140625\n",
            "Training epoch 14849/1000000, d_loss: -128.40155029296875,  g_loss: 132.78173828125\n",
            "Training epoch 14850/1000000, d_loss: 105.01602172851562,  g_loss: 36.25563049316406\n",
            "Training epoch 14851/1000000, d_loss: -40.89997100830078,  g_loss: 46.80828857421875\n",
            "Training epoch 14852/1000000, d_loss: -403.1463928222656,  g_loss: -100.1095199584961\n",
            "Training epoch 14853/1000000, d_loss: -237.7802734375,  g_loss: -32.170780181884766\n",
            "Training epoch 14854/1000000, d_loss: -322.8641052246094,  g_loss: -94.06694030761719\n",
            "Training epoch 14855/1000000, d_loss: -109.06245422363281,  g_loss: -50.072044372558594\n",
            "Training epoch 14856/1000000, d_loss: -30.085792541503906,  g_loss: 6.822268486022949\n",
            "Training epoch 14857/1000000, d_loss: -120.13600158691406,  g_loss: 36.701202392578125\n",
            "Training epoch 14858/1000000, d_loss: -49.79143524169922,  g_loss: 47.45362854003906\n",
            "Training epoch 14859/1000000, d_loss: -128.83584594726562,  g_loss: 42.07339859008789\n",
            "Training epoch 14860/1000000, d_loss: -7.36944580078125,  g_loss: 64.97451782226562\n",
            "Training epoch 14861/1000000, d_loss: -176.05601501464844,  g_loss: 38.356719970703125\n",
            "Training epoch 14862/1000000, d_loss: -193.16419982910156,  g_loss: -8.192514419555664\n",
            "Training epoch 14863/1000000, d_loss: -166.1320343017578,  g_loss: 69.55386352539062\n",
            "Training epoch 14864/1000000, d_loss: -125.59491729736328,  g_loss: 136.661376953125\n",
            "Training epoch 14865/1000000, d_loss: -68.14250183105469,  g_loss: 40.25834655761719\n",
            "Training epoch 14866/1000000, d_loss: -86.79811096191406,  g_loss: 54.119956970214844\n",
            "Training epoch 14867/1000000, d_loss: -230.6143798828125,  g_loss: 38.281986236572266\n",
            "Training epoch 14868/1000000, d_loss: -226.92086791992188,  g_loss: -59.93706512451172\n",
            "Training epoch 14869/1000000, d_loss: -405.1275634765625,  g_loss: -58.57904052734375\n",
            "Training epoch 14870/1000000, d_loss: -98.0341567993164,  g_loss: 172.93826293945312\n",
            "Training epoch 14871/1000000, d_loss: -121.18064880371094,  g_loss: 115.8857421875\n",
            "Training epoch 14872/1000000, d_loss: 35.40899658203125,  g_loss: -20.56805992126465\n",
            "Training epoch 14873/1000000, d_loss: -113.01312255859375,  g_loss: -42.62274169921875\n",
            "Training epoch 14874/1000000, d_loss: -73.3599853515625,  g_loss: 41.40653610229492\n",
            "Training epoch 14875/1000000, d_loss: -50.49932098388672,  g_loss: 57.93295669555664\n",
            "Training epoch 14876/1000000, d_loss: -5.455081939697266,  g_loss: 75.0687484741211\n",
            "Training epoch 14877/1000000, d_loss: -121.51873779296875,  g_loss: 31.485891342163086\n",
            "Training epoch 14878/1000000, d_loss: -611.9319458007812,  g_loss: -87.99659729003906\n",
            "Training epoch 14879/1000000, d_loss: 239.39666748046875,  g_loss: 1.8798346519470215\n",
            "Training epoch 14880/1000000, d_loss: -208.83494567871094,  g_loss: 32.37765884399414\n",
            "Training epoch 14881/1000000, d_loss: -50.79848098754883,  g_loss: -11.864594459533691\n",
            "Training epoch 14882/1000000, d_loss: -64.10742950439453,  g_loss: 37.5931396484375\n",
            "Training epoch 14883/1000000, d_loss: -87.83311462402344,  g_loss: 47.195655822753906\n",
            "Training epoch 14884/1000000, d_loss: -73.61807250976562,  g_loss: 43.64624786376953\n",
            "Training epoch 14885/1000000, d_loss: -68.19134521484375,  g_loss: 8.116442680358887\n",
            "Training epoch 14886/1000000, d_loss: -255.2234344482422,  g_loss: -16.04391098022461\n",
            "Training epoch 14887/1000000, d_loss: -132.7421112060547,  g_loss: 27.123140335083008\n",
            "Training epoch 14888/1000000, d_loss: -154.48912048339844,  g_loss: 35.815311431884766\n",
            "Training epoch 14889/1000000, d_loss: -24.780075073242188,  g_loss: -31.220136642456055\n",
            "Training epoch 14890/1000000, d_loss: -146.92074584960938,  g_loss: -10.325180053710938\n",
            "Training epoch 14891/1000000, d_loss: -93.73494720458984,  g_loss: -23.55824851989746\n",
            "Training epoch 14892/1000000, d_loss: -55.53852081298828,  g_loss: 32.51835632324219\n",
            "Training epoch 14893/1000000, d_loss: -386.0274658203125,  g_loss: -30.921627044677734\n",
            "Training epoch 14894/1000000, d_loss: -915.20654296875,  g_loss: -63.18696975708008\n",
            "Training epoch 14895/1000000, d_loss: 103.3747787475586,  g_loss: -96.19064331054688\n",
            "Training epoch 14896/1000000, d_loss: -53.838314056396484,  g_loss: -16.525466918945312\n",
            "Training epoch 14897/1000000, d_loss: -451.2549743652344,  g_loss: -455.04193115234375\n",
            "Training epoch 14898/1000000, d_loss: -28.640270233154297,  g_loss: 19.77875518798828\n",
            "Training epoch 14899/1000000, d_loss: -76.637451171875,  g_loss: 137.19229125976562\n",
            "Training epoch 14900/1000000, d_loss: -203.38394165039062,  g_loss: 430.0682373046875\n",
            "Training epoch 14901/1000000, d_loss: -69.18292236328125,  g_loss: 64.83785247802734\n",
            "Training epoch 14902/1000000, d_loss: -77.36712646484375,  g_loss: -16.002477645874023\n",
            "Training epoch 14903/1000000, d_loss: -265.61968994140625,  g_loss: -154.7144317626953\n",
            "Training epoch 14904/1000000, d_loss: -95.6051254272461,  g_loss: -42.788108825683594\n",
            "Training epoch 14905/1000000, d_loss: -120.74119567871094,  g_loss: 3.6963233947753906\n",
            "Training epoch 14906/1000000, d_loss: -33.332305908203125,  g_loss: 39.266075134277344\n",
            "Training epoch 14907/1000000, d_loss: -199.57696533203125,  g_loss: 263.49188232421875\n",
            "Training epoch 14908/1000000, d_loss: -150.73330688476562,  g_loss: 72.48242950439453\n",
            "Training epoch 14909/1000000, d_loss: -159.32904052734375,  g_loss: -12.60781192779541\n",
            "Training epoch 14910/1000000, d_loss: -44.916847229003906,  g_loss: -5.403650760650635\n",
            "Training epoch 14911/1000000, d_loss: -190.49871826171875,  g_loss: -31.922138214111328\n",
            "Training epoch 14912/1000000, d_loss: -51.90700912475586,  g_loss: -41.15946960449219\n",
            "Training epoch 14913/1000000, d_loss: -71.68467712402344,  g_loss: 21.935749053955078\n",
            "Training epoch 14914/1000000, d_loss: -96.36579895019531,  g_loss: 103.48408508300781\n",
            "Training epoch 14915/1000000, d_loss: -133.081787109375,  g_loss: 15.38215446472168\n",
            "Training epoch 14916/1000000, d_loss: -86.50628662109375,  g_loss: 46.564144134521484\n",
            "Training epoch 14917/1000000, d_loss: 44.32965087890625,  g_loss: 18.173683166503906\n",
            "Training epoch 14918/1000000, d_loss: -152.07107543945312,  g_loss: -17.348209381103516\n",
            "Training epoch 14919/1000000, d_loss: -160.05442810058594,  g_loss: 22.649682998657227\n",
            "Training epoch 14920/1000000, d_loss: 805.1505126953125,  g_loss: -42.28464889526367\n",
            "Training epoch 14921/1000000, d_loss: -290.11834716796875,  g_loss: -24.885744094848633\n",
            "Training epoch 14922/1000000, d_loss: 688.06494140625,  g_loss: 27.34143829345703\n",
            "Training epoch 14923/1000000, d_loss: -36.26466751098633,  g_loss: 6.5063629150390625\n",
            "Training epoch 14924/1000000, d_loss: -21.672317504882812,  g_loss: 13.478360176086426\n",
            "Training epoch 14925/1000000, d_loss: -19.045263290405273,  g_loss: 46.69132995605469\n",
            "Training epoch 14926/1000000, d_loss: -161.26226806640625,  g_loss: 25.460054397583008\n",
            "Training epoch 14927/1000000, d_loss: -303.7021484375,  g_loss: -18.624462127685547\n",
            "Training epoch 14928/1000000, d_loss: -28.29822540283203,  g_loss: -7.170339584350586\n",
            "Training epoch 14929/1000000, d_loss: -142.5522918701172,  g_loss: 4.664133071899414\n",
            "Training epoch 14930/1000000, d_loss: -110.75895690917969,  g_loss: 110.40283203125\n",
            "Training epoch 14931/1000000, d_loss: -59.95889663696289,  g_loss: 43.196109771728516\n",
            "Training epoch 14932/1000000, d_loss: -108.63917541503906,  g_loss: 49.775691986083984\n",
            "Training epoch 14933/1000000, d_loss: -265.2749328613281,  g_loss: 356.9068298339844\n",
            "Training epoch 14934/1000000, d_loss: -87.6570816040039,  g_loss: -38.145179748535156\n",
            "Training epoch 14935/1000000, d_loss: -139.0498809814453,  g_loss: -122.48436737060547\n",
            "Training epoch 14936/1000000, d_loss: -307.8829345703125,  g_loss: -112.66313934326172\n",
            "Training epoch 14937/1000000, d_loss: -825.1566162109375,  g_loss: -254.73301696777344\n",
            "Training epoch 14938/1000000, d_loss: -85.82682800292969,  g_loss: -222.06741333007812\n",
            "Training epoch 14939/1000000, d_loss: 135.98394775390625,  g_loss: 30.125411987304688\n",
            "Training epoch 14940/1000000, d_loss: -212.36395263671875,  g_loss: -58.711997985839844\n",
            "Training epoch 14941/1000000, d_loss: 67.16189575195312,  g_loss: -26.4575138092041\n",
            "Training epoch 14942/1000000, d_loss: -87.3758316040039,  g_loss: 278.80987548828125\n",
            "Training epoch 14943/1000000, d_loss: -446.8565368652344,  g_loss: -58.684288024902344\n",
            "Training epoch 14944/1000000, d_loss: 13.101644515991211,  g_loss: 6.273347854614258\n",
            "Training epoch 14945/1000000, d_loss: -145.3719940185547,  g_loss: 12.65028190612793\n",
            "Training epoch 14946/1000000, d_loss: -321.38031005859375,  g_loss: 403.3815002441406\n",
            "Training epoch 14947/1000000, d_loss: -156.42811584472656,  g_loss: 218.02125549316406\n",
            "Training epoch 14948/1000000, d_loss: -654.4609375,  g_loss: 738.8184204101562\n",
            "Training epoch 14949/1000000, d_loss: -108.39036560058594,  g_loss: 102.3536605834961\n",
            "Training epoch 14950/1000000, d_loss: -442.8321228027344,  g_loss: 303.2801818847656\n",
            "Training epoch 14951/1000000, d_loss: -60.822509765625,  g_loss: 48.82267761230469\n",
            "Training epoch 14952/1000000, d_loss: -45.41218948364258,  g_loss: -6.368396759033203\n",
            "Training epoch 14953/1000000, d_loss: -83.72196960449219,  g_loss: 28.277620315551758\n",
            "Training epoch 14954/1000000, d_loss: -16.242599487304688,  g_loss: -51.22206115722656\n",
            "Training epoch 14955/1000000, d_loss: -104.27159881591797,  g_loss: -52.32390594482422\n",
            "Training epoch 14956/1000000, d_loss: -161.10302734375,  g_loss: 44.958740234375\n",
            "Training epoch 14957/1000000, d_loss: -104.47254943847656,  g_loss: -39.457611083984375\n",
            "Training epoch 14958/1000000, d_loss: -62.525062561035156,  g_loss: -87.849853515625\n",
            "Training epoch 14959/1000000, d_loss: 278.10235595703125,  g_loss: -102.98263549804688\n",
            "Training epoch 14960/1000000, d_loss: -360.29217529296875,  g_loss: -287.5048828125\n",
            "Training epoch 14961/1000000, d_loss: 10.781574249267578,  g_loss: 9.14639663696289\n",
            "Training epoch 14962/1000000, d_loss: -128.92149353027344,  g_loss: 22.503498077392578\n",
            "Training epoch 14963/1000000, d_loss: -88.39848327636719,  g_loss: 58.874271392822266\n",
            "Training epoch 14964/1000000, d_loss: -101.80561828613281,  g_loss: 116.12471008300781\n",
            "Training epoch 14965/1000000, d_loss: -229.06997680664062,  g_loss: 60.05926513671875\n",
            "Training epoch 14966/1000000, d_loss: -102.04595947265625,  g_loss: 45.243141174316406\n",
            "Training epoch 14967/1000000, d_loss: -563.4695434570312,  g_loss: -165.74603271484375\n",
            "Training epoch 14968/1000000, d_loss: 128.42117309570312,  g_loss: -0.9723100662231445\n",
            "Training epoch 14969/1000000, d_loss: -128.3656005859375,  g_loss: 14.802711486816406\n",
            "Training epoch 14970/1000000, d_loss: -253.95294189453125,  g_loss: -33.36219787597656\n",
            "Training epoch 14971/1000000, d_loss: -86.48500061035156,  g_loss: 52.361228942871094\n",
            "Training epoch 14972/1000000, d_loss: -152.42506408691406,  g_loss: 77.0425033569336\n",
            "Training epoch 14973/1000000, d_loss: -24.455734252929688,  g_loss: 90.99854278564453\n",
            "Training epoch 14974/1000000, d_loss: -71.00579833984375,  g_loss: 121.16437530517578\n",
            "Training epoch 14975/1000000, d_loss: -145.9057159423828,  g_loss: 49.36834716796875\n",
            "Training epoch 14976/1000000, d_loss: -77.3896255493164,  g_loss: 17.113536834716797\n",
            "Training epoch 14977/1000000, d_loss: -96.32954406738281,  g_loss: 34.067222595214844\n",
            "Training epoch 14978/1000000, d_loss: -49.08658218383789,  g_loss: 52.464088439941406\n",
            "Training epoch 14979/1000000, d_loss: -93.83647155761719,  g_loss: -4.751519203186035\n",
            "Training epoch 14980/1000000, d_loss: -625.4679565429688,  g_loss: -290.20672607421875\n",
            "Training epoch 14981/1000000, d_loss: 29.50720977783203,  g_loss: -32.242919921875\n",
            "Training epoch 14982/1000000, d_loss: -75.63888549804688,  g_loss: -16.79345703125\n",
            "Training epoch 14983/1000000, d_loss: -78.55078125,  g_loss: 5.466933250427246\n",
            "Training epoch 14984/1000000, d_loss: -109.4549331665039,  g_loss: 200.9566650390625\n",
            "Training epoch 14985/1000000, d_loss: -49.86395263671875,  g_loss: 19.128244400024414\n",
            "Training epoch 14986/1000000, d_loss: -32.3001594543457,  g_loss: 0.9010376930236816\n",
            "Training epoch 14987/1000000, d_loss: -1405.7724609375,  g_loss: -101.09351348876953\n",
            "Training epoch 14988/1000000, d_loss: -80.92799377441406,  g_loss: -44.605262756347656\n",
            "Training epoch 14989/1000000, d_loss: -49.35527420043945,  g_loss: -31.085453033447266\n",
            "Training epoch 14990/1000000, d_loss: -70.80152893066406,  g_loss: -8.786943435668945\n",
            "Training epoch 14991/1000000, d_loss: -138.26426696777344,  g_loss: -4.380457878112793\n",
            "Training epoch 14992/1000000, d_loss: -230.4352264404297,  g_loss: -116.35253143310547\n",
            "Training epoch 14993/1000000, d_loss: -85.63864135742188,  g_loss: -52.885894775390625\n",
            "Training epoch 14994/1000000, d_loss: 40.08203125,  g_loss: 4.139516830444336\n",
            "Training epoch 14995/1000000, d_loss: -159.5960235595703,  g_loss: 2.46274733543396\n",
            "Training epoch 14996/1000000, d_loss: -145.34739685058594,  g_loss: 64.1712646484375\n",
            "Training epoch 14997/1000000, d_loss: -115.20087432861328,  g_loss: 59.31757354736328\n",
            "Training epoch 14998/1000000, d_loss: -268.6968994140625,  g_loss: 5.271990776062012\n",
            "Training epoch 14999/1000000, d_loss: -46.33837890625,  g_loss: -20.91492462158203\n",
            "Training epoch 15000/1000000, d_loss: -13.533157348632812,  g_loss: -2.0879268646240234\n",
            "Training epoch 15001/1000000, d_loss: -41.350830078125,  g_loss: 50.48666763305664\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 62/62 [00:00<00:00, 238.25it/s]\n",
            "Meshing: 100%|██████████| 831/831 [00:00<00:00, 1455.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_15001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_15001/assets\n",
            "Training epoch 15002/1000000, d_loss: -2023.4454345703125,  g_loss: -644.3487548828125\n",
            "Training epoch 15003/1000000, d_loss: 84.99811553955078,  g_loss: -38.72496795654297\n",
            "Training epoch 15004/1000000, d_loss: 24.000028610229492,  g_loss: -174.74356079101562\n",
            "Training epoch 15005/1000000, d_loss: 243.61044311523438,  g_loss: -178.62391662597656\n",
            "Training epoch 15006/1000000, d_loss: -164.54443359375,  g_loss: -29.436840057373047\n",
            "Training epoch 15007/1000000, d_loss: -473.4492492675781,  g_loss: -63.45062255859375\n",
            "Training epoch 15008/1000000, d_loss: 282.6832275390625,  g_loss: -209.2794189453125\n",
            "Training epoch 15009/1000000, d_loss: 191.1538543701172,  g_loss: -349.5321044921875\n",
            "Training epoch 15010/1000000, d_loss: 45.53947448730469,  g_loss: -65.51773834228516\n",
            "Training epoch 15011/1000000, d_loss: -83.30526733398438,  g_loss: 69.569580078125\n",
            "Training epoch 15012/1000000, d_loss: -88.70294189453125,  g_loss: 64.24388885498047\n",
            "Training epoch 15013/1000000, d_loss: -307.34429931640625,  g_loss: -150.6654510498047\n",
            "Training epoch 15014/1000000, d_loss: 70.20923614501953,  g_loss: 80.01011657714844\n",
            "Training epoch 15015/1000000, d_loss: -310.2694396972656,  g_loss: 412.43115234375\n",
            "Training epoch 15016/1000000, d_loss: -110.83799743652344,  g_loss: 178.41802978515625\n",
            "Training epoch 15017/1000000, d_loss: -165.03555297851562,  g_loss: 59.141326904296875\n",
            "Training epoch 15018/1000000, d_loss: -191.50140380859375,  g_loss: 241.40789794921875\n",
            "Training epoch 15019/1000000, d_loss: -88.81121826171875,  g_loss: 92.06988525390625\n",
            "Training epoch 15020/1000000, d_loss: -89.01963806152344,  g_loss: 42.106929779052734\n",
            "Training epoch 15021/1000000, d_loss: -138.00030517578125,  g_loss: 0.26939868927001953\n",
            "Training epoch 15022/1000000, d_loss: -133.4933319091797,  g_loss: 28.9061222076416\n",
            "Training epoch 15023/1000000, d_loss: -119.65834045410156,  g_loss: -1.4453692436218262\n",
            "Training epoch 15024/1000000, d_loss: -231.09906005859375,  g_loss: 23.3306884765625\n",
            "Training epoch 15025/1000000, d_loss: -81.06214904785156,  g_loss: 137.89205932617188\n",
            "Training epoch 15026/1000000, d_loss: -54.86051940917969,  g_loss: 17.60883140563965\n",
            "Training epoch 15027/1000000, d_loss: -90.32356262207031,  g_loss: 8.205677032470703\n",
            "Training epoch 15028/1000000, d_loss: -30.98155403137207,  g_loss: 23.987667083740234\n",
            "Training epoch 15029/1000000, d_loss: -420.11700439453125,  g_loss: -34.164207458496094\n",
            "Training epoch 15030/1000000, d_loss: -176.2056121826172,  g_loss: -58.905181884765625\n",
            "Training epoch 15031/1000000, d_loss: -115.3826904296875,  g_loss: -57.99517822265625\n",
            "Training epoch 15032/1000000, d_loss: -58.91260528564453,  g_loss: -53.305599212646484\n",
            "Training epoch 15033/1000000, d_loss: -120.07308959960938,  g_loss: 19.731725692749023\n",
            "Training epoch 15034/1000000, d_loss: -62.525489807128906,  g_loss: 12.368696212768555\n",
            "Training epoch 15035/1000000, d_loss: -161.594970703125,  g_loss: 6.7778215408325195\n",
            "Training epoch 15036/1000000, d_loss: -119.83171844482422,  g_loss: 33.43190002441406\n",
            "Training epoch 15037/1000000, d_loss: 20.4107666015625,  g_loss: 115.44664001464844\n",
            "Training epoch 15038/1000000, d_loss: -133.25308227539062,  g_loss: 151.267333984375\n",
            "Training epoch 15039/1000000, d_loss: -81.71267700195312,  g_loss: 12.269004821777344\n",
            "Training epoch 15040/1000000, d_loss: -114.50323486328125,  g_loss: 5.213163375854492\n",
            "Training epoch 15041/1000000, d_loss: -148.8168487548828,  g_loss: -2.8395347595214844\n",
            "Training epoch 15042/1000000, d_loss: -12.107025146484375,  g_loss: 31.447378158569336\n",
            "Training epoch 15043/1000000, d_loss: -95.97111511230469,  g_loss: 27.681291580200195\n",
            "Training epoch 15044/1000000, d_loss: -103.1126708984375,  g_loss: 17.542259216308594\n",
            "Training epoch 15045/1000000, d_loss: -118.13043212890625,  g_loss: 0.9753918647766113\n",
            "Training epoch 15046/1000000, d_loss: -68.92909240722656,  g_loss: 51.91050720214844\n",
            "Training epoch 15047/1000000, d_loss: -115.30195617675781,  g_loss: 79.23289489746094\n",
            "Training epoch 15048/1000000, d_loss: -45.7797737121582,  g_loss: 64.90431213378906\n",
            "Training epoch 15049/1000000, d_loss: -63.05377197265625,  g_loss: 12.89712905883789\n",
            "Training epoch 15050/1000000, d_loss: -29.514570236206055,  g_loss: 64.18936920166016\n",
            "Training epoch 15051/1000000, d_loss: -250.7153778076172,  g_loss: 10.325145721435547\n",
            "Training epoch 15052/1000000, d_loss: -189.56088256835938,  g_loss: -19.762540817260742\n",
            "Training epoch 15053/1000000, d_loss: -34.787410736083984,  g_loss: 12.830716133117676\n",
            "Training epoch 15054/1000000, d_loss: -110.77484893798828,  g_loss: -1.4426326751708984\n",
            "Training epoch 15055/1000000, d_loss: -218.13182067871094,  g_loss: -112.0985336303711\n",
            "Training epoch 15056/1000000, d_loss: -81.04056549072266,  g_loss: 138.02801513671875\n",
            "Training epoch 15057/1000000, d_loss: -308.9471435546875,  g_loss: -38.75236511230469\n",
            "Training epoch 15058/1000000, d_loss: -41.64088439941406,  g_loss: 86.04671478271484\n",
            "Training epoch 15059/1000000, d_loss: -168.7247314453125,  g_loss: -36.41877365112305\n",
            "Training epoch 15060/1000000, d_loss: -115.06289672851562,  g_loss: 18.23127555847168\n",
            "Training epoch 15061/1000000, d_loss: -111.72759246826172,  g_loss: 78.22335815429688\n",
            "Training epoch 15062/1000000, d_loss: -828.8187866210938,  g_loss: -175.7726593017578\n",
            "Training epoch 15063/1000000, d_loss: -37.00323486328125,  g_loss: -135.58763122558594\n",
            "Training epoch 15064/1000000, d_loss: -65.06185150146484,  g_loss: 34.043853759765625\n",
            "Training epoch 15065/1000000, d_loss: -67.34294128417969,  g_loss: 8.960021018981934\n",
            "Training epoch 15066/1000000, d_loss: -105.06083679199219,  g_loss: 83.60852813720703\n",
            "Training epoch 15067/1000000, d_loss: 482.73089599609375,  g_loss: 22.026512145996094\n",
            "Training epoch 15068/1000000, d_loss: -248.95880126953125,  g_loss: 13.986665725708008\n",
            "Training epoch 15069/1000000, d_loss: -205.99891662597656,  g_loss: 15.271345138549805\n",
            "Training epoch 15070/1000000, d_loss: -98.55718994140625,  g_loss: 11.030261039733887\n",
            "Training epoch 15071/1000000, d_loss: -179.40089416503906,  g_loss: 150.39878845214844\n",
            "Training epoch 15072/1000000, d_loss: -434.01165771484375,  g_loss: 853.9511108398438\n",
            "Training epoch 15073/1000000, d_loss: -69.63631439208984,  g_loss: 66.81450653076172\n",
            "Training epoch 15074/1000000, d_loss: -66.64788818359375,  g_loss: 54.789756774902344\n",
            "Training epoch 15075/1000000, d_loss: -112.40965270996094,  g_loss: 128.4496307373047\n",
            "Training epoch 15076/1000000, d_loss: -179.59298706054688,  g_loss: 103.83673095703125\n",
            "Training epoch 15077/1000000, d_loss: -120.60287475585938,  g_loss: 60.39453887939453\n",
            "Training epoch 15078/1000000, d_loss: -63.098995208740234,  g_loss: 78.96449279785156\n",
            "Training epoch 15079/1000000, d_loss: -91.97994232177734,  g_loss: 68.22968292236328\n",
            "Training epoch 15080/1000000, d_loss: -309.14971923828125,  g_loss: 45.62432861328125\n",
            "Training epoch 15081/1000000, d_loss: -36.2535400390625,  g_loss: 9.235668182373047\n",
            "Training epoch 15082/1000000, d_loss: -75.27662658691406,  g_loss: 3.15116024017334\n",
            "Training epoch 15083/1000000, d_loss: -239.15921020507812,  g_loss: -27.368228912353516\n",
            "Training epoch 15084/1000000, d_loss: -11.581466674804688,  g_loss: 27.377180099487305\n",
            "Training epoch 15085/1000000, d_loss: -202.03005981445312,  g_loss: -14.623308181762695\n",
            "Training epoch 15086/1000000, d_loss: -115.92469787597656,  g_loss: -9.721284866333008\n",
            "Training epoch 15087/1000000, d_loss: -66.29661560058594,  g_loss: 36.935516357421875\n",
            "Training epoch 15088/1000000, d_loss: -596.64013671875,  g_loss: -31.141036987304688\n",
            "Training epoch 15089/1000000, d_loss: -385.9890441894531,  g_loss: -52.37078857421875\n",
            "Training epoch 15090/1000000, d_loss: 384.7626647949219,  g_loss: 80.92498779296875\n",
            "Training epoch 15091/1000000, d_loss: -8.484825134277344,  g_loss: 37.448570251464844\n",
            "Training epoch 15092/1000000, d_loss: -109.14364624023438,  g_loss: 98.28218841552734\n",
            "Training epoch 15093/1000000, d_loss: -150.0869140625,  g_loss: 163.39373779296875\n",
            "Training epoch 15094/1000000, d_loss: -30.704387664794922,  g_loss: -25.038076400756836\n",
            "Training epoch 15095/1000000, d_loss: -122.76560974121094,  g_loss: 8.680453300476074\n",
            "Training epoch 15096/1000000, d_loss: -67.81969451904297,  g_loss: 4.084075927734375\n",
            "Training epoch 15097/1000000, d_loss: -496.0932922363281,  g_loss: -68.94830322265625\n",
            "Training epoch 15098/1000000, d_loss: 9.622261047363281,  g_loss: -10.893789291381836\n",
            "Training epoch 15099/1000000, d_loss: -212.57534790039062,  g_loss: -32.405189514160156\n",
            "Training epoch 15100/1000000, d_loss: -87.04946899414062,  g_loss: 62.203163146972656\n",
            "Training epoch 15101/1000000, d_loss: -110.38442993164062,  g_loss: 22.92676544189453\n",
            "Training epoch 15102/1000000, d_loss: -162.5338592529297,  g_loss: 48.411407470703125\n",
            "Training epoch 15103/1000000, d_loss: -148.3853759765625,  g_loss: -43.84828567504883\n",
            "Training epoch 15104/1000000, d_loss: -256.8614501953125,  g_loss: 200.5862274169922\n",
            "Training epoch 15105/1000000, d_loss: -444.0404357910156,  g_loss: -99.96060180664062\n",
            "Training epoch 15106/1000000, d_loss: -757.23583984375,  g_loss: -242.28720092773438\n",
            "Training epoch 15107/1000000, d_loss: -252.89044189453125,  g_loss: -21.54620361328125\n",
            "Training epoch 15108/1000000, d_loss: -73.58114624023438,  g_loss: -1.5854125022888184\n",
            "Training epoch 15109/1000000, d_loss: -48.39935302734375,  g_loss: -55.948238372802734\n",
            "Training epoch 15110/1000000, d_loss: -40.716365814208984,  g_loss: -66.8341064453125\n",
            "Training epoch 15111/1000000, d_loss: 2.009100914001465,  g_loss: -82.64451599121094\n",
            "Training epoch 15112/1000000, d_loss: -33.00225067138672,  g_loss: 34.01243591308594\n",
            "Training epoch 15113/1000000, d_loss: -75.90611267089844,  g_loss: 16.868602752685547\n",
            "Training epoch 15114/1000000, d_loss: -404.19964599609375,  g_loss: -34.54554748535156\n",
            "Training epoch 15115/1000000, d_loss: -34.31025695800781,  g_loss: -15.323705673217773\n",
            "Training epoch 15116/1000000, d_loss: -102.89606475830078,  g_loss: 23.983604431152344\n",
            "Training epoch 15117/1000000, d_loss: -213.20608520507812,  g_loss: -42.84607696533203\n",
            "Training epoch 15118/1000000, d_loss: -350.3485412597656,  g_loss: -62.3188362121582\n",
            "Training epoch 15119/1000000, d_loss: -399.96160888671875,  g_loss: -139.07315063476562\n",
            "Training epoch 15120/1000000, d_loss: -463.04132080078125,  g_loss: -170.80783081054688\n",
            "Training epoch 15121/1000000, d_loss: -20.73543930053711,  g_loss: 8.30306625366211\n",
            "Training epoch 15122/1000000, d_loss: -72.98092651367188,  g_loss: 27.993576049804688\n",
            "Training epoch 15123/1000000, d_loss: -183.78317260742188,  g_loss: 49.54132843017578\n",
            "Training epoch 15124/1000000, d_loss: -53.05047607421875,  g_loss: 125.82122802734375\n",
            "Training epoch 15125/1000000, d_loss: -75.68494415283203,  g_loss: 52.93019104003906\n",
            "Training epoch 15126/1000000, d_loss: -1106.43701171875,  g_loss: -210.27108764648438\n",
            "Training epoch 15127/1000000, d_loss: 49.051177978515625,  g_loss: -65.723876953125\n",
            "Training epoch 15128/1000000, d_loss: -220.7003936767578,  g_loss: -57.488311767578125\n",
            "Training epoch 15129/1000000, d_loss: -320.6647033691406,  g_loss: -36.377960205078125\n",
            "Training epoch 15130/1000000, d_loss: -188.35157775878906,  g_loss: 335.0101318359375\n",
            "Training epoch 15131/1000000, d_loss: 120.41876220703125,  g_loss: 213.9520263671875\n",
            "Training epoch 15132/1000000, d_loss: -63.38266372680664,  g_loss: 114.17213439941406\n",
            "Training epoch 15133/1000000, d_loss: -98.1622543334961,  g_loss: 33.11903762817383\n",
            "Training epoch 15134/1000000, d_loss: -42.709617614746094,  g_loss: -23.960664749145508\n",
            "Training epoch 15135/1000000, d_loss: -93.01053619384766,  g_loss: 1.0458641052246094\n",
            "Training epoch 15136/1000000, d_loss: 29.487472534179688,  g_loss: 61.214881896972656\n",
            "Training epoch 15137/1000000, d_loss: -146.56219482421875,  g_loss: 41.200599670410156\n",
            "Training epoch 15138/1000000, d_loss: -214.25070190429688,  g_loss: -1.2781314849853516\n",
            "Training epoch 15139/1000000, d_loss: -14.791679382324219,  g_loss: 46.87677764892578\n",
            "Training epoch 15140/1000000, d_loss: -76.48461151123047,  g_loss: 164.68553161621094\n",
            "Training epoch 15141/1000000, d_loss: -85.34536743164062,  g_loss: 49.98712158203125\n",
            "Training epoch 15142/1000000, d_loss: -169.91856384277344,  g_loss: 33.401214599609375\n",
            "Training epoch 15143/1000000, d_loss: -493.7817077636719,  g_loss: -85.79237365722656\n",
            "Training epoch 15144/1000000, d_loss: -838.0836181640625,  g_loss: -247.19703674316406\n",
            "Training epoch 15145/1000000, d_loss: -39.551025390625,  g_loss: -60.23573303222656\n",
            "Training epoch 15146/1000000, d_loss: -5.0686821937561035,  g_loss: -73.92839050292969\n",
            "Training epoch 15147/1000000, d_loss: -123.07159423828125,  g_loss: 14.768173217773438\n",
            "Training epoch 15148/1000000, d_loss: 60.64249038696289,  g_loss: -12.217612266540527\n",
            "Training epoch 15149/1000000, d_loss: -159.4525909423828,  g_loss: -27.306652069091797\n",
            "Training epoch 15150/1000000, d_loss: -100.93800354003906,  g_loss: -23.039386749267578\n",
            "Training epoch 15151/1000000, d_loss: -99.72148895263672,  g_loss: 74.27288055419922\n",
            "Training epoch 15152/1000000, d_loss: -436.0772705078125,  g_loss: -71.95960998535156\n",
            "Training epoch 15153/1000000, d_loss: -279.1452331542969,  g_loss: -42.69194030761719\n",
            "Training epoch 15154/1000000, d_loss: -206.1803741455078,  g_loss: -41.98187255859375\n",
            "Training epoch 15155/1000000, d_loss: -57.18099594116211,  g_loss: -106.04222106933594\n",
            "Training epoch 15156/1000000, d_loss: -96.0479507446289,  g_loss: -57.01970672607422\n",
            "Training epoch 15157/1000000, d_loss: -133.8675537109375,  g_loss: -27.082870483398438\n",
            "Training epoch 15158/1000000, d_loss: -134.93212890625,  g_loss: 21.460628509521484\n",
            "Training epoch 15159/1000000, d_loss: -107.22754669189453,  g_loss: 49.6822509765625\n",
            "Training epoch 15160/1000000, d_loss: -105.87214660644531,  g_loss: 88.4307861328125\n",
            "Training epoch 15161/1000000, d_loss: -124.91268920898438,  g_loss: 35.77565383911133\n",
            "Training epoch 15162/1000000, d_loss: -53.71715545654297,  g_loss: -89.61483001708984\n",
            "Training epoch 15163/1000000, d_loss: -110.6666488647461,  g_loss: -38.10930633544922\n",
            "Training epoch 15164/1000000, d_loss: -133.73487854003906,  g_loss: -109.18841552734375\n",
            "Training epoch 15165/1000000, d_loss: -95.28973388671875,  g_loss: 21.48782730102539\n",
            "Training epoch 15166/1000000, d_loss: -94.00552368164062,  g_loss: -3.993558883666992\n",
            "Training epoch 15167/1000000, d_loss: -154.75628662109375,  g_loss: -7.710484027862549\n",
            "Training epoch 15168/1000000, d_loss: -222.75942993164062,  g_loss: -58.49930191040039\n",
            "Training epoch 15169/1000000, d_loss: -62.58726501464844,  g_loss: 13.539677619934082\n",
            "Training epoch 15170/1000000, d_loss: -158.7745819091797,  g_loss: -0.059065818786621094\n",
            "Training epoch 15171/1000000, d_loss: -100.65399169921875,  g_loss: -59.313472747802734\n",
            "Training epoch 15172/1000000, d_loss: -225.25936889648438,  g_loss: -62.51690673828125\n",
            "Training epoch 15173/1000000, d_loss: -37.464881896972656,  g_loss: 11.27087688446045\n",
            "Training epoch 15174/1000000, d_loss: -225.9791259765625,  g_loss: 250.7445831298828\n",
            "Training epoch 15175/1000000, d_loss: -61.83489990234375,  g_loss: 70.82525634765625\n",
            "Training epoch 15176/1000000, d_loss: -37.139404296875,  g_loss: 17.347789764404297\n",
            "Training epoch 15177/1000000, d_loss: -315.4533996582031,  g_loss: -7.883398056030273\n",
            "Training epoch 15178/1000000, d_loss: -458.94525146484375,  g_loss: -54.294212341308594\n",
            "Training epoch 15179/1000000, d_loss: -464.979248046875,  g_loss: -132.65126037597656\n",
            "Training epoch 15180/1000000, d_loss: -0.5703163146972656,  g_loss: 0.5133085250854492\n",
            "Training epoch 15181/1000000, d_loss: 58.94001770019531,  g_loss: 64.52799987792969\n",
            "Training epoch 15182/1000000, d_loss: -98.87419128417969,  g_loss: -10.422412872314453\n",
            "Training epoch 15183/1000000, d_loss: -44.753639221191406,  g_loss: 67.64018249511719\n",
            "Training epoch 15184/1000000, d_loss: -120.41144561767578,  g_loss: 41.1070442199707\n",
            "Training epoch 15185/1000000, d_loss: -74.47484588623047,  g_loss: 46.12806701660156\n",
            "Training epoch 15186/1000000, d_loss: -78.47943115234375,  g_loss: 99.86969757080078\n",
            "Training epoch 15187/1000000, d_loss: -98.79257202148438,  g_loss: 42.29590606689453\n",
            "Training epoch 15188/1000000, d_loss: -111.8022689819336,  g_loss: 60.077972412109375\n",
            "Training epoch 15189/1000000, d_loss: -32.38028335571289,  g_loss: 52.8489990234375\n",
            "Training epoch 15190/1000000, d_loss: -63.11827087402344,  g_loss: 26.141143798828125\n",
            "Training epoch 15191/1000000, d_loss: -253.43353271484375,  g_loss: 24.709800720214844\n",
            "Training epoch 15192/1000000, d_loss: -40.413063049316406,  g_loss: 34.08531188964844\n",
            "Training epoch 15193/1000000, d_loss: -82.97959899902344,  g_loss: 30.40300941467285\n",
            "Training epoch 15194/1000000, d_loss: -3.1216201782226562,  g_loss: 43.66313934326172\n",
            "Training epoch 15195/1000000, d_loss: -195.55763244628906,  g_loss: -30.95989227294922\n",
            "Training epoch 15196/1000000, d_loss: -265.7933349609375,  g_loss: 23.701683044433594\n",
            "Training epoch 15197/1000000, d_loss: -167.925048828125,  g_loss: -50.133121490478516\n",
            "Training epoch 15198/1000000, d_loss: -22.276899337768555,  g_loss: 34.656978607177734\n",
            "Training epoch 15199/1000000, d_loss: -75.92770385742188,  g_loss: 14.430810928344727\n",
            "Training epoch 15200/1000000, d_loss: -129.1310272216797,  g_loss: -8.352092742919922\n",
            "Training epoch 15201/1000000, d_loss: -30.175270080566406,  g_loss: 17.305591583251953\n",
            "Training epoch 15202/1000000, d_loss: -975.9412231445312,  g_loss: -127.31886291503906\n",
            "Training epoch 15203/1000000, d_loss: 42.63603210449219,  g_loss: 34.76732635498047\n",
            "Training epoch 15204/1000000, d_loss: -145.7517852783203,  g_loss: 20.330432891845703\n",
            "Training epoch 15205/1000000, d_loss: -105.98627471923828,  g_loss: -17.582136154174805\n",
            "Training epoch 15206/1000000, d_loss: -83.30387115478516,  g_loss: -24.3173828125\n",
            "Training epoch 15207/1000000, d_loss: -64.61359405517578,  g_loss: -14.919931411743164\n",
            "Training epoch 15208/1000000, d_loss: -80.8446273803711,  g_loss: 27.87543487548828\n",
            "Training epoch 15209/1000000, d_loss: -828.2958374023438,  g_loss: -17.964628219604492\n",
            "Training epoch 15210/1000000, d_loss: 18.301132202148438,  g_loss: -230.07528686523438\n",
            "Training epoch 15211/1000000, d_loss: -47.241363525390625,  g_loss: -22.221290588378906\n",
            "Training epoch 15212/1000000, d_loss: -284.7220764160156,  g_loss: 57.242156982421875\n",
            "Training epoch 15213/1000000, d_loss: -100.82518768310547,  g_loss: 83.6801986694336\n",
            "Training epoch 15214/1000000, d_loss: -355.96881103515625,  g_loss: -6.990068435668945\n",
            "Training epoch 15215/1000000, d_loss: -189.17529296875,  g_loss: -215.8663787841797\n",
            "Training epoch 15216/1000000, d_loss: 159.09994506835938,  g_loss: -12.31005573272705\n",
            "Training epoch 15217/1000000, d_loss: -106.09469604492188,  g_loss: 29.118148803710938\n",
            "Training epoch 15218/1000000, d_loss: -80.21253967285156,  g_loss: 78.46385192871094\n",
            "Training epoch 15219/1000000, d_loss: -193.30661010742188,  g_loss: -5.1147308349609375\n",
            "Training epoch 15220/1000000, d_loss: -101.99171447753906,  g_loss: -12.436452865600586\n",
            "Training epoch 15221/1000000, d_loss: -66.81875610351562,  g_loss: 88.66468048095703\n",
            "Training epoch 15222/1000000, d_loss: -84.09708404541016,  g_loss: 202.5126953125\n",
            "Training epoch 15223/1000000, d_loss: -152.0279998779297,  g_loss: 105.56212615966797\n",
            "Training epoch 15224/1000000, d_loss: -62.35857009887695,  g_loss: 69.52081298828125\n",
            "Training epoch 15225/1000000, d_loss: -116.04485321044922,  g_loss: 57.77855682373047\n",
            "Training epoch 15226/1000000, d_loss: -132.0940399169922,  g_loss: 67.36956024169922\n",
            "Training epoch 15227/1000000, d_loss: -65.4819564819336,  g_loss: 45.5962028503418\n",
            "Training epoch 15228/1000000, d_loss: -731.74462890625,  g_loss: -82.39353942871094\n",
            "Training epoch 15229/1000000, d_loss: -26.18413543701172,  g_loss: -10.308513641357422\n",
            "Training epoch 15230/1000000, d_loss: 3004.31689453125,  g_loss: 3.0650882720947266\n",
            "Training epoch 15231/1000000, d_loss: -182.76028442382812,  g_loss: -34.02802276611328\n",
            "Training epoch 15232/1000000, d_loss: 4.115959167480469,  g_loss: 4.435220718383789\n",
            "Training epoch 15233/1000000, d_loss: 48.342002868652344,  g_loss: 23.764774322509766\n",
            "Training epoch 15234/1000000, d_loss: -123.76072692871094,  g_loss: 60.70743942260742\n",
            "Training epoch 15235/1000000, d_loss: -126.90986633300781,  g_loss: 68.4463882446289\n",
            "Training epoch 15236/1000000, d_loss: -95.23968505859375,  g_loss: 44.648189544677734\n",
            "Training epoch 15237/1000000, d_loss: -175.31002807617188,  g_loss: -34.52458953857422\n",
            "Training epoch 15238/1000000, d_loss: 29.289817810058594,  g_loss: -37.69614791870117\n",
            "Training epoch 15239/1000000, d_loss: -101.74464416503906,  g_loss: 5.478038787841797\n",
            "Training epoch 15240/1000000, d_loss: -115.87802124023438,  g_loss: -10.317138671875\n",
            "Training epoch 15241/1000000, d_loss: -107.84153747558594,  g_loss: -53.83688735961914\n",
            "Training epoch 15242/1000000, d_loss: -255.8048553466797,  g_loss: -116.79537200927734\n",
            "Training epoch 15243/1000000, d_loss: -663.181884765625,  g_loss: -412.5320739746094\n",
            "Training epoch 15244/1000000, d_loss: -164.76339721679688,  g_loss: -19.66757583618164\n",
            "Training epoch 15245/1000000, d_loss: 2152.11669921875,  g_loss: 85.8467788696289\n",
            "Training epoch 15246/1000000, d_loss: -22.994979858398438,  g_loss: 59.15594482421875\n",
            "Training epoch 15247/1000000, d_loss: -52.47193145751953,  g_loss: 31.070528030395508\n",
            "Training epoch 15248/1000000, d_loss: -250.10142517089844,  g_loss: -9.273974418640137\n",
            "Training epoch 15249/1000000, d_loss: -31.327402114868164,  g_loss: -55.98672103881836\n",
            "Training epoch 15250/1000000, d_loss: -90.7314682006836,  g_loss: -21.608867645263672\n",
            "Training epoch 15251/1000000, d_loss: -77.46538543701172,  g_loss: 152.55303955078125\n",
            "Training epoch 15252/1000000, d_loss: -119.00264739990234,  g_loss: -25.77365493774414\n",
            "Training epoch 15253/1000000, d_loss: -41.78710174560547,  g_loss: -10.242613792419434\n",
            "Training epoch 15254/1000000, d_loss: -153.91497802734375,  g_loss: -16.175304412841797\n",
            "Training epoch 15255/1000000, d_loss: -59.94048309326172,  g_loss: -18.13521957397461\n",
            "Training epoch 15256/1000000, d_loss: -27.1827392578125,  g_loss: 75.21141815185547\n",
            "Training epoch 15257/1000000, d_loss: -92.3739013671875,  g_loss: 75.84405517578125\n",
            "Training epoch 15258/1000000, d_loss: -354.8078308105469,  g_loss: -23.747966766357422\n",
            "Training epoch 15259/1000000, d_loss: -121.2152099609375,  g_loss: 99.22134399414062\n",
            "Training epoch 15260/1000000, d_loss: -50.69220733642578,  g_loss: 36.46803665161133\n",
            "Training epoch 15261/1000000, d_loss: -115.16361236572266,  g_loss: 33.847877502441406\n",
            "Training epoch 15262/1000000, d_loss: -299.2293395996094,  g_loss: 30.585548400878906\n",
            "Training epoch 15263/1000000, d_loss: -232.88543701171875,  g_loss: -42.346981048583984\n",
            "Training epoch 15264/1000000, d_loss: -95.10018920898438,  g_loss: 63.27919387817383\n",
            "Training epoch 15265/1000000, d_loss: -738.0578002929688,  g_loss: -67.31687927246094\n",
            "Training epoch 15266/1000000, d_loss: -29.508155822753906,  g_loss: 108.60111999511719\n",
            "Training epoch 15267/1000000, d_loss: -720.3613891601562,  g_loss: 68.02007293701172\n",
            "Training epoch 15268/1000000, d_loss: 95.97850799560547,  g_loss: -33.22023010253906\n",
            "Training epoch 15269/1000000, d_loss: -245.20510864257812,  g_loss: 423.9000244140625\n",
            "Training epoch 15270/1000000, d_loss: -93.94295501708984,  g_loss: 24.649953842163086\n",
            "Training epoch 15271/1000000, d_loss: -12.294925689697266,  g_loss: 58.17915344238281\n",
            "Training epoch 15272/1000000, d_loss: -89.46148681640625,  g_loss: 103.55032348632812\n",
            "Training epoch 15273/1000000, d_loss: -208.92752075195312,  g_loss: 118.39678955078125\n",
            "Training epoch 15274/1000000, d_loss: -253.66795349121094,  g_loss: 339.1117858886719\n",
            "Training epoch 15275/1000000, d_loss: -248.49978637695312,  g_loss: 233.72079467773438\n",
            "Training epoch 15276/1000000, d_loss: -264.44110107421875,  g_loss: 294.6341247558594\n",
            "Training epoch 15277/1000000, d_loss: -51.68409729003906,  g_loss: 15.06979751586914\n",
            "Training epoch 15278/1000000, d_loss: -216.4905548095703,  g_loss: -20.239341735839844\n",
            "Training epoch 15279/1000000, d_loss: -110.82545471191406,  g_loss: -59.78886413574219\n",
            "Training epoch 15280/1000000, d_loss: -18.027368545532227,  g_loss: 26.010364532470703\n",
            "Training epoch 15281/1000000, d_loss: -149.48165893554688,  g_loss: -30.983367919921875\n",
            "Training epoch 15282/1000000, d_loss: -68.14599609375,  g_loss: -8.181666374206543\n",
            "Training epoch 15283/1000000, d_loss: -70.31192779541016,  g_loss: 12.899128913879395\n",
            "Training epoch 15284/1000000, d_loss: -283.4417724609375,  g_loss: -96.18307495117188\n",
            "Training epoch 15285/1000000, d_loss: -96.8546371459961,  g_loss: -9.757551193237305\n",
            "Training epoch 15286/1000000, d_loss: -89.31172180175781,  g_loss: -1.7678414583206177\n",
            "Training epoch 15287/1000000, d_loss: -367.5897521972656,  g_loss: -62.76520919799805\n",
            "Training epoch 15288/1000000, d_loss: -146.8043975830078,  g_loss: -27.58123779296875\n",
            "Training epoch 15289/1000000, d_loss: -65.6570816040039,  g_loss: 134.6963348388672\n",
            "Training epoch 15290/1000000, d_loss: -65.35088348388672,  g_loss: 82.09940338134766\n",
            "Training epoch 15291/1000000, d_loss: -282.22821044921875,  g_loss: 15.877572059631348\n",
            "Training epoch 15292/1000000, d_loss: -121.62812805175781,  g_loss: -36.82896423339844\n",
            "Training epoch 15293/1000000, d_loss: -87.4576644897461,  g_loss: -28.72216033935547\n",
            "Training epoch 15294/1000000, d_loss: -86.8553237915039,  g_loss: -10.252692222595215\n",
            "Training epoch 15295/1000000, d_loss: -436.75238037109375,  g_loss: -60.61958694458008\n",
            "Training epoch 15296/1000000, d_loss: -175.51780700683594,  g_loss: 6.912441253662109\n",
            "Training epoch 15297/1000000, d_loss: -41.64673614501953,  g_loss: 34.09152603149414\n",
            "Training epoch 15298/1000000, d_loss: -505.2187194824219,  g_loss: -117.18846130371094\n",
            "Training epoch 15299/1000000, d_loss: -73.0013656616211,  g_loss: -30.367094039916992\n",
            "Training epoch 15300/1000000, d_loss: -187.14781188964844,  g_loss: -15.747135162353516\n",
            "Training epoch 15301/1000000, d_loss: -252.62680053710938,  g_loss: 5.276320934295654\n",
            "Training epoch 15302/1000000, d_loss: -217.30014038085938,  g_loss: -35.69411849975586\n",
            "Training epoch 15303/1000000, d_loss: -125.53469848632812,  g_loss: -58.66234588623047\n",
            "Training epoch 15304/1000000, d_loss: 18.056365966796875,  g_loss: 99.79325866699219\n",
            "Training epoch 15305/1000000, d_loss: -91.54350280761719,  g_loss: 114.27088928222656\n",
            "Training epoch 15306/1000000, d_loss: -112.85803985595703,  g_loss: 186.5613555908203\n",
            "Training epoch 15307/1000000, d_loss: -89.63746643066406,  g_loss: 34.736576080322266\n",
            "Training epoch 15308/1000000, d_loss: -125.80551147460938,  g_loss: 37.594444274902344\n",
            "Training epoch 15309/1000000, d_loss: -50.745208740234375,  g_loss: 55.76393127441406\n",
            "Training epoch 15310/1000000, d_loss: -467.0855712890625,  g_loss: -42.05974578857422\n",
            "Training epoch 15311/1000000, d_loss: -415.7936096191406,  g_loss: -105.37413024902344\n",
            "Training epoch 15312/1000000, d_loss: -775.0136108398438,  g_loss: -257.01324462890625\n",
            "Training epoch 15313/1000000, d_loss: -412.7906188964844,  g_loss: -234.30841064453125\n",
            "Training epoch 15314/1000000, d_loss: -137.23660278320312,  g_loss: -42.77054214477539\n",
            "Training epoch 15315/1000000, d_loss: -39.210693359375,  g_loss: 24.76966667175293\n",
            "Training epoch 15316/1000000, d_loss: -126.8485107421875,  g_loss: 73.3846435546875\n",
            "Training epoch 15317/1000000, d_loss: -60.75682067871094,  g_loss: 109.12522888183594\n",
            "Training epoch 15318/1000000, d_loss: -80.26080322265625,  g_loss: 74.45731353759766\n",
            "Training epoch 15319/1000000, d_loss: -229.46011352539062,  g_loss: 232.65753173828125\n",
            "Training epoch 15320/1000000, d_loss: -95.75070190429688,  g_loss: 16.582895278930664\n",
            "Training epoch 15321/1000000, d_loss: -392.5076599121094,  g_loss: -138.30601501464844\n",
            "Training epoch 15322/1000000, d_loss: -66.68792724609375,  g_loss: -43.45353698730469\n",
            "Training epoch 15323/1000000, d_loss: -172.14614868164062,  g_loss: -15.84202766418457\n",
            "Training epoch 15324/1000000, d_loss: -93.69903564453125,  g_loss: 117.6549072265625\n",
            "Training epoch 15325/1000000, d_loss: -340.16412353515625,  g_loss: 128.62806701660156\n",
            "Training epoch 15326/1000000, d_loss: -102.35868835449219,  g_loss: 55.766807556152344\n",
            "Training epoch 15327/1000000, d_loss: -189.19418334960938,  g_loss: 25.692413330078125\n",
            "Training epoch 15328/1000000, d_loss: 20.100666046142578,  g_loss: 42.107234954833984\n",
            "Training epoch 15329/1000000, d_loss: -304.6761779785156,  g_loss: -16.692577362060547\n",
            "Training epoch 15330/1000000, d_loss: -373.799072265625,  g_loss: -146.48159790039062\n",
            "Training epoch 15331/1000000, d_loss: -184.87196350097656,  g_loss: -74.47425079345703\n",
            "Training epoch 15332/1000000, d_loss: -215.68338012695312,  g_loss: -6.379831314086914\n",
            "Training epoch 15333/1000000, d_loss: -21.840682983398438,  g_loss: 29.101438522338867\n",
            "Training epoch 15334/1000000, d_loss: -25.93535614013672,  g_loss: 40.42302322387695\n",
            "Training epoch 15335/1000000, d_loss: -78.89067077636719,  g_loss: 50.653926849365234\n",
            "Training epoch 15336/1000000, d_loss: -96.69039916992188,  g_loss: 36.39823532104492\n",
            "Training epoch 15337/1000000, d_loss: -51.9222412109375,  g_loss: 44.33465576171875\n",
            "Training epoch 15338/1000000, d_loss: -87.96733093261719,  g_loss: 66.66046142578125\n",
            "Training epoch 15339/1000000, d_loss: 257.7739562988281,  g_loss: 62.025726318359375\n",
            "Training epoch 15340/1000000, d_loss: -119.26709747314453,  g_loss: 69.53926849365234\n",
            "Training epoch 15341/1000000, d_loss: -139.48377990722656,  g_loss: 19.229324340820312\n",
            "Training epoch 15342/1000000, d_loss: -157.59127807617188,  g_loss: 2.132061004638672\n",
            "Training epoch 15343/1000000, d_loss: -241.2523193359375,  g_loss: -25.78411865234375\n",
            "Training epoch 15344/1000000, d_loss: 13.68603515625,  g_loss: 50.83832550048828\n",
            "Training epoch 15345/1000000, d_loss: -102.5815200805664,  g_loss: 33.11323165893555\n",
            "Training epoch 15346/1000000, d_loss: -38.244361877441406,  g_loss: 48.0982780456543\n",
            "Training epoch 15347/1000000, d_loss: -168.42459106445312,  g_loss: 45.07514572143555\n",
            "Training epoch 15348/1000000, d_loss: -66.95170593261719,  g_loss: 75.7079086303711\n",
            "Training epoch 15349/1000000, d_loss: -114.17396545410156,  g_loss: 70.233642578125\n",
            "Training epoch 15350/1000000, d_loss: -159.9540557861328,  g_loss: 81.75074768066406\n",
            "Training epoch 15351/1000000, d_loss: -741.5816040039062,  g_loss: -64.90756225585938\n",
            "Training epoch 15352/1000000, d_loss: 706.5242919921875,  g_loss: -7.938644886016846\n",
            "Training epoch 15353/1000000, d_loss: 13.754348754882812,  g_loss: 56.19502258300781\n",
            "Training epoch 15354/1000000, d_loss: -112.93730926513672,  g_loss: 76.71399688720703\n",
            "Training epoch 15355/1000000, d_loss: -27.497669219970703,  g_loss: 101.2845458984375\n",
            "Training epoch 15356/1000000, d_loss: -366.5615539550781,  g_loss: 45.17427062988281\n",
            "Training epoch 15357/1000000, d_loss: -150.05345153808594,  g_loss: 57.116729736328125\n",
            "Training epoch 15358/1000000, d_loss: -488.4110412597656,  g_loss: -256.51239013671875\n",
            "Training epoch 15359/1000000, d_loss: 6.566154479980469,  g_loss: 31.19171142578125\n",
            "Training epoch 15360/1000000, d_loss: -91.36395263671875,  g_loss: 57.083621978759766\n",
            "Training epoch 15361/1000000, d_loss: -87.20557403564453,  g_loss: 60.14348220825195\n",
            "Training epoch 15362/1000000, d_loss: -255.0247344970703,  g_loss: 520.9301147460938\n",
            "Training epoch 15363/1000000, d_loss: -46.90137481689453,  g_loss: 64.5556411743164\n",
            "Training epoch 15364/1000000, d_loss: -163.14981079101562,  g_loss: 42.40695571899414\n",
            "Training epoch 15365/1000000, d_loss: -133.77047729492188,  g_loss: 12.380271911621094\n",
            "Training epoch 15366/1000000, d_loss: -157.31597900390625,  g_loss: -1.961172103881836\n",
            "Training epoch 15367/1000000, d_loss: -60.74462890625,  g_loss: 3.5738730430603027\n",
            "Training epoch 15368/1000000, d_loss: -351.7263488769531,  g_loss: 4.395804405212402\n",
            "Training epoch 15369/1000000, d_loss: -68.02486419677734,  g_loss: 10.480316162109375\n",
            "Training epoch 15370/1000000, d_loss: -66.04989624023438,  g_loss: 43.85365295410156\n",
            "Training epoch 15371/1000000, d_loss: -17.879302978515625,  g_loss: 88.62217712402344\n",
            "Training epoch 15372/1000000, d_loss: -211.27755737304688,  g_loss: 54.332069396972656\n",
            "Training epoch 15373/1000000, d_loss: -271.0162048339844,  g_loss: 177.24879455566406\n",
            "Training epoch 15374/1000000, d_loss: -103.56497192382812,  g_loss: 28.90055274963379\n",
            "Training epoch 15375/1000000, d_loss: -255.7314910888672,  g_loss: 20.759727478027344\n",
            "Training epoch 15376/1000000, d_loss: -38.917266845703125,  g_loss: 29.647727966308594\n",
            "Training epoch 15377/1000000, d_loss: -451.808349609375,  g_loss: -201.9918975830078\n",
            "Training epoch 15378/1000000, d_loss: 408.8116149902344,  g_loss: 96.0330581665039\n",
            "Training epoch 15379/1000000, d_loss: -218.67807006835938,  g_loss: 18.260305404663086\n",
            "Training epoch 15380/1000000, d_loss: -105.14259338378906,  g_loss: 16.67091178894043\n",
            "Training epoch 15381/1000000, d_loss: -109.248046875,  g_loss: 48.767478942871094\n",
            "Training epoch 15382/1000000, d_loss: -551.3179321289062,  g_loss: -158.99461364746094\n",
            "Training epoch 15383/1000000, d_loss: -121.15034484863281,  g_loss: 100.39321899414062\n",
            "Training epoch 15384/1000000, d_loss: -473.7535095214844,  g_loss: 41.81040954589844\n",
            "Training epoch 15385/1000000, d_loss: -474.588623046875,  g_loss: 108.22635650634766\n",
            "Training epoch 15386/1000000, d_loss: -248.60150146484375,  g_loss: 267.3506164550781\n",
            "Training epoch 15387/1000000, d_loss: -540.6842041015625,  g_loss: 942.200439453125\n",
            "Training epoch 15388/1000000, d_loss: -18.93177032470703,  g_loss: -53.453941345214844\n",
            "Training epoch 15389/1000000, d_loss: -57.57322692871094,  g_loss: -19.646156311035156\n",
            "Training epoch 15390/1000000, d_loss: 60.514801025390625,  g_loss: 6.953497886657715\n",
            "Training epoch 15391/1000000, d_loss: -89.92499542236328,  g_loss: 31.600757598876953\n",
            "Training epoch 15392/1000000, d_loss: -62.934059143066406,  g_loss: 55.80471420288086\n",
            "Training epoch 15393/1000000, d_loss: -161.15969848632812,  g_loss: 19.653396606445312\n",
            "Training epoch 15394/1000000, d_loss: -31.731399536132812,  g_loss: 60.77437973022461\n",
            "Training epoch 15395/1000000, d_loss: -112.13986206054688,  g_loss: 60.45069122314453\n",
            "Training epoch 15396/1000000, d_loss: -180.60842895507812,  g_loss: 26.259078979492188\n",
            "Training epoch 15397/1000000, d_loss: -462.95556640625,  g_loss: -78.11676025390625\n",
            "Training epoch 15398/1000000, d_loss: -17.984413146972656,  g_loss: 49.00377655029297\n",
            "Training epoch 15399/1000000, d_loss: -223.00643920898438,  g_loss: 27.1737060546875\n",
            "Training epoch 15400/1000000, d_loss: -99.62847900390625,  g_loss: 39.14912414550781\n",
            "Training epoch 15401/1000000, d_loss: -186.29986572265625,  g_loss: 61.849056243896484\n",
            "Training epoch 15402/1000000, d_loss: -163.7794952392578,  g_loss: 106.41716003417969\n",
            "Training epoch 15403/1000000, d_loss: -77.12543487548828,  g_loss: 111.7536849975586\n",
            "Training epoch 15404/1000000, d_loss: -773.6478881835938,  g_loss: -13.739137649536133\n",
            "Training epoch 15405/1000000, d_loss: -361.7542724609375,  g_loss: -52.72074890136719\n",
            "Training epoch 15406/1000000, d_loss: -45.426971435546875,  g_loss: 23.894800186157227\n",
            "Training epoch 15407/1000000, d_loss: -170.0369873046875,  g_loss: -3.6397006511688232\n",
            "Training epoch 15408/1000000, d_loss: -76.50069427490234,  g_loss: -9.721113204956055\n",
            "Training epoch 15409/1000000, d_loss: -44.90663146972656,  g_loss: 31.721839904785156\n",
            "Training epoch 15410/1000000, d_loss: -226.23825073242188,  g_loss: 11.49201488494873\n",
            "Training epoch 15411/1000000, d_loss: -251.11050415039062,  g_loss: -79.29071807861328\n",
            "Training epoch 15412/1000000, d_loss: -133.68060302734375,  g_loss: 39.21493148803711\n",
            "Training epoch 15413/1000000, d_loss: -51.86152648925781,  g_loss: 11.214221954345703\n",
            "Training epoch 15414/1000000, d_loss: -89.19328308105469,  g_loss: 33.15970993041992\n",
            "Training epoch 15415/1000000, d_loss: -134.10914611816406,  g_loss: 44.99186706542969\n",
            "Training epoch 15416/1000000, d_loss: -191.1188201904297,  g_loss: 20.324369430541992\n",
            "Training epoch 15417/1000000, d_loss: -376.32244873046875,  g_loss: -176.3697052001953\n",
            "Training epoch 15418/1000000, d_loss: -87.87321472167969,  g_loss: 100.43128967285156\n",
            "Training epoch 15419/1000000, d_loss: -65.19041442871094,  g_loss: 143.32814025878906\n",
            "Training epoch 15420/1000000, d_loss: -177.72622680664062,  g_loss: 163.25405883789062\n",
            "Training epoch 15421/1000000, d_loss: -973.1845092773438,  g_loss: -132.2124786376953\n",
            "Training epoch 15422/1000000, d_loss: -101.96553039550781,  g_loss: 52.55357360839844\n",
            "Training epoch 15423/1000000, d_loss: -358.63751220703125,  g_loss: 16.22146224975586\n",
            "Training epoch 15424/1000000, d_loss: -5.022623062133789,  g_loss: 61.74127960205078\n",
            "Training epoch 15425/1000000, d_loss: -441.05047607421875,  g_loss: -41.81205749511719\n",
            "Training epoch 15426/1000000, d_loss: -724.3483276367188,  g_loss: 49.41957092285156\n",
            "Training epoch 15427/1000000, d_loss: -79.34249114990234,  g_loss: 123.96078491210938\n",
            "Training epoch 15428/1000000, d_loss: -80.85303497314453,  g_loss: 14.871706008911133\n",
            "Training epoch 15429/1000000, d_loss: -122.91056823730469,  g_loss: 117.73212432861328\n",
            "Training epoch 15430/1000000, d_loss: 170.22084045410156,  g_loss: -1.2651748657226562\n",
            "Training epoch 15431/1000000, d_loss: -80.12457275390625,  g_loss: 48.14353942871094\n",
            "Training epoch 15432/1000000, d_loss: -91.42230987548828,  g_loss: 54.12424850463867\n",
            "Training epoch 15433/1000000, d_loss: 13.268333435058594,  g_loss: 54.47801208496094\n",
            "Training epoch 15434/1000000, d_loss: -48.504173278808594,  g_loss: 14.274291038513184\n",
            "Training epoch 15435/1000000, d_loss: -348.004150390625,  g_loss: -31.750431060791016\n",
            "Training epoch 15436/1000000, d_loss: -56.15509033203125,  g_loss: 106.80671691894531\n",
            "Training epoch 15437/1000000, d_loss: 155.7268524169922,  g_loss: 37.47228240966797\n",
            "Training epoch 15438/1000000, d_loss: -159.87017822265625,  g_loss: 70.9777603149414\n",
            "Training epoch 15439/1000000, d_loss: -273.1031799316406,  g_loss: -66.79557800292969\n",
            "Training epoch 15440/1000000, d_loss: -952.37158203125,  g_loss: -271.6548767089844\n",
            "Training epoch 15441/1000000, d_loss: -53.49391555786133,  g_loss: -133.0760498046875\n",
            "Training epoch 15442/1000000, d_loss: -14.722908020019531,  g_loss: 2.9467086791992188\n",
            "Training epoch 15443/1000000, d_loss: -77.28899383544922,  g_loss: 38.89402770996094\n",
            "Training epoch 15444/1000000, d_loss: -54.12204360961914,  g_loss: 23.75455093383789\n",
            "Training epoch 15445/1000000, d_loss: -109.15837860107422,  g_loss: 0.956061840057373\n",
            "Training epoch 15446/1000000, d_loss: -79.31631469726562,  g_loss: 47.43423080444336\n",
            "Training epoch 15447/1000000, d_loss: -122.99642181396484,  g_loss: -8.04289436340332\n",
            "Training epoch 15448/1000000, d_loss: -370.7374267578125,  g_loss: -60.12153625488281\n",
            "Training epoch 15449/1000000, d_loss: -352.8147277832031,  g_loss: -196.81752014160156\n",
            "Training epoch 15450/1000000, d_loss: -826.1898193359375,  g_loss: -474.0829162597656\n",
            "Training epoch 15451/1000000, d_loss: 81.59910583496094,  g_loss: -71.3578109741211\n",
            "Training epoch 15452/1000000, d_loss: 45.153446197509766,  g_loss: -44.693939208984375\n",
            "Training epoch 15453/1000000, d_loss: -52.839176177978516,  g_loss: 67.08930206298828\n",
            "Training epoch 15454/1000000, d_loss: -77.73189544677734,  g_loss: 140.40872192382812\n",
            "Training epoch 15455/1000000, d_loss: -204.6207733154297,  g_loss: 388.1704406738281\n",
            "Training epoch 15456/1000000, d_loss: -230.95603942871094,  g_loss: 238.9821014404297\n",
            "Training epoch 15457/1000000, d_loss: -427.6527404785156,  g_loss: 976.054443359375\n",
            "Training epoch 15458/1000000, d_loss: -99.78329467773438,  g_loss: 64.32598114013672\n",
            "Training epoch 15459/1000000, d_loss: -180.23648071289062,  g_loss: -11.975696563720703\n",
            "Training epoch 15460/1000000, d_loss: -247.32708740234375,  g_loss: -34.48411560058594\n",
            "Training epoch 15461/1000000, d_loss: -134.05215454101562,  g_loss: -17.153343200683594\n",
            "Training epoch 15462/1000000, d_loss: -59.27443313598633,  g_loss: 9.622075080871582\n",
            "Training epoch 15463/1000000, d_loss: 0.0546875,  g_loss: -5.76386833190918\n",
            "Training epoch 15464/1000000, d_loss: -64.78998565673828,  g_loss: 17.14921760559082\n",
            "Training epoch 15465/1000000, d_loss: -62.626529693603516,  g_loss: 8.074113845825195\n",
            "Training epoch 15466/1000000, d_loss: -154.58807373046875,  g_loss: 122.05607604980469\n",
            "Training epoch 15467/1000000, d_loss: -88.31623077392578,  g_loss: 103.34765625\n",
            "Training epoch 15468/1000000, d_loss: -124.40103149414062,  g_loss: -5.791399955749512\n",
            "Training epoch 15469/1000000, d_loss: -112.13919067382812,  g_loss: 2.2883434295654297\n",
            "Training epoch 15470/1000000, d_loss: -144.47549438476562,  g_loss: 11.302895545959473\n",
            "Training epoch 15471/1000000, d_loss: -271.665771484375,  g_loss: 115.15695190429688\n",
            "Training epoch 15472/1000000, d_loss: -327.1123962402344,  g_loss: -11.857682228088379\n",
            "Training epoch 15473/1000000, d_loss: -2.123016357421875,  g_loss: 85.11805725097656\n",
            "Training epoch 15474/1000000, d_loss: -85.83161163330078,  g_loss: 75.0844955444336\n",
            "Training epoch 15475/1000000, d_loss: -745.135498046875,  g_loss: -334.8180847167969\n",
            "Training epoch 15476/1000000, d_loss: 16.298969268798828,  g_loss: 11.610695838928223\n",
            "Training epoch 15477/1000000, d_loss: -84.09210968017578,  g_loss: -11.625988006591797\n",
            "Training epoch 15478/1000000, d_loss: -138.35821533203125,  g_loss: 9.303832054138184\n",
            "Training epoch 15479/1000000, d_loss: -231.04339599609375,  g_loss: 88.11894226074219\n",
            "Training epoch 15480/1000000, d_loss: -98.88482666015625,  g_loss: -13.28225040435791\n",
            "Training epoch 15481/1000000, d_loss: -182.82293701171875,  g_loss: -27.249374389648438\n",
            "Training epoch 15482/1000000, d_loss: -162.27880859375,  g_loss: 2.11676025390625\n",
            "Training epoch 15483/1000000, d_loss: -181.21449279785156,  g_loss: -8.918500900268555\n",
            "Training epoch 15484/1000000, d_loss: -161.83984375,  g_loss: 7.25226354598999\n",
            "Training epoch 15485/1000000, d_loss: -306.49517822265625,  g_loss: -112.86865234375\n",
            "Training epoch 15486/1000000, d_loss: -144.6493682861328,  g_loss: 32.01582336425781\n",
            "Training epoch 15487/1000000, d_loss: -151.84115600585938,  g_loss: 109.46908569335938\n",
            "Training epoch 15488/1000000, d_loss: -19.039588928222656,  g_loss: 76.36787414550781\n",
            "Training epoch 15489/1000000, d_loss: 16.372203826904297,  g_loss: 15.837286949157715\n",
            "Training epoch 15490/1000000, d_loss: -93.28715515136719,  g_loss: 60.59165954589844\n",
            "Training epoch 15491/1000000, d_loss: -63.12797546386719,  g_loss: 76.50908660888672\n",
            "Training epoch 15492/1000000, d_loss: -547.6138305664062,  g_loss: -42.634117126464844\n",
            "Training epoch 15493/1000000, d_loss: -53.5739631652832,  g_loss: 57.37978744506836\n",
            "Training epoch 15494/1000000, d_loss: -232.54483032226562,  g_loss: -15.610651016235352\n",
            "Training epoch 15495/1000000, d_loss: -134.6966094970703,  g_loss: 35.856407165527344\n",
            "Training epoch 15496/1000000, d_loss: -90.95787048339844,  g_loss: 87.72657775878906\n",
            "Training epoch 15497/1000000, d_loss: -101.13040924072266,  g_loss: 75.96392822265625\n",
            "Training epoch 15498/1000000, d_loss: -187.91580200195312,  g_loss: 375.05584716796875\n",
            "Training epoch 15499/1000000, d_loss: -105.077880859375,  g_loss: 122.52967834472656\n",
            "Training epoch 15500/1000000, d_loss: -150.5438995361328,  g_loss: -19.877506256103516\n",
            "Training epoch 15501/1000000, d_loss: -355.23248291015625,  g_loss: -77.99374389648438\n",
            "Training epoch 15502/1000000, d_loss: 6.583076477050781,  g_loss: 50.552635192871094\n",
            "Training epoch 15503/1000000, d_loss: 7346.81201171875,  g_loss: 21.6937198638916\n",
            "Training epoch 15504/1000000, d_loss: -96.01516723632812,  g_loss: 47.97113037109375\n",
            "Training epoch 15505/1000000, d_loss: -86.51734924316406,  g_loss: 51.243507385253906\n",
            "Training epoch 15506/1000000, d_loss: 131.62924194335938,  g_loss: 55.223270416259766\n",
            "Training epoch 15507/1000000, d_loss: -30.2142333984375,  g_loss: 76.23675537109375\n",
            "Training epoch 15508/1000000, d_loss: -111.46519470214844,  g_loss: 72.76516723632812\n",
            "Training epoch 15509/1000000, d_loss: -209.12518310546875,  g_loss: 51.44506072998047\n",
            "Training epoch 15510/1000000, d_loss: -86.64653015136719,  g_loss: 90.18213653564453\n",
            "Training epoch 15511/1000000, d_loss: -155.44509887695312,  g_loss: 345.12139892578125\n",
            "Training epoch 15512/1000000, d_loss: -252.16702270507812,  g_loss: 35.76581954956055\n",
            "Training epoch 15513/1000000, d_loss: -45.82999801635742,  g_loss: 39.4071044921875\n",
            "Training epoch 15514/1000000, d_loss: -45.56210708618164,  g_loss: 23.69363021850586\n",
            "Training epoch 15515/1000000, d_loss: -304.74200439453125,  g_loss: 12.07828140258789\n",
            "Training epoch 15516/1000000, d_loss: -112.55992889404297,  g_loss: 12.437766075134277\n",
            "Training epoch 15517/1000000, d_loss: -98.66285705566406,  g_loss: 22.942176818847656\n",
            "Training epoch 15518/1000000, d_loss: -76.40364074707031,  g_loss: 19.208059310913086\n",
            "Training epoch 15519/1000000, d_loss: -66.59445190429688,  g_loss: 50.18851852416992\n",
            "Training epoch 15520/1000000, d_loss: -30.994609832763672,  g_loss: 46.89967727661133\n",
            "Training epoch 15521/1000000, d_loss: -96.79354095458984,  g_loss: 51.39439392089844\n",
            "Training epoch 15522/1000000, d_loss: -99.91144561767578,  g_loss: 107.01533508300781\n",
            "Training epoch 15523/1000000, d_loss: -110.40272521972656,  g_loss: -4.783369541168213\n",
            "Training epoch 15524/1000000, d_loss: -155.96791076660156,  g_loss: -8.451622009277344\n",
            "Training epoch 15525/1000000, d_loss: -251.3179931640625,  g_loss: -42.74638366699219\n",
            "Training epoch 15526/1000000, d_loss: -102.50180053710938,  g_loss: 42.57945251464844\n",
            "Training epoch 15527/1000000, d_loss: -84.71342468261719,  g_loss: -6.91581392288208\n",
            "Training epoch 15528/1000000, d_loss: -82.17233276367188,  g_loss: 18.74515151977539\n",
            "Training epoch 15529/1000000, d_loss: -55.762550354003906,  g_loss: 53.502960205078125\n",
            "Training epoch 15530/1000000, d_loss: -148.86880493164062,  g_loss: 30.661630630493164\n",
            "Training epoch 15531/1000000, d_loss: -130.602783203125,  g_loss: -14.135747909545898\n",
            "Training epoch 15532/1000000, d_loss: -54.01879119873047,  g_loss: 132.4419708251953\n",
            "Training epoch 15533/1000000, d_loss: -659.2855224609375,  g_loss: 70.80818176269531\n",
            "Training epoch 15534/1000000, d_loss: -19.07162094116211,  g_loss: 47.232933044433594\n",
            "Training epoch 15535/1000000, d_loss: -48.31849670410156,  g_loss: 51.24451446533203\n",
            "Training epoch 15536/1000000, d_loss: -149.48812866210938,  g_loss: 27.212804794311523\n",
            "Training epoch 15537/1000000, d_loss: -128.16903686523438,  g_loss: -4.129342079162598\n",
            "Training epoch 15538/1000000, d_loss: -239.837158203125,  g_loss: 66.22257995605469\n",
            "Training epoch 15539/1000000, d_loss: 9.242637634277344,  g_loss: 77.9869384765625\n",
            "Training epoch 15540/1000000, d_loss: -50.46171569824219,  g_loss: 56.36871337890625\n",
            "Training epoch 15541/1000000, d_loss: 68.83261108398438,  g_loss: 53.7301025390625\n",
            "Training epoch 15542/1000000, d_loss: -13.997631072998047,  g_loss: 82.93466186523438\n",
            "Training epoch 15543/1000000, d_loss: -28.94144058227539,  g_loss: 80.04073333740234\n",
            "Training epoch 15544/1000000, d_loss: -225.32582092285156,  g_loss: 22.805877685546875\n",
            "Training epoch 15545/1000000, d_loss: -61.84443664550781,  g_loss: 38.027191162109375\n",
            "Training epoch 15546/1000000, d_loss: -71.47383117675781,  g_loss: 45.810340881347656\n",
            "Training epoch 15547/1000000, d_loss: -92.26888275146484,  g_loss: 36.46596145629883\n",
            "Training epoch 15548/1000000, d_loss: -71.31950378417969,  g_loss: 30.696918487548828\n",
            "Training epoch 15549/1000000, d_loss: -97.7804183959961,  g_loss: 85.36123657226562\n",
            "Training epoch 15550/1000000, d_loss: -75.30081176757812,  g_loss: 76.29605102539062\n",
            "Training epoch 15551/1000000, d_loss: -65.954833984375,  g_loss: 89.87907409667969\n",
            "Training epoch 15552/1000000, d_loss: -45.09929275512695,  g_loss: 34.735992431640625\n",
            "Training epoch 15553/1000000, d_loss: -62.87400817871094,  g_loss: 94.7598648071289\n",
            "Training epoch 15554/1000000, d_loss: -57.856536865234375,  g_loss: 104.11994934082031\n",
            "Training epoch 15555/1000000, d_loss: -12.653348922729492,  g_loss: 82.66114807128906\n",
            "Training epoch 15556/1000000, d_loss: -40.14459228515625,  g_loss: 47.07879638671875\n",
            "Training epoch 15557/1000000, d_loss: -66.70231628417969,  g_loss: 31.520587921142578\n",
            "Training epoch 15558/1000000, d_loss: -242.1575927734375,  g_loss: -26.973962783813477\n",
            "Training epoch 15559/1000000, d_loss: -109.55059814453125,  g_loss: 43.46858596801758\n",
            "Training epoch 15560/1000000, d_loss: -129.01504516601562,  g_loss: 60.221595764160156\n",
            "Training epoch 15561/1000000, d_loss: -122.7741928100586,  g_loss: 7.241589546203613\n",
            "Training epoch 15562/1000000, d_loss: -122.58763122558594,  g_loss: 33.16977310180664\n",
            "Training epoch 15563/1000000, d_loss: -549.9443359375,  g_loss: -9.923346519470215\n",
            "Training epoch 15564/1000000, d_loss: -251.9784393310547,  g_loss: 11.337325096130371\n",
            "Training epoch 15565/1000000, d_loss: -75.73543548583984,  g_loss: 16.14103889465332\n",
            "Training epoch 15566/1000000, d_loss: -94.09774780273438,  g_loss: 42.1009521484375\n",
            "Training epoch 15567/1000000, d_loss: -84.83747100830078,  g_loss: 88.3311767578125\n",
            "Training epoch 15568/1000000, d_loss: -157.07373046875,  g_loss: 29.433855056762695\n",
            "Training epoch 15569/1000000, d_loss: -400.1719665527344,  g_loss: 57.5620002746582\n",
            "Training epoch 15570/1000000, d_loss: -110.84929656982422,  g_loss: -40.991825103759766\n",
            "Training epoch 15571/1000000, d_loss: -22.34300994873047,  g_loss: 22.383543014526367\n",
            "Training epoch 15572/1000000, d_loss: -102.76785278320312,  g_loss: 52.54851531982422\n",
            "Training epoch 15573/1000000, d_loss: -227.05291748046875,  g_loss: -57.247833251953125\n",
            "Training epoch 15574/1000000, d_loss: 103.53132629394531,  g_loss: 36.986900329589844\n",
            "Training epoch 15575/1000000, d_loss: -119.93106079101562,  g_loss: 65.99861907958984\n",
            "Training epoch 15576/1000000, d_loss: -162.93231201171875,  g_loss: -22.513551712036133\n",
            "Training epoch 15577/1000000, d_loss: -102.4291763305664,  g_loss: 72.06732177734375\n",
            "Training epoch 15578/1000000, d_loss: -99.7136001586914,  g_loss: 54.314796447753906\n",
            "Training epoch 15579/1000000, d_loss: -164.26927185058594,  g_loss: 76.06150817871094\n",
            "Training epoch 15580/1000000, d_loss: -64.6182861328125,  g_loss: 103.08541870117188\n",
            "Training epoch 15581/1000000, d_loss: -194.06182861328125,  g_loss: -20.428945541381836\n",
            "Training epoch 15582/1000000, d_loss: -140.84664916992188,  g_loss: 30.79857063293457\n",
            "Training epoch 15583/1000000, d_loss: -154.3845672607422,  g_loss: -39.34986877441406\n",
            "Training epoch 15584/1000000, d_loss: -92.44597625732422,  g_loss: 111.89231872558594\n",
            "Training epoch 15585/1000000, d_loss: -308.8571472167969,  g_loss: 44.77592086791992\n",
            "Training epoch 15586/1000000, d_loss: -72.76266479492188,  g_loss: 50.80568313598633\n",
            "Training epoch 15587/1000000, d_loss: -54.02031707763672,  g_loss: 24.758909225463867\n",
            "Training epoch 15588/1000000, d_loss: -123.35665893554688,  g_loss: 19.968517303466797\n",
            "Training epoch 15589/1000000, d_loss: -466.55474853515625,  g_loss: -230.52777099609375\n",
            "Training epoch 15590/1000000, d_loss: -27.766983032226562,  g_loss: 80.62498474121094\n",
            "Training epoch 15591/1000000, d_loss: -258.4635314941406,  g_loss: 64.49797821044922\n",
            "Training epoch 15592/1000000, d_loss: -52.63131332397461,  g_loss: 27.765405654907227\n",
            "Training epoch 15593/1000000, d_loss: -188.67098999023438,  g_loss: 65.24876403808594\n",
            "Training epoch 15594/1000000, d_loss: -197.92352294921875,  g_loss: 63.389976501464844\n",
            "Training epoch 15595/1000000, d_loss: -55.26636505126953,  g_loss: 53.037818908691406\n",
            "Training epoch 15596/1000000, d_loss: -175.8502655029297,  g_loss: 32.89944839477539\n",
            "Training epoch 15597/1000000, d_loss: -11.16796875,  g_loss: 78.06684112548828\n",
            "Training epoch 15598/1000000, d_loss: -95.22586822509766,  g_loss: 38.16950607299805\n",
            "Training epoch 15599/1000000, d_loss: -444.8970031738281,  g_loss: 12.315969467163086\n",
            "Training epoch 15600/1000000, d_loss: -158.10140991210938,  g_loss: -19.848196029663086\n",
            "Training epoch 15601/1000000, d_loss: -633.4381103515625,  g_loss: -34.55619430541992\n",
            "Training epoch 15602/1000000, d_loss: 48.984466552734375,  g_loss: -45.976600646972656\n",
            "Training epoch 15603/1000000, d_loss: -324.6815490722656,  g_loss: -250.2296142578125\n",
            "Training epoch 15604/1000000, d_loss: -146.19247436523438,  g_loss: -86.35337829589844\n",
            "Training epoch 15605/1000000, d_loss: -118.1146469116211,  g_loss: 117.15299987792969\n",
            "Training epoch 15606/1000000, d_loss: -286.1981201171875,  g_loss: 379.5376281738281\n",
            "Training epoch 15607/1000000, d_loss: -72.87680053710938,  g_loss: 121.17787170410156\n",
            "Training epoch 15608/1000000, d_loss: -99.2999496459961,  g_loss: -22.086118698120117\n",
            "Training epoch 15609/1000000, d_loss: -94.27073669433594,  g_loss: -5.243783950805664\n",
            "Training epoch 15610/1000000, d_loss: -68.439697265625,  g_loss: 37.84748077392578\n",
            "Training epoch 15611/1000000, d_loss: -378.810546875,  g_loss: 181.18309020996094\n",
            "Training epoch 15612/1000000, d_loss: -143.62449645996094,  g_loss: -62.051143646240234\n",
            "Training epoch 15613/1000000, d_loss: -71.33567810058594,  g_loss: 49.66889190673828\n",
            "Training epoch 15614/1000000, d_loss: -72.84737396240234,  g_loss: 11.43044376373291\n",
            "Training epoch 15615/1000000, d_loss: -973.0449829101562,  g_loss: -133.93775939941406\n",
            "Training epoch 15616/1000000, d_loss: -639.9832763671875,  g_loss: -178.9799346923828\n",
            "Training epoch 15617/1000000, d_loss: 58.32798767089844,  g_loss: -8.85600471496582\n",
            "Training epoch 15618/1000000, d_loss: -82.23690795898438,  g_loss: 29.32366371154785\n",
            "Training epoch 15619/1000000, d_loss: -193.00685119628906,  g_loss: -11.741538047790527\n",
            "Training epoch 15620/1000000, d_loss: 39.886680603027344,  g_loss: -8.206135749816895\n",
            "Training epoch 15621/1000000, d_loss: -178.14956665039062,  g_loss: 17.620811462402344\n",
            "Training epoch 15622/1000000, d_loss: 92.60125732421875,  g_loss: 25.307697296142578\n",
            "Training epoch 15623/1000000, d_loss: -135.3311309814453,  g_loss: 140.93214416503906\n",
            "Training epoch 15624/1000000, d_loss: -533.0011596679688,  g_loss: 4.503807067871094\n",
            "Training epoch 15625/1000000, d_loss: -457.3512268066406,  g_loss: -5.5244598388671875\n",
            "Training epoch 15626/1000000, d_loss: -448.603759765625,  g_loss: 20.031719207763672\n",
            "Training epoch 15627/1000000, d_loss: -616.8970947265625,  g_loss: -487.459716796875\n",
            "Training epoch 15628/1000000, d_loss: 3.43328857421875,  g_loss: -120.26825714111328\n",
            "Training epoch 15629/1000000, d_loss: 39.977447509765625,  g_loss: 63.30648422241211\n",
            "Training epoch 15630/1000000, d_loss: -96.23768615722656,  g_loss: 199.24514770507812\n",
            "Training epoch 15631/1000000, d_loss: -368.18768310546875,  g_loss: 6.071954727172852\n",
            "Training epoch 15632/1000000, d_loss: 56.8101806640625,  g_loss: 48.91810607910156\n",
            "Training epoch 15633/1000000, d_loss: 60.97593688964844,  g_loss: -4.042898178100586\n",
            "Training epoch 15634/1000000, d_loss: -625.0208129882812,  g_loss: -136.6560516357422\n",
            "Training epoch 15635/1000000, d_loss: 70.51835632324219,  g_loss: -128.33627319335938\n",
            "Training epoch 15636/1000000, d_loss: -63.196895599365234,  g_loss: 119.52342987060547\n",
            "Training epoch 15637/1000000, d_loss: -219.41656494140625,  g_loss: 334.4453125\n",
            "Training epoch 15638/1000000, d_loss: -94.72500610351562,  g_loss: 59.74260330200195\n",
            "Training epoch 15639/1000000, d_loss: -214.1473388671875,  g_loss: 65.89450073242188\n",
            "Training epoch 15640/1000000, d_loss: -136.10035705566406,  g_loss: 59.6708984375\n",
            "Training epoch 15641/1000000, d_loss: -174.37167358398438,  g_loss: 112.33081817626953\n",
            "Training epoch 15642/1000000, d_loss: -190.67039489746094,  g_loss: 252.19424438476562\n",
            "Training epoch 15643/1000000, d_loss: -124.33721923828125,  g_loss: 49.21965789794922\n",
            "Training epoch 15644/1000000, d_loss: -540.0659790039062,  g_loss: 744.8284912109375\n",
            "Training epoch 15645/1000000, d_loss: -103.8104476928711,  g_loss: 133.43673706054688\n",
            "Training epoch 15646/1000000, d_loss: -16.17325210571289,  g_loss: 165.49063110351562\n",
            "Training epoch 15647/1000000, d_loss: -26.567611694335938,  g_loss: 98.56434631347656\n",
            "Training epoch 15648/1000000, d_loss: -147.898193359375,  g_loss: 96.1089096069336\n",
            "Training epoch 15649/1000000, d_loss: -137.12460327148438,  g_loss: 58.47667694091797\n",
            "Training epoch 15650/1000000, d_loss: -42.90397644042969,  g_loss: 54.483680725097656\n",
            "Training epoch 15651/1000000, d_loss: -139.505615234375,  g_loss: 90.56461334228516\n",
            "Training epoch 15652/1000000, d_loss: -260.5428161621094,  g_loss: 28.743518829345703\n",
            "Training epoch 15653/1000000, d_loss: -436.5997314453125,  g_loss: -126.21356201171875\n",
            "Training epoch 15654/1000000, d_loss: -96.08270263671875,  g_loss: 2.301597833633423\n",
            "Training epoch 15655/1000000, d_loss: -24.143871307373047,  g_loss: 39.9742431640625\n",
            "Training epoch 15656/1000000, d_loss: -74.03064727783203,  g_loss: 84.08769226074219\n",
            "Training epoch 15657/1000000, d_loss: -178.3302764892578,  g_loss: 60.755035400390625\n",
            "Training epoch 15658/1000000, d_loss: -343.4773254394531,  g_loss: -70.19577026367188\n",
            "Training epoch 15659/1000000, d_loss: -275.22998046875,  g_loss: -21.572647094726562\n",
            "Training epoch 15660/1000000, d_loss: -31.602949142456055,  g_loss: 21.627471923828125\n",
            "Training epoch 15661/1000000, d_loss: 195.8527374267578,  g_loss: 14.669933319091797\n",
            "Training epoch 15662/1000000, d_loss: -17.98834228515625,  g_loss: 10.672880172729492\n",
            "Training epoch 15663/1000000, d_loss: -134.0569305419922,  g_loss: -37.500282287597656\n",
            "Training epoch 15664/1000000, d_loss: -130.13687133789062,  g_loss: 176.56556701660156\n",
            "Training epoch 15665/1000000, d_loss: -424.26593017578125,  g_loss: 258.5754699707031\n",
            "Training epoch 15666/1000000, d_loss: 62.00169372558594,  g_loss: -49.103904724121094\n",
            "Training epoch 15667/1000000, d_loss: -232.51414489746094,  g_loss: 265.9493103027344\n",
            "Training epoch 15668/1000000, d_loss: -184.80491638183594,  g_loss: -35.6485595703125\n",
            "Training epoch 15669/1000000, d_loss: -316.33636474609375,  g_loss: -89.01397705078125\n",
            "Training epoch 15670/1000000, d_loss: -250.2596435546875,  g_loss: -169.15557861328125\n",
            "Training epoch 15671/1000000, d_loss: -50.444847106933594,  g_loss: -21.93753433227539\n",
            "Training epoch 15672/1000000, d_loss: -674.03515625,  g_loss: -305.73431396484375\n",
            "Training epoch 15673/1000000, d_loss: 27.529773712158203,  g_loss: 11.327957153320312\n",
            "Training epoch 15674/1000000, d_loss: 38.838165283203125,  g_loss: 11.730198860168457\n",
            "Training epoch 15675/1000000, d_loss: -169.2556915283203,  g_loss: 209.71597290039062\n",
            "Training epoch 15676/1000000, d_loss: -231.2392578125,  g_loss: 49.922401428222656\n",
            "Training epoch 15677/1000000, d_loss: 32.83216857910156,  g_loss: 127.02388000488281\n",
            "Training epoch 15678/1000000, d_loss: -56.757625579833984,  g_loss: 129.05319213867188\n",
            "Training epoch 15679/1000000, d_loss: -148.98348999023438,  g_loss: 209.16964721679688\n",
            "Training epoch 15680/1000000, d_loss: -196.44097900390625,  g_loss: 232.5075225830078\n",
            "Training epoch 15681/1000000, d_loss: -139.3282928466797,  g_loss: 118.14949035644531\n",
            "Training epoch 15682/1000000, d_loss: -340.74871826171875,  g_loss: -18.41475486755371\n",
            "Training epoch 15683/1000000, d_loss: -138.58901977539062,  g_loss: -21.266149520874023\n",
            "Training epoch 15684/1000000, d_loss: -177.51425170898438,  g_loss: 7.969742774963379\n",
            "Training epoch 15685/1000000, d_loss: -46.89535903930664,  g_loss: 21.215824127197266\n",
            "Training epoch 15686/1000000, d_loss: 44.00715637207031,  g_loss: 35.35346221923828\n",
            "Training epoch 15687/1000000, d_loss: -41.77122116088867,  g_loss: 97.83265686035156\n",
            "Training epoch 15688/1000000, d_loss: -32.23674011230469,  g_loss: 171.47018432617188\n",
            "Training epoch 15689/1000000, d_loss: -112.51773834228516,  g_loss: 20.28860092163086\n",
            "Training epoch 15690/1000000, d_loss: -48.6381950378418,  g_loss: 7.087408065795898\n",
            "Training epoch 15691/1000000, d_loss: -152.50091552734375,  g_loss: 19.249492645263672\n",
            "Training epoch 15692/1000000, d_loss: -652.9456787109375,  g_loss: -2.3985707759857178\n",
            "Training epoch 15693/1000000, d_loss: -41.802093505859375,  g_loss: 14.58499526977539\n",
            "Training epoch 15694/1000000, d_loss: -121.79627227783203,  g_loss: -17.681499481201172\n",
            "Training epoch 15695/1000000, d_loss: -73.69145202636719,  g_loss: -7.107148170471191\n",
            "Training epoch 15696/1000000, d_loss: -95.89083099365234,  g_loss: -17.363689422607422\n",
            "Training epoch 15697/1000000, d_loss: -69.4664077758789,  g_loss: 12.816350936889648\n",
            "Training epoch 15698/1000000, d_loss: -83.60565185546875,  g_loss: 92.44232940673828\n",
            "Training epoch 15699/1000000, d_loss: -155.80899047851562,  g_loss: 14.053349494934082\n",
            "Training epoch 15700/1000000, d_loss: -63.765708923339844,  g_loss: 93.45953369140625\n",
            "Training epoch 15701/1000000, d_loss: -154.63748168945312,  g_loss: -0.01700592041015625\n",
            "Training epoch 15702/1000000, d_loss: -50.51007080078125,  g_loss: 56.59892654418945\n",
            "Training epoch 15703/1000000, d_loss: -381.73065185546875,  g_loss: 4.641016960144043\n",
            "Training epoch 15704/1000000, d_loss: -39.11675262451172,  g_loss: 18.27121353149414\n",
            "Training epoch 15705/1000000, d_loss: -654.2904052734375,  g_loss: -66.46432495117188\n",
            "Training epoch 15706/1000000, d_loss: -19.101234436035156,  g_loss: 25.224708557128906\n",
            "Training epoch 15707/1000000, d_loss: -353.7238464355469,  g_loss: -77.24996185302734\n",
            "Training epoch 15708/1000000, d_loss: 23.886383056640625,  g_loss: -10.098163604736328\n",
            "Training epoch 15709/1000000, d_loss: -122.13047790527344,  g_loss: 41.244529724121094\n",
            "Training epoch 15710/1000000, d_loss: -1214.69482421875,  g_loss: -302.747314453125\n",
            "Training epoch 15711/1000000, d_loss: 131.01763916015625,  g_loss: 14.668661117553711\n",
            "Training epoch 15712/1000000, d_loss: 193.80587768554688,  g_loss: -58.48215866088867\n",
            "Training epoch 15713/1000000, d_loss: -36.22026443481445,  g_loss: 4.553601264953613\n",
            "Training epoch 15714/1000000, d_loss: -166.4748077392578,  g_loss: -44.56379699707031\n",
            "Training epoch 15715/1000000, d_loss: -489.0126647949219,  g_loss: -13.28611946105957\n",
            "Training epoch 15716/1000000, d_loss: -8.748313903808594,  g_loss: 87.17112731933594\n",
            "Training epoch 15717/1000000, d_loss: 67.17190551757812,  g_loss: 107.64512634277344\n",
            "Training epoch 15718/1000000, d_loss: -139.9193878173828,  g_loss: -14.797560691833496\n",
            "Training epoch 15719/1000000, d_loss: -118.52241516113281,  g_loss: 27.072917938232422\n",
            "Training epoch 15720/1000000, d_loss: -246.58050537109375,  g_loss: -24.317623138427734\n",
            "Training epoch 15721/1000000, d_loss: -100.36771392822266,  g_loss: 50.778587341308594\n",
            "Training epoch 15722/1000000, d_loss: -123.70184326171875,  g_loss: -28.26727867126465\n",
            "Training epoch 15723/1000000, d_loss: -627.9481811523438,  g_loss: -52.63423156738281\n",
            "Training epoch 15724/1000000, d_loss: -608.808349609375,  g_loss: 1.904327392578125\n",
            "Training epoch 15725/1000000, d_loss: -111.30561828613281,  g_loss: 7.623509407043457\n",
            "Training epoch 15726/1000000, d_loss: 35.02887725830078,  g_loss: 67.73835754394531\n",
            "Training epoch 15727/1000000, d_loss: -2706.451171875,  g_loss: -252.08645629882812\n",
            "Training epoch 15728/1000000, d_loss: 347.5208740234375,  g_loss: -308.1207275390625\n",
            "Training epoch 15729/1000000, d_loss: -145.42091369628906,  g_loss: -154.74697875976562\n",
            "Training epoch 15730/1000000, d_loss: -63.489845275878906,  g_loss: 97.42623138427734\n",
            "Training epoch 15731/1000000, d_loss: -46.642311096191406,  g_loss: 156.7073211669922\n",
            "Training epoch 15732/1000000, d_loss: 98.13758850097656,  g_loss: 191.5094451904297\n",
            "Training epoch 15733/1000000, d_loss: -86.79293823242188,  g_loss: 61.27968978881836\n",
            "Training epoch 15734/1000000, d_loss: -107.7772216796875,  g_loss: 82.98382568359375\n",
            "Training epoch 15735/1000000, d_loss: -150.39620971679688,  g_loss: 87.99803161621094\n",
            "Training epoch 15736/1000000, d_loss: -544.4160766601562,  g_loss: 519.4263916015625\n",
            "Training epoch 15737/1000000, d_loss: -65.6785888671875,  g_loss: 131.90809631347656\n",
            "Training epoch 15738/1000000, d_loss: -278.6333923339844,  g_loss: 57.511940002441406\n",
            "Training epoch 15739/1000000, d_loss: -67.0517578125,  g_loss: 13.732061386108398\n",
            "Training epoch 15740/1000000, d_loss: -85.90019226074219,  g_loss: 48.95948028564453\n",
            "Training epoch 15741/1000000, d_loss: -192.25482177734375,  g_loss: 152.39407348632812\n",
            "Training epoch 15742/1000000, d_loss: -348.28350830078125,  g_loss: 268.2825012207031\n",
            "Training epoch 15743/1000000, d_loss: -162.32293701171875,  g_loss: 43.35186004638672\n",
            "Training epoch 15744/1000000, d_loss: -183.69293212890625,  g_loss: 23.145719528198242\n",
            "Training epoch 15745/1000000, d_loss: -24.87053871154785,  g_loss: 38.34846496582031\n",
            "Training epoch 15746/1000000, d_loss: -196.62185668945312,  g_loss: 73.55371856689453\n",
            "Training epoch 15747/1000000, d_loss: -132.32025146484375,  g_loss: 15.968158721923828\n",
            "Training epoch 15748/1000000, d_loss: -172.05520629882812,  g_loss: -11.437949180603027\n",
            "Training epoch 15749/1000000, d_loss: -219.19198608398438,  g_loss: 13.354389190673828\n",
            "Training epoch 15750/1000000, d_loss: -659.306396484375,  g_loss: -77.06329345703125\n",
            "Training epoch 15751/1000000, d_loss: -22.383865356445312,  g_loss: 33.05962371826172\n",
            "Training epoch 15752/1000000, d_loss: -43.05422592163086,  g_loss: 23.17856788635254\n",
            "Training epoch 15753/1000000, d_loss: -1587.7034912109375,  g_loss: 15.370952606201172\n",
            "Training epoch 15754/1000000, d_loss: -44.32550811767578,  g_loss: -42.21461486816406\n",
            "Training epoch 15755/1000000, d_loss: -1045.3785400390625,  g_loss: -60.10145568847656\n",
            "Training epoch 15756/1000000, d_loss: -276.1730651855469,  g_loss: -4.3345184326171875\n",
            "Training epoch 15757/1000000, d_loss: -34.17916488647461,  g_loss: -18.974863052368164\n",
            "Training epoch 15758/1000000, d_loss: -57.104087829589844,  g_loss: -44.03666305541992\n",
            "Training epoch 15759/1000000, d_loss: -56.06504440307617,  g_loss: 29.764108657836914\n",
            "Training epoch 15760/1000000, d_loss: 5.043516159057617,  g_loss: 8.253265380859375\n",
            "Training epoch 15761/1000000, d_loss: -38.3095588684082,  g_loss: 36.55921173095703\n",
            "Training epoch 15762/1000000, d_loss: -88.6076431274414,  g_loss: 4.732488632202148\n",
            "Training epoch 15763/1000000, d_loss: 47.160064697265625,  g_loss: -25.10317611694336\n",
            "Training epoch 15764/1000000, d_loss: -143.6400146484375,  g_loss: 41.200958251953125\n",
            "Training epoch 15765/1000000, d_loss: -172.60140991210938,  g_loss: 115.02108764648438\n",
            "Training epoch 15766/1000000, d_loss: -127.63524627685547,  g_loss: -17.480871200561523\n",
            "Training epoch 15767/1000000, d_loss: -87.04740142822266,  g_loss: 9.19723892211914\n",
            "Training epoch 15768/1000000, d_loss: -102.35145568847656,  g_loss: -46.396629333496094\n",
            "Training epoch 15769/1000000, d_loss: -812.2430419921875,  g_loss: -197.22024536132812\n",
            "Training epoch 15770/1000000, d_loss: -721.7501220703125,  g_loss: -389.8305358886719\n",
            "Training epoch 15771/1000000, d_loss: -136.56594848632812,  g_loss: -106.156005859375\n",
            "Training epoch 15772/1000000, d_loss: -25.426589965820312,  g_loss: 189.15945434570312\n",
            "Training epoch 15773/1000000, d_loss: -105.14126586914062,  g_loss: 260.74200439453125\n",
            "Training epoch 15774/1000000, d_loss: -142.0771484375,  g_loss: 65.34612274169922\n",
            "Training epoch 15775/1000000, d_loss: -6.102882385253906,  g_loss: 176.27047729492188\n",
            "Training epoch 15776/1000000, d_loss: -157.67245483398438,  g_loss: 320.17608642578125\n",
            "Training epoch 15777/1000000, d_loss: -266.6316833496094,  g_loss: 743.8187255859375\n",
            "Training epoch 15778/1000000, d_loss: -140.78070068359375,  g_loss: 30.833585739135742\n",
            "Training epoch 15779/1000000, d_loss: -67.76399230957031,  g_loss: 90.26582336425781\n",
            "Training epoch 15780/1000000, d_loss: -68.93789672851562,  g_loss: 165.16273498535156\n",
            "Training epoch 15781/1000000, d_loss: -73.72948455810547,  g_loss: 105.29910278320312\n",
            "Training epoch 15782/1000000, d_loss: -75.18592071533203,  g_loss: 93.49725341796875\n",
            "Training epoch 15783/1000000, d_loss: -212.0376434326172,  g_loss: 73.32012176513672\n",
            "Training epoch 15784/1000000, d_loss: -490.52325439453125,  g_loss: 148.92100524902344\n",
            "Training epoch 15785/1000000, d_loss: 48.569313049316406,  g_loss: 26.208580017089844\n",
            "Training epoch 15786/1000000, d_loss: -40.40039825439453,  g_loss: 23.902910232543945\n",
            "Training epoch 15787/1000000, d_loss: -69.90274047851562,  g_loss: 53.13941192626953\n",
            "Training epoch 15788/1000000, d_loss: -51.57794952392578,  g_loss: 69.14451599121094\n",
            "Training epoch 15789/1000000, d_loss: -84.27401733398438,  g_loss: 60.97467803955078\n",
            "Training epoch 15790/1000000, d_loss: -487.78887939453125,  g_loss: -95.69001770019531\n",
            "Training epoch 15791/1000000, d_loss: -131.0201416015625,  g_loss: -27.333341598510742\n",
            "Training epoch 15792/1000000, d_loss: -558.940185546875,  g_loss: 21.56142234802246\n",
            "Training epoch 15793/1000000, d_loss: -153.13975524902344,  g_loss: -5.466553688049316\n",
            "Training epoch 15794/1000000, d_loss: -109.92941284179688,  g_loss: 8.462400436401367\n",
            "Training epoch 15795/1000000, d_loss: 73.40316772460938,  g_loss: 27.96505355834961\n",
            "Training epoch 15796/1000000, d_loss: -286.0761413574219,  g_loss: -141.97998046875\n",
            "Training epoch 15797/1000000, d_loss: 507.6816101074219,  g_loss: 14.160552978515625\n",
            "Training epoch 15798/1000000, d_loss: -177.2935333251953,  g_loss: 69.10025024414062\n",
            "Training epoch 15799/1000000, d_loss: -92.38938903808594,  g_loss: 27.63408851623535\n",
            "Training epoch 15800/1000000, d_loss: -29.86394500732422,  g_loss: 55.87755584716797\n",
            "Training epoch 15801/1000000, d_loss: -536.3541870117188,  g_loss: -10.19918441772461\n",
            "Training epoch 15802/1000000, d_loss: -25.20049476623535,  g_loss: -11.723329544067383\n",
            "Training epoch 15803/1000000, d_loss: -554.5244750976562,  g_loss: -15.21505355834961\n",
            "Training epoch 15804/1000000, d_loss: -186.1189727783203,  g_loss: 63.29653549194336\n",
            "Training epoch 15805/1000000, d_loss: -334.33807373046875,  g_loss: -63.422245025634766\n",
            "Training epoch 15806/1000000, d_loss: -441.00286865234375,  g_loss: -256.8114318847656\n",
            "Training epoch 15807/1000000, d_loss: 89.67840576171875,  g_loss: -201.4437713623047\n",
            "Training epoch 15808/1000000, d_loss: -106.42757415771484,  g_loss: 160.97650146484375\n",
            "Training epoch 15809/1000000, d_loss: -123.22127532958984,  g_loss: 59.51586151123047\n",
            "Training epoch 15810/1000000, d_loss: -131.06654357910156,  g_loss: 128.036865234375\n",
            "Training epoch 15811/1000000, d_loss: 10.138778686523438,  g_loss: 7.482272148132324\n",
            "Training epoch 15812/1000000, d_loss: -199.25689697265625,  g_loss: 0.9703810214996338\n",
            "Training epoch 15813/1000000, d_loss: -22.54661750793457,  g_loss: 4.691978454589844\n",
            "Training epoch 15814/1000000, d_loss: -91.55625915527344,  g_loss: 132.28500366210938\n",
            "Training epoch 15815/1000000, d_loss: -126.27192687988281,  g_loss: 59.75978469848633\n",
            "Training epoch 15816/1000000, d_loss: -149.03428649902344,  g_loss: 15.807890892028809\n",
            "Training epoch 15817/1000000, d_loss: -369.5845031738281,  g_loss: -52.388572692871094\n",
            "Training epoch 15818/1000000, d_loss: -91.19994354248047,  g_loss: 74.123291015625\n",
            "Training epoch 15819/1000000, d_loss: -164.30967712402344,  g_loss: 70.33872985839844\n",
            "Training epoch 15820/1000000, d_loss: -81.5736083984375,  g_loss: 135.66419982910156\n",
            "Training epoch 15821/1000000, d_loss: -296.3614807128906,  g_loss: 31.407512664794922\n",
            "Training epoch 15822/1000000, d_loss: -65.39098358154297,  g_loss: 118.2856674194336\n",
            "Training epoch 15823/1000000, d_loss: -136.8802490234375,  g_loss: 107.75260162353516\n",
            "Training epoch 15824/1000000, d_loss: -158.0145263671875,  g_loss: 19.129995346069336\n",
            "Training epoch 15825/1000000, d_loss: -127.06443786621094,  g_loss: 7.0430908203125\n",
            "Training epoch 15826/1000000, d_loss: -170.5143280029297,  g_loss: 38.39057159423828\n",
            "Training epoch 15827/1000000, d_loss: -201.57778930664062,  g_loss: 124.42454528808594\n",
            "Training epoch 15828/1000000, d_loss: -46.674835205078125,  g_loss: 72.9114761352539\n",
            "Training epoch 15829/1000000, d_loss: -130.91055297851562,  g_loss: 291.6945495605469\n",
            "Training epoch 15830/1000000, d_loss: -301.7325744628906,  g_loss: -13.114086151123047\n",
            "Training epoch 15831/1000000, d_loss: -631.140380859375,  g_loss: -43.35546112060547\n",
            "Training epoch 15832/1000000, d_loss: 44.37724304199219,  g_loss: -10.960027694702148\n",
            "Training epoch 15833/1000000, d_loss: 218.13868713378906,  g_loss: 18.170101165771484\n",
            "Training epoch 15834/1000000, d_loss: 16.02613067626953,  g_loss: 17.759933471679688\n",
            "Training epoch 15835/1000000, d_loss: -40.04888916015625,  g_loss: 2.622279644012451\n",
            "Training epoch 15836/1000000, d_loss: -245.0214385986328,  g_loss: 1.8182446956634521\n",
            "Training epoch 15837/1000000, d_loss: -58.33483123779297,  g_loss: -10.0655517578125\n",
            "Training epoch 15838/1000000, d_loss: -62.48621368408203,  g_loss: -7.971493721008301\n",
            "Training epoch 15839/1000000, d_loss: -130.72332763671875,  g_loss: 14.51962947845459\n",
            "Training epoch 15840/1000000, d_loss: -149.0771484375,  g_loss: -0.9634122848510742\n",
            "Training epoch 15841/1000000, d_loss: -930.4771118164062,  g_loss: -97.60957336425781\n",
            "Training epoch 15842/1000000, d_loss: 48.03499221801758,  g_loss: 4.480963230133057\n",
            "Training epoch 15843/1000000, d_loss: -128.04811096191406,  g_loss: -21.474231719970703\n",
            "Training epoch 15844/1000000, d_loss: -157.33981323242188,  g_loss: -54.14529037475586\n",
            "Training epoch 15845/1000000, d_loss: -152.7602996826172,  g_loss: -5.215414047241211\n",
            "Training epoch 15846/1000000, d_loss: -36.31182861328125,  g_loss: 28.746307373046875\n",
            "Training epoch 15847/1000000, d_loss: -95.623046875,  g_loss: 22.523258209228516\n",
            "Training epoch 15848/1000000, d_loss: -99.01175689697266,  g_loss: 12.845996856689453\n",
            "Training epoch 15849/1000000, d_loss: -385.36279296875,  g_loss: -27.09453010559082\n",
            "Training epoch 15850/1000000, d_loss: -163.74252319335938,  g_loss: 13.7694091796875\n",
            "Training epoch 15851/1000000, d_loss: -82.74272918701172,  g_loss: -13.87919807434082\n",
            "Training epoch 15852/1000000, d_loss: -102.37232208251953,  g_loss: -37.08693313598633\n",
            "Training epoch 15853/1000000, d_loss: 74.830078125,  g_loss: 1.6413341760635376\n",
            "Training epoch 15854/1000000, d_loss: -115.81596374511719,  g_loss: -23.017780303955078\n",
            "Training epoch 15855/1000000, d_loss: -2063.502197265625,  g_loss: -103.12162780761719\n",
            "Training epoch 15856/1000000, d_loss: -45.693572998046875,  g_loss: -0.3647029399871826\n",
            "Training epoch 15857/1000000, d_loss: -146.79714965820312,  g_loss: -54.6065788269043\n",
            "Training epoch 15858/1000000, d_loss: 15.738700866699219,  g_loss: -13.969364166259766\n",
            "Training epoch 15859/1000000, d_loss: -749.1952514648438,  g_loss: -72.69520568847656\n",
            "Training epoch 15860/1000000, d_loss: -85.12744140625,  g_loss: -110.40853881835938\n",
            "Training epoch 15861/1000000, d_loss: -3.6951904296875,  g_loss: 17.59326171875\n",
            "Training epoch 15862/1000000, d_loss: -108.92060089111328,  g_loss: 29.34091567993164\n",
            "Training epoch 15863/1000000, d_loss: -31.582275390625,  g_loss: -35.655372619628906\n",
            "Training epoch 15864/1000000, d_loss: -160.78509521484375,  g_loss: -52.19182586669922\n",
            "Training epoch 15865/1000000, d_loss: -149.33203125,  g_loss: 70.7915267944336\n",
            "Training epoch 15866/1000000, d_loss: -65.48545837402344,  g_loss: 29.3577938079834\n",
            "Training epoch 15867/1000000, d_loss: -16.271732330322266,  g_loss: 6.090680122375488\n",
            "Training epoch 15868/1000000, d_loss: -241.42442321777344,  g_loss: 11.775386810302734\n",
            "Training epoch 15869/1000000, d_loss: -164.17616271972656,  g_loss: 8.519049644470215\n",
            "Training epoch 15870/1000000, d_loss: -197.3137969970703,  g_loss: -24.402441024780273\n",
            "Training epoch 15871/1000000, d_loss: -112.10072326660156,  g_loss: 6.977544784545898\n",
            "Training epoch 15872/1000000, d_loss: -646.2822265625,  g_loss: -141.11398315429688\n",
            "Training epoch 15873/1000000, d_loss: -0.8750534057617188,  g_loss: 49.07050323486328\n",
            "Training epoch 15874/1000000, d_loss: -77.80827331542969,  g_loss: 25.71270179748535\n",
            "Training epoch 15875/1000000, d_loss: -295.1885681152344,  g_loss: -16.231964111328125\n",
            "Training epoch 15876/1000000, d_loss: -46.266136169433594,  g_loss: -12.787046432495117\n",
            "Training epoch 15877/1000000, d_loss: -125.6727066040039,  g_loss: 18.96529197692871\n",
            "Training epoch 15878/1000000, d_loss: -1936.1806640625,  g_loss: -76.6071548461914\n",
            "Training epoch 15879/1000000, d_loss: 133.87753295898438,  g_loss: -32.640838623046875\n",
            "Training epoch 15880/1000000, d_loss: -177.37918090820312,  g_loss: -17.193687438964844\n",
            "Training epoch 15881/1000000, d_loss: -8.92586612701416,  g_loss: 49.54536437988281\n",
            "Training epoch 15882/1000000, d_loss: 38.43285369873047,  g_loss: 135.7481689453125\n",
            "Training epoch 15883/1000000, d_loss: -9.016250610351562,  g_loss: 2.791457176208496\n",
            "Training epoch 15884/1000000, d_loss: -34.258636474609375,  g_loss: -23.22861671447754\n",
            "Training epoch 15885/1000000, d_loss: -48.61932373046875,  g_loss: 55.06096649169922\n",
            "Training epoch 15886/1000000, d_loss: -106.65067291259766,  g_loss: -26.030088424682617\n",
            "Training epoch 15887/1000000, d_loss: -394.5467224121094,  g_loss: -45.553253173828125\n",
            "Training epoch 15888/1000000, d_loss: -179.02536010742188,  g_loss: -41.335044860839844\n",
            "Training epoch 15889/1000000, d_loss: -72.70459747314453,  g_loss: -32.82045364379883\n",
            "Training epoch 15890/1000000, d_loss: -121.49067687988281,  g_loss: 302.7027587890625\n",
            "Training epoch 15891/1000000, d_loss: -94.74760437011719,  g_loss: -23.129709243774414\n",
            "Training epoch 15892/1000000, d_loss: -51.203819274902344,  g_loss: 34.47026062011719\n",
            "Training epoch 15893/1000000, d_loss: -112.38752746582031,  g_loss: -28.110755920410156\n",
            "Training epoch 15894/1000000, d_loss: -80.47531127929688,  g_loss: -21.62108039855957\n",
            "Training epoch 15895/1000000, d_loss: -194.33721923828125,  g_loss: 125.25662994384766\n",
            "Training epoch 15896/1000000, d_loss: -51.30842590332031,  g_loss: -107.32679748535156\n",
            "Training epoch 15897/1000000, d_loss: -83.95584106445312,  g_loss: -21.941198348999023\n",
            "Training epoch 15898/1000000, d_loss: -53.643898010253906,  g_loss: -63.019493103027344\n",
            "Training epoch 15899/1000000, d_loss: -46.668983459472656,  g_loss: -41.06550598144531\n",
            "Training epoch 15900/1000000, d_loss: -295.3837890625,  g_loss: -60.78936767578125\n",
            "Training epoch 15901/1000000, d_loss: -590.5768432617188,  g_loss: -81.52437591552734\n",
            "Training epoch 15902/1000000, d_loss: -141.58648681640625,  g_loss: -62.706634521484375\n",
            "Training epoch 15903/1000000, d_loss: -193.0493621826172,  g_loss: -41.9196891784668\n",
            "Training epoch 15904/1000000, d_loss: -620.1554565429688,  g_loss: -114.30282592773438\n",
            "Training epoch 15905/1000000, d_loss: 21.699859619140625,  g_loss: -67.7856216430664\n",
            "Training epoch 15906/1000000, d_loss: -77.27119445800781,  g_loss: 31.436161041259766\n",
            "Training epoch 15907/1000000, d_loss: -294.6572570800781,  g_loss: 271.53106689453125\n",
            "Training epoch 15908/1000000, d_loss: -20.605945587158203,  g_loss: 103.81553649902344\n",
            "Training epoch 15909/1000000, d_loss: -56.521888732910156,  g_loss: -55.7498664855957\n",
            "Training epoch 15910/1000000, d_loss: -154.4766082763672,  g_loss: 18.144264221191406\n",
            "Training epoch 15911/1000000, d_loss: -114.3209457397461,  g_loss: -33.06853485107422\n",
            "Training epoch 15912/1000000, d_loss: -186.09591674804688,  g_loss: 7.983695983886719\n",
            "Training epoch 15913/1000000, d_loss: -217.95394897460938,  g_loss: -90.70054626464844\n",
            "Training epoch 15914/1000000, d_loss: -250.0577392578125,  g_loss: 58.442588806152344\n",
            "Training epoch 15915/1000000, d_loss: -137.06851196289062,  g_loss: -44.66404724121094\n",
            "Training epoch 15916/1000000, d_loss: -210.11529541015625,  g_loss: -8.541236877441406\n",
            "Training epoch 15917/1000000, d_loss: -14.209451675415039,  g_loss: -0.8178021907806396\n",
            "Training epoch 15918/1000000, d_loss: -15.609451293945312,  g_loss: 23.115314483642578\n",
            "Training epoch 15919/1000000, d_loss: -1363.0283203125,  g_loss: -110.04756164550781\n",
            "Training epoch 15920/1000000, d_loss: -205.58145141601562,  g_loss: -72.27755737304688\n",
            "Training epoch 15921/1000000, d_loss: -117.64018249511719,  g_loss: 3.6276493072509766\n",
            "Training epoch 15922/1000000, d_loss: -176.42039489746094,  g_loss: 14.41115665435791\n",
            "Training epoch 15923/1000000, d_loss: -32.97881317138672,  g_loss: 63.61460494995117\n",
            "Training epoch 15924/1000000, d_loss: -89.61241912841797,  g_loss: 23.526416778564453\n",
            "Training epoch 15925/1000000, d_loss: -87.13317108154297,  g_loss: 58.359134674072266\n",
            "Training epoch 15926/1000000, d_loss: -73.93880462646484,  g_loss: -14.775805473327637\n",
            "Training epoch 15927/1000000, d_loss: -91.58306884765625,  g_loss: 1.3380658626556396\n",
            "Training epoch 15928/1000000, d_loss: -74.46588897705078,  g_loss: 8.729385375976562\n",
            "Training epoch 15929/1000000, d_loss: -1323.36572265625,  g_loss: -79.75276184082031\n",
            "Training epoch 15930/1000000, d_loss: -40.324256896972656,  g_loss: -51.89091873168945\n",
            "Training epoch 15931/1000000, d_loss: -24.358760833740234,  g_loss: -32.82896423339844\n",
            "Training epoch 15932/1000000, d_loss: -148.52871704101562,  g_loss: 9.706659317016602\n",
            "Training epoch 15933/1000000, d_loss: -75.28492736816406,  g_loss: 15.854377746582031\n",
            "Training epoch 15934/1000000, d_loss: -42.54233932495117,  g_loss: -1.3788728713989258\n",
            "Training epoch 15935/1000000, d_loss: -251.81240844726562,  g_loss: -10.694782257080078\n",
            "Training epoch 15936/1000000, d_loss: -941.4754028320312,  g_loss: -117.54593658447266\n",
            "Training epoch 15937/1000000, d_loss: -1091.9248046875,  g_loss: -138.40892028808594\n",
            "Training epoch 15938/1000000, d_loss: -220.219482421875,  g_loss: -137.05136108398438\n",
            "Training epoch 15939/1000000, d_loss: -157.71859741210938,  g_loss: -373.8933410644531\n",
            "Training epoch 15940/1000000, d_loss: -92.50796508789062,  g_loss: 60.300209045410156\n",
            "Training epoch 15941/1000000, d_loss: -716.1094970703125,  g_loss: -409.08636474609375\n",
            "Training epoch 15942/1000000, d_loss: -4395.810546875,  g_loss: -1264.74609375\n",
            "Training epoch 15943/1000000, d_loss: 6799.466796875,  g_loss: -1767.326171875\n",
            "Training epoch 15944/1000000, d_loss: 83.9786376953125,  g_loss: -98.3303451538086\n",
            "Training epoch 15945/1000000, d_loss: 127.57551574707031,  g_loss: -197.10018920898438\n",
            "Training epoch 15946/1000000, d_loss: 389.54046630859375,  g_loss: -63.274349212646484\n",
            "Training epoch 15947/1000000, d_loss: 45.202789306640625,  g_loss: 412.2243347167969\n",
            "Training epoch 15948/1000000, d_loss: 1171.0831298828125,  g_loss: 176.27236938476562\n",
            "Training epoch 15949/1000000, d_loss: -422.71405029296875,  g_loss: 479.46746826171875\n",
            "Training epoch 15950/1000000, d_loss: -342.7983093261719,  g_loss: 697.8402099609375\n",
            "Training epoch 15951/1000000, d_loss: 581.7828369140625,  g_loss: 198.39987182617188\n",
            "Training epoch 15952/1000000, d_loss: -129.9678192138672,  g_loss: 73.61236572265625\n",
            "Training epoch 15953/1000000, d_loss: -76.50321960449219,  g_loss: 49.263145446777344\n",
            "Training epoch 15954/1000000, d_loss: -212.33267211914062,  g_loss: 54.42219543457031\n",
            "Training epoch 15955/1000000, d_loss: -176.81292724609375,  g_loss: 92.59848022460938\n",
            "Training epoch 15956/1000000, d_loss: -78.93367004394531,  g_loss: -31.65654182434082\n",
            "Training epoch 15957/1000000, d_loss: -128.51742553710938,  g_loss: 68.41421508789062\n",
            "Training epoch 15958/1000000, d_loss: -206.60488891601562,  g_loss: 125.67582702636719\n",
            "Training epoch 15959/1000000, d_loss: 178.7713623046875,  g_loss: 165.05899047851562\n",
            "Training epoch 15960/1000000, d_loss: -343.64678955078125,  g_loss: 494.00103759765625\n",
            "Training epoch 15961/1000000, d_loss: -443.2121276855469,  g_loss: 471.30352783203125\n",
            "Training epoch 15962/1000000, d_loss: -66.95768737792969,  g_loss: 151.09767150878906\n",
            "Training epoch 15963/1000000, d_loss: -193.18069458007812,  g_loss: 158.634033203125\n",
            "Training epoch 15964/1000000, d_loss: -86.04629516601562,  g_loss: 199.7154998779297\n",
            "Training epoch 15965/1000000, d_loss: 660.679931640625,  g_loss: -196.4845733642578\n",
            "Training epoch 15966/1000000, d_loss: -189.55328369140625,  g_loss: 14.347628593444824\n",
            "Training epoch 15967/1000000, d_loss: -128.19412231445312,  g_loss: -12.142584800720215\n",
            "Training epoch 15968/1000000, d_loss: -152.09194946289062,  g_loss: 58.79706573486328\n",
            "Training epoch 15969/1000000, d_loss: -120.35252380371094,  g_loss: 9.747051239013672\n",
            "Training epoch 15970/1000000, d_loss: -220.21441650390625,  g_loss: -0.3656444549560547\n",
            "Training epoch 15971/1000000, d_loss: -167.1331329345703,  g_loss: 26.363399505615234\n",
            "Training epoch 15972/1000000, d_loss: -79.97540283203125,  g_loss: 83.41747283935547\n",
            "Training epoch 15973/1000000, d_loss: -264.1037902832031,  g_loss: 98.56673431396484\n",
            "Training epoch 15974/1000000, d_loss: -84.70687866210938,  g_loss: 40.04225158691406\n",
            "Training epoch 15975/1000000, d_loss: -81.10856628417969,  g_loss: -16.006572723388672\n",
            "Training epoch 15976/1000000, d_loss: -52.764007568359375,  g_loss: 88.6824722290039\n",
            "Training epoch 15977/1000000, d_loss: -104.39017486572266,  g_loss: 20.745988845825195\n",
            "Training epoch 15978/1000000, d_loss: -336.2907409667969,  g_loss: 145.6634979248047\n",
            "Training epoch 15979/1000000, d_loss: -89.89866638183594,  g_loss: -112.26966094970703\n",
            "Training epoch 15980/1000000, d_loss: -69.29318237304688,  g_loss: -65.14872741699219\n",
            "Training epoch 15981/1000000, d_loss: -27.224899291992188,  g_loss: -42.97493362426758\n",
            "Training epoch 15982/1000000, d_loss: -11.161991119384766,  g_loss: -2.0920209884643555\n",
            "Training epoch 15983/1000000, d_loss: -170.52601623535156,  g_loss: -9.262145042419434\n",
            "Training epoch 15984/1000000, d_loss: -4.043830871582031,  g_loss: 30.788869857788086\n",
            "Training epoch 15985/1000000, d_loss: -83.42292785644531,  g_loss: 17.635295867919922\n",
            "Training epoch 15986/1000000, d_loss: -82.30363464355469,  g_loss: -18.825321197509766\n",
            "Training epoch 15987/1000000, d_loss: -158.37619018554688,  g_loss: -44.847694396972656\n",
            "Training epoch 15988/1000000, d_loss: -152.1800079345703,  g_loss: 5.958850860595703\n",
            "Training epoch 15989/1000000, d_loss: -129.9350128173828,  g_loss: 1.8659586906433105\n",
            "Training epoch 15990/1000000, d_loss: -210.54124450683594,  g_loss: -21.8020076751709\n",
            "Training epoch 15991/1000000, d_loss: -14.700813293457031,  g_loss: 2.352825164794922\n",
            "Training epoch 15992/1000000, d_loss: -113.98348999023438,  g_loss: -7.055971145629883\n",
            "Training epoch 15993/1000000, d_loss: -198.93519592285156,  g_loss: 59.26074981689453\n",
            "Training epoch 15994/1000000, d_loss: -153.0679473876953,  g_loss: 33.4702033996582\n",
            "Training epoch 15995/1000000, d_loss: -427.7817077636719,  g_loss: -87.02500915527344\n",
            "Training epoch 15996/1000000, d_loss: -63.16638946533203,  g_loss: -29.702667236328125\n",
            "Training epoch 15997/1000000, d_loss: -36.77357482910156,  g_loss: -11.042184829711914\n",
            "Training epoch 15998/1000000, d_loss: -105.96539306640625,  g_loss: 124.32500457763672\n",
            "Training epoch 15999/1000000, d_loss: -12.82571792602539,  g_loss: -18.66158676147461\n",
            "Training epoch 16000/1000000, d_loss: -352.2308654785156,  g_loss: -24.070497512817383\n",
            "Training epoch 16001/1000000, d_loss: -158.510498046875,  g_loss: 199.80982971191406\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 27ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 64/64 [00:00<00:00, 315.06it/s]\n",
            "Meshing: 100%|██████████| 15887/15887 [00:04<00:00, 3462.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_16001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_16001/assets\n",
            "Training epoch 16002/1000000, d_loss: -41.772552490234375,  g_loss: 1.987135887145996\n",
            "Training epoch 16003/1000000, d_loss: -85.11634826660156,  g_loss: 32.976627349853516\n",
            "Training epoch 16004/1000000, d_loss: -1201.23095703125,  g_loss: -79.84915161132812\n",
            "Training epoch 16005/1000000, d_loss: -59.34614562988281,  g_loss: -53.68952941894531\n",
            "Training epoch 16006/1000000, d_loss: -396.5933532714844,  g_loss: -70.68522644042969\n",
            "Training epoch 16007/1000000, d_loss: -74.70321655273438,  g_loss: 0.5682721138000488\n",
            "Training epoch 16008/1000000, d_loss: -39.82909393310547,  g_loss: -5.4286789894104\n",
            "Training epoch 16009/1000000, d_loss: -363.9013671875,  g_loss: -297.58282470703125\n",
            "Training epoch 16010/1000000, d_loss: -3.7813949584960938,  g_loss: 20.231090545654297\n",
            "Training epoch 16011/1000000, d_loss: -81.93389129638672,  g_loss: 117.45098876953125\n",
            "Training epoch 16012/1000000, d_loss: 242.35812377929688,  g_loss: 92.82195281982422\n",
            "Training epoch 16013/1000000, d_loss: -87.05219268798828,  g_loss: 26.59789276123047\n",
            "Training epoch 16014/1000000, d_loss: -49.69004440307617,  g_loss: 8.446608543395996\n",
            "Training epoch 16015/1000000, d_loss: -488.1899719238281,  g_loss: -6.371894836425781\n",
            "Training epoch 16016/1000000, d_loss: 1242.5283203125,  g_loss: -151.49034118652344\n",
            "Training epoch 16017/1000000, d_loss: -198.4845733642578,  g_loss: -56.38179397583008\n",
            "Training epoch 16018/1000000, d_loss: -49.50484848022461,  g_loss: -224.55413818359375\n",
            "Training epoch 16019/1000000, d_loss: -204.95843505859375,  g_loss: -93.36575317382812\n",
            "Training epoch 16020/1000000, d_loss: 691.03662109375,  g_loss: -77.14402770996094\n",
            "Training epoch 16021/1000000, d_loss: -89.35453796386719,  g_loss: 31.298095703125\n",
            "Training epoch 16022/1000000, d_loss: -149.09213256835938,  g_loss: 83.75251770019531\n",
            "Training epoch 16023/1000000, d_loss: -38.00511932373047,  g_loss: 3.6595540046691895\n",
            "Training epoch 16024/1000000, d_loss: -54.02737808227539,  g_loss: 69.68968963623047\n",
            "Training epoch 16025/1000000, d_loss: -293.37896728515625,  g_loss: 399.775146484375\n",
            "Training epoch 16026/1000000, d_loss: -71.52611541748047,  g_loss: -7.454071044921875\n",
            "Training epoch 16027/1000000, d_loss: -28.060928344726562,  g_loss: -3.1914730072021484\n",
            "Training epoch 16028/1000000, d_loss: -24.57769203186035,  g_loss: -24.90453338623047\n",
            "Training epoch 16029/1000000, d_loss: -100.27787780761719,  g_loss: -33.36942672729492\n",
            "Training epoch 16030/1000000, d_loss: -104.13909912109375,  g_loss: -23.96554946899414\n",
            "Training epoch 16031/1000000, d_loss: -290.41424560546875,  g_loss: -88.1910400390625\n",
            "Training epoch 16032/1000000, d_loss: -691.4839477539062,  g_loss: -226.9959716796875\n",
            "Training epoch 16033/1000000, d_loss: 109.43678283691406,  g_loss: -1.3343696594238281\n",
            "Training epoch 16034/1000000, d_loss: -271.80419921875,  g_loss: 282.4046630859375\n",
            "Training epoch 16035/1000000, d_loss: -101.88626098632812,  g_loss: 87.4866943359375\n",
            "Training epoch 16036/1000000, d_loss: -122.673828125,  g_loss: 115.0788345336914\n",
            "Training epoch 16037/1000000, d_loss: -56.85191345214844,  g_loss: 40.64692687988281\n",
            "Training epoch 16038/1000000, d_loss: -158.50509643554688,  g_loss: 111.43346405029297\n",
            "Training epoch 16039/1000000, d_loss: -230.4783935546875,  g_loss: 151.51478576660156\n",
            "Training epoch 16040/1000000, d_loss: 75.55208587646484,  g_loss: 103.97220611572266\n",
            "Training epoch 16041/1000000, d_loss: -141.34133911132812,  g_loss: 10.843032836914062\n",
            "Training epoch 16042/1000000, d_loss: -71.77162170410156,  g_loss: 44.65525817871094\n",
            "Training epoch 16043/1000000, d_loss: -109.38756561279297,  g_loss: 16.448001861572266\n",
            "Training epoch 16044/1000000, d_loss: -196.2111053466797,  g_loss: -25.246694564819336\n",
            "Training epoch 16045/1000000, d_loss: -530.4514770507812,  g_loss: -166.70130920410156\n",
            "Training epoch 16046/1000000, d_loss: -54.450035095214844,  g_loss: -23.740863800048828\n",
            "Training epoch 16047/1000000, d_loss: -207.089111328125,  g_loss: -156.99514770507812\n",
            "Training epoch 16048/1000000, d_loss: -213.80526733398438,  g_loss: -86.19754791259766\n",
            "Training epoch 16049/1000000, d_loss: 73.217041015625,  g_loss: 18.799358367919922\n",
            "Training epoch 16050/1000000, d_loss: -147.16232299804688,  g_loss: 38.3824462890625\n",
            "Training epoch 16051/1000000, d_loss: -156.72097778320312,  g_loss: 2.462294578552246\n",
            "Training epoch 16052/1000000, d_loss: -17.905868530273438,  g_loss: 24.48919105529785\n",
            "Training epoch 16053/1000000, d_loss: -290.77386474609375,  g_loss: 7.015529632568359\n",
            "Training epoch 16054/1000000, d_loss: -901.2860717773438,  g_loss: -49.60540771484375\n",
            "Training epoch 16055/1000000, d_loss: -38.62150192260742,  g_loss: 3.5460901260375977\n",
            "Training epoch 16056/1000000, d_loss: -59.196861267089844,  g_loss: -15.344718933105469\n",
            "Training epoch 16057/1000000, d_loss: -60.46017074584961,  g_loss: 84.40158081054688\n",
            "Training epoch 16058/1000000, d_loss: -342.4007873535156,  g_loss: -3.9370036125183105\n",
            "Training epoch 16059/1000000, d_loss: -25.699874877929688,  g_loss: 33.542205810546875\n",
            "Training epoch 16060/1000000, d_loss: 1.94158935546875,  g_loss: 67.10479736328125\n",
            "Training epoch 16061/1000000, d_loss: -203.76016235351562,  g_loss: 35.52385711669922\n",
            "Training epoch 16062/1000000, d_loss: -99.43434143066406,  g_loss: 61.510841369628906\n",
            "Training epoch 16063/1000000, d_loss: -88.89016723632812,  g_loss: 141.9395751953125\n",
            "Training epoch 16064/1000000, d_loss: -200.703857421875,  g_loss: 18.397214889526367\n",
            "Training epoch 16065/1000000, d_loss: -43.56745147705078,  g_loss: -11.073934555053711\n",
            "Training epoch 16066/1000000, d_loss: -372.4156188964844,  g_loss: -63.981834411621094\n",
            "Training epoch 16067/1000000, d_loss: 4034.091796875,  g_loss: 19.364261627197266\n",
            "Training epoch 16068/1000000, d_loss: -133.22134399414062,  g_loss: 25.24233627319336\n",
            "Training epoch 16069/1000000, d_loss: -131.9043731689453,  g_loss: 31.216571807861328\n",
            "Training epoch 16070/1000000, d_loss: 9.847614288330078,  g_loss: 129.77357482910156\n",
            "Training epoch 16071/1000000, d_loss: -148.26577758789062,  g_loss: 143.0360565185547\n",
            "Training epoch 16072/1000000, d_loss: -55.626075744628906,  g_loss: 74.88604736328125\n",
            "Training epoch 16073/1000000, d_loss: -46.21626281738281,  g_loss: 63.065391540527344\n",
            "Training epoch 16074/1000000, d_loss: -109.22781372070312,  g_loss: 227.06394958496094\n",
            "Training epoch 16075/1000000, d_loss: -196.61834716796875,  g_loss: 71.6119155883789\n",
            "Training epoch 16076/1000000, d_loss: -278.09417724609375,  g_loss: -25.357257843017578\n",
            "Training epoch 16077/1000000, d_loss: -349.6099548339844,  g_loss: -25.764625549316406\n",
            "Training epoch 16078/1000000, d_loss: -8.736583709716797,  g_loss: 29.091833114624023\n",
            "Training epoch 16079/1000000, d_loss: 36.395286560058594,  g_loss: 38.51639175415039\n",
            "Training epoch 16080/1000000, d_loss: -46.26911163330078,  g_loss: 34.02442169189453\n",
            "Training epoch 16081/1000000, d_loss: -39.99345397949219,  g_loss: 40.35441970825195\n",
            "Training epoch 16082/1000000, d_loss: -268.7929992675781,  g_loss: 18.985628128051758\n",
            "Training epoch 16083/1000000, d_loss: -904.072265625,  g_loss: -85.9320068359375\n",
            "Training epoch 16084/1000000, d_loss: -38.42218780517578,  g_loss: -6.082008361816406\n",
            "Training epoch 16085/1000000, d_loss: -21.536758422851562,  g_loss: -4.841762542724609\n",
            "Training epoch 16086/1000000, d_loss: -105.50767517089844,  g_loss: 20.883258819580078\n",
            "Training epoch 16087/1000000, d_loss: -55.59092330932617,  g_loss: 21.640296936035156\n",
            "Training epoch 16088/1000000, d_loss: -157.24334716796875,  g_loss: 39.06816864013672\n",
            "Training epoch 16089/1000000, d_loss: -55.949981689453125,  g_loss: 43.63895034790039\n",
            "Training epoch 16090/1000000, d_loss: -134.57850646972656,  g_loss: 113.16304779052734\n",
            "Training epoch 16091/1000000, d_loss: -51.57247543334961,  g_loss: 23.687854766845703\n",
            "Training epoch 16092/1000000, d_loss: -36.31013488769531,  g_loss: 5.051727294921875\n",
            "Training epoch 16093/1000000, d_loss: -75.05941772460938,  g_loss: 32.413883209228516\n",
            "Training epoch 16094/1000000, d_loss: -257.771728515625,  g_loss: -32.45729446411133\n",
            "Training epoch 16095/1000000, d_loss: -913.248046875,  g_loss: -132.58853149414062\n",
            "Training epoch 16096/1000000, d_loss: 7.6776323318481445,  g_loss: -115.54476928710938\n",
            "Training epoch 16097/1000000, d_loss: -387.78851318359375,  g_loss: -46.077392578125\n",
            "Training epoch 16098/1000000, d_loss: -366.1633605957031,  g_loss: -76.61499786376953\n",
            "Training epoch 16099/1000000, d_loss: 511.0083923339844,  g_loss: 68.40602111816406\n",
            "Training epoch 16100/1000000, d_loss: -98.54737854003906,  g_loss: 50.088951110839844\n",
            "Training epoch 16101/1000000, d_loss: -24.881271362304688,  g_loss: 27.70035171508789\n",
            "Training epoch 16102/1000000, d_loss: -43.93994140625,  g_loss: 17.235870361328125\n",
            "Training epoch 16103/1000000, d_loss: -170.752197265625,  g_loss: -10.540151596069336\n",
            "Training epoch 16104/1000000, d_loss: -230.3804931640625,  g_loss: -70.51880645751953\n",
            "Training epoch 16105/1000000, d_loss: -115.69464874267578,  g_loss: -29.131027221679688\n",
            "Training epoch 16106/1000000, d_loss: -133.9169921875,  g_loss: -83.24712371826172\n",
            "Training epoch 16107/1000000, d_loss: -110.58233642578125,  g_loss: 51.772090911865234\n",
            "Training epoch 16108/1000000, d_loss: -103.95320129394531,  g_loss: 10.484771728515625\n",
            "Training epoch 16109/1000000, d_loss: -137.22218322753906,  g_loss: 41.266510009765625\n",
            "Training epoch 16110/1000000, d_loss: -45.66915512084961,  g_loss: 22.164575576782227\n",
            "Training epoch 16111/1000000, d_loss: -92.9730453491211,  g_loss: 22.962974548339844\n",
            "Training epoch 16112/1000000, d_loss: -182.86256408691406,  g_loss: 59.068031311035156\n",
            "Training epoch 16113/1000000, d_loss: -115.16351318359375,  g_loss: 29.05763816833496\n",
            "Training epoch 16114/1000000, d_loss: -228.17816162109375,  g_loss: 247.6125030517578\n",
            "Training epoch 16115/1000000, d_loss: -85.19827270507812,  g_loss: 74.28044128417969\n",
            "Training epoch 16116/1000000, d_loss: -121.91425323486328,  g_loss: 122.09485626220703\n",
            "Training epoch 16117/1000000, d_loss: -97.44059753417969,  g_loss: 82.62357330322266\n",
            "Training epoch 16118/1000000, d_loss: -29.0010986328125,  g_loss: 21.93785858154297\n",
            "Training epoch 16119/1000000, d_loss: -168.5012969970703,  g_loss: -19.555137634277344\n",
            "Training epoch 16120/1000000, d_loss: -61.873077392578125,  g_loss: 38.167999267578125\n",
            "Training epoch 16121/1000000, d_loss: -151.05787658691406,  g_loss: 39.683937072753906\n",
            "Training epoch 16122/1000000, d_loss: -134.70428466796875,  g_loss: 25.368295669555664\n",
            "Training epoch 16123/1000000, d_loss: 46.8466796875,  g_loss: 58.946189880371094\n",
            "Training epoch 16124/1000000, d_loss: -280.93719482421875,  g_loss: 35.68443298339844\n",
            "Training epoch 16125/1000000, d_loss: -298.77581787109375,  g_loss: 54.704437255859375\n",
            "Training epoch 16126/1000000, d_loss: -113.0311050415039,  g_loss: 66.1871337890625\n",
            "Training epoch 16127/1000000, d_loss: -168.97439575195312,  g_loss: 8.265963554382324\n",
            "Training epoch 16128/1000000, d_loss: -1291.1278076171875,  g_loss: -55.374481201171875\n",
            "Training epoch 16129/1000000, d_loss: 2301.152099609375,  g_loss: -45.023414611816406\n",
            "Training epoch 16130/1000000, d_loss: -50.874271392822266,  g_loss: 11.856444358825684\n",
            "Training epoch 16131/1000000, d_loss: -33.144996643066406,  g_loss: -28.550220489501953\n",
            "Training epoch 16132/1000000, d_loss: 18.519277572631836,  g_loss: -83.21981811523438\n",
            "Training epoch 16133/1000000, d_loss: 131.92169189453125,  g_loss: -72.54974365234375\n",
            "Training epoch 16134/1000000, d_loss: -293.7467041015625,  g_loss: -23.376379013061523\n",
            "Training epoch 16135/1000000, d_loss: -19.555709838867188,  g_loss: -57.32926559448242\n",
            "Training epoch 16136/1000000, d_loss: 4.126850128173828,  g_loss: 17.130754470825195\n",
            "Training epoch 16137/1000000, d_loss: -132.0706787109375,  g_loss: -21.350114822387695\n",
            "Training epoch 16138/1000000, d_loss: -121.48716735839844,  g_loss: -21.968505859375\n",
            "Training epoch 16139/1000000, d_loss: -150.56507873535156,  g_loss: -37.19322204589844\n",
            "Training epoch 16140/1000000, d_loss: -126.9361343383789,  g_loss: -51.69630432128906\n",
            "Training epoch 16141/1000000, d_loss: -122.70620727539062,  g_loss: 61.18719482421875\n",
            "Training epoch 16142/1000000, d_loss: -267.5416564941406,  g_loss: -60.207977294921875\n",
            "Training epoch 16143/1000000, d_loss: -166.22911071777344,  g_loss: -47.34145736694336\n",
            "Training epoch 16144/1000000, d_loss: 26.18779754638672,  g_loss: -29.02013397216797\n",
            "Training epoch 16145/1000000, d_loss: 34.723602294921875,  g_loss: -8.819500923156738\n",
            "Training epoch 16146/1000000, d_loss: -151.2960662841797,  g_loss: 13.78805923461914\n",
            "Training epoch 16147/1000000, d_loss: -69.9823989868164,  g_loss: -41.244140625\n",
            "Training epoch 16148/1000000, d_loss: -51.554344177246094,  g_loss: -61.680992126464844\n",
            "Training epoch 16149/1000000, d_loss: -97.7988510131836,  g_loss: 133.4611358642578\n",
            "Training epoch 16150/1000000, d_loss: -85.04197692871094,  g_loss: 24.283721923828125\n",
            "Training epoch 16151/1000000, d_loss: -114.62493896484375,  g_loss: 113.05685424804688\n",
            "Training epoch 16152/1000000, d_loss: -182.81134033203125,  g_loss: -4.590269088745117\n",
            "Training epoch 16153/1000000, d_loss: -337.2967834472656,  g_loss: -100.83641052246094\n",
            "Training epoch 16154/1000000, d_loss: -229.1402587890625,  g_loss: 144.05816650390625\n",
            "Training epoch 16155/1000000, d_loss: -179.34632873535156,  g_loss: 351.0692443847656\n",
            "Training epoch 16156/1000000, d_loss: 15.033744812011719,  g_loss: 43.61518096923828\n",
            "Training epoch 16157/1000000, d_loss: -42.2194709777832,  g_loss: 43.93034362792969\n",
            "Training epoch 16158/1000000, d_loss: 10.436561584472656,  g_loss: 5.452067852020264\n",
            "Training epoch 16159/1000000, d_loss: -83.44866943359375,  g_loss: 0.23924970626831055\n",
            "Training epoch 16160/1000000, d_loss: -100.60649871826172,  g_loss: -1.8125240802764893\n",
            "Training epoch 16161/1000000, d_loss: -63.338096618652344,  g_loss: 50.216514587402344\n",
            "Training epoch 16162/1000000, d_loss: -120.2258529663086,  g_loss: 12.536901473999023\n",
            "Training epoch 16163/1000000, d_loss: -236.6129150390625,  g_loss: -63.14280700683594\n",
            "Training epoch 16164/1000000, d_loss: -3.5070152282714844,  g_loss: 50.01823043823242\n",
            "Training epoch 16165/1000000, d_loss: -132.618408203125,  g_loss: 115.29430389404297\n",
            "Training epoch 16166/1000000, d_loss: -108.30149841308594,  g_loss: 31.836257934570312\n",
            "Training epoch 16167/1000000, d_loss: -218.1665496826172,  g_loss: -26.231468200683594\n",
            "Training epoch 16168/1000000, d_loss: -57.43867492675781,  g_loss: 56.76966094970703\n",
            "Training epoch 16169/1000000, d_loss: -133.10366821289062,  g_loss: 107.95223999023438\n",
            "Training epoch 16170/1000000, d_loss: -81.7581787109375,  g_loss: 97.44181060791016\n",
            "Training epoch 16171/1000000, d_loss: -62.858306884765625,  g_loss: 144.1720733642578\n",
            "Training epoch 16172/1000000, d_loss: -39.99711990356445,  g_loss: 26.666786193847656\n",
            "Training epoch 16173/1000000, d_loss: -222.54632568359375,  g_loss: 12.83419418334961\n",
            "Training epoch 16174/1000000, d_loss: -43.70623779296875,  g_loss: -41.42487335205078\n",
            "Training epoch 16175/1000000, d_loss: -29.668834686279297,  g_loss: 2.1439995765686035\n",
            "Training epoch 16176/1000000, d_loss: -153.98097229003906,  g_loss: -27.76858901977539\n",
            "Training epoch 16177/1000000, d_loss: -118.22399139404297,  g_loss: 0.15403151512145996\n",
            "Training epoch 16178/1000000, d_loss: -111.83612060546875,  g_loss: -4.875819683074951\n",
            "Training epoch 16179/1000000, d_loss: -125.61904907226562,  g_loss: -46.97651290893555\n",
            "Training epoch 16180/1000000, d_loss: -112.3639907836914,  g_loss: -39.947845458984375\n",
            "Training epoch 16181/1000000, d_loss: -77.56913757324219,  g_loss: -6.279781341552734\n",
            "Training epoch 16182/1000000, d_loss: -118.40000915527344,  g_loss: 34.814727783203125\n",
            "Training epoch 16183/1000000, d_loss: -338.2314453125,  g_loss: -30.797731399536133\n",
            "Training epoch 16184/1000000, d_loss: -219.75845336914062,  g_loss: -56.71559143066406\n",
            "Training epoch 16185/1000000, d_loss: -709.7041625976562,  g_loss: -208.85548400878906\n",
            "Training epoch 16186/1000000, d_loss: -40.512935638427734,  g_loss: -22.530305862426758\n",
            "Training epoch 16187/1000000, d_loss: 270.40814208984375,  g_loss: 13.121856689453125\n",
            "Training epoch 16188/1000000, d_loss: -251.84127807617188,  g_loss: -37.17505645751953\n",
            "Training epoch 16189/1000000, d_loss: -110.17803955078125,  g_loss: 41.295894622802734\n",
            "Training epoch 16190/1000000, d_loss: -2968.363037109375,  g_loss: -670.390869140625\n",
            "Training epoch 16191/1000000, d_loss: 243.4838409423828,  g_loss: -205.91819763183594\n",
            "Training epoch 16192/1000000, d_loss: -197.9969024658203,  g_loss: -211.638916015625\n",
            "Training epoch 16193/1000000, d_loss: -19.83618927001953,  g_loss: 4.741703033447266\n",
            "Training epoch 16194/1000000, d_loss: -133.11477661132812,  g_loss: 29.53513526916504\n",
            "Training epoch 16195/1000000, d_loss: -90.72834777832031,  g_loss: 60.68257141113281\n",
            "Training epoch 16196/1000000, d_loss: -103.27076721191406,  g_loss: -4.7136688232421875\n",
            "Training epoch 16197/1000000, d_loss: -155.21481323242188,  g_loss: 185.7783660888672\n",
            "Training epoch 16198/1000000, d_loss: -206.79800415039062,  g_loss: 285.0218505859375\n",
            "Training epoch 16199/1000000, d_loss: -148.9781494140625,  g_loss: 304.0805969238281\n",
            "Training epoch 16200/1000000, d_loss: -85.19332885742188,  g_loss: -41.67571258544922\n",
            "Training epoch 16201/1000000, d_loss: -152.54513549804688,  g_loss: 110.51161193847656\n",
            "Training epoch 16202/1000000, d_loss: -206.00277709960938,  g_loss: -62.597537994384766\n",
            "Training epoch 16203/1000000, d_loss: -63.762107849121094,  g_loss: -29.82376480102539\n",
            "Training epoch 16204/1000000, d_loss: -133.1237335205078,  g_loss: 62.3656120300293\n",
            "Training epoch 16205/1000000, d_loss: -486.4922790527344,  g_loss: -195.2398223876953\n",
            "Training epoch 16206/1000000, d_loss: -328.6516418457031,  g_loss: -132.89230346679688\n",
            "Training epoch 16207/1000000, d_loss: -68.25315856933594,  g_loss: 3.5298502445220947\n",
            "Training epoch 16208/1000000, d_loss: -34.94215393066406,  g_loss: 39.65047836303711\n",
            "Training epoch 16209/1000000, d_loss: -45.939239501953125,  g_loss: 13.578655242919922\n",
            "Training epoch 16210/1000000, d_loss: -146.9910888671875,  g_loss: 15.51319694519043\n",
            "Training epoch 16211/1000000, d_loss: -79.45509338378906,  g_loss: 43.37299346923828\n",
            "Training epoch 16212/1000000, d_loss: -92.85398864746094,  g_loss: -37.11113739013672\n",
            "Training epoch 16213/1000000, d_loss: -673.5942993164062,  g_loss: -31.236587524414062\n",
            "Training epoch 16214/1000000, d_loss: -117.67491149902344,  g_loss: 9.052457809448242\n",
            "Training epoch 16215/1000000, d_loss: -71.93108367919922,  g_loss: 23.8931941986084\n",
            "Training epoch 16216/1000000, d_loss: -44.950111389160156,  g_loss: 46.38517761230469\n",
            "Training epoch 16217/1000000, d_loss: -251.78326416015625,  g_loss: 181.850341796875\n",
            "Training epoch 16218/1000000, d_loss: -29.908390045166016,  g_loss: -70.52871704101562\n",
            "Training epoch 16219/1000000, d_loss: -43.697906494140625,  g_loss: -32.589080810546875\n",
            "Training epoch 16220/1000000, d_loss: -75.17361450195312,  g_loss: -54.34728240966797\n",
            "Training epoch 16221/1000000, d_loss: -23.023534774780273,  g_loss: -64.60009765625\n",
            "Training epoch 16222/1000000, d_loss: -134.4556121826172,  g_loss: -63.00323486328125\n",
            "Training epoch 16223/1000000, d_loss: -784.982666015625,  g_loss: -209.97702026367188\n",
            "Training epoch 16224/1000000, d_loss: -44.21131896972656,  g_loss: -19.76310157775879\n",
            "Training epoch 16225/1000000, d_loss: -552.6267700195312,  g_loss: -186.99070739746094\n",
            "Training epoch 16226/1000000, d_loss: -22.715831756591797,  g_loss: -14.51382827758789\n",
            "Training epoch 16227/1000000, d_loss: -10.661590576171875,  g_loss: 11.007979393005371\n",
            "Training epoch 16228/1000000, d_loss: -66.25064849853516,  g_loss: 44.9301872253418\n",
            "Training epoch 16229/1000000, d_loss: -366.95941162109375,  g_loss: 33.4242057800293\n",
            "Training epoch 16230/1000000, d_loss: -43.19136047363281,  g_loss: 63.57966232299805\n",
            "Training epoch 16231/1000000, d_loss: -772.2949829101562,  g_loss: -314.0682373046875\n",
            "Training epoch 16232/1000000, d_loss: -369.46868896484375,  g_loss: -34.01488494873047\n",
            "Training epoch 16233/1000000, d_loss: -44.25520324707031,  g_loss: -77.4625244140625\n",
            "Training epoch 16234/1000000, d_loss: 412.2193603515625,  g_loss: 83.34803009033203\n",
            "Training epoch 16235/1000000, d_loss: -191.75588989257812,  g_loss: 243.2392578125\n",
            "Training epoch 16236/1000000, d_loss: -52.0704345703125,  g_loss: 109.67205810546875\n",
            "Training epoch 16237/1000000, d_loss: -140.75323486328125,  g_loss: 116.16336059570312\n",
            "Training epoch 16238/1000000, d_loss: -6.386207580566406,  g_loss: 49.95186233520508\n",
            "Training epoch 16239/1000000, d_loss: -96.82174682617188,  g_loss: 100.70094299316406\n",
            "Training epoch 16240/1000000, d_loss: -215.61244201660156,  g_loss: 65.36666107177734\n",
            "Training epoch 16241/1000000, d_loss: -95.48912048339844,  g_loss: 37.18891143798828\n",
            "Training epoch 16242/1000000, d_loss: -28.317928314208984,  g_loss: 47.038902282714844\n",
            "Training epoch 16243/1000000, d_loss: -74.05257415771484,  g_loss: 43.943912506103516\n",
            "Training epoch 16244/1000000, d_loss: -330.0451965332031,  g_loss: -6.580430030822754\n",
            "Training epoch 16245/1000000, d_loss: -29.48492431640625,  g_loss: -10.928105354309082\n",
            "Training epoch 16246/1000000, d_loss: -404.18560791015625,  g_loss: -179.77554321289062\n",
            "Training epoch 16247/1000000, d_loss: -68.80701446533203,  g_loss: 30.583465576171875\n",
            "Training epoch 16248/1000000, d_loss: -107.4365234375,  g_loss: 58.38407897949219\n",
            "Training epoch 16249/1000000, d_loss: -148.4879150390625,  g_loss: 122.82628631591797\n",
            "Training epoch 16250/1000000, d_loss: 1461.944580078125,  g_loss: 166.3536376953125\n",
            "Training epoch 16251/1000000, d_loss: -130.696533203125,  g_loss: 72.5135726928711\n",
            "Training epoch 16252/1000000, d_loss: -176.5998992919922,  g_loss: 203.30003356933594\n",
            "Training epoch 16253/1000000, d_loss: -117.03437042236328,  g_loss: 35.42778396606445\n",
            "Training epoch 16254/1000000, d_loss: -193.38465881347656,  g_loss: -14.371630668640137\n",
            "Training epoch 16255/1000000, d_loss: -351.2294921875,  g_loss: -24.500411987304688\n",
            "Training epoch 16256/1000000, d_loss: -11.385602951049805,  g_loss: 80.78638458251953\n",
            "Training epoch 16257/1000000, d_loss: -169.96519470214844,  g_loss: -3.548633575439453\n",
            "Training epoch 16258/1000000, d_loss: -43.32624053955078,  g_loss: 17.147415161132812\n",
            "Training epoch 16259/1000000, d_loss: -150.92190551757812,  g_loss: 35.956886291503906\n",
            "Training epoch 16260/1000000, d_loss: -176.1100616455078,  g_loss: -23.108341217041016\n",
            "Training epoch 16261/1000000, d_loss: -317.742431640625,  g_loss: -64.38233947753906\n",
            "Training epoch 16262/1000000, d_loss: -287.67401123046875,  g_loss: -96.61658477783203\n",
            "Training epoch 16263/1000000, d_loss: -232.77291870117188,  g_loss: 92.51921081542969\n",
            "Training epoch 16264/1000000, d_loss: -53.78278350830078,  g_loss: 4.976036548614502\n",
            "Training epoch 16265/1000000, d_loss: -178.7717742919922,  g_loss: -59.041015625\n",
            "Training epoch 16266/1000000, d_loss: -8.648666381835938,  g_loss: 41.472755432128906\n",
            "Training epoch 16267/1000000, d_loss: -538.5078735351562,  g_loss: -51.893333435058594\n",
            "Training epoch 16268/1000000, d_loss: -69.12886810302734,  g_loss: 50.90093994140625\n",
            "Training epoch 16269/1000000, d_loss: -123.30859375,  g_loss: -56.08296203613281\n",
            "Training epoch 16270/1000000, d_loss: -271.4064025878906,  g_loss: -87.7686767578125\n",
            "Training epoch 16271/1000000, d_loss: -40.643463134765625,  g_loss: -14.619478225708008\n",
            "Training epoch 16272/1000000, d_loss: -139.47293090820312,  g_loss: -52.86198425292969\n",
            "Training epoch 16273/1000000, d_loss: -144.31198120117188,  g_loss: -80.5294418334961\n",
            "Training epoch 16274/1000000, d_loss: -90.65057373046875,  g_loss: -58.30455017089844\n",
            "Training epoch 16275/1000000, d_loss: -204.6220703125,  g_loss: -107.28585815429688\n",
            "Training epoch 16276/1000000, d_loss: -328.25982666015625,  g_loss: -94.52429962158203\n",
            "Training epoch 16277/1000000, d_loss: -390.0841979980469,  g_loss: -190.6116485595703\n",
            "Training epoch 16278/1000000, d_loss: -167.96385192871094,  g_loss: -249.77841186523438\n",
            "Training epoch 16279/1000000, d_loss: -265.95318603515625,  g_loss: 90.05238342285156\n",
            "Training epoch 16280/1000000, d_loss: -103.16098022460938,  g_loss: -34.701171875\n",
            "Training epoch 16281/1000000, d_loss: -76.29227447509766,  g_loss: -14.891575813293457\n",
            "Training epoch 16282/1000000, d_loss: -103.85078430175781,  g_loss: -15.193132400512695\n",
            "Training epoch 16283/1000000, d_loss: -157.6584014892578,  g_loss: 23.914506912231445\n",
            "Training epoch 16284/1000000, d_loss: -145.43092346191406,  g_loss: 11.434514999389648\n",
            "Training epoch 16285/1000000, d_loss: -78.19358825683594,  g_loss: 11.170406341552734\n",
            "Training epoch 16286/1000000, d_loss: -67.30638885498047,  g_loss: 19.59079933166504\n",
            "Training epoch 16287/1000000, d_loss: -77.63373565673828,  g_loss: 67.38087463378906\n",
            "Training epoch 16288/1000000, d_loss: -377.8350830078125,  g_loss: -2.78493595123291\n",
            "Training epoch 16289/1000000, d_loss: -86.34282684326172,  g_loss: -34.947654724121094\n",
            "Training epoch 16290/1000000, d_loss: -57.79001235961914,  g_loss: 39.06846618652344\n",
            "Training epoch 16291/1000000, d_loss: -155.65695190429688,  g_loss: 51.401634216308594\n",
            "Training epoch 16292/1000000, d_loss: -54.746055603027344,  g_loss: -12.157718658447266\n",
            "Training epoch 16293/1000000, d_loss: -59.65895462036133,  g_loss: 34.06829071044922\n",
            "Training epoch 16294/1000000, d_loss: -173.6722412109375,  g_loss: 22.321653366088867\n",
            "Training epoch 16295/1000000, d_loss: -726.3463745117188,  g_loss: -74.63170623779297\n",
            "Training epoch 16296/1000000, d_loss: -170.06187438964844,  g_loss: -185.26953125\n",
            "Training epoch 16297/1000000, d_loss: -40.955848693847656,  g_loss: -12.035051345825195\n",
            "Training epoch 16298/1000000, d_loss: -54.064483642578125,  g_loss: -67.41834259033203\n",
            "Training epoch 16299/1000000, d_loss: 743.98388671875,  g_loss: 31.81719970703125\n",
            "Training epoch 16300/1000000, d_loss: -116.97785949707031,  g_loss: -1.1329517364501953\n",
            "Training epoch 16301/1000000, d_loss: -214.42843627929688,  g_loss: 361.4971923828125\n",
            "Training epoch 16302/1000000, d_loss: -354.851806640625,  g_loss: 598.64892578125\n",
            "Training epoch 16303/1000000, d_loss: -69.76484680175781,  g_loss: 13.614824295043945\n",
            "Training epoch 16304/1000000, d_loss: -125.22913360595703,  g_loss: -22.880638122558594\n",
            "Training epoch 16305/1000000, d_loss: -104.28804016113281,  g_loss: -41.261329650878906\n",
            "Training epoch 16306/1000000, d_loss: -173.95445251464844,  g_loss: -72.92980194091797\n",
            "Training epoch 16307/1000000, d_loss: -176.79367065429688,  g_loss: -82.31217956542969\n",
            "Training epoch 16308/1000000, d_loss: -89.9188232421875,  g_loss: -33.661224365234375\n",
            "Training epoch 16309/1000000, d_loss: 10.257492065429688,  g_loss: 36.709136962890625\n",
            "Training epoch 16310/1000000, d_loss: -63.95829391479492,  g_loss: 46.701995849609375\n",
            "Training epoch 16311/1000000, d_loss: -164.76898193359375,  g_loss: 41.9925537109375\n",
            "Training epoch 16312/1000000, d_loss: -129.6086883544922,  g_loss: 26.49445343017578\n",
            "Training epoch 16313/1000000, d_loss: -21.327163696289062,  g_loss: 70.85491180419922\n",
            "Training epoch 16314/1000000, d_loss: -23.8928279876709,  g_loss: 48.57646179199219\n",
            "Training epoch 16315/1000000, d_loss: -132.6242218017578,  g_loss: 37.386985778808594\n",
            "Training epoch 16316/1000000, d_loss: -115.57994842529297,  g_loss: 25.486679077148438\n",
            "Training epoch 16317/1000000, d_loss: -103.6661148071289,  g_loss: 24.409963607788086\n",
            "Training epoch 16318/1000000, d_loss: -257.9788818359375,  g_loss: 5.281569480895996\n",
            "Training epoch 16319/1000000, d_loss: -300.874755859375,  g_loss: -6.475831985473633\n",
            "Training epoch 16320/1000000, d_loss: -79.51655578613281,  g_loss: 33.55635070800781\n",
            "Training epoch 16321/1000000, d_loss: -137.7530517578125,  g_loss: -26.4537353515625\n",
            "Training epoch 16322/1000000, d_loss: -147.84280395507812,  g_loss: 48.10613250732422\n",
            "Training epoch 16323/1000000, d_loss: -144.1790008544922,  g_loss: -75.01744079589844\n",
            "Training epoch 16324/1000000, d_loss: -589.8347778320312,  g_loss: -119.84808349609375\n",
            "Training epoch 16325/1000000, d_loss: -64.02213287353516,  g_loss: -42.982994079589844\n",
            "Training epoch 16326/1000000, d_loss: -372.8436279296875,  g_loss: -131.40615844726562\n",
            "Training epoch 16327/1000000, d_loss: -779.1416015625,  g_loss: 45.63923263549805\n",
            "Training epoch 16328/1000000, d_loss: -401.52545166015625,  g_loss: -105.55548095703125\n",
            "Training epoch 16329/1000000, d_loss: -209.99758911132812,  g_loss: -177.01959228515625\n",
            "Training epoch 16330/1000000, d_loss: -351.7386474609375,  g_loss: 103.89659118652344\n",
            "Training epoch 16331/1000000, d_loss: -763.3797607421875,  g_loss: 20.311656951904297\n",
            "Training epoch 16332/1000000, d_loss: 65.45772552490234,  g_loss: 3.6016900539398193\n",
            "Training epoch 16333/1000000, d_loss: -56.51111602783203,  g_loss: 93.29859161376953\n",
            "Training epoch 16334/1000000, d_loss: -182.460205078125,  g_loss: 190.13027954101562\n",
            "Training epoch 16335/1000000, d_loss: -228.1600799560547,  g_loss: 270.1262512207031\n",
            "Training epoch 16336/1000000, d_loss: -77.00212097167969,  g_loss: 71.96443176269531\n",
            "Training epoch 16337/1000000, d_loss: -100.74949645996094,  g_loss: -10.521451950073242\n",
            "Training epoch 16338/1000000, d_loss: -89.63375854492188,  g_loss: 24.810606002807617\n",
            "Training epoch 16339/1000000, d_loss: -311.5430603027344,  g_loss: 318.42681884765625\n",
            "Training epoch 16340/1000000, d_loss: -192.1097869873047,  g_loss: 57.193824768066406\n",
            "Training epoch 16341/1000000, d_loss: 115.59220886230469,  g_loss: 60.3528938293457\n",
            "Training epoch 16342/1000000, d_loss: -143.45997619628906,  g_loss: 46.845703125\n",
            "Training epoch 16343/1000000, d_loss: -95.2187728881836,  g_loss: 76.65428161621094\n",
            "Training epoch 16344/1000000, d_loss: -2.821187973022461,  g_loss: 27.174854278564453\n",
            "Training epoch 16345/1000000, d_loss: -120.35267639160156,  g_loss: 42.674434661865234\n",
            "Training epoch 16346/1000000, d_loss: -10.516212463378906,  g_loss: 17.441883087158203\n",
            "Training epoch 16347/1000000, d_loss: -80.06321716308594,  g_loss: 42.898033142089844\n",
            "Training epoch 16348/1000000, d_loss: -115.60671997070312,  g_loss: 38.22480010986328\n",
            "Training epoch 16349/1000000, d_loss: -95.0670394897461,  g_loss: 25.044116973876953\n",
            "Training epoch 16350/1000000, d_loss: -209.17056274414062,  g_loss: 34.737518310546875\n",
            "Training epoch 16351/1000000, d_loss: -207.8647003173828,  g_loss: -28.96051788330078\n",
            "Training epoch 16352/1000000, d_loss: -60.535743713378906,  g_loss: 27.322603225708008\n",
            "Training epoch 16353/1000000, d_loss: -219.9796142578125,  g_loss: -16.460477828979492\n",
            "Training epoch 16354/1000000, d_loss: -65.2196273803711,  g_loss: 10.120474815368652\n",
            "Training epoch 16355/1000000, d_loss: -124.95703125,  g_loss: 13.07193374633789\n",
            "Training epoch 16356/1000000, d_loss: -75.77830505371094,  g_loss: -33.06843948364258\n",
            "Training epoch 16357/1000000, d_loss: -281.4717712402344,  g_loss: -120.37379455566406\n",
            "Training epoch 16358/1000000, d_loss: -636.5336303710938,  g_loss: -238.8282928466797\n",
            "Training epoch 16359/1000000, d_loss: -361.9541320800781,  g_loss: -147.23065185546875\n",
            "Training epoch 16360/1000000, d_loss: -75.0794677734375,  g_loss: -92.90364074707031\n",
            "Training epoch 16361/1000000, d_loss: -140.64918518066406,  g_loss: -69.25275421142578\n",
            "Training epoch 16362/1000000, d_loss: -8.275711059570312,  g_loss: 4.526028633117676\n",
            "Training epoch 16363/1000000, d_loss: -12.872993469238281,  g_loss: 44.7567253112793\n",
            "Training epoch 16364/1000000, d_loss: -147.79327392578125,  g_loss: 146.2805938720703\n",
            "Training epoch 16365/1000000, d_loss: -66.5249252319336,  g_loss: 15.955157279968262\n",
            "Training epoch 16366/1000000, d_loss: -83.38944244384766,  g_loss: 52.967193603515625\n",
            "Training epoch 16367/1000000, d_loss: -311.7345886230469,  g_loss: -38.95232391357422\n",
            "Training epoch 16368/1000000, d_loss: -202.7828369140625,  g_loss: -48.71366882324219\n",
            "Training epoch 16369/1000000, d_loss: -554.9026489257812,  g_loss: -202.97169494628906\n",
            "Training epoch 16370/1000000, d_loss: -106.16532897949219,  g_loss: -97.88534545898438\n",
            "Training epoch 16371/1000000, d_loss: 158.6584014892578,  g_loss: -13.42831802368164\n",
            "Training epoch 16372/1000000, d_loss: -270.71441650390625,  g_loss: -54.19593048095703\n",
            "Training epoch 16373/1000000, d_loss: -4.840476989746094,  g_loss: -1.0357561111450195\n",
            "Training epoch 16374/1000000, d_loss: -123.87544250488281,  g_loss: -11.257599830627441\n",
            "Training epoch 16375/1000000, d_loss: -60.9060173034668,  g_loss: 25.489879608154297\n",
            "Training epoch 16376/1000000, d_loss: -311.69549560546875,  g_loss: 45.47036361694336\n",
            "Training epoch 16377/1000000, d_loss: -389.2869567871094,  g_loss: 38.97506332397461\n",
            "Training epoch 16378/1000000, d_loss: -49.328311920166016,  g_loss: -1.3841943740844727\n",
            "Training epoch 16379/1000000, d_loss: -109.835693359375,  g_loss: 83.2069091796875\n",
            "Training epoch 16380/1000000, d_loss: -25.068405151367188,  g_loss: 45.2515754699707\n",
            "Training epoch 16381/1000000, d_loss: -180.29193115234375,  g_loss: 53.3719596862793\n",
            "Training epoch 16382/1000000, d_loss: -160.86940002441406,  g_loss: 229.07565307617188\n",
            "Training epoch 16383/1000000, d_loss: -116.71627807617188,  g_loss: -31.848188400268555\n",
            "Training epoch 16384/1000000, d_loss: -90.6788330078125,  g_loss: 58.556983947753906\n",
            "Training epoch 16385/1000000, d_loss: -63.44017791748047,  g_loss: 20.089500427246094\n",
            "Training epoch 16386/1000000, d_loss: -234.50250244140625,  g_loss: -60.471534729003906\n",
            "Training epoch 16387/1000000, d_loss: -136.16151428222656,  g_loss: 1.7689342498779297\n",
            "Training epoch 16388/1000000, d_loss: -124.154541015625,  g_loss: 142.9987030029297\n",
            "Training epoch 16389/1000000, d_loss: -184.39381408691406,  g_loss: -32.065452575683594\n",
            "Training epoch 16390/1000000, d_loss: -377.17266845703125,  g_loss: -51.349632263183594\n",
            "Training epoch 16391/1000000, d_loss: -316.0769958496094,  g_loss: -46.539642333984375\n",
            "Training epoch 16392/1000000, d_loss: -392.12286376953125,  g_loss: -158.05560302734375\n",
            "Training epoch 16393/1000000, d_loss: -2.021392822265625,  g_loss: -45.983001708984375\n",
            "Training epoch 16394/1000000, d_loss: -51.61564254760742,  g_loss: 46.64593505859375\n",
            "Training epoch 16395/1000000, d_loss: -816.458740234375,  g_loss: -226.80845642089844\n",
            "Training epoch 16396/1000000, d_loss: -193.17636108398438,  g_loss: -64.15773010253906\n",
            "Training epoch 16397/1000000, d_loss: -292.260986328125,  g_loss: 0.8669939041137695\n",
            "Training epoch 16398/1000000, d_loss: -96.1605224609375,  g_loss: 43.64109802246094\n",
            "Training epoch 16399/1000000, d_loss: 26.554092407226562,  g_loss: 128.90118408203125\n",
            "Training epoch 16400/1000000, d_loss: -384.4306640625,  g_loss: 16.129270553588867\n",
            "Training epoch 16401/1000000, d_loss: -19.371749877929688,  g_loss: 56.794349670410156\n",
            "Training epoch 16402/1000000, d_loss: -173.88775634765625,  g_loss: 55.223751068115234\n",
            "Training epoch 16403/1000000, d_loss: -172.32406616210938,  g_loss: 8.937840461730957\n",
            "Training epoch 16404/1000000, d_loss: 7.937126159667969,  g_loss: 2.3127098083496094\n",
            "Training epoch 16405/1000000, d_loss: -66.59059143066406,  g_loss: 44.428123474121094\n",
            "Training epoch 16406/1000000, d_loss: -221.9862518310547,  g_loss: 75.3989028930664\n",
            "Training epoch 16407/1000000, d_loss: -177.12176513671875,  g_loss: 70.15830993652344\n",
            "Training epoch 16408/1000000, d_loss: -111.42707824707031,  g_loss: 4.134254455566406\n",
            "Training epoch 16409/1000000, d_loss: -206.83445739746094,  g_loss: 15.331746101379395\n",
            "Training epoch 16410/1000000, d_loss: -86.98481750488281,  g_loss: 23.417095184326172\n",
            "Training epoch 16411/1000000, d_loss: -70.0103988647461,  g_loss: 32.98859405517578\n",
            "Training epoch 16412/1000000, d_loss: -55.79346466064453,  g_loss: 31.585716247558594\n",
            "Training epoch 16413/1000000, d_loss: -132.36097717285156,  g_loss: 16.503965377807617\n",
            "Training epoch 16414/1000000, d_loss: -93.98506164550781,  g_loss: -0.6310234069824219\n",
            "Training epoch 16415/1000000, d_loss: -137.60411071777344,  g_loss: 25.18059539794922\n",
            "Training epoch 16416/1000000, d_loss: -278.50018310546875,  g_loss: 24.026037216186523\n",
            "Training epoch 16417/1000000, d_loss: -85.35752868652344,  g_loss: 40.68779754638672\n",
            "Training epoch 16418/1000000, d_loss: -53.19546890258789,  g_loss: 41.376708984375\n",
            "Training epoch 16419/1000000, d_loss: -53.631072998046875,  g_loss: 53.41209411621094\n",
            "Training epoch 16420/1000000, d_loss: -214.91748046875,  g_loss: -14.220232963562012\n",
            "Training epoch 16421/1000000, d_loss: -101.62174987792969,  g_loss: 40.4329833984375\n",
            "Training epoch 16422/1000000, d_loss: -117.26692199707031,  g_loss: -42.692909240722656\n",
            "Training epoch 16423/1000000, d_loss: -174.84628295898438,  g_loss: -78.68771362304688\n",
            "Training epoch 16424/1000000, d_loss: -81.57249450683594,  g_loss: -63.977149963378906\n",
            "Training epoch 16425/1000000, d_loss: 783.1727294921875,  g_loss: -32.676795959472656\n",
            "Training epoch 16426/1000000, d_loss: 42.20404052734375,  g_loss: -53.04503631591797\n",
            "Training epoch 16427/1000000, d_loss: -49.23602294921875,  g_loss: -41.408512115478516\n",
            "Training epoch 16428/1000000, d_loss: -138.91033935546875,  g_loss: 17.698467254638672\n",
            "Training epoch 16429/1000000, d_loss: 34.12701416015625,  g_loss: -13.219860076904297\n",
            "Training epoch 16430/1000000, d_loss: -106.31443786621094,  g_loss: -31.124042510986328\n",
            "Training epoch 16431/1000000, d_loss: -31.712177276611328,  g_loss: -34.7796630859375\n",
            "Training epoch 16432/1000000, d_loss: -113.08949279785156,  g_loss: 1.121236801147461\n",
            "Training epoch 16433/1000000, d_loss: -79.14468383789062,  g_loss: -10.130697250366211\n",
            "Training epoch 16434/1000000, d_loss: -105.0685806274414,  g_loss: -28.177528381347656\n",
            "Training epoch 16435/1000000, d_loss: -113.19293212890625,  g_loss: 1.973222255706787\n",
            "Training epoch 16436/1000000, d_loss: -125.97716522216797,  g_loss: -21.853626251220703\n",
            "Training epoch 16437/1000000, d_loss: -686.09130859375,  g_loss: -49.502777099609375\n",
            "Training epoch 16438/1000000, d_loss: -74.56729125976562,  g_loss: 47.06438446044922\n",
            "Training epoch 16439/1000000, d_loss: -56.37382507324219,  g_loss: -26.113483428955078\n",
            "Training epoch 16440/1000000, d_loss: -97.17546081542969,  g_loss: 28.799150466918945\n",
            "Training epoch 16441/1000000, d_loss: -3546.956298828125,  g_loss: -293.8541259765625\n",
            "Training epoch 16442/1000000, d_loss: 259.9183654785156,  g_loss: -161.18467712402344\n",
            "Training epoch 16443/1000000, d_loss: 73.14218139648438,  g_loss: -186.42884826660156\n",
            "Training epoch 16444/1000000, d_loss: 10877.865234375,  g_loss: 14.841489791870117\n",
            "Training epoch 16445/1000000, d_loss: -25.70594596862793,  g_loss: -22.632253646850586\n",
            "Training epoch 16446/1000000, d_loss: 34.94150161743164,  g_loss: -63.93199920654297\n",
            "Training epoch 16447/1000000, d_loss: 18.471817016601562,  g_loss: 1.372568130493164\n",
            "Training epoch 16448/1000000, d_loss: -78.26499938964844,  g_loss: 84.72655487060547\n",
            "Training epoch 16449/1000000, d_loss: -127.975341796875,  g_loss: 23.544158935546875\n",
            "Training epoch 16450/1000000, d_loss: -135.14268493652344,  g_loss: 28.731603622436523\n",
            "Training epoch 16451/1000000, d_loss: -100.0804672241211,  g_loss: 52.42704772949219\n",
            "Training epoch 16452/1000000, d_loss: -18.682479858398438,  g_loss: 1.4521434307098389\n",
            "Training epoch 16453/1000000, d_loss: -146.89871215820312,  g_loss: 59.470306396484375\n",
            "Training epoch 16454/1000000, d_loss: -157.48411560058594,  g_loss: 10.656021118164062\n",
            "Training epoch 16455/1000000, d_loss: 103.79328918457031,  g_loss: 11.81631851196289\n",
            "Training epoch 16456/1000000, d_loss: -69.80477905273438,  g_loss: -0.14981460571289062\n",
            "Training epoch 16457/1000000, d_loss: -246.38888549804688,  g_loss: -31.041532516479492\n",
            "Training epoch 16458/1000000, d_loss: -219.63833618164062,  g_loss: -27.878780364990234\n",
            "Training epoch 16459/1000000, d_loss: -15.546875,  g_loss: 42.78125\n",
            "Training epoch 16460/1000000, d_loss: -43.79228210449219,  g_loss: 54.94378662109375\n",
            "Training epoch 16461/1000000, d_loss: -213.81842041015625,  g_loss: 42.717594146728516\n",
            "Training epoch 16462/1000000, d_loss: -102.75015258789062,  g_loss: 51.26742172241211\n",
            "Training epoch 16463/1000000, d_loss: -55.294654846191406,  g_loss: 54.338287353515625\n",
            "Training epoch 16464/1000000, d_loss: -390.2213134765625,  g_loss: 1.1417236328125\n",
            "Training epoch 16465/1000000, d_loss: -77.62171936035156,  g_loss: 34.40795135498047\n",
            "Training epoch 16466/1000000, d_loss: -51.818878173828125,  g_loss: 39.78907775878906\n",
            "Training epoch 16467/1000000, d_loss: -80.21794891357422,  g_loss: 64.87571716308594\n",
            "Training epoch 16468/1000000, d_loss: -39.38941192626953,  g_loss: 64.15763092041016\n",
            "Training epoch 16469/1000000, d_loss: -166.2900848388672,  g_loss: 227.38633728027344\n",
            "Training epoch 16470/1000000, d_loss: -217.9964599609375,  g_loss: 74.7721939086914\n",
            "Training epoch 16471/1000000, d_loss: -231.45335388183594,  g_loss: -85.33251953125\n",
            "Training epoch 16472/1000000, d_loss: -399.6514892578125,  g_loss: -65.73263549804688\n",
            "Training epoch 16473/1000000, d_loss: -774.6402587890625,  g_loss: -376.4615478515625\n",
            "Training epoch 16474/1000000, d_loss: 503.53033447265625,  g_loss: -38.19593811035156\n",
            "Training epoch 16475/1000000, d_loss: -126.78070068359375,  g_loss: -119.55741882324219\n",
            "Training epoch 16476/1000000, d_loss: -84.60946655273438,  g_loss: -156.3018035888672\n",
            "Training epoch 16477/1000000, d_loss: -79.50480651855469,  g_loss: 73.82176208496094\n",
            "Training epoch 16478/1000000, d_loss: -182.80770874023438,  g_loss: 127.77806091308594\n",
            "Training epoch 16479/1000000, d_loss: 67.81597137451172,  g_loss: -18.230531692504883\n",
            "Training epoch 16480/1000000, d_loss: -406.8248596191406,  g_loss: -90.5487060546875\n",
            "Training epoch 16481/1000000, d_loss: -232.33346557617188,  g_loss: 24.258604049682617\n",
            "Training epoch 16482/1000000, d_loss: -380.0775146484375,  g_loss: 95.26919555664062\n",
            "Training epoch 16483/1000000, d_loss: 6.1126708984375,  g_loss: -2.8263931274414062\n",
            "Training epoch 16484/1000000, d_loss: -154.6866455078125,  g_loss: 53.17579650878906\n",
            "Training epoch 16485/1000000, d_loss: -58.76175308227539,  g_loss: 58.72770309448242\n",
            "Training epoch 16486/1000000, d_loss: -292.70843505859375,  g_loss: -7.784902572631836\n",
            "Training epoch 16487/1000000, d_loss: -84.29662322998047,  g_loss: 80.11302947998047\n",
            "Training epoch 16488/1000000, d_loss: -167.13174438476562,  g_loss: -29.491615295410156\n",
            "Training epoch 16489/1000000, d_loss: -146.0778350830078,  g_loss: 69.25128936767578\n",
            "Training epoch 16490/1000000, d_loss: -315.179443359375,  g_loss: -71.16948699951172\n",
            "Training epoch 16491/1000000, d_loss: -56.532867431640625,  g_loss: 9.189632415771484\n",
            "Training epoch 16492/1000000, d_loss: 113.23362731933594,  g_loss: 3.193718194961548\n",
            "Training epoch 16493/1000000, d_loss: -51.71290588378906,  g_loss: 51.98698425292969\n",
            "Training epoch 16494/1000000, d_loss: -290.103515625,  g_loss: 9.446182250976562\n",
            "Training epoch 16495/1000000, d_loss: -139.9756622314453,  g_loss: 86.3686294555664\n",
            "Training epoch 16496/1000000, d_loss: 5.976123809814453,  g_loss: -7.8296356201171875\n",
            "Training epoch 16497/1000000, d_loss: -28.350494384765625,  g_loss: 33.6165771484375\n",
            "Training epoch 16498/1000000, d_loss: -151.75973510742188,  g_loss: 152.7583465576172\n",
            "Training epoch 16499/1000000, d_loss: -149.39352416992188,  g_loss: 39.36732482910156\n",
            "Training epoch 16500/1000000, d_loss: -191.27528381347656,  g_loss: 136.8206329345703\n",
            "Training epoch 16501/1000000, d_loss: -93.90887451171875,  g_loss: 71.55070495605469\n",
            "Training epoch 16502/1000000, d_loss: -1.306854248046875,  g_loss: 45.56637954711914\n",
            "Training epoch 16503/1000000, d_loss: -359.73248291015625,  g_loss: -8.149352073669434\n",
            "Training epoch 16504/1000000, d_loss: -245.8047637939453,  g_loss: -56.481231689453125\n",
            "Training epoch 16505/1000000, d_loss: -91.98578643798828,  g_loss: 33.89591598510742\n",
            "Training epoch 16506/1000000, d_loss: -519.0515747070312,  g_loss: 33.02751922607422\n",
            "Training epoch 16507/1000000, d_loss: -1445.915283203125,  g_loss: -73.24903106689453\n",
            "Training epoch 16508/1000000, d_loss: -101.4298324584961,  g_loss: 14.875572204589844\n",
            "Training epoch 16509/1000000, d_loss: -41.20121765136719,  g_loss: -100.21849060058594\n",
            "Training epoch 16510/1000000, d_loss: -161.47640991210938,  g_loss: 28.755407333374023\n",
            "Training epoch 16511/1000000, d_loss: -68.32122802734375,  g_loss: 47.818870544433594\n",
            "Training epoch 16512/1000000, d_loss: -386.852294921875,  g_loss: 41.46738052368164\n",
            "Training epoch 16513/1000000, d_loss: -80.61799621582031,  g_loss: 2.589448928833008\n",
            "Training epoch 16514/1000000, d_loss: -97.23953247070312,  g_loss: 125.61029052734375\n",
            "Training epoch 16515/1000000, d_loss: -136.91213989257812,  g_loss: 76.6978988647461\n",
            "Training epoch 16516/1000000, d_loss: -320.2557678222656,  g_loss: 231.82540893554688\n",
            "Training epoch 16517/1000000, d_loss: -157.90597534179688,  g_loss: 36.396873474121094\n",
            "Training epoch 16518/1000000, d_loss: -149.49862670898438,  g_loss: 51.93948745727539\n",
            "Training epoch 16519/1000000, d_loss: -587.090087890625,  g_loss: -204.7615203857422\n",
            "Training epoch 16520/1000000, d_loss: -8.000434875488281,  g_loss: -7.12955904006958\n",
            "Training epoch 16521/1000000, d_loss: -904.7583618164062,  g_loss: -284.6519775390625\n",
            "Training epoch 16522/1000000, d_loss: -57.8651008605957,  g_loss: 90.57691955566406\n",
            "Training epoch 16523/1000000, d_loss: -49.98870849609375,  g_loss: 113.94194030761719\n",
            "Training epoch 16524/1000000, d_loss: -392.2431335449219,  g_loss: -16.118144989013672\n",
            "Training epoch 16525/1000000, d_loss: -141.32106018066406,  g_loss: 197.97900390625\n",
            "Training epoch 16526/1000000, d_loss: -259.9267883300781,  g_loss: -206.61740112304688\n",
            "Training epoch 16527/1000000, d_loss: -175.045166015625,  g_loss: 178.6096649169922\n",
            "Training epoch 16528/1000000, d_loss: -195.61016845703125,  g_loss: 64.5621566772461\n",
            "Training epoch 16529/1000000, d_loss: 49.30418395996094,  g_loss: 151.66555786132812\n",
            "Training epoch 16530/1000000, d_loss: -303.9864196777344,  g_loss: 396.75946044921875\n",
            "Training epoch 16531/1000000, d_loss: -173.95655822753906,  g_loss: 60.110992431640625\n",
            "Training epoch 16532/1000000, d_loss: -137.82211303710938,  g_loss: 33.953487396240234\n",
            "Training epoch 16533/1000000, d_loss: -135.66322326660156,  g_loss: 6.774031639099121\n",
            "Training epoch 16534/1000000, d_loss: -212.37432861328125,  g_loss: 30.430171966552734\n",
            "Training epoch 16535/1000000, d_loss: -743.2840576171875,  g_loss: -96.02913665771484\n",
            "Training epoch 16536/1000000, d_loss: -596.6094970703125,  g_loss: -1190.063232421875\n",
            "Training epoch 16537/1000000, d_loss: -278.44329833984375,  g_loss: 246.93313598632812\n",
            "Training epoch 16538/1000000, d_loss: -120.10641479492188,  g_loss: 174.9991912841797\n",
            "Training epoch 16539/1000000, d_loss: -191.94252014160156,  g_loss: 44.50245666503906\n",
            "Training epoch 16540/1000000, d_loss: -50.85803985595703,  g_loss: -7.067147731781006\n",
            "Training epoch 16541/1000000, d_loss: -163.6434783935547,  g_loss: 35.76902770996094\n",
            "Training epoch 16542/1000000, d_loss: -97.37608337402344,  g_loss: 28.521564483642578\n",
            "Training epoch 16543/1000000, d_loss: -351.1453552246094,  g_loss: 543.5886840820312\n",
            "Training epoch 16544/1000000, d_loss: -378.3993225097656,  g_loss: 744.4655151367188\n",
            "Training epoch 16545/1000000, d_loss: -163.82608032226562,  g_loss: -7.336104869842529\n",
            "Training epoch 16546/1000000, d_loss: -232.89901733398438,  g_loss: 171.86065673828125\n",
            "Training epoch 16547/1000000, d_loss: -598.3108520507812,  g_loss: -66.5013427734375\n",
            "Training epoch 16548/1000000, d_loss: -94.33551025390625,  g_loss: -70.44631958007812\n",
            "Training epoch 16549/1000000, d_loss: -614.0211181640625,  g_loss: -83.55084228515625\n",
            "Training epoch 16550/1000000, d_loss: 40.69580078125,  g_loss: -47.813167572021484\n",
            "Training epoch 16551/1000000, d_loss: -258.1923828125,  g_loss: -35.228057861328125\n",
            "Training epoch 16552/1000000, d_loss: 429.05316162109375,  g_loss: 51.357666015625\n",
            "Training epoch 16553/1000000, d_loss: -84.197021484375,  g_loss: 78.37423706054688\n",
            "Training epoch 16554/1000000, d_loss: -11.208511352539062,  g_loss: 45.37186813354492\n",
            "Training epoch 16555/1000000, d_loss: -107.48428344726562,  g_loss: 30.290660858154297\n",
            "Training epoch 16556/1000000, d_loss: -127.99015808105469,  g_loss: 38.73549270629883\n",
            "Training epoch 16557/1000000, d_loss: -191.2848663330078,  g_loss: 60.955848693847656\n",
            "Training epoch 16558/1000000, d_loss: -253.20611572265625,  g_loss: -135.3038330078125\n",
            "Training epoch 16559/1000000, d_loss: -78.68528747558594,  g_loss: -13.671110153198242\n",
            "Training epoch 16560/1000000, d_loss: -293.09039306640625,  g_loss: -95.04502868652344\n",
            "Training epoch 16561/1000000, d_loss: -65.73597717285156,  g_loss: 0.7299952507019043\n",
            "Training epoch 16562/1000000, d_loss: -259.9910888671875,  g_loss: -69.31524658203125\n",
            "Training epoch 16563/1000000, d_loss: 50.83997344970703,  g_loss: 14.848855972290039\n",
            "Training epoch 16564/1000000, d_loss: -184.0838165283203,  g_loss: 77.9739990234375\n",
            "Training epoch 16565/1000000, d_loss: -447.557861328125,  g_loss: 171.92636108398438\n",
            "Training epoch 16566/1000000, d_loss: -115.6094970703125,  g_loss: -1.2468143701553345\n",
            "Training epoch 16567/1000000, d_loss: -219.53399658203125,  g_loss: 202.81410217285156\n",
            "Training epoch 16568/1000000, d_loss: -49.856658935546875,  g_loss: -0.41174745559692383\n",
            "Training epoch 16569/1000000, d_loss: -124.0306396484375,  g_loss: 78.77586364746094\n",
            "Training epoch 16570/1000000, d_loss: -84.67143249511719,  g_loss: 99.95587158203125\n",
            "Training epoch 16571/1000000, d_loss: -119.5244140625,  g_loss: 164.5594482421875\n",
            "Training epoch 16572/1000000, d_loss: -721.5509033203125,  g_loss: -222.8341827392578\n",
            "Training epoch 16573/1000000, d_loss: 33733.4140625,  g_loss: -171.29959106445312\n",
            "Training epoch 16574/1000000, d_loss: 464.6007995605469,  g_loss: -477.74420166015625\n",
            "Training epoch 16575/1000000, d_loss: -34.38541793823242,  g_loss: -10.977205276489258\n",
            "Training epoch 16576/1000000, d_loss: -119.45555114746094,  g_loss: 185.73794555664062\n",
            "Training epoch 16577/1000000, d_loss: -25.282470703125,  g_loss: 262.84228515625\n",
            "Training epoch 16578/1000000, d_loss: 2772.722900390625,  g_loss: 42.21931457519531\n",
            "Training epoch 16579/1000000, d_loss: 16.543407440185547,  g_loss: 42.435462951660156\n",
            "Training epoch 16580/1000000, d_loss: -88.05398559570312,  g_loss: 134.43406677246094\n",
            "Training epoch 16581/1000000, d_loss: 0.3478851318359375,  g_loss: 180.6981964111328\n",
            "Training epoch 16582/1000000, d_loss: -118.16490173339844,  g_loss: 59.973812103271484\n",
            "Training epoch 16583/1000000, d_loss: -438.1861267089844,  g_loss: 349.7424621582031\n",
            "Training epoch 16584/1000000, d_loss: -343.1004943847656,  g_loss: 219.12713623046875\n",
            "Training epoch 16585/1000000, d_loss: 18.53881072998047,  g_loss: 61.54547119140625\n",
            "Training epoch 16586/1000000, d_loss: -12.517333984375,  g_loss: 68.65399932861328\n",
            "Training epoch 16587/1000000, d_loss: -64.6856689453125,  g_loss: 24.435956954956055\n",
            "Training epoch 16588/1000000, d_loss: -188.2672882080078,  g_loss: 30.992948532104492\n",
            "Training epoch 16589/1000000, d_loss: -84.48265075683594,  g_loss: 2.7204108238220215\n",
            "Training epoch 16590/1000000, d_loss: -158.14154052734375,  g_loss: 86.87905883789062\n",
            "Training epoch 16591/1000000, d_loss: -60.79158020019531,  g_loss: -30.236820220947266\n",
            "Training epoch 16592/1000000, d_loss: -419.9543762207031,  g_loss: 50.326663970947266\n",
            "Training epoch 16593/1000000, d_loss: -184.57516479492188,  g_loss: 128.36524963378906\n",
            "Training epoch 16594/1000000, d_loss: -316.9842834472656,  g_loss: -19.67473793029785\n",
            "Training epoch 16595/1000000, d_loss: -331.04852294921875,  g_loss: -21.440128326416016\n",
            "Training epoch 16596/1000000, d_loss: -254.76300048828125,  g_loss: 13.734258651733398\n",
            "Training epoch 16597/1000000, d_loss: -109.46473693847656,  g_loss: 27.543819427490234\n",
            "Training epoch 16598/1000000, d_loss: -1058.6990966796875,  g_loss: -172.18377685546875\n",
            "Training epoch 16599/1000000, d_loss: 20.5823974609375,  g_loss: -84.67255401611328\n",
            "Training epoch 16600/1000000, d_loss: -56.39973449707031,  g_loss: -5.961925506591797\n",
            "Training epoch 16601/1000000, d_loss: -812.3948364257812,  g_loss: -9.852394104003906\n",
            "Training epoch 16602/1000000, d_loss: -96.68682861328125,  g_loss: 95.27823638916016\n",
            "Training epoch 16603/1000000, d_loss: -133.3486785888672,  g_loss: -3.9268760681152344\n",
            "Training epoch 16604/1000000, d_loss: -139.33352661132812,  g_loss: 66.19105529785156\n",
            "Training epoch 16605/1000000, d_loss: -307.38134765625,  g_loss: 2.848653793334961\n",
            "Training epoch 16606/1000000, d_loss: -346.9119873046875,  g_loss: -53.15229034423828\n",
            "Training epoch 16607/1000000, d_loss: -23.942794799804688,  g_loss: 120.5400390625\n",
            "Training epoch 16608/1000000, d_loss: -48.51601028442383,  g_loss: -22.83704948425293\n",
            "Training epoch 16609/1000000, d_loss: -81.79930877685547,  g_loss: 1.5195567607879639\n",
            "Training epoch 16610/1000000, d_loss: -100.44813537597656,  g_loss: 129.77078247070312\n",
            "Training epoch 16611/1000000, d_loss: 102.69520568847656,  g_loss: -19.035539627075195\n",
            "Training epoch 16612/1000000, d_loss: -285.5527038574219,  g_loss: -35.36968994140625\n",
            "Training epoch 16613/1000000, d_loss: 1198.516357421875,  g_loss: -18.917346954345703\n",
            "Training epoch 16614/1000000, d_loss: -642.158203125,  g_loss: -145.6675262451172\n",
            "Training epoch 16615/1000000, d_loss: -189.2284393310547,  g_loss: -304.7165832519531\n",
            "Training epoch 16616/1000000, d_loss: -28.236587524414062,  g_loss: -35.045074462890625\n",
            "Training epoch 16617/1000000, d_loss: -115.67483520507812,  g_loss: -40.09147262573242\n",
            "Training epoch 16618/1000000, d_loss: -4.912651062011719,  g_loss: -19.995256423950195\n",
            "Training epoch 16619/1000000, d_loss: 2607.89599609375,  g_loss: 39.1872673034668\n",
            "Training epoch 16620/1000000, d_loss: 249.211669921875,  g_loss: 260.05499267578125\n",
            "Training epoch 16621/1000000, d_loss: -113.65182495117188,  g_loss: 39.95379638671875\n",
            "Training epoch 16622/1000000, d_loss: -108.746337890625,  g_loss: 152.99411010742188\n",
            "Training epoch 16623/1000000, d_loss: -92.24393463134766,  g_loss: 136.3211212158203\n",
            "Training epoch 16624/1000000, d_loss: -144.5463104248047,  g_loss: 175.07659912109375\n",
            "Training epoch 16625/1000000, d_loss: -615.088623046875,  g_loss: 24.122453689575195\n",
            "Training epoch 16626/1000000, d_loss: -58.173274993896484,  g_loss: 25.316604614257812\n",
            "Training epoch 16627/1000000, d_loss: -55.4127197265625,  g_loss: 39.2661247253418\n",
            "Training epoch 16628/1000000, d_loss: -261.3817138671875,  g_loss: 4.98919677734375\n",
            "Training epoch 16629/1000000, d_loss: -224.38816833496094,  g_loss: -46.23353576660156\n",
            "Training epoch 16630/1000000, d_loss: -266.3250732421875,  g_loss: 4.498043060302734\n",
            "Training epoch 16631/1000000, d_loss: -132.50357055664062,  g_loss: 296.67169189453125\n",
            "Training epoch 16632/1000000, d_loss: -69.15229797363281,  g_loss: -1.6729764938354492\n",
            "Training epoch 16633/1000000, d_loss: -71.50413513183594,  g_loss: -4.43231201171875\n",
            "Training epoch 16634/1000000, d_loss: -57.54029846191406,  g_loss: 81.20875549316406\n",
            "Training epoch 16635/1000000, d_loss: 2893.371337890625,  g_loss: 196.76072692871094\n",
            "Training epoch 16636/1000000, d_loss: 44.71718978881836,  g_loss: 69.25452423095703\n",
            "Training epoch 16637/1000000, d_loss: -80.56486511230469,  g_loss: 102.11698913574219\n",
            "Training epoch 16638/1000000, d_loss: -294.9334411621094,  g_loss: 486.7541198730469\n",
            "Training epoch 16639/1000000, d_loss: -69.81536865234375,  g_loss: 53.405921936035156\n",
            "Training epoch 16640/1000000, d_loss: -241.71917724609375,  g_loss: 17.59185218811035\n",
            "Training epoch 16641/1000000, d_loss: -106.26408386230469,  g_loss: 12.352256774902344\n",
            "Training epoch 16642/1000000, d_loss: -161.91152954101562,  g_loss: -2.4996557235717773\n",
            "Training epoch 16643/1000000, d_loss: -157.4811248779297,  g_loss: 9.20875358581543\n",
            "Training epoch 16644/1000000, d_loss: -404.64306640625,  g_loss: -117.77813720703125\n",
            "Training epoch 16645/1000000, d_loss: -12.580215454101562,  g_loss: 9.269490242004395\n",
            "Training epoch 16646/1000000, d_loss: -144.485595703125,  g_loss: 23.503557205200195\n",
            "Training epoch 16647/1000000, d_loss: -309.93994140625,  g_loss: -144.62423706054688\n",
            "Training epoch 16648/1000000, d_loss: -449.23907470703125,  g_loss: -67.99372863769531\n",
            "Training epoch 16649/1000000, d_loss: -31.365089416503906,  g_loss: 105.92119598388672\n",
            "Training epoch 16650/1000000, d_loss: -1283.050048828125,  g_loss: -369.65899658203125\n",
            "Training epoch 16651/1000000, d_loss: -7.053533554077148,  g_loss: 103.02406311035156\n",
            "Training epoch 16652/1000000, d_loss: -97.49842834472656,  g_loss: 45.950233459472656\n",
            "Training epoch 16653/1000000, d_loss: 228.3082275390625,  g_loss: 41.75267791748047\n",
            "Training epoch 16654/1000000, d_loss: -143.6442108154297,  g_loss: -66.47352600097656\n",
            "Training epoch 16655/1000000, d_loss: -156.55117797851562,  g_loss: 196.2017364501953\n",
            "Training epoch 16656/1000000, d_loss: 66.14518737792969,  g_loss: 74.74038696289062\n",
            "Training epoch 16657/1000000, d_loss: -239.6387939453125,  g_loss: 612.630859375\n",
            "Training epoch 16658/1000000, d_loss: -161.29017639160156,  g_loss: 33.44540786743164\n",
            "Training epoch 16659/1000000, d_loss: -124.58771514892578,  g_loss: 51.89033508300781\n",
            "Training epoch 16660/1000000, d_loss: -95.28398132324219,  g_loss: 56.296722412109375\n",
            "Training epoch 16661/1000000, d_loss: -38.33972930908203,  g_loss: -1.6446738243103027\n",
            "Training epoch 16662/1000000, d_loss: -72.1285400390625,  g_loss: 1.2918205261230469\n",
            "Training epoch 16663/1000000, d_loss: -207.6396942138672,  g_loss: 88.33695983886719\n",
            "Training epoch 16664/1000000, d_loss: -132.05471801757812,  g_loss: -33.61823272705078\n",
            "Training epoch 16665/1000000, d_loss: -148.570068359375,  g_loss: 100.63258361816406\n",
            "Training epoch 16666/1000000, d_loss: -24.827194213867188,  g_loss: -28.124229431152344\n",
            "Training epoch 16667/1000000, d_loss: -155.20921325683594,  g_loss: -43.49215316772461\n",
            "Training epoch 16668/1000000, d_loss: -728.19287109375,  g_loss: -135.26974487304688\n",
            "Training epoch 16669/1000000, d_loss: -391.70220947265625,  g_loss: -178.5093536376953\n",
            "Training epoch 16670/1000000, d_loss: -169.96505737304688,  g_loss: -156.714599609375\n",
            "Training epoch 16671/1000000, d_loss: -63.13195037841797,  g_loss: -143.15158081054688\n",
            "Training epoch 16672/1000000, d_loss: -112.72492980957031,  g_loss: 247.14291381835938\n",
            "Training epoch 16673/1000000, d_loss: -129.12884521484375,  g_loss: 23.500675201416016\n",
            "Training epoch 16674/1000000, d_loss: -100.45861053466797,  g_loss: 63.155914306640625\n",
            "Training epoch 16675/1000000, d_loss: -169.20855712890625,  g_loss: 72.94325256347656\n",
            "Training epoch 16676/1000000, d_loss: -313.20654296875,  g_loss: 209.95799255371094\n",
            "Training epoch 16677/1000000, d_loss: -194.78369140625,  g_loss: 165.0531463623047\n",
            "Training epoch 16678/1000000, d_loss: -106.92195129394531,  g_loss: 63.28465270996094\n",
            "Training epoch 16679/1000000, d_loss: -793.8302001953125,  g_loss: -193.55653381347656\n",
            "Training epoch 16680/1000000, d_loss: -127.87275695800781,  g_loss: 58.98798751831055\n",
            "Training epoch 16681/1000000, d_loss: 17.32611083984375,  g_loss: 137.43357849121094\n",
            "Training epoch 16682/1000000, d_loss: -381.19830322265625,  g_loss: -28.878385543823242\n",
            "Training epoch 16683/1000000, d_loss: -140.87860107421875,  g_loss: 152.90194702148438\n",
            "Training epoch 16684/1000000, d_loss: -151.7783203125,  g_loss: 107.33348846435547\n",
            "Training epoch 16685/1000000, d_loss: -81.87411499023438,  g_loss: 14.953142166137695\n",
            "Training epoch 16686/1000000, d_loss: -65.5047607421875,  g_loss: 40.630409240722656\n",
            "Training epoch 16687/1000000, d_loss: -104.52725982666016,  g_loss: 147.8130340576172\n",
            "Training epoch 16688/1000000, d_loss: -178.13037109375,  g_loss: 105.67048645019531\n",
            "Training epoch 16689/1000000, d_loss: -56.268985748291016,  g_loss: 108.98204803466797\n",
            "Training epoch 16690/1000000, d_loss: -416.76904296875,  g_loss: 154.16738891601562\n",
            "Training epoch 16691/1000000, d_loss: -47.28089904785156,  g_loss: -26.596242904663086\n",
            "Training epoch 16692/1000000, d_loss: -149.28553771972656,  g_loss: 33.21318817138672\n",
            "Training epoch 16693/1000000, d_loss: -123.59635925292969,  g_loss: 26.416927337646484\n",
            "Training epoch 16694/1000000, d_loss: -1679.386962890625,  g_loss: -228.31993103027344\n",
            "Training epoch 16695/1000000, d_loss: -31.544483184814453,  g_loss: -41.15259552001953\n",
            "Training epoch 16696/1000000, d_loss: -75.73826599121094,  g_loss: 6.013725280761719\n",
            "Training epoch 16697/1000000, d_loss: -106.18501281738281,  g_loss: 98.07495880126953\n",
            "Training epoch 16698/1000000, d_loss: -181.69004821777344,  g_loss: 6.543235778808594\n",
            "Training epoch 16699/1000000, d_loss: -155.90802001953125,  g_loss: 38.20134735107422\n",
            "Training epoch 16700/1000000, d_loss: -301.95892333984375,  g_loss: -51.062129974365234\n",
            "Training epoch 16701/1000000, d_loss: 29.304927825927734,  g_loss: 18.59657859802246\n",
            "Training epoch 16702/1000000, d_loss: 25.398662567138672,  g_loss: 43.32193374633789\n",
            "Training epoch 16703/1000000, d_loss: 87.29664611816406,  g_loss: 55.15614318847656\n",
            "Training epoch 16704/1000000, d_loss: -189.9229736328125,  g_loss: 163.55612182617188\n",
            "Training epoch 16705/1000000, d_loss: -349.00067138671875,  g_loss: 29.838912963867188\n",
            "Training epoch 16706/1000000, d_loss: 24.131072998046875,  g_loss: 35.74878692626953\n",
            "Training epoch 16707/1000000, d_loss: -280.71343994140625,  g_loss: 29.770381927490234\n",
            "Training epoch 16708/1000000, d_loss: -116.39398193359375,  g_loss: 33.28329086303711\n",
            "Training epoch 16709/1000000, d_loss: -151.0066375732422,  g_loss: 19.081148147583008\n",
            "Training epoch 16710/1000000, d_loss: -57.438419342041016,  g_loss: -6.348428249359131\n",
            "Training epoch 16711/1000000, d_loss: -157.01327514648438,  g_loss: 29.704608917236328\n",
            "Training epoch 16712/1000000, d_loss: -250.1117706298828,  g_loss: 338.6252136230469\n",
            "Training epoch 16713/1000000, d_loss: -150.62789916992188,  g_loss: 62.437076568603516\n",
            "Training epoch 16714/1000000, d_loss: -22.76491928100586,  g_loss: 24.41671371459961\n",
            "Training epoch 16715/1000000, d_loss: -107.82036590576172,  g_loss: -4.269894123077393\n",
            "Training epoch 16716/1000000, d_loss: -136.3175048828125,  g_loss: 72.90737915039062\n",
            "Training epoch 16717/1000000, d_loss: -60.1815185546875,  g_loss: 25.150598526000977\n",
            "Training epoch 16718/1000000, d_loss: -298.75537109375,  g_loss: -15.009014129638672\n",
            "Training epoch 16719/1000000, d_loss: -64.25849914550781,  g_loss: 1.0457515716552734\n",
            "Training epoch 16720/1000000, d_loss: -444.67010498046875,  g_loss: -63.36311721801758\n",
            "Training epoch 16721/1000000, d_loss: -2401.99365234375,  g_loss: -206.94140625\n",
            "Training epoch 16722/1000000, d_loss: 784.7581787109375,  g_loss: -155.36012268066406\n",
            "Training epoch 16723/1000000, d_loss: 984.2728881835938,  g_loss: -44.706077575683594\n",
            "Training epoch 16724/1000000, d_loss: -102.85775756835938,  g_loss: -68.55958557128906\n",
            "Training epoch 16725/1000000, d_loss: 107.39828491210938,  g_loss: 49.81887435913086\n",
            "Training epoch 16726/1000000, d_loss: 9.934165954589844,  g_loss: 36.30638885498047\n",
            "Training epoch 16727/1000000, d_loss: -244.5099334716797,  g_loss: -47.62939453125\n",
            "Training epoch 16728/1000000, d_loss: -1619.39453125,  g_loss: -297.3883056640625\n",
            "Training epoch 16729/1000000, d_loss: -58.052894592285156,  g_loss: -139.6458282470703\n",
            "Training epoch 16730/1000000, d_loss: -207.724853515625,  g_loss: 52.661224365234375\n",
            "Training epoch 16731/1000000, d_loss: -216.4171600341797,  g_loss: -259.35833740234375\n",
            "Training epoch 16732/1000000, d_loss: -129.02468872070312,  g_loss: 52.030853271484375\n",
            "Training epoch 16733/1000000, d_loss: -474.048828125,  g_loss: -16.660415649414062\n",
            "Training epoch 16734/1000000, d_loss: -76.67132568359375,  g_loss: 3.939657211303711\n",
            "Training epoch 16735/1000000, d_loss: -11.525955200195312,  g_loss: -49.96822738647461\n",
            "Training epoch 16736/1000000, d_loss: -158.07395935058594,  g_loss: 134.46624755859375\n",
            "Training epoch 16737/1000000, d_loss: -263.2724304199219,  g_loss: 232.40736389160156\n",
            "Training epoch 16738/1000000, d_loss: -33.52403259277344,  g_loss: 109.31087493896484\n",
            "Training epoch 16739/1000000, d_loss: -413.96881103515625,  g_loss: 348.923583984375\n",
            "Training epoch 16740/1000000, d_loss: -24.84760284423828,  g_loss: -24.315698623657227\n",
            "Training epoch 16741/1000000, d_loss: -223.10888671875,  g_loss: -37.07963943481445\n",
            "Training epoch 16742/1000000, d_loss: -99.00796508789062,  g_loss: 12.809589385986328\n",
            "Training epoch 16743/1000000, d_loss: -124.72550964355469,  g_loss: 17.98213768005371\n",
            "Training epoch 16744/1000000, d_loss: -264.75274658203125,  g_loss: -31.971460342407227\n",
            "Training epoch 16745/1000000, d_loss: -100.95809173583984,  g_loss: 16.686599731445312\n",
            "Training epoch 16746/1000000, d_loss: -201.6551513671875,  g_loss: 122.77531433105469\n",
            "Training epoch 16747/1000000, d_loss: -956.6918334960938,  g_loss: -466.4595947265625\n",
            "Training epoch 16748/1000000, d_loss: 19.832763671875,  g_loss: 51.65144729614258\n",
            "Training epoch 16749/1000000, d_loss: 37.0732421875,  g_loss: -35.943092346191406\n",
            "Training epoch 16750/1000000, d_loss: -282.5544128417969,  g_loss: 56.645668029785156\n",
            "Training epoch 16751/1000000, d_loss: 341.6651916503906,  g_loss: -90.52114868164062\n",
            "Training epoch 16752/1000000, d_loss: -74.59432983398438,  g_loss: 27.500110626220703\n",
            "Training epoch 16753/1000000, d_loss: -121.0430908203125,  g_loss: 22.377288818359375\n",
            "Training epoch 16754/1000000, d_loss: -447.9228210449219,  g_loss: -267.73980712890625\n",
            "Training epoch 16755/1000000, d_loss: -40.713623046875,  g_loss: 68.98310089111328\n",
            "Training epoch 16756/1000000, d_loss: -330.20562744140625,  g_loss: 504.2454528808594\n",
            "Training epoch 16757/1000000, d_loss: -217.3061981201172,  g_loss: 116.98627471923828\n",
            "Training epoch 16758/1000000, d_loss: -533.840087890625,  g_loss: 638.4199829101562\n",
            "Training epoch 16759/1000000, d_loss: 39.385276794433594,  g_loss: 15.103143692016602\n",
            "Training epoch 16760/1000000, d_loss: -121.82183837890625,  g_loss: 98.23011779785156\n",
            "Training epoch 16761/1000000, d_loss: -133.03465270996094,  g_loss: 135.8489990234375\n",
            "Training epoch 16762/1000000, d_loss: -52.52971649169922,  g_loss: -15.123613357543945\n",
            "Training epoch 16763/1000000, d_loss: -103.86341857910156,  g_loss: 53.88676452636719\n",
            "Training epoch 16764/1000000, d_loss: -101.29930114746094,  g_loss: -65.6319808959961\n",
            "Training epoch 16765/1000000, d_loss: -151.89195251464844,  g_loss: 14.08802604675293\n",
            "Training epoch 16766/1000000, d_loss: -692.584716796875,  g_loss: -38.55552291870117\n",
            "Training epoch 16767/1000000, d_loss: 26.47905731201172,  g_loss: -17.150466918945312\n",
            "Training epoch 16768/1000000, d_loss: -78.02275085449219,  g_loss: -38.658180236816406\n",
            "Training epoch 16769/1000000, d_loss: -20.31977653503418,  g_loss: -0.39653539657592773\n",
            "Training epoch 16770/1000000, d_loss: 1.24554443359375,  g_loss: 43.7155876159668\n",
            "Training epoch 16771/1000000, d_loss: -291.3012390136719,  g_loss: 24.759904861450195\n",
            "Training epoch 16772/1000000, d_loss: -127.00924682617188,  g_loss: 20.334978103637695\n",
            "Training epoch 16773/1000000, d_loss: 160.8573760986328,  g_loss: 67.30873107910156\n",
            "Training epoch 16774/1000000, d_loss: -81.21859741210938,  g_loss: 36.066314697265625\n",
            "Training epoch 16775/1000000, d_loss: -43.916717529296875,  g_loss: 97.20854949951172\n",
            "Training epoch 16776/1000000, d_loss: -114.46623992919922,  g_loss: 23.139568328857422\n",
            "Training epoch 16777/1000000, d_loss: -51.7564582824707,  g_loss: 157.73182678222656\n",
            "Training epoch 16778/1000000, d_loss: -271.13726806640625,  g_loss: 283.14080810546875\n",
            "Training epoch 16779/1000000, d_loss: -126.24658966064453,  g_loss: 66.238037109375\n",
            "Training epoch 16780/1000000, d_loss: -68.485107421875,  g_loss: 31.73149871826172\n",
            "Training epoch 16781/1000000, d_loss: -62.36125946044922,  g_loss: 39.192626953125\n",
            "Training epoch 16782/1000000, d_loss: -47.540157318115234,  g_loss: 65.11846923828125\n",
            "Training epoch 16783/1000000, d_loss: -57.486839294433594,  g_loss: 60.47785949707031\n",
            "Training epoch 16784/1000000, d_loss: -124.64317321777344,  g_loss: 46.764442443847656\n",
            "Training epoch 16785/1000000, d_loss: -1156.0506591796875,  g_loss: -441.07696533203125\n",
            "Training epoch 16786/1000000, d_loss: -53.24717712402344,  g_loss: 39.24441146850586\n",
            "Training epoch 16787/1000000, d_loss: -136.34185791015625,  g_loss: 58.74774932861328\n",
            "Training epoch 16788/1000000, d_loss: -694.6505126953125,  g_loss: 2.8377814292907715\n",
            "Training epoch 16789/1000000, d_loss: -51.334930419921875,  g_loss: 26.630666732788086\n",
            "Training epoch 16790/1000000, d_loss: 30.191852569580078,  g_loss: 56.24026870727539\n",
            "Training epoch 16791/1000000, d_loss: 30.298751831054688,  g_loss: 61.054481506347656\n",
            "Training epoch 16792/1000000, d_loss: 77.3148193359375,  g_loss: 96.55054473876953\n",
            "Training epoch 16793/1000000, d_loss: -60.99079132080078,  g_loss: 49.93159484863281\n",
            "Training epoch 16794/1000000, d_loss: -428.3041687011719,  g_loss: -0.4253089427947998\n",
            "Training epoch 16795/1000000, d_loss: -80.45796203613281,  g_loss: 20.455524444580078\n",
            "Training epoch 16796/1000000, d_loss: -83.92347717285156,  g_loss: -67.55352783203125\n",
            "Training epoch 16797/1000000, d_loss: -249.61053466796875,  g_loss: -24.220924377441406\n",
            "Training epoch 16798/1000000, d_loss: -31.878673553466797,  g_loss: 42.235111236572266\n",
            "Training epoch 16799/1000000, d_loss: 49.277400970458984,  g_loss: 33.522708892822266\n",
            "Training epoch 16800/1000000, d_loss: -134.61827087402344,  g_loss: 113.51205444335938\n",
            "Training epoch 16801/1000000, d_loss: -176.1478729248047,  g_loss: 82.99214935302734\n",
            "Training epoch 16802/1000000, d_loss: -81.7476806640625,  g_loss: 35.57801055908203\n",
            "Training epoch 16803/1000000, d_loss: -1034.244873046875,  g_loss: -768.0056762695312\n",
            "Training epoch 16804/1000000, d_loss: -286.593505859375,  g_loss: -367.4636535644531\n",
            "Training epoch 16805/1000000, d_loss: -186.2395477294922,  g_loss: -1012.4104614257812\n",
            "Training epoch 16806/1000000, d_loss: -394.2801513671875,  g_loss: -248.91065979003906\n",
            "Training epoch 16807/1000000, d_loss: 208.84872436523438,  g_loss: -5.3095855712890625\n",
            "Training epoch 16808/1000000, d_loss: -15.153165817260742,  g_loss: -442.1439514160156\n",
            "Training epoch 16809/1000000, d_loss: -7.261407852172852,  g_loss: -2.421034097671509\n",
            "Training epoch 16810/1000000, d_loss: -193.95411682128906,  g_loss: -184.3041229248047\n",
            "Training epoch 16811/1000000, d_loss: 37.789527893066406,  g_loss: -88.04623413085938\n",
            "Training epoch 16812/1000000, d_loss: -27.179290771484375,  g_loss: 125.7442398071289\n",
            "Training epoch 16813/1000000, d_loss: -35.825927734375,  g_loss: 121.81068420410156\n",
            "Training epoch 16814/1000000, d_loss: -122.02782440185547,  g_loss: 84.24530029296875\n",
            "Training epoch 16815/1000000, d_loss: -311.0147399902344,  g_loss: 467.38104248046875\n",
            "Training epoch 16816/1000000, d_loss: -433.7991638183594,  g_loss: 584.2515869140625\n",
            "Training epoch 16817/1000000, d_loss: -7.333244323730469,  g_loss: 46.31712341308594\n",
            "Training epoch 16818/1000000, d_loss: -86.79559326171875,  g_loss: 42.94318389892578\n",
            "Training epoch 16819/1000000, d_loss: -140.29324340820312,  g_loss: 127.8616943359375\n",
            "Training epoch 16820/1000000, d_loss: 10.435455322265625,  g_loss: -3.1004104614257812\n",
            "Training epoch 16821/1000000, d_loss: -315.46820068359375,  g_loss: -66.39823150634766\n",
            "Training epoch 16822/1000000, d_loss: -61.059837341308594,  g_loss: 13.079896926879883\n",
            "Training epoch 16823/1000000, d_loss: -83.32843017578125,  g_loss: 126.37982940673828\n",
            "Training epoch 16824/1000000, d_loss: -102.71644592285156,  g_loss: 83.72138214111328\n",
            "Training epoch 16825/1000000, d_loss: -202.80453491210938,  g_loss: 370.9443664550781\n",
            "Training epoch 16826/1000000, d_loss: -166.63294982910156,  g_loss: 309.62158203125\n",
            "Training epoch 16827/1000000, d_loss: 314.69415283203125,  g_loss: -21.869163513183594\n",
            "Training epoch 16828/1000000, d_loss: 216.04910278320312,  g_loss: -20.351856231689453\n",
            "Training epoch 16829/1000000, d_loss: -278.8086242675781,  g_loss: 15.381567001342773\n",
            "Training epoch 16830/1000000, d_loss: -129.1911163330078,  g_loss: 42.38666534423828\n",
            "Training epoch 16831/1000000, d_loss: -118.70732116699219,  g_loss: 5.947724342346191\n",
            "Training epoch 16832/1000000, d_loss: -66.52908325195312,  g_loss: 40.74079895019531\n",
            "Training epoch 16833/1000000, d_loss: -10.105659484863281,  g_loss: 83.2850570678711\n",
            "Training epoch 16834/1000000, d_loss: -128.03855895996094,  g_loss: 132.04507446289062\n",
            "Training epoch 16835/1000000, d_loss: -243.99801635742188,  g_loss: 88.92948913574219\n",
            "Training epoch 16836/1000000, d_loss: -91.88712310791016,  g_loss: 15.86404800415039\n",
            "Training epoch 16837/1000000, d_loss: -224.21945190429688,  g_loss: -0.5481011867523193\n",
            "Training epoch 16838/1000000, d_loss: 88.3809814453125,  g_loss: 37.14837646484375\n",
            "Training epoch 16839/1000000, d_loss: -309.041015625,  g_loss: 8.83838176727295\n",
            "Training epoch 16840/1000000, d_loss: -69.81129455566406,  g_loss: 104.96829223632812\n",
            "Training epoch 16841/1000000, d_loss: 77.91650390625,  g_loss: 55.58483123779297\n",
            "Training epoch 16842/1000000, d_loss: -68.86344909667969,  g_loss: 96.32403564453125\n",
            "Training epoch 16843/1000000, d_loss: -155.08212280273438,  g_loss: -14.095182418823242\n",
            "Training epoch 16844/1000000, d_loss: -121.40349578857422,  g_loss: 72.23482513427734\n",
            "Training epoch 16845/1000000, d_loss: -180.75535583496094,  g_loss: 29.998493194580078\n",
            "Training epoch 16846/1000000, d_loss: -153.21401977539062,  g_loss: 31.92767333984375\n",
            "Training epoch 16847/1000000, d_loss: -86.53182983398438,  g_loss: 21.093605041503906\n",
            "Training epoch 16848/1000000, d_loss: -59.322540283203125,  g_loss: 55.20246887207031\n",
            "Training epoch 16849/1000000, d_loss: -143.64108276367188,  g_loss: -2.76364803314209\n",
            "Training epoch 16850/1000000, d_loss: -171.2810516357422,  g_loss: 225.5394287109375\n",
            "Training epoch 16851/1000000, d_loss: -108.38807678222656,  g_loss: 3.3879294395446777\n",
            "Training epoch 16852/1000000, d_loss: -169.00814819335938,  g_loss: 148.0562286376953\n",
            "Training epoch 16853/1000000, d_loss: -209.62539672851562,  g_loss: -14.344686508178711\n",
            "Training epoch 16854/1000000, d_loss: -47.32059860229492,  g_loss: 66.74935150146484\n",
            "Training epoch 16855/1000000, d_loss: -692.007080078125,  g_loss: 31.045995712280273\n",
            "Training epoch 16856/1000000, d_loss: -50.10689926147461,  g_loss: -32.77987289428711\n",
            "Training epoch 16857/1000000, d_loss: -34.70371627807617,  g_loss: 51.491943359375\n",
            "Training epoch 16858/1000000, d_loss: -37.33797073364258,  g_loss: 81.38925170898438\n",
            "Training epoch 16859/1000000, d_loss: -478.5539855957031,  g_loss: 23.91747283935547\n",
            "Training epoch 16860/1000000, d_loss: -102.63449096679688,  g_loss: 79.45352935791016\n",
            "Training epoch 16861/1000000, d_loss: -13.917373657226562,  g_loss: 59.545135498046875\n",
            "Training epoch 16862/1000000, d_loss: -331.9651184082031,  g_loss: 70.631103515625\n",
            "Training epoch 16863/1000000, d_loss: -214.72930908203125,  g_loss: 29.932968139648438\n",
            "Training epoch 16864/1000000, d_loss: -244.76417541503906,  g_loss: -32.13418960571289\n",
            "Training epoch 16865/1000000, d_loss: -11.00857162475586,  g_loss: -5.344547748565674\n",
            "Training epoch 16866/1000000, d_loss: -147.49346923828125,  g_loss: 48.98497009277344\n",
            "Training epoch 16867/1000000, d_loss: -73.4300537109375,  g_loss: 64.10801696777344\n",
            "Training epoch 16868/1000000, d_loss: -33.35127258300781,  g_loss: 25.003238677978516\n",
            "Training epoch 16869/1000000, d_loss: -51.620628356933594,  g_loss: 56.742523193359375\n",
            "Training epoch 16870/1000000, d_loss: -150.74400329589844,  g_loss: 80.73748779296875\n",
            "Training epoch 16871/1000000, d_loss: -290.97064208984375,  g_loss: -3.2423248291015625\n",
            "Training epoch 16872/1000000, d_loss: -232.451171875,  g_loss: -16.05642318725586\n",
            "Training epoch 16873/1000000, d_loss: 27.642539978027344,  g_loss: 12.543737411499023\n",
            "Training epoch 16874/1000000, d_loss: -40.576839447021484,  g_loss: 34.42216873168945\n",
            "Training epoch 16875/1000000, d_loss: -136.66026306152344,  g_loss: 13.268708229064941\n",
            "Training epoch 16876/1000000, d_loss: -150.20477294921875,  g_loss: -14.662985801696777\n",
            "Training epoch 16877/1000000, d_loss: -116.63871765136719,  g_loss: 52.10990524291992\n",
            "Training epoch 16878/1000000, d_loss: 179.5743408203125,  g_loss: 69.833251953125\n",
            "Training epoch 16879/1000000, d_loss: -47.200355529785156,  g_loss: 59.83879089355469\n",
            "Training epoch 16880/1000000, d_loss: -61.103546142578125,  g_loss: 47.05351257324219\n",
            "Training epoch 16881/1000000, d_loss: -35.83533477783203,  g_loss: 34.63172149658203\n",
            "Training epoch 16882/1000000, d_loss: -596.2256469726562,  g_loss: -17.77007293701172\n",
            "Training epoch 16883/1000000, d_loss: -69.18705749511719,  g_loss: 31.502159118652344\n",
            "Training epoch 16884/1000000, d_loss: -73.05838775634766,  g_loss: -51.80470275878906\n",
            "Training epoch 16885/1000000, d_loss: -85.23465728759766,  g_loss: -17.752120971679688\n",
            "Training epoch 16886/1000000, d_loss: -226.38775634765625,  g_loss: -36.0253791809082\n",
            "Training epoch 16887/1000000, d_loss: 723.9892578125,  g_loss: -0.9520072937011719\n",
            "Training epoch 16888/1000000, d_loss: -96.87005615234375,  g_loss: 40.21931457519531\n",
            "Training epoch 16889/1000000, d_loss: -16.817550659179688,  g_loss: 78.76411437988281\n",
            "Training epoch 16890/1000000, d_loss: -154.92294311523438,  g_loss: 137.88636779785156\n",
            "Training epoch 16891/1000000, d_loss: 0.78857421875,  g_loss: 78.27129364013672\n",
            "Training epoch 16892/1000000, d_loss: -120.42032623291016,  g_loss: 66.63872528076172\n",
            "Training epoch 16893/1000000, d_loss: -290.9372253417969,  g_loss: 11.605634689331055\n",
            "Training epoch 16894/1000000, d_loss: -201.65185546875,  g_loss: -9.92668628692627\n",
            "Training epoch 16895/1000000, d_loss: -101.14769744873047,  g_loss: 32.482154846191406\n",
            "Training epoch 16896/1000000, d_loss: -373.7405090332031,  g_loss: 362.31011962890625\n",
            "Training epoch 16897/1000000, d_loss: 30.565582275390625,  g_loss: -42.10865020751953\n",
            "Training epoch 16898/1000000, d_loss: -16.57437515258789,  g_loss: -34.57794189453125\n",
            "Training epoch 16899/1000000, d_loss: -124.30779266357422,  g_loss: 9.73778247833252\n",
            "Training epoch 16900/1000000, d_loss: 55.71742248535156,  g_loss: 42.960906982421875\n",
            "Training epoch 16901/1000000, d_loss: -98.71158599853516,  g_loss: 19.776344299316406\n",
            "Training epoch 16902/1000000, d_loss: -129.39248657226562,  g_loss: -7.757359504699707\n",
            "Training epoch 16903/1000000, d_loss: -195.31369018554688,  g_loss: -1.498673439025879\n",
            "Training epoch 16904/1000000, d_loss: -294.675537109375,  g_loss: -45.37740707397461\n",
            "Training epoch 16905/1000000, d_loss: 287.28131103515625,  g_loss: 29.78717041015625\n",
            "Training epoch 16906/1000000, d_loss: -94.261474609375,  g_loss: 18.818044662475586\n",
            "Training epoch 16907/1000000, d_loss: -104.8471908569336,  g_loss: 4.185001373291016\n",
            "Training epoch 16908/1000000, d_loss: -28.917076110839844,  g_loss: 47.49560546875\n",
            "Training epoch 16909/1000000, d_loss: -196.41464233398438,  g_loss: 33.05097961425781\n",
            "Training epoch 16910/1000000, d_loss: -409.2857666015625,  g_loss: 11.625737190246582\n",
            "Training epoch 16911/1000000, d_loss: -15.454185485839844,  g_loss: 41.538429260253906\n",
            "Training epoch 16912/1000000, d_loss: -71.04467010498047,  g_loss: 102.39701843261719\n",
            "Training epoch 16913/1000000, d_loss: -144.5532989501953,  g_loss: 46.05104064941406\n",
            "Training epoch 16914/1000000, d_loss: -110.97042083740234,  g_loss: 43.32203674316406\n",
            "Training epoch 16915/1000000, d_loss: -76.46163177490234,  g_loss: 40.35028839111328\n",
            "Training epoch 16916/1000000, d_loss: -207.12220764160156,  g_loss: -31.905838012695312\n",
            "Training epoch 16917/1000000, d_loss: -94.48747253417969,  g_loss: 24.563493728637695\n",
            "Training epoch 16918/1000000, d_loss: -82.19807434082031,  g_loss: 3.3024086952209473\n",
            "Training epoch 16919/1000000, d_loss: -420.15252685546875,  g_loss: -61.848628997802734\n",
            "Training epoch 16920/1000000, d_loss: -97.30650329589844,  g_loss: -65.70278930664062\n",
            "Training epoch 16921/1000000, d_loss: -75.13200378417969,  g_loss: 11.285552978515625\n",
            "Training epoch 16922/1000000, d_loss: -45.89631652832031,  g_loss: 59.767921447753906\n",
            "Training epoch 16923/1000000, d_loss: -30.90838623046875,  g_loss: 33.09458923339844\n",
            "Training epoch 16924/1000000, d_loss: -128.17697143554688,  g_loss: 28.97447967529297\n",
            "Training epoch 16925/1000000, d_loss: -166.24383544921875,  g_loss: 38.96998596191406\n",
            "Training epoch 16926/1000000, d_loss: -535.4523315429688,  g_loss: -44.353355407714844\n",
            "Training epoch 16927/1000000, d_loss: 33.25707244873047,  g_loss: 77.89055633544922\n",
            "Training epoch 16928/1000000, d_loss: -266.2774658203125,  g_loss: -8.574817657470703\n",
            "Training epoch 16929/1000000, d_loss: -389.7992858886719,  g_loss: -7.154606819152832\n",
            "Training epoch 16930/1000000, d_loss: -204.9775390625,  g_loss: -38.44911575317383\n",
            "Training epoch 16931/1000000, d_loss: -493.75341796875,  g_loss: -72.98931884765625\n",
            "Training epoch 16932/1000000, d_loss: -87.62003326416016,  g_loss: 21.856111526489258\n",
            "Training epoch 16933/1000000, d_loss: -323.9963684082031,  g_loss: 425.88897705078125\n",
            "Training epoch 16934/1000000, d_loss: -330.7550354003906,  g_loss: 1.989710807800293\n",
            "Training epoch 16935/1000000, d_loss: -300.6231994628906,  g_loss: -116.01753997802734\n",
            "Training epoch 16936/1000000, d_loss: -211.33526611328125,  g_loss: -214.78030395507812\n",
            "Training epoch 16937/1000000, d_loss: -11.154464721679688,  g_loss: 32.83207702636719\n",
            "Training epoch 16938/1000000, d_loss: -68.11685180664062,  g_loss: 13.236905097961426\n",
            "Training epoch 16939/1000000, d_loss: -87.4097900390625,  g_loss: 267.8436584472656\n",
            "Training epoch 16940/1000000, d_loss: -702.6845092773438,  g_loss: 1500.133056640625\n",
            "Training epoch 16941/1000000, d_loss: 576.8477783203125,  g_loss: -35.02630615234375\n",
            "Training epoch 16942/1000000, d_loss: 18.305959701538086,  g_loss: 1.7722597122192383\n",
            "Training epoch 16943/1000000, d_loss: -71.81471252441406,  g_loss: -7.414427757263184\n",
            "Training epoch 16944/1000000, d_loss: -740.681640625,  g_loss: -100.92826080322266\n",
            "Training epoch 16945/1000000, d_loss: 472.1036376953125,  g_loss: 35.78343200683594\n",
            "Training epoch 16946/1000000, d_loss: -231.77911376953125,  g_loss: 147.05703735351562\n",
            "Training epoch 16947/1000000, d_loss: -517.57275390625,  g_loss: 787.761474609375\n",
            "Training epoch 16948/1000000, d_loss: 108.12754821777344,  g_loss: 91.77052307128906\n",
            "Training epoch 16949/1000000, d_loss: -47.90648651123047,  g_loss: 31.779565811157227\n",
            "Training epoch 16950/1000000, d_loss: -51.251922607421875,  g_loss: 88.86603546142578\n",
            "Training epoch 16951/1000000, d_loss: -118.68672943115234,  g_loss: -62.591148376464844\n",
            "Training epoch 16952/1000000, d_loss: -204.64910888671875,  g_loss: 41.898826599121094\n",
            "Training epoch 16953/1000000, d_loss: -121.02873992919922,  g_loss: -15.136083602905273\n",
            "Training epoch 16954/1000000, d_loss: -1186.390869140625,  g_loss: -35.045654296875\n",
            "Training epoch 16955/1000000, d_loss: -56.46487045288086,  g_loss: 5.898280143737793\n",
            "Training epoch 16956/1000000, d_loss: -27.58974838256836,  g_loss: -7.241024971008301\n",
            "Training epoch 16957/1000000, d_loss: -16.561153411865234,  g_loss: 24.816762924194336\n",
            "Training epoch 16958/1000000, d_loss: -171.90943908691406,  g_loss: 1.6363821029663086\n",
            "Training epoch 16959/1000000, d_loss: -319.0478515625,  g_loss: -112.62397766113281\n",
            "Training epoch 16960/1000000, d_loss: -248.19589233398438,  g_loss: -22.29043960571289\n",
            "Training epoch 16961/1000000, d_loss: -96.34016418457031,  g_loss: -33.24514389038086\n",
            "Training epoch 16962/1000000, d_loss: -94.01467895507812,  g_loss: -38.972625732421875\n",
            "Training epoch 16963/1000000, d_loss: -133.58241271972656,  g_loss: 30.584125518798828\n",
            "Training epoch 16964/1000000, d_loss: -210.43246459960938,  g_loss: 28.401592254638672\n",
            "Training epoch 16965/1000000, d_loss: -411.38031005859375,  g_loss: 995.351318359375\n",
            "Training epoch 16966/1000000, d_loss: 82.44139099121094,  g_loss: -4.996860504150391\n",
            "Training epoch 16967/1000000, d_loss: -87.54270935058594,  g_loss: 22.053634643554688\n",
            "Training epoch 16968/1000000, d_loss: -158.37059020996094,  g_loss: -29.04522705078125\n",
            "Training epoch 16969/1000000, d_loss: -72.69845581054688,  g_loss: -0.13147735595703125\n",
            "Training epoch 16970/1000000, d_loss: -191.1876220703125,  g_loss: -80.07240295410156\n",
            "Training epoch 16971/1000000, d_loss: 420.84368896484375,  g_loss: 32.522281646728516\n",
            "Training epoch 16972/1000000, d_loss: -41.670169830322266,  g_loss: 34.1688346862793\n",
            "Training epoch 16973/1000000, d_loss: -45.11505126953125,  g_loss: 39.40520095825195\n",
            "Training epoch 16974/1000000, d_loss: -194.02821350097656,  g_loss: 114.78856658935547\n",
            "Training epoch 16975/1000000, d_loss: -177.3351287841797,  g_loss: 205.364990234375\n",
            "Training epoch 16976/1000000, d_loss: 45.527252197265625,  g_loss: 13.48694896697998\n",
            "Training epoch 16977/1000000, d_loss: -41.42180633544922,  g_loss: 29.47027015686035\n",
            "Training epoch 16978/1000000, d_loss: -9.775848388671875,  g_loss: 31.277822494506836\n",
            "Training epoch 16979/1000000, d_loss: -97.42525482177734,  g_loss: 7.445487976074219\n",
            "Training epoch 16980/1000000, d_loss: -482.5567932128906,  g_loss: -85.17593383789062\n",
            "Training epoch 16981/1000000, d_loss: -19.17438507080078,  g_loss: 2.369842290878296\n",
            "Training epoch 16982/1000000, d_loss: -98.18070983886719,  g_loss: 28.387859344482422\n",
            "Training epoch 16983/1000000, d_loss: -34.607872009277344,  g_loss: 152.85357666015625\n",
            "Training epoch 16984/1000000, d_loss: -91.56010437011719,  g_loss: -0.9746913909912109\n",
            "Training epoch 16985/1000000, d_loss: -23.131927490234375,  g_loss: 5.504928112030029\n",
            "Training epoch 16986/1000000, d_loss: -10.04119873046875,  g_loss: 54.91387176513672\n",
            "Training epoch 16987/1000000, d_loss: -137.83192443847656,  g_loss: 50.833152770996094\n",
            "Training epoch 16988/1000000, d_loss: -129.00946044921875,  g_loss: 128.3491668701172\n",
            "Training epoch 16989/1000000, d_loss: -50.087554931640625,  g_loss: 97.12899780273438\n",
            "Training epoch 16990/1000000, d_loss: -13.739471435546875,  g_loss: -11.032822608947754\n",
            "Training epoch 16991/1000000, d_loss: -27.203834533691406,  g_loss: 17.941150665283203\n",
            "Training epoch 16992/1000000, d_loss: -121.29400634765625,  g_loss: -3.049578905105591\n",
            "Training epoch 16993/1000000, d_loss: -24.776123046875,  g_loss: -58.071067810058594\n",
            "Training epoch 16994/1000000, d_loss: -125.41303253173828,  g_loss: -9.027776718139648\n",
            "Training epoch 16995/1000000, d_loss: -209.91888427734375,  g_loss: -28.117633819580078\n",
            "Training epoch 16996/1000000, d_loss: -288.5799255371094,  g_loss: -23.572650909423828\n",
            "Training epoch 16997/1000000, d_loss: -55.75347900390625,  g_loss: 6.6159186363220215\n",
            "Training epoch 16998/1000000, d_loss: -150.18911743164062,  g_loss: -12.07459831237793\n",
            "Training epoch 16999/1000000, d_loss: -145.52169799804688,  g_loss: 29.52002716064453\n",
            "Training epoch 17000/1000000, d_loss: -217.89114379882812,  g_loss: 9.64492130279541\n",
            "Training epoch 17001/1000000, d_loss: -278.59478759765625,  g_loss: -37.876609802246094\n",
            "Sampling...\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 31/31 [00:00<00:00, 595.45it/s]\n",
            "Meshing: 100%|██████████| 697/697 [00:00<00:00, 5086.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_17001/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/discriminator_epoch_17001/assets\n",
            "Training epoch 17002/1000000, d_loss: -113.71065521240234,  g_loss: -4.394912242889404\n",
            "Training epoch 17003/1000000, d_loss: -119.52833557128906,  g_loss: 4.6538004875183105\n",
            "Training epoch 17004/1000000, d_loss: -29.8170166015625,  g_loss: 36.74287796020508\n",
            "Training epoch 17005/1000000, d_loss: -115.60608673095703,  g_loss: 236.50270080566406\n",
            "Training epoch 17006/1000000, d_loss: -204.8673858642578,  g_loss: 16.32577896118164\n",
            "Training epoch 17007/1000000, d_loss: -111.50860595703125,  g_loss: -46.21577453613281\n",
            "Training epoch 17008/1000000, d_loss: -95.96099853515625,  g_loss: -87.69141387939453\n",
            "Training epoch 17009/1000000, d_loss: -89.70298767089844,  g_loss: -21.78197479248047\n",
            "Training epoch 17010/1000000, d_loss: -69.54600524902344,  g_loss: 11.662435531616211\n",
            "Training epoch 17011/1000000, d_loss: -21.081157684326172,  g_loss: 47.144012451171875\n",
            "Training epoch 17012/1000000, d_loss: -69.59933471679688,  g_loss: 6.871225833892822\n",
            "Training epoch 17013/1000000, d_loss: -145.88250732421875,  g_loss: 10.60389518737793\n",
            "Training epoch 17014/1000000, d_loss: -213.7658233642578,  g_loss: 39.95304870605469\n",
            "Training epoch 17015/1000000, d_loss: -92.24264526367188,  g_loss: -6.402811050415039\n",
            "Training epoch 17016/1000000, d_loss: -154.3492889404297,  g_loss: 46.00171661376953\n",
            "Training epoch 17017/1000000, d_loss: -9.604293823242188,  g_loss: -3.690709114074707\n",
            "Training epoch 17018/1000000, d_loss: -256.8124694824219,  g_loss: -22.672800064086914\n",
            "Training epoch 17019/1000000, d_loss: -966.837890625,  g_loss: -229.57940673828125\n",
            "Training epoch 17020/1000000, d_loss: -112.75042724609375,  g_loss: -128.41781616210938\n",
            "Training epoch 17021/1000000, d_loss: -125.28822326660156,  g_loss: -120.71685791015625\n",
            "Training epoch 17022/1000000, d_loss: -236.06201171875,  g_loss: -98.92713928222656\n",
            "Training epoch 17023/1000000, d_loss: -36.5303840637207,  g_loss: -87.401123046875\n",
            "Training epoch 17024/1000000, d_loss: -2.3925628662109375,  g_loss: -25.764793395996094\n",
            "Training epoch 17025/1000000, d_loss: -96.69087982177734,  g_loss: 66.6624984741211\n",
            "Training epoch 17026/1000000, d_loss: -272.1138916015625,  g_loss: -41.4302978515625\n",
            "Training epoch 17027/1000000, d_loss: -223.7117462158203,  g_loss: -34.047279357910156\n",
            "Training epoch 17028/1000000, d_loss: -1.1240463256835938,  g_loss: -67.73789978027344\n",
            "Training epoch 17029/1000000, d_loss: -76.48745727539062,  g_loss: -68.25576782226562\n",
            "Training epoch 17030/1000000, d_loss: -237.8740234375,  g_loss: -139.30447387695312\n",
            "Training epoch 17031/1000000, d_loss: -542.0963134765625,  g_loss: -112.48182678222656\n",
            "Training epoch 17032/1000000, d_loss: -91.33885192871094,  g_loss: -89.92626190185547\n",
            "Training epoch 17033/1000000, d_loss: -519.7894287109375,  g_loss: -308.76251220703125\n",
            "Training epoch 17034/1000000, d_loss: -99.75515747070312,  g_loss: 10.65689468383789\n",
            "Training epoch 17035/1000000, d_loss: -118.3515625,  g_loss: 63.17803192138672\n",
            "Training epoch 17036/1000000, d_loss: -221.1842041015625,  g_loss: -4.039954662322998\n",
            "Training epoch 17037/1000000, d_loss: -118.65884399414062,  g_loss: -26.307281494140625\n",
            "Training epoch 17038/1000000, d_loss: -103.68798065185547,  g_loss: -13.636340141296387\n",
            "Training epoch 17039/1000000, d_loss: -134.6768798828125,  g_loss: 40.86894607543945\n",
            "Training epoch 17040/1000000, d_loss: -321.864990234375,  g_loss: 68.9137954711914\n",
            "Training epoch 17041/1000000, d_loss: -58.794471740722656,  g_loss: 15.251083374023438\n",
            "Training epoch 17042/1000000, d_loss: -213.05209350585938,  g_loss: -77.6458740234375\n",
            "Training epoch 17043/1000000, d_loss: -273.8646240234375,  g_loss: -141.10220336914062\n",
            "Training epoch 17044/1000000, d_loss: 381.4251708984375,  g_loss: 7.546267032623291\n",
            "Training epoch 17045/1000000, d_loss: 64.05851745605469,  g_loss: -7.52583646774292\n",
            "Training epoch 17046/1000000, d_loss: -131.63632202148438,  g_loss: 1.2271593809127808\n",
            "Training epoch 17047/1000000, d_loss: -86.06741333007812,  g_loss: 0.18493996560573578\n",
            "Training epoch 17048/1000000, d_loss: -86.53193664550781,  g_loss: 10.067636489868164\n",
            "Training epoch 17049/1000000, d_loss: -42.53143310546875,  g_loss: 22.568050384521484\n",
            "Training epoch 17050/1000000, d_loss: -214.13206481933594,  g_loss: 265.7877502441406\n",
            "Training epoch 17051/1000000, d_loss: -239.50631713867188,  g_loss: 124.09161376953125\n",
            "Training epoch 17052/1000000, d_loss: -21.266666412353516,  g_loss: 60.10575485229492\n",
            "Training epoch 17053/1000000, d_loss: -76.18563842773438,  g_loss: 90.0730972290039\n",
            "Training epoch 17054/1000000, d_loss: -24.341976165771484,  g_loss: 51.52849578857422\n",
            "Training epoch 17055/1000000, d_loss: -52.89731979370117,  g_loss: 35.79987335205078\n",
            "Training epoch 17056/1000000, d_loss: -229.735595703125,  g_loss: -14.917875289916992\n",
            "Training epoch 17057/1000000, d_loss: -69.83711242675781,  g_loss: 30.468168258666992\n",
            "Training epoch 17058/1000000, d_loss: -256.8583984375,  g_loss: -3.4769792556762695\n",
            "Training epoch 17059/1000000, d_loss: -175.71359252929688,  g_loss: -33.00702667236328\n",
            "Training epoch 17060/1000000, d_loss: -147.44618225097656,  g_loss: 129.5743408203125\n",
            "Training epoch 17061/1000000, d_loss: -712.9805908203125,  g_loss: -2.75144100189209\n",
            "Training epoch 17062/1000000, d_loss: -54.346107482910156,  g_loss: -43.215545654296875\n",
            "Training epoch 17063/1000000, d_loss: -266.9801330566406,  g_loss: -66.14443969726562\n",
            "Training epoch 17064/1000000, d_loss: -304.2030334472656,  g_loss: -31.240447998046875\n",
            "Training epoch 17065/1000000, d_loss: -1472.3790283203125,  g_loss: -417.16644287109375\n",
            "Training epoch 17066/1000000, d_loss: 40.008460998535156,  g_loss: -96.27568054199219\n",
            "Training epoch 17067/1000000, d_loss: -305.38238525390625,  g_loss: -49.51408767700195\n",
            "Training epoch 17068/1000000, d_loss: -57.6193733215332,  g_loss: -39.291015625\n",
            "Training epoch 17069/1000000, d_loss: -135.77499389648438,  g_loss: 142.71434020996094\n",
            "Training epoch 17070/1000000, d_loss: -146.65025329589844,  g_loss: 19.688438415527344\n",
            "Training epoch 17071/1000000, d_loss: 67.5302505493164,  g_loss: -17.060876846313477\n",
            "Training epoch 17072/1000000, d_loss: -204.05303955078125,  g_loss: -77.07176208496094\n",
            "Training epoch 17073/1000000, d_loss: -347.9349670410156,  g_loss: -149.2624053955078\n",
            "Training epoch 17074/1000000, d_loss: -321.3428039550781,  g_loss: -93.74446868896484\n",
            "Training epoch 17075/1000000, d_loss: -1239.01416015625,  g_loss: -554.1051635742188\n",
            "Training epoch 17076/1000000, d_loss: 69.3028564453125,  g_loss: -255.6566619873047\n",
            "Training epoch 17077/1000000, d_loss: -54.32374954223633,  g_loss: 25.492324829101562\n",
            "Training epoch 17078/1000000, d_loss: -13.920188903808594,  g_loss: -4.543020248413086\n",
            "Training epoch 17079/1000000, d_loss: -116.69561767578125,  g_loss: 79.33515167236328\n",
            "Training epoch 17080/1000000, d_loss: -92.75070190429688,  g_loss: 84.95156860351562\n",
            "Training epoch 17081/1000000, d_loss: -143.3482208251953,  g_loss: 221.89779663085938\n",
            "Training epoch 17082/1000000, d_loss: -10.320053100585938,  g_loss: -28.897117614746094\n",
            "Training epoch 17083/1000000, d_loss: -159.2823944091797,  g_loss: 39.583213806152344\n",
            "Training epoch 17084/1000000, d_loss: -156.17420959472656,  g_loss: -72.42487335205078\n",
            "Training epoch 17085/1000000, d_loss: -143.2855224609375,  g_loss: 10.444799423217773\n",
            "Training epoch 17086/1000000, d_loss: -62.460182189941406,  g_loss: -27.76188850402832\n",
            "Training epoch 17087/1000000, d_loss: -120.75956726074219,  g_loss: 34.27484130859375\n",
            "Training epoch 17088/1000000, d_loss: -64.91275787353516,  g_loss: 52.49344253540039\n",
            "Training epoch 17089/1000000, d_loss: -279.109130859375,  g_loss: -181.75186157226562\n",
            "Training epoch 17090/1000000, d_loss: -334.6400146484375,  g_loss: 629.511474609375\n",
            "Training epoch 17091/1000000, d_loss: -189.32986450195312,  g_loss: 313.7616271972656\n",
            "Training epoch 17092/1000000, d_loss: -244.98822021484375,  g_loss: -16.262380599975586\n",
            "Training epoch 17093/1000000, d_loss: 106.29127502441406,  g_loss: -13.721306800842285\n",
            "Training epoch 17094/1000000, d_loss: -406.6457214355469,  g_loss: 1.9344401359558105\n",
            "Training epoch 17095/1000000, d_loss: -999.86865234375,  g_loss: -285.37725830078125\n",
            "Training epoch 17096/1000000, d_loss: -74.73577880859375,  g_loss: -62.400413513183594\n",
            "Training epoch 17097/1000000, d_loss: -565.0879516601562,  g_loss: -184.8139190673828\n",
            "Training epoch 17098/1000000, d_loss: -152.6202392578125,  g_loss: 2.0150909423828125\n",
            "Training epoch 17099/1000000, d_loss: -235.53067016601562,  g_loss: -233.1924285888672\n",
            "Training epoch 17100/1000000, d_loss: 12.827333450317383,  g_loss: -64.6600570678711\n",
            "Training epoch 17101/1000000, d_loss: -125.54078674316406,  g_loss: 22.48954200744629\n",
            "Training epoch 17102/1000000, d_loss: -241.53802490234375,  g_loss: 271.49432373046875\n",
            "Training epoch 17103/1000000, d_loss: -182.8064422607422,  g_loss: 54.584529876708984\n",
            "Training epoch 17104/1000000, d_loss: -144.6091766357422,  g_loss: 31.2571964263916\n",
            "Training epoch 17105/1000000, d_loss: -97.61705017089844,  g_loss: 223.73770141601562\n",
            "Training epoch 17106/1000000, d_loss: -248.21310424804688,  g_loss: 379.1267395019531\n",
            "Training epoch 17107/1000000, d_loss: -323.4542541503906,  g_loss: -5.829891204833984\n",
            "Training epoch 17108/1000000, d_loss: -494.2841491699219,  g_loss: -36.68510437011719\n",
            "Training epoch 17109/1000000, d_loss: -513.3074951171875,  g_loss: -92.92622375488281\n",
            "Training epoch 17110/1000000, d_loss: -2436.86328125,  g_loss: -1580.5037841796875\n",
            "Training epoch 17111/1000000, d_loss: 339.6366271972656,  g_loss: -345.98626708984375\n",
            "Training epoch 17112/1000000, d_loss: -86.04963684082031,  g_loss: -129.34266662597656\n",
            "Training epoch 17113/1000000, d_loss: -516.7305297851562,  g_loss: -43.83281326293945\n",
            "Training epoch 17114/1000000, d_loss: 140.631591796875,  g_loss: 15.397439956665039\n",
            "Training epoch 17115/1000000, d_loss: -571.5584106445312,  g_loss: -141.09201049804688\n",
            "Training epoch 17116/1000000, d_loss: 0.7187957763671875,  g_loss: 137.83660888671875\n",
            "Training epoch 17117/1000000, d_loss: -195.65748596191406,  g_loss: 239.33737182617188\n",
            "Training epoch 17118/1000000, d_loss: 15.877105712890625,  g_loss: 136.0291748046875\n",
            "Training epoch 17119/1000000, d_loss: 719.3511352539062,  g_loss: 5.207150459289551\n",
            "Training epoch 17120/1000000, d_loss: -105.4380874633789,  g_loss: 115.01541900634766\n",
            "Training epoch 17121/1000000, d_loss: -166.434326171875,  g_loss: 253.1513214111328\n",
            "Training epoch 17122/1000000, d_loss: 35.294273376464844,  g_loss: 43.335777282714844\n",
            "Training epoch 17123/1000000, d_loss: -186.09283447265625,  g_loss: 23.528297424316406\n",
            "Training epoch 17124/1000000, d_loss: -119.58380889892578,  g_loss: 22.584714889526367\n",
            "Training epoch 17125/1000000, d_loss: -66.85948181152344,  g_loss: 122.21884155273438\n",
            "Training epoch 17126/1000000, d_loss: -107.71200561523438,  g_loss: 100.14331817626953\n",
            "Training epoch 17127/1000000, d_loss: -305.3733825683594,  g_loss: 248.81832885742188\n",
            "Training epoch 17128/1000000, d_loss: 6.869728088378906,  g_loss: -12.732695579528809\n",
            "Training epoch 17129/1000000, d_loss: -186.85910034179688,  g_loss: -52.783836364746094\n",
            "Training epoch 17130/1000000, d_loss: -177.6875457763672,  g_loss: 7.335335731506348\n",
            "Training epoch 17131/1000000, d_loss: -18.95458984375,  g_loss: -30.9135799407959\n",
            "Training epoch 17132/1000000, d_loss: -39.68634796142578,  g_loss: 60.92101287841797\n",
            "Training epoch 17133/1000000, d_loss: -31.37401580810547,  g_loss: 47.918678283691406\n",
            "Training epoch 17134/1000000, d_loss: -149.1538543701172,  g_loss: -5.090938568115234\n",
            "Training epoch 17135/1000000, d_loss: 44.218231201171875,  g_loss: 67.28056335449219\n",
            "Training epoch 17136/1000000, d_loss: -68.78392028808594,  g_loss: 47.070289611816406\n",
            "Training epoch 17137/1000000, d_loss: -359.16644287109375,  g_loss: 236.4782257080078\n",
            "Training epoch 17138/1000000, d_loss: 7.0955810546875,  g_loss: 117.69551849365234\n",
            "Training epoch 17139/1000000, d_loss: -28.4437255859375,  g_loss: 24.171348571777344\n",
            "Training epoch 17140/1000000, d_loss: -236.71023559570312,  g_loss: 200.25498962402344\n",
            "Training epoch 17141/1000000, d_loss: -67.62777709960938,  g_loss: -10.974786758422852\n",
            "Training epoch 17142/1000000, d_loss: -124.32354736328125,  g_loss: 46.45095443725586\n",
            "Training epoch 17143/1000000, d_loss: -267.6331787109375,  g_loss: -57.77842330932617\n",
            "Training epoch 17144/1000000, d_loss: -141.17059326171875,  g_loss: -64.56803894042969\n",
            "Training epoch 17145/1000000, d_loss: -20.399995803833008,  g_loss: -19.09765625\n",
            "Training epoch 17146/1000000, d_loss: -78.99365997314453,  g_loss: -42.67158508300781\n",
            "Training epoch 17147/1000000, d_loss: -1.6285362243652344,  g_loss: 30.252277374267578\n",
            "Training epoch 17148/1000000, d_loss: -54.855899810791016,  g_loss: 82.62513732910156\n",
            "Training epoch 17149/1000000, d_loss: -216.12428283691406,  g_loss: 203.992431640625\n",
            "Training epoch 17150/1000000, d_loss: -40.93062973022461,  g_loss: 21.97466278076172\n",
            "Training epoch 17151/1000000, d_loss: -78.42082214355469,  g_loss: 62.991920471191406\n",
            "Training epoch 17152/1000000, d_loss: -18.19487762451172,  g_loss: 26.69034767150879\n",
            "Training epoch 17153/1000000, d_loss: -47.54437255859375,  g_loss: 29.908153533935547\n",
            "Training epoch 17154/1000000, d_loss: -33.873680114746094,  g_loss: 65.20945739746094\n",
            "Training epoch 17155/1000000, d_loss: -130.18392944335938,  g_loss: 76.92835998535156\n",
            "Training epoch 17156/1000000, d_loss: -15.815910339355469,  g_loss: 99.00225830078125\n",
            "Training epoch 17157/1000000, d_loss: -190.59642028808594,  g_loss: 74.98151397705078\n",
            "Training epoch 17158/1000000, d_loss: -504.48309326171875,  g_loss: 9.923419952392578\n",
            "Training epoch 17159/1000000, d_loss: -17.863941192626953,  g_loss: 42.034080505371094\n",
            "Training epoch 17160/1000000, d_loss: -83.20095825195312,  g_loss: 57.233856201171875\n",
            "Training epoch 17161/1000000, d_loss: -878.4168701171875,  g_loss: -129.49989318847656\n",
            "Training epoch 17162/1000000, d_loss: -340.578125,  g_loss: -21.622900009155273\n",
            "Training epoch 17163/1000000, d_loss: -301.8851318359375,  g_loss: -3.6667959690093994\n",
            "Training epoch 17164/1000000, d_loss: 18.288421630859375,  g_loss: 28.8663330078125\n",
            "Training epoch 17165/1000000, d_loss: -302.5682373046875,  g_loss: 9.698238372802734\n",
            "Training epoch 17166/1000000, d_loss: -117.5462875366211,  g_loss: -3.5897436141967773\n",
            "Training epoch 17167/1000000, d_loss: -1.1546173095703125,  g_loss: -26.53057861328125\n",
            "Training epoch 17168/1000000, d_loss: -2362.00146484375,  g_loss: -118.6913833618164\n",
            "Training epoch 17169/1000000, d_loss: 7.6455078125,  g_loss: -78.56278228759766\n",
            "Training epoch 17170/1000000, d_loss: -78.90311431884766,  g_loss: -53.87364959716797\n",
            "Training epoch 17171/1000000, d_loss: 26.194252014160156,  g_loss: 37.20336151123047\n",
            "Training epoch 17172/1000000, d_loss: -92.61858367919922,  g_loss: -1.9140281677246094\n",
            "Training epoch 17173/1000000, d_loss: -94.09203338623047,  g_loss: 13.818079948425293\n",
            "Training epoch 17174/1000000, d_loss: -42.64762878417969,  g_loss: 7.531237602233887\n",
            "Training epoch 17175/1000000, d_loss: -59.99808883666992,  g_loss: -25.31121253967285\n",
            "Training epoch 17176/1000000, d_loss: -88.89720916748047,  g_loss: -53.21922302246094\n",
            "Training epoch 17177/1000000, d_loss: -174.80484008789062,  g_loss: -33.09221649169922\n",
            "Training epoch 17178/1000000, d_loss: -263.4327392578125,  g_loss: -81.79678344726562\n",
            "Training epoch 17179/1000000, d_loss: -0.8339576721191406,  g_loss: -15.265411376953125\n",
            "Training epoch 17180/1000000, d_loss: -169.73916625976562,  g_loss: -69.70498657226562\n",
            "Training epoch 17181/1000000, d_loss: -76.71248626708984,  g_loss: -42.637325286865234\n",
            "Training epoch 17182/1000000, d_loss: 60.975738525390625,  g_loss: -38.395145416259766\n",
            "Training epoch 17183/1000000, d_loss: -8.976669311523438,  g_loss: 63.83905792236328\n",
            "Training epoch 17184/1000000, d_loss: -138.46778869628906,  g_loss: 128.29986572265625\n",
            "Training epoch 17185/1000000, d_loss: -323.56646728515625,  g_loss: -148.47787475585938\n",
            "Training epoch 17186/1000000, d_loss: -108.80708312988281,  g_loss: -44.54427719116211\n",
            "Training epoch 17187/1000000, d_loss: -112.19102478027344,  g_loss: -14.538299560546875\n",
            "Training epoch 17188/1000000, d_loss: 10.585586547851562,  g_loss: -66.7975082397461\n",
            "Training epoch 17189/1000000, d_loss: -130.70188903808594,  g_loss: -52.75019836425781\n",
            "Training epoch 17190/1000000, d_loss: -131.6896514892578,  g_loss: 103.62798309326172\n",
            "Training epoch 17191/1000000, d_loss: -539.5657348632812,  g_loss: -304.9626770019531\n",
            "Training epoch 17192/1000000, d_loss: -109.82785034179688,  g_loss: 0.5317401885986328\n",
            "Training epoch 17193/1000000, d_loss: -2179.7275390625,  g_loss: -644.3638916015625\n",
            "Training epoch 17194/1000000, d_loss: -474.0896911621094,  g_loss: -226.618408203125\n",
            "Training epoch 17195/1000000, d_loss: -26.860149383544922,  g_loss: 14.51047134399414\n",
            "Training epoch 17196/1000000, d_loss: -185.6382598876953,  g_loss: -23.453704833984375\n",
            "Training epoch 17197/1000000, d_loss: -28.501087188720703,  g_loss: -28.854820251464844\n",
            "Training epoch 17198/1000000, d_loss: -13.326156616210938,  g_loss: 25.6492977142334\n",
            "Training epoch 17199/1000000, d_loss: -364.24188232421875,  g_loss: 732.7047119140625\n",
            "Training epoch 17200/1000000, d_loss: -107.72267150878906,  g_loss: -4.815371990203857\n",
            "Training epoch 17201/1000000, d_loss: -127.00733184814453,  g_loss: -41.04020690917969\n",
            "Training epoch 17202/1000000, d_loss: -134.10009765625,  g_loss: 179.8766326904297\n",
            "Training epoch 17203/1000000, d_loss: -1045.3017578125,  g_loss: -26.713476181030273\n",
            "Training epoch 17204/1000000, d_loss: 654.1978759765625,  g_loss: -49.21340560913086\n",
            "Training epoch 17205/1000000, d_loss: -129.46246337890625,  g_loss: -24.887372970581055\n",
            "Training epoch 17206/1000000, d_loss: 40.982337951660156,  g_loss: -58.9751091003418\n",
            "Training epoch 17207/1000000, d_loss: 70.86509704589844,  g_loss: 8.381793975830078\n",
            "Training epoch 17208/1000000, d_loss: -59.85762023925781,  g_loss: -41.57626724243164\n",
            "Training epoch 17209/1000000, d_loss: 121.90611267089844,  g_loss: -86.6036605834961\n",
            "Training epoch 17210/1000000, d_loss: -177.53536987304688,  g_loss: -107.61630249023438\n",
            "Training epoch 17211/1000000, d_loss: -112.77081298828125,  g_loss: -16.47018814086914\n",
            "Training epoch 17212/1000000, d_loss: 146.20364379882812,  g_loss: -2.7456836700439453\n",
            "Training epoch 17213/1000000, d_loss: -82.39707946777344,  g_loss: -29.21766471862793\n",
            "Training epoch 17214/1000000, d_loss: -89.28343963623047,  g_loss: -25.467910766601562\n",
            "Training epoch 17215/1000000, d_loss: -140.41004943847656,  g_loss: -8.744176864624023\n",
            "Training epoch 17216/1000000, d_loss: -100.63182067871094,  g_loss: 1.231074571609497\n",
            "Training epoch 17217/1000000, d_loss: -466.45269775390625,  g_loss: 177.0742950439453\n",
            "Training epoch 17218/1000000, d_loss: -10.363353729248047,  g_loss: 35.45519256591797\n",
            "Training epoch 17219/1000000, d_loss: -345.15374755859375,  g_loss: -3.785737991333008\n",
            "Training epoch 17220/1000000, d_loss: -329.75689697265625,  g_loss: -47.20263671875\n",
            "Training epoch 17221/1000000, d_loss: -64.86053466796875,  g_loss: 85.82454681396484\n",
            "Training epoch 17222/1000000, d_loss: -359.4120178222656,  g_loss: -329.239501953125\n",
            "Training epoch 17223/1000000, d_loss: -235.8230743408203,  g_loss: -133.33567810058594\n",
            "Training epoch 17224/1000000, d_loss: -109.23178100585938,  g_loss: 46.853065490722656\n",
            "Training epoch 17225/1000000, d_loss: -143.63453674316406,  g_loss: -7.446563720703125\n",
            "Training epoch 17226/1000000, d_loss: -152.80068969726562,  g_loss: -77.98248291015625\n",
            "Training epoch 17227/1000000, d_loss: -69.5988540649414,  g_loss: 75.29666137695312\n",
            "Training epoch 17228/1000000, d_loss: -402.8194885253906,  g_loss: 703.2169799804688\n",
            "Training epoch 17229/1000000, d_loss: 98.59588623046875,  g_loss: 91.46857452392578\n",
            "Training epoch 17230/1000000, d_loss: -47.40494155883789,  g_loss: 27.460433959960938\n",
            "Training epoch 17231/1000000, d_loss: -169.90272521972656,  g_loss: 47.41529846191406\n",
            "Training epoch 17232/1000000, d_loss: -264.44720458984375,  g_loss: 19.590736389160156\n",
            "Training epoch 17233/1000000, d_loss: -10.6097412109375,  g_loss: 73.95728302001953\n",
            "Training epoch 17234/1000000, d_loss: -1310.97998046875,  g_loss: -75.65702819824219\n",
            "Training epoch 17235/1000000, d_loss: -64.56932067871094,  g_loss: -21.36433219909668\n",
            "Training epoch 17236/1000000, d_loss: -369.18878173828125,  g_loss: -26.903270721435547\n",
            "Training epoch 17237/1000000, d_loss: -42.98999786376953,  g_loss: -34.810447692871094\n",
            "Training epoch 17238/1000000, d_loss: 25.340606689453125,  g_loss: -31.513103485107422\n",
            "Training epoch 17239/1000000, d_loss: -155.80203247070312,  g_loss: -36.284942626953125\n",
            "Training epoch 17240/1000000, d_loss: -117.09439086914062,  g_loss: 58.17050552368164\n",
            "Training epoch 17241/1000000, d_loss: -519.003173828125,  g_loss: -90.6075210571289\n",
            "Training epoch 17242/1000000, d_loss: -224.50357055664062,  g_loss: -99.62442779541016\n",
            "Training epoch 17243/1000000, d_loss: -977.6978759765625,  g_loss: -235.3839874267578\n",
            "Training epoch 17244/1000000, d_loss: -150.82249450683594,  g_loss: -221.33712768554688\n",
            "Training epoch 17245/1000000, d_loss: -123.97229766845703,  g_loss: -157.17347717285156\n",
            "Training epoch 17246/1000000, d_loss: -3645.680908203125,  g_loss: -1177.675537109375\n",
            "Training epoch 17247/1000000, d_loss: 135.04244995117188,  g_loss: -34.37667465209961\n",
            "Training epoch 17248/1000000, d_loss: 288.2489013671875,  g_loss: -234.3522491455078\n",
            "Training epoch 17249/1000000, d_loss: -90.21353149414062,  g_loss: 52.23670959472656\n",
            "Training epoch 17250/1000000, d_loss: -49.264610290527344,  g_loss: 27.056324005126953\n",
            "Training epoch 17251/1000000, d_loss: -33.79048156738281,  g_loss: 71.46636962890625\n",
            "Training epoch 17252/1000000, d_loss: -132.80723571777344,  g_loss: 115.97836303710938\n",
            "Training epoch 17253/1000000, d_loss: -274.0539855957031,  g_loss: -5.612884521484375\n",
            "Training epoch 17254/1000000, d_loss: -214.91671752929688,  g_loss: 73.4582290649414\n",
            "Training epoch 17255/1000000, d_loss: -91.63400268554688,  g_loss: 137.53314208984375\n",
            "Training epoch 17256/1000000, d_loss: -286.74554443359375,  g_loss: 135.3309783935547\n",
            "Training epoch 17257/1000000, d_loss: -438.02227783203125,  g_loss: 475.6974182128906\n",
            "Training epoch 17258/1000000, d_loss: 422.9212951660156,  g_loss: -12.791182518005371\n",
            "Training epoch 17259/1000000, d_loss: -147.17652893066406,  g_loss: 49.8166618347168\n",
            "Training epoch 17260/1000000, d_loss: 432.197265625,  g_loss: 41.40763473510742\n",
            "Training epoch 17261/1000000, d_loss: -64.19895935058594,  g_loss: 51.25840759277344\n",
            "Training epoch 17262/1000000, d_loss: -138.57481384277344,  g_loss: 127.0328369140625\n",
            "Training epoch 17263/1000000, d_loss: -91.08519744873047,  g_loss: 38.93268966674805\n",
            "Training epoch 17264/1000000, d_loss: -49.981170654296875,  g_loss: 38.011756896972656\n",
            "Training epoch 17265/1000000, d_loss: -229.69400024414062,  g_loss: 58.8992805480957\n",
            "Training epoch 17266/1000000, d_loss: -42.77770233154297,  g_loss: 24.702571868896484\n",
            "Training epoch 17267/1000000, d_loss: -81.50273895263672,  g_loss: 93.69647216796875\n",
            "Training epoch 17268/1000000, d_loss: -93.0535888671875,  g_loss: -54.834228515625\n",
            "Training epoch 17269/1000000, d_loss: -245.80123901367188,  g_loss: -56.51018524169922\n",
            "Training epoch 17270/1000000, d_loss: -188.0395050048828,  g_loss: -61.31529235839844\n",
            "Training epoch 17271/1000000, d_loss: -81.01057434082031,  g_loss: -40.58434295654297\n",
            "Training epoch 17272/1000000, d_loss: -771.650146484375,  g_loss: -90.46180725097656\n",
            "Training epoch 17273/1000000, d_loss: 6.227476119995117,  g_loss: -60.622528076171875\n",
            "Training epoch 17274/1000000, d_loss: -399.6999206542969,  g_loss: -85.8857421875\n",
            "Training epoch 17275/1000000, d_loss: -48.957725524902344,  g_loss: -108.84112548828125\n",
            "Training epoch 17276/1000000, d_loss: -306.61187744140625,  g_loss: 178.02130126953125\n",
            "Training epoch 17277/1000000, d_loss: -242.2684783935547,  g_loss: 291.9358215332031\n",
            "Training epoch 17278/1000000, d_loss: -2.6677474975585938,  g_loss: -36.29133605957031\n",
            "Training epoch 17279/1000000, d_loss: -49.32078552246094,  g_loss: 12.76057243347168\n",
            "Training epoch 17280/1000000, d_loss: -38.08475875854492,  g_loss: -1.9998626708984375\n",
            "Training epoch 17281/1000000, d_loss: -65.40106201171875,  g_loss: 94.86360168457031\n",
            "Training epoch 17282/1000000, d_loss: 20.071449279785156,  g_loss: 69.67948913574219\n",
            "Training epoch 17283/1000000, d_loss: -88.3701171875,  g_loss: 69.39585876464844\n",
            "Training epoch 17284/1000000, d_loss: -169.77438354492188,  g_loss: 27.174907684326172\n",
            "Training epoch 17285/1000000, d_loss: -185.66079711914062,  g_loss: 38.79923629760742\n",
            "Training epoch 17286/1000000, d_loss: -66.26044464111328,  g_loss: 0.6091921329498291\n",
            "Training epoch 17287/1000000, d_loss: -63.367767333984375,  g_loss: 56.94371032714844\n",
            "Training epoch 17288/1000000, d_loss: -352.60333251953125,  g_loss: -30.770065307617188\n",
            "Training epoch 17289/1000000, d_loss: -543.5985717773438,  g_loss: -52.604209899902344\n",
            "Training epoch 17290/1000000, d_loss: -150.59396362304688,  g_loss: -46.78572463989258\n",
            "Training epoch 17291/1000000, d_loss: -50.91474914550781,  g_loss: -39.843833923339844\n",
            "Training epoch 17292/1000000, d_loss: -3.5871200561523438,  g_loss: 21.928123474121094\n",
            "Training epoch 17293/1000000, d_loss: -107.27489471435547,  g_loss: 122.68065643310547\n",
            "Training epoch 17294/1000000, d_loss: -119.63201904296875,  g_loss: 148.71722412109375\n",
            "Training epoch 17295/1000000, d_loss: -50.428558349609375,  g_loss: 10.531534194946289\n",
            "Training epoch 17296/1000000, d_loss: -255.38754272460938,  g_loss: 5.498577117919922\n",
            "Training epoch 17297/1000000, d_loss: -770.06591796875,  g_loss: -112.58808898925781\n",
            "Training epoch 17298/1000000, d_loss: -118.56268310546875,  g_loss: -30.013046264648438\n",
            "Training epoch 17299/1000000, d_loss: -85.53720092773438,  g_loss: 270.0635681152344\n",
            "Training epoch 17300/1000000, d_loss: -274.40289306640625,  g_loss: 408.73895263671875\n",
            "Training epoch 17301/1000000, d_loss: -397.0382080078125,  g_loss: -102.56492614746094\n",
            "Training epoch 17302/1000000, d_loss: 83.56107330322266,  g_loss: -6.892708778381348\n",
            "Training epoch 17303/1000000, d_loss: -405.7685852050781,  g_loss: -71.98310089111328\n",
            "Training epoch 17304/1000000, d_loss: -478.1248474121094,  g_loss: -166.63978576660156\n",
            "Training epoch 17305/1000000, d_loss: -164.7767791748047,  g_loss: -109.06486511230469\n",
            "Training epoch 17306/1000000, d_loss: -169.42138671875,  g_loss: -66.75302124023438\n",
            "Training epoch 17307/1000000, d_loss: -443.81036376953125,  g_loss: -114.04419708251953\n",
            "Training epoch 17308/1000000, d_loss: -201.73976135253906,  g_loss: -7.871580123901367\n",
            "Training epoch 17309/1000000, d_loss: 116.13290405273438,  g_loss: 123.45832824707031\n",
            "Training epoch 17310/1000000, d_loss: -606.796875,  g_loss: -66.90637969970703\n",
            "Training epoch 17311/1000000, d_loss: -43.584197998046875,  g_loss: 10.325072288513184\n",
            "Training epoch 17312/1000000, d_loss: -922.123291015625,  g_loss: -159.56861877441406\n",
            "Training epoch 17313/1000000, d_loss: 12059.03515625,  g_loss: 102.41590118408203\n",
            "Training epoch 17314/1000000, d_loss: 160.20870971679688,  g_loss: 29.673912048339844\n",
            "Training epoch 17315/1000000, d_loss: -121.57244110107422,  g_loss: 115.91925048828125\n",
            "Training epoch 17316/1000000, d_loss: -39.84905242919922,  g_loss: 54.39339828491211\n",
            "Training epoch 17317/1000000, d_loss: -127.66468048095703,  g_loss: 53.006263732910156\n",
            "Training epoch 17318/1000000, d_loss: -63.426727294921875,  g_loss: -28.896163940429688\n",
            "Training epoch 17319/1000000, d_loss: -206.63150024414062,  g_loss: -51.949851989746094\n",
            "Training epoch 17320/1000000, d_loss: -23.587615966796875,  g_loss: -2.5911407470703125\n",
            "Training epoch 17321/1000000, d_loss: -303.7814025878906,  g_loss: 40.167930603027344\n",
            "Training epoch 17322/1000000, d_loss: -93.86808013916016,  g_loss: -1.150644302368164\n",
            "Training epoch 17323/1000000, d_loss: -84.05010986328125,  g_loss: 27.909000396728516\n",
            "Training epoch 17324/1000000, d_loss: -328.0221862792969,  g_loss: -87.11187744140625\n",
            "Training epoch 17325/1000000, d_loss: -20.111026763916016,  g_loss: -23.60875129699707\n",
            "Training epoch 17326/1000000, d_loss: -1146.3497314453125,  g_loss: -247.41116333007812\n",
            "Training epoch 17327/1000000, d_loss: 105.71484375,  g_loss: -172.5074462890625\n",
            "Training epoch 17328/1000000, d_loss: 197.74295043945312,  g_loss: -19.832664489746094\n",
            "Training epoch 17329/1000000, d_loss: -83.56471252441406,  g_loss: 111.87358856201172\n",
            "Training epoch 17330/1000000, d_loss: -32.80171585083008,  g_loss: -22.604114532470703\n",
            "Training epoch 17331/1000000, d_loss: -268.9793701171875,  g_loss: -54.810699462890625\n",
            "Training epoch 17332/1000000, d_loss: -191.27859497070312,  g_loss: 11.203088760375977\n",
            "Training epoch 17333/1000000, d_loss: -218.33688354492188,  g_loss: 254.91566467285156\n",
            "Training epoch 17334/1000000, d_loss: -285.8918151855469,  g_loss: -6.835145950317383\n",
            "Training epoch 17335/1000000, d_loss: -247.7064971923828,  g_loss: 288.2732238769531\n",
            "Training epoch 17336/1000000, d_loss: -94.25151062011719,  g_loss: 191.28106689453125\n",
            "Training epoch 17337/1000000, d_loss: -165.932861328125,  g_loss: -69.79048156738281\n",
            "Training epoch 17338/1000000, d_loss: -66.92074584960938,  g_loss: -9.060198783874512\n",
            "Training epoch 17339/1000000, d_loss: -99.28651428222656,  g_loss: 88.20329284667969\n",
            "Training epoch 17340/1000000, d_loss: -160.9246063232422,  g_loss: 63.22382354736328\n",
            "Training epoch 17341/1000000, d_loss: -240.0562744140625,  g_loss: -43.677391052246094\n",
            "Training epoch 17342/1000000, d_loss: -217.56622314453125,  g_loss: 73.6861572265625\n",
            "Training epoch 17343/1000000, d_loss: -290.4646911621094,  g_loss: 307.7757873535156\n",
            "Training epoch 17344/1000000, d_loss: 60.95333480834961,  g_loss: 9.846623420715332\n",
            "Training epoch 17345/1000000, d_loss: 259.25048828125,  g_loss: 25.062610626220703\n",
            "Training epoch 17346/1000000, d_loss: -68.42001342773438,  g_loss: 18.94164276123047\n",
            "Training epoch 17347/1000000, d_loss: -98.80656433105469,  g_loss: 18.690357208251953\n",
            "Training epoch 17348/1000000, d_loss: -1.20635986328125,  g_loss: 133.17330932617188\n",
            "Training epoch 17349/1000000, d_loss: -148.749267578125,  g_loss: 72.5303726196289\n",
            "Training epoch 17350/1000000, d_loss: 261.76763916015625,  g_loss: 76.05604553222656\n",
            "Training epoch 17351/1000000, d_loss: -97.08403015136719,  g_loss: 46.41604995727539\n",
            "Training epoch 17352/1000000, d_loss: -126.09867095947266,  g_loss: 42.774742126464844\n",
            "Training epoch 17353/1000000, d_loss: -329.0151062011719,  g_loss: 65.88876342773438\n",
            "Training epoch 17354/1000000, d_loss: -61.43577575683594,  g_loss: 40.03211212158203\n",
            "Training epoch 17355/1000000, d_loss: -322.5928955078125,  g_loss: -81.89047241210938\n",
            "Training epoch 17356/1000000, d_loss: -72.53448486328125,  g_loss: 96.05166625976562\n",
            "Training epoch 17357/1000000, d_loss: -72.12320709228516,  g_loss: 65.47215270996094\n",
            "Training epoch 17358/1000000, d_loss: -66.90335083007812,  g_loss: 59.29857635498047\n",
            "Training epoch 17359/1000000, d_loss: -151.68682861328125,  g_loss: 55.3216552734375\n",
            "Training epoch 17360/1000000, d_loss: -102.37535858154297,  g_loss: 13.76873779296875\n",
            "Training epoch 17361/1000000, d_loss: -84.41567993164062,  g_loss: 6.112358093261719\n",
            "Training epoch 17362/1000000, d_loss: -146.51402282714844,  g_loss: 63.09577941894531\n",
            "Training epoch 17363/1000000, d_loss: -350.4261779785156,  g_loss: -44.18886947631836\n",
            "Training epoch 17364/1000000, d_loss: 1890.10302734375,  g_loss: 46.065406799316406\n",
            "Training epoch 17365/1000000, d_loss: -58.755340576171875,  g_loss: 52.24641036987305\n",
            "Training epoch 17366/1000000, d_loss: 62.59222412109375,  g_loss: 55.753684997558594\n",
            "Training epoch 17367/1000000, d_loss: -52.8050651550293,  g_loss: 47.56157684326172\n",
            "Training epoch 17368/1000000, d_loss: -37.967132568359375,  g_loss: 34.149566650390625\n",
            "Training epoch 17369/1000000, d_loss: -175.97169494628906,  g_loss: 55.893943786621094\n",
            "Training epoch 17370/1000000, d_loss: -222.36900329589844,  g_loss: 20.449195861816406\n",
            "Training epoch 17371/1000000, d_loss: -45.31349563598633,  g_loss: 53.284027099609375\n",
            "Training epoch 17372/1000000, d_loss: -159.1639404296875,  g_loss: -16.18182373046875\n",
            "Training epoch 17373/1000000, d_loss: -164.921142578125,  g_loss: 9.881244659423828\n",
            "Training epoch 17374/1000000, d_loss: -71.05875396728516,  g_loss: 42.16893768310547\n",
            "Training epoch 17375/1000000, d_loss: -70.95806884765625,  g_loss: 85.21441650390625\n",
            "Training epoch 17376/1000000, d_loss: -2.6604690551757812,  g_loss: 76.62032318115234\n",
            "Training epoch 17377/1000000, d_loss: -137.15365600585938,  g_loss: 60.85591506958008\n",
            "Training epoch 17378/1000000, d_loss: -63.498146057128906,  g_loss: 44.72633361816406\n",
            "Training epoch 17379/1000000, d_loss: -146.7436981201172,  g_loss: 2.1625304222106934\n",
            "Training epoch 17380/1000000, d_loss: -17.532649993896484,  g_loss: 66.21954345703125\n",
            "Training epoch 17381/1000000, d_loss: -66.56415557861328,  g_loss: 66.43647003173828\n",
            "Training epoch 17382/1000000, d_loss: -87.31578063964844,  g_loss: 50.1192626953125\n",
            "Training epoch 17383/1000000, d_loss: -46.995304107666016,  g_loss: 12.09299087524414\n",
            "Training epoch 17384/1000000, d_loss: -238.47012329101562,  g_loss: -46.9537239074707\n",
            "Training epoch 17385/1000000, d_loss: -107.96473693847656,  g_loss: 0.2006845474243164\n",
            "Training epoch 17386/1000000, d_loss: -1037.350341796875,  g_loss: -86.56143951416016\n",
            "Training epoch 17387/1000000, d_loss: -45.61629867553711,  g_loss: -37.930206298828125\n",
            "Training epoch 17388/1000000, d_loss: -399.6435546875,  g_loss: -40.75426483154297\n",
            "Training epoch 17389/1000000, d_loss: 46494.58984375,  g_loss: 53.310546875\n",
            "Training epoch 17390/1000000, d_loss: -698.4905395507812,  g_loss: -46.00273895263672\n",
            "Training epoch 17391/1000000, d_loss: 577.3828125,  g_loss: -52.255279541015625\n",
            "Training epoch 17392/1000000, d_loss: 29.853469848632812,  g_loss: -42.39076232910156\n",
            "Training epoch 17393/1000000, d_loss: 138.4071502685547,  g_loss: -95.18351745605469\n",
            "Training epoch 17394/1000000, d_loss: -590.3421630859375,  g_loss: 80.61347198486328\n",
            "Training epoch 17395/1000000, d_loss: 4.033180236816406,  g_loss: -20.92495346069336\n",
            "Training epoch 17396/1000000, d_loss: -242.1667938232422,  g_loss: 107.54901885986328\n",
            "Training epoch 17397/1000000, d_loss: -255.49203491210938,  g_loss: 40.36277389526367\n",
            "Training epoch 17398/1000000, d_loss: -158.26930236816406,  g_loss: 25.15822982788086\n",
            "Training epoch 17399/1000000, d_loss: -181.68675231933594,  g_loss: 40.02305603027344\n",
            "Training epoch 17400/1000000, d_loss: -312.0864562988281,  g_loss: -71.06546783447266\n",
            "Training epoch 17401/1000000, d_loss: -190.5833282470703,  g_loss: 118.70799255371094\n",
            "Training epoch 17402/1000000, d_loss: -414.91802978515625,  g_loss: -180.45755004882812\n",
            "Training epoch 17403/1000000, d_loss: -295.7035217285156,  g_loss: -140.20730590820312\n",
            "Training epoch 17404/1000000, d_loss: -27.02283477783203,  g_loss: -111.89727783203125\n",
            "Training epoch 17405/1000000, d_loss: -330.4766845703125,  g_loss: -244.96212768554688\n",
            "Training epoch 17406/1000000, d_loss: -38.587127685546875,  g_loss: -157.11126708984375\n",
            "Training epoch 17407/1000000, d_loss: -45.16105651855469,  g_loss: -83.63613891601562\n",
            "Training epoch 17408/1000000, d_loss: -72.50686645507812,  g_loss: -76.61436462402344\n",
            "Training epoch 17409/1000000, d_loss: -227.23284912109375,  g_loss: -14.493240356445312\n",
            "Training epoch 17410/1000000, d_loss: -412.48895263671875,  g_loss: -55.593814849853516\n",
            "Training epoch 17411/1000000, d_loss: -45.70851135253906,  g_loss: 14.880199432373047\n",
            "Training epoch 17412/1000000, d_loss: -136.07736206054688,  g_loss: -138.18539428710938\n",
            "Training epoch 17413/1000000, d_loss: -107.65644836425781,  g_loss: -23.4325008392334\n",
            "Training epoch 17414/1000000, d_loss: -180.72988891601562,  g_loss: -73.66654968261719\n",
            "Training epoch 17415/1000000, d_loss: -90.50813293457031,  g_loss: -38.328086853027344\n",
            "Training epoch 17416/1000000, d_loss: -174.317138671875,  g_loss: 249.0800323486328\n",
            "Training epoch 17417/1000000, d_loss: -477.91851806640625,  g_loss: -8.958911895751953\n",
            "Training epoch 17418/1000000, d_loss: -29.856712341308594,  g_loss: 17.195720672607422\n",
            "Training epoch 17419/1000000, d_loss: -1453.577392578125,  g_loss: -364.62030029296875\n",
            "Training epoch 17420/1000000, d_loss: 68.30936431884766,  g_loss: 38.26106262207031\n",
            "Training epoch 17421/1000000, d_loss: -51.26771545410156,  g_loss: 62.431541442871094\n",
            "Training epoch 17422/1000000, d_loss: -52.694217681884766,  g_loss: 15.735447883605957\n",
            "Training epoch 17423/1000000, d_loss: -27.533973693847656,  g_loss: 34.18359375\n",
            "Training epoch 17424/1000000, d_loss: -2.43182373046875,  g_loss: 120.69367980957031\n",
            "Training epoch 17425/1000000, d_loss: -350.3901672363281,  g_loss: 624.8919677734375\n",
            "Training epoch 17426/1000000, d_loss: -258.62139892578125,  g_loss: 313.0394592285156\n",
            "Training epoch 17427/1000000, d_loss: -95.82095336914062,  g_loss: 103.19825744628906\n",
            "Training epoch 17428/1000000, d_loss: -136.22494506835938,  g_loss: 35.10591125488281\n",
            "Training epoch 17429/1000000, d_loss: -536.6964721679688,  g_loss: 16.40819549560547\n",
            "Training epoch 17430/1000000, d_loss: -862.3084716796875,  g_loss: -4.371527671813965\n",
            "Training epoch 17431/1000000, d_loss: -47.03418731689453,  g_loss: -14.886249542236328\n",
            "Training epoch 17432/1000000, d_loss: 22.13299560546875,  g_loss: 128.10545349121094\n",
            "Training epoch 17433/1000000, d_loss: 1.4513893127441406,  g_loss: 75.43446350097656\n",
            "Training epoch 17434/1000000, d_loss: -76.49146270751953,  g_loss: 116.53823852539062\n",
            "Training epoch 17435/1000000, d_loss: -121.5816879272461,  g_loss: 105.55078887939453\n",
            "Training epoch 17436/1000000, d_loss: -95.1492691040039,  g_loss: 63.770999908447266\n",
            "Training epoch 17437/1000000, d_loss: -86.57028198242188,  g_loss: 105.20278930664062\n",
            "Training epoch 17438/1000000, d_loss: -139.12574768066406,  g_loss: 76.75529479980469\n",
            "Training epoch 17439/1000000, d_loss: -145.9106903076172,  g_loss: 96.01775360107422\n",
            "Training epoch 17440/1000000, d_loss: 35.170040130615234,  g_loss: 4.494388580322266\n",
            "Training epoch 17441/1000000, d_loss: -78.99773406982422,  g_loss: 8.871438026428223\n",
            "Training epoch 17442/1000000, d_loss: -119.9981460571289,  g_loss: 37.979896545410156\n",
            "Training epoch 17443/1000000, d_loss: -117.54827117919922,  g_loss: 5.088258743286133\n",
            "Training epoch 17444/1000000, d_loss: -1914.0367431640625,  g_loss: -245.589111328125\n",
            "Training epoch 17445/1000000, d_loss: -110.01760864257812,  g_loss: -16.176055908203125\n",
            "Training epoch 17446/1000000, d_loss: -18.064128875732422,  g_loss: -40.40884780883789\n",
            "Training epoch 17447/1000000, d_loss: -126.08988952636719,  g_loss: -8.896984100341797\n",
            "Training epoch 17448/1000000, d_loss: -155.9154815673828,  g_loss: -15.367762565612793\n",
            "Training epoch 17449/1000000, d_loss: -54.022186279296875,  g_loss: -24.35857582092285\n",
            "Training epoch 17450/1000000, d_loss: -83.50950622558594,  g_loss: 57.8470458984375\n",
            "Training epoch 17451/1000000, d_loss: -8.38232421875,  g_loss: 42.26030731201172\n",
            "Training epoch 17452/1000000, d_loss: -230.16665649414062,  g_loss: 90.49784851074219\n",
            "Training epoch 17453/1000000, d_loss: -130.40267944335938,  g_loss: 234.6916961669922\n",
            "Training epoch 17454/1000000, d_loss: -6.927886962890625,  g_loss: 14.88808822631836\n",
            "Training epoch 17455/1000000, d_loss: -114.67133331298828,  g_loss: -14.851058959960938\n",
            "Training epoch 17456/1000000, d_loss: -120.36549377441406,  g_loss: 3.165316581726074\n",
            "Training epoch 17457/1000000, d_loss: -332.78436279296875,  g_loss: -68.85855102539062\n",
            "Training epoch 17458/1000000, d_loss: -1308.45556640625,  g_loss: -180.64117431640625\n",
            "Training epoch 17459/1000000, d_loss: 35.059574127197266,  g_loss: -75.16439819335938\n",
            "Training epoch 17460/1000000, d_loss: 2864.4072265625,  g_loss: 48.11289978027344\n",
            "Training epoch 17461/1000000, d_loss: -16.23199462890625,  g_loss: -14.721343994140625\n",
            "Training epoch 17462/1000000, d_loss: 128.48239135742188,  g_loss: 31.700326919555664\n",
            "Training epoch 17463/1000000, d_loss: 3.9386215209960938,  g_loss: 26.588726043701172\n",
            "Training epoch 17464/1000000, d_loss: -0.7002182006835938,  g_loss: 36.52263641357422\n",
            "Training epoch 17465/1000000, d_loss: -35.17913055419922,  g_loss: 19.80195426940918\n",
            "Training epoch 17466/1000000, d_loss: -11.9249267578125,  g_loss: 13.013362884521484\n",
            "Training epoch 17467/1000000, d_loss: 0.5822715759277344,  g_loss: -1.4254930019378662\n",
            "Training epoch 17468/1000000, d_loss: 4.127143859863281,  g_loss: 21.923301696777344\n",
            "Training epoch 17469/1000000, d_loss: -291.7647705078125,  g_loss: 6.529612064361572\n",
            "Training epoch 17470/1000000, d_loss: -64.02368927001953,  g_loss: 11.166190147399902\n",
            "Training epoch 17471/1000000, d_loss: -279.54150390625,  g_loss: 36.74542999267578\n",
            "Training epoch 17472/1000000, d_loss: -109.67696380615234,  g_loss: -0.9692392349243164\n",
            "Training epoch 17473/1000000, d_loss: -101.49210357666016,  g_loss: -22.685230255126953\n",
            "Training epoch 17474/1000000, d_loss: -150.85372924804688,  g_loss: 26.471696853637695\n",
            "Training epoch 17475/1000000, d_loss: -102.43775177001953,  g_loss: -10.580718994140625\n",
            "Training epoch 17476/1000000, d_loss: -99.23474884033203,  g_loss: 65.04517364501953\n",
            "Training epoch 17477/1000000, d_loss: -310.4800720214844,  g_loss: -99.8523178100586\n",
            "Training epoch 17478/1000000, d_loss: -159.3372039794922,  g_loss: -47.397701263427734\n",
            "Training epoch 17479/1000000, d_loss: -248.71319580078125,  g_loss: -14.033367156982422\n",
            "Training epoch 17480/1000000, d_loss: -64.27153015136719,  g_loss: 8.300389289855957\n",
            "Training epoch 17481/1000000, d_loss: -115.22802734375,  g_loss: 157.31822204589844\n",
            "Training epoch 17482/1000000, d_loss: 24.350677490234375,  g_loss: -28.924068450927734\n",
            "Training epoch 17483/1000000, d_loss: -21.272781372070312,  g_loss: 5.192662239074707\n",
            "Training epoch 17484/1000000, d_loss: -401.17242431640625,  g_loss: -49.18642807006836\n",
            "Training epoch 17485/1000000, d_loss: -65.60943603515625,  g_loss: -22.973793029785156\n",
            "Training epoch 17486/1000000, d_loss: -8.23675537109375,  g_loss: 26.362918853759766\n",
            "Training epoch 17487/1000000, d_loss: -115.97990417480469,  g_loss: 46.284881591796875\n",
            "Training epoch 17488/1000000, d_loss: -37.936439514160156,  g_loss: -17.568307876586914\n",
            "Training epoch 17489/1000000, d_loss: -817.1118774414062,  g_loss: -85.11572265625\n",
            "Training epoch 17490/1000000, d_loss: -159.35305786132812,  g_loss: -29.372928619384766\n",
            "Training epoch 17491/1000000, d_loss: -92.97578430175781,  g_loss: 12.23440170288086\n",
            "Training epoch 17492/1000000, d_loss: 33.33631896972656,  g_loss: -44.29164123535156\n",
            "Training epoch 17493/1000000, d_loss: -20.75274658203125,  g_loss: 69.93485260009766\n",
            "Training epoch 17494/1000000, d_loss: -47.409969329833984,  g_loss: 19.63177490234375\n",
            "Training epoch 17495/1000000, d_loss: -145.48143005371094,  g_loss: 138.1276092529297\n",
            "Training epoch 17496/1000000, d_loss: -192.65682983398438,  g_loss: -10.029281616210938\n",
            "Training epoch 17497/1000000, d_loss: -102.59144592285156,  g_loss: 118.52474975585938\n",
            "Training epoch 17498/1000000, d_loss: -119.74012756347656,  g_loss: 178.4726104736328\n",
            "Training epoch 17499/1000000, d_loss: -84.58889770507812,  g_loss: 89.6183090209961\n",
            "Training epoch 17500/1000000, d_loss: -71.46018981933594,  g_loss: 32.71092987060547\n",
            "Training epoch 17501/1000000, d_loss: -261.9627380371094,  g_loss: 45.03321075439453\n",
            "Training epoch 17502/1000000, d_loss: -69.8758773803711,  g_loss: 40.05116271972656\n",
            "Training epoch 17503/1000000, d_loss: -81.74462127685547,  g_loss: 24.949892044067383\n",
            "Training epoch 17504/1000000, d_loss: -207.06576538085938,  g_loss: 2.572080612182617\n",
            "Training epoch 17505/1000000, d_loss: -105.88516998291016,  g_loss: 7.1097092628479\n",
            "Training epoch 17506/1000000, d_loss: -190.63153076171875,  g_loss: 58.06763458251953\n",
            "Training epoch 17507/1000000, d_loss: -145.73147583007812,  g_loss: 50.324501037597656\n",
            "Training epoch 17508/1000000, d_loss: -165.53909301757812,  g_loss: 3.7772560119628906\n",
            "Training epoch 17509/1000000, d_loss: -66.07075500488281,  g_loss: 40.14599609375\n",
            "Training epoch 17510/1000000, d_loss: -59.139976501464844,  g_loss: 10.545003890991211\n",
            "Training epoch 17511/1000000, d_loss: -229.57839965820312,  g_loss: -22.8214111328125\n",
            "Training epoch 17512/1000000, d_loss: -168.6143341064453,  g_loss: 66.95960235595703\n",
            "Training epoch 17513/1000000, d_loss: -99.16154479980469,  g_loss: -41.14802932739258\n",
            "Training epoch 17514/1000000, d_loss: -86.21736145019531,  g_loss: 84.28527069091797\n",
            "Training epoch 17515/1000000, d_loss: -418.09014892578125,  g_loss: -64.05879211425781\n",
            "Training epoch 17516/1000000, d_loss: -108.77029418945312,  g_loss: -12.586445808410645\n",
            "Training epoch 17517/1000000, d_loss: 13.822139739990234,  g_loss: 1.7414641380310059\n",
            "Training epoch 17518/1000000, d_loss: -47.041019439697266,  g_loss: 13.591653823852539\n",
            "Training epoch 17519/1000000, d_loss: -407.4027404785156,  g_loss: -101.70015716552734\n",
            "Training epoch 17520/1000000, d_loss: 89.76806640625,  g_loss: -33.837196350097656\n",
            "Training epoch 17521/1000000, d_loss: -54.26457977294922,  g_loss: -27.39284896850586\n",
            "Training epoch 17522/1000000, d_loss: -62.57265090942383,  g_loss: 1.9188404083251953\n",
            "Training epoch 17523/1000000, d_loss: -185.33897399902344,  g_loss: -46.559051513671875\n",
            "Training epoch 17524/1000000, d_loss: 35.10614013671875,  g_loss: -6.328276634216309\n",
            "Training epoch 17525/1000000, d_loss: -112.6532974243164,  g_loss: 0.5907986164093018\n",
            "Training epoch 17526/1000000, d_loss: -331.0002746582031,  g_loss: -138.93043518066406\n",
            "Training epoch 17527/1000000, d_loss: -190.38766479492188,  g_loss: -74.78213500976562\n",
            "Training epoch 17528/1000000, d_loss: -113.9696044921875,  g_loss: -5.001209259033203\n",
            "Training epoch 17529/1000000, d_loss: -44.88766860961914,  g_loss: 83.41667938232422\n",
            "Training epoch 17530/1000000, d_loss: -27.13021469116211,  g_loss: 83.59803009033203\n",
            "Training epoch 17531/1000000, d_loss: -217.09637451171875,  g_loss: 22.578248977661133\n",
            "Training epoch 17532/1000000, d_loss: 124.142822265625,  g_loss: 15.189386367797852\n",
            "Training epoch 17533/1000000, d_loss: 689.80029296875,  g_loss: 0.47845083475112915\n",
            "Training epoch 17534/1000000, d_loss: -13.163190841674805,  g_loss: -1.336815357208252\n",
            "Training epoch 17535/1000000, d_loss: -55.26026916503906,  g_loss: 18.184803009033203\n",
            "Training epoch 17536/1000000, d_loss: -212.6165771484375,  g_loss: 20.371143341064453\n",
            "Training epoch 17537/1000000, d_loss: -713.5066528320312,  g_loss: -28.82712745666504\n",
            "Training epoch 17538/1000000, d_loss: -55.74356460571289,  g_loss: -64.06023406982422\n",
            "Training epoch 17539/1000000, d_loss: -207.57992553710938,  g_loss: -239.85423278808594\n",
            "Training epoch 17540/1000000, d_loss: -24.03049087524414,  g_loss: -58.134090423583984\n",
            "Training epoch 17541/1000000, d_loss: -109.98081970214844,  g_loss: 15.465938568115234\n",
            "Training epoch 17542/1000000, d_loss: -85.55362701416016,  g_loss: 9.810198783874512\n",
            "Training epoch 17543/1000000, d_loss: -174.59512329101562,  g_loss: -48.25982666015625\n",
            "Training epoch 17544/1000000, d_loss: -182.47447204589844,  g_loss: 40.674598693847656\n",
            "Training epoch 17545/1000000, d_loss: -233.67079162597656,  g_loss: 51.13396453857422\n",
            "Training epoch 17546/1000000, d_loss: -734.4393920898438,  g_loss: -181.34727478027344\n",
            "Training epoch 17547/1000000, d_loss: -56.29130554199219,  g_loss: -12.104039192199707\n",
            "Training epoch 17548/1000000, d_loss: -22.960205078125,  g_loss: -17.130342483520508\n",
            "Training epoch 17549/1000000, d_loss: -375.89703369140625,  g_loss: -70.89543914794922\n",
            "Training epoch 17550/1000000, d_loss: 168.9300994873047,  g_loss: 13.975976943969727\n",
            "Training epoch 17551/1000000, d_loss: -59.413856506347656,  g_loss: 33.18900680541992\n",
            "Training epoch 17552/1000000, d_loss: -175.60450744628906,  g_loss: 31.04218864440918\n",
            "Training epoch 17553/1000000, d_loss: -94.6971664428711,  g_loss: -1.5796470642089844\n",
            "Training epoch 17554/1000000, d_loss: -125.67180633544922,  g_loss: 41.05392837524414\n",
            "Training epoch 17555/1000000, d_loss: -70.31016540527344,  g_loss: 3.4851889610290527\n",
            "Training epoch 17556/1000000, d_loss: -415.53546142578125,  g_loss: -32.75996398925781\n",
            "Training epoch 17557/1000000, d_loss: -89.08352661132812,  g_loss: 56.31304168701172\n",
            "Training epoch 17558/1000000, d_loss: -139.88186645507812,  g_loss: 47.4034309387207\n",
            "Training epoch 17559/1000000, d_loss: -90.92584228515625,  g_loss: 17.86346435546875\n",
            "Training epoch 17560/1000000, d_loss: -1731.2603759765625,  g_loss: -481.49285888671875\n",
            "Training epoch 17561/1000000, d_loss: 219.25537109375,  g_loss: -115.08495330810547\n",
            "Training epoch 17562/1000000, d_loss: 125.27102661132812,  g_loss: -130.3863525390625\n",
            "Training epoch 17563/1000000, d_loss: 233.22128295898438,  g_loss: -185.1887664794922\n",
            "Training epoch 17564/1000000, d_loss: -152.98562622070312,  g_loss: 22.450748443603516\n",
            "Training epoch 17565/1000000, d_loss: -28.697799682617188,  g_loss: -71.88628387451172\n",
            "Training epoch 17566/1000000, d_loss: 61.00001525878906,  g_loss: 22.284238815307617\n",
            "Training epoch 17567/1000000, d_loss: -126.09501647949219,  g_loss: 45.16426467895508\n",
            "Training epoch 17568/1000000, d_loss: -105.25552368164062,  g_loss: 232.15496826171875\n",
            "Training epoch 17569/1000000, d_loss: -63.131614685058594,  g_loss: 37.981353759765625\n",
            "Training epoch 17570/1000000, d_loss: -238.08599853515625,  g_loss: -20.933101654052734\n",
            "Training epoch 17571/1000000, d_loss: -13.22174072265625,  g_loss: -1.6251163482666016\n",
            "Training epoch 17572/1000000, d_loss: -54.880531311035156,  g_loss: 105.85271453857422\n",
            "Training epoch 17573/1000000, d_loss: -341.5229187011719,  g_loss: 340.9158935546875\n",
            "Training epoch 17574/1000000, d_loss: -106.92859649658203,  g_loss: 7.895412445068359\n",
            "Training epoch 17575/1000000, d_loss: -432.2104797363281,  g_loss: 194.63479614257812\n",
            "Training epoch 17576/1000000, d_loss: -249.97531127929688,  g_loss: 15.97939682006836\n",
            "Training epoch 17577/1000000, d_loss: -239.98361206054688,  g_loss: -80.16331481933594\n",
            "Training epoch 17578/1000000, d_loss: -68.79791259765625,  g_loss: -16.49736976623535\n",
            "Training epoch 17579/1000000, d_loss: -131.43443298339844,  g_loss: -84.19902038574219\n",
            "Training epoch 17580/1000000, d_loss: -157.39080810546875,  g_loss: -25.664417266845703\n",
            "Training epoch 17581/1000000, d_loss: -216.67742919921875,  g_loss: 201.34213256835938\n",
            "Training epoch 17582/1000000, d_loss: 2966.05029296875,  g_loss: 33.85834884643555\n",
            "Training epoch 17583/1000000, d_loss: -162.86724853515625,  g_loss: 242.07525634765625\n",
            "Training epoch 17584/1000000, d_loss: 190.5614471435547,  g_loss: 98.53457641601562\n",
            "Training epoch 17585/1000000, d_loss: -141.98118591308594,  g_loss: 164.53060913085938\n",
            "Training epoch 17586/1000000, d_loss: -223.00143432617188,  g_loss: 84.78594970703125\n",
            "Training epoch 17587/1000000, d_loss: -117.39493560791016,  g_loss: 33.23151779174805\n",
            "Training epoch 17588/1000000, d_loss: -171.5026397705078,  g_loss: 19.0892276763916\n",
            "Training epoch 17589/1000000, d_loss: -216.22708129882812,  g_loss: -0.05086994171142578\n",
            "Training epoch 17590/1000000, d_loss: -1073.70654296875,  g_loss: -422.03961181640625\n",
            "Training epoch 17591/1000000, d_loss: -587.6395263671875,  g_loss: -125.61724853515625\n",
            "Training epoch 17592/1000000, d_loss: 200.3524169921875,  g_loss: 67.9544448852539\n",
            "Training epoch 17593/1000000, d_loss: -265.89508056640625,  g_loss: 403.0972900390625\n",
            "Training epoch 17594/1000000, d_loss: -2.415050506591797,  g_loss: -17.257673263549805\n",
            "Training epoch 17595/1000000, d_loss: -497.4836120605469,  g_loss: -192.54713439941406\n",
            "Training epoch 17596/1000000, d_loss: -13.168136596679688,  g_loss: 40.25041961669922\n",
            "Training epoch 17597/1000000, d_loss: -136.92735290527344,  g_loss: 93.98043060302734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBA_0Xa78i2V"
      },
      "source": [
        "##Generating random samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYy9ycd7oGGD"
      },
      "source": [
        "def generate_samples(G_path, out_path, num=100, latent_dim=32):\n",
        "  if not os.path.exists(out_path):\n",
        "        os.makedirs(out_path)\n",
        "  G = tf.keras.models.load_model(G_path)\n",
        "  z = np.random.normal(0.0, 1.0, size=[num, latent_dim])\n",
        "  voxel_models = G.predict(z, verbose=1)\n",
        "  voxel_models = voxel_models.reshape(voxel_models.shape[:-1])\n",
        "  voxel_models = np.rint(voxel_models)\n",
        "  for i, model in enumerate(voxel_models):\n",
        "    if(model.sum()<2500): continue\n",
        "    save_voxel(model, f'{out_path}/sample_{i}.obj')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNCmjTBgLJmP",
        "outputId": "facc201a-7861-42aa-880c-5104664ea5c1"
      },
      "source": [
        "generate_samples('/content/gdrive/MyDrive/AI_Artathon/voxel_model/model_checkpoints/generator_epoch_17001', '/content/gdrive/MyDrive/AI_Artathon/voxel_model/out/17002', num=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 9s 3s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding exterior voxels: 100%|██████████| 64/64 [00:00<00:00, 294.52it/s]\n",
            "Meshing: 100%|██████████| 2613/2613 [00:00<00:00, 3979.80it/s]\n",
            "Finding exterior voxels: 100%|██████████| 23/23 [00:00<00:00, 450.20it/s]\n",
            "Meshing: 100%|██████████| 3828/3828 [00:00<00:00, 6978.13it/s]\n",
            "Finding exterior voxels: 100%|██████████| 16/16 [00:00<00:00, 690.02it/s]\n",
            "Meshing: 100%|██████████| 2402/2402 [00:00<00:00, 7189.42it/s]\n",
            "Finding exterior voxels: 100%|██████████| 19/19 [00:00<00:00, 546.23it/s]\n",
            "Meshing: 100%|██████████| 3453/3453 [00:00<00:00, 7183.31it/s]\n",
            "Finding exterior voxels: 100%|██████████| 17/17 [00:00<00:00, 597.64it/s]\n",
            "Meshing: 100%|██████████| 2594/2594 [00:00<00:00, 4476.43it/s]\n",
            "Finding exterior voxels: 100%|██████████| 23/23 [00:00<00:00, 495.65it/s]\n",
            "Meshing: 100%|██████████| 2439/2439 [00:00<00:00, 6618.33it/s]\n",
            "Finding exterior voxels: 100%|██████████| 26/26 [00:00<00:00, 424.22it/s]\n",
            "Meshing: 100%|██████████| 2853/2853 [00:00<00:00, 6854.58it/s]\n",
            "Finding exterior voxels: 100%|██████████| 45/45 [00:00<00:00, 268.90it/s]\n",
            "Meshing: 100%|██████████| 11751/11751 [00:02<00:00, 4212.97it/s]\n",
            "Finding exterior voxels: 100%|██████████| 26/26 [00:00<00:00, 456.88it/s]\n",
            "Meshing: 100%|██████████| 3169/3169 [00:00<00:00, 4670.40it/s]\n",
            "Finding exterior voxels: 100%|██████████| 27/27 [00:00<00:00, 390.78it/s]\n",
            "Meshing: 100%|██████████| 6349/6349 [00:00<00:00, 6478.75it/s]\n",
            "Finding exterior voxels: 100%|██████████| 19/19 [00:00<00:00, 656.94it/s]\n",
            "Meshing: 100%|██████████| 3727/3727 [00:00<00:00, 7185.11it/s]\n",
            "Finding exterior voxels: 100%|██████████| 23/23 [00:00<00:00, 246.65it/s]\n",
            "Meshing: 100%|██████████| 4833/4833 [00:00<00:00, 5025.52it/s]\n",
            "Finding exterior voxels: 100%|██████████| 20/20 [00:00<00:00, 488.11it/s]\n",
            "Meshing: 100%|██████████| 4303/4303 [00:00<00:00, 7142.16it/s]\n",
            "Finding exterior voxels: 100%|██████████| 24/24 [00:00<00:00, 449.21it/s]\n",
            "Meshing: 100%|██████████| 10906/10906 [00:01<00:00, 6143.54it/s]\n",
            "Finding exterior voxels: 100%|██████████| 34/34 [00:00<00:00, 326.53it/s]\n",
            "Meshing: 100%|██████████| 3178/3178 [00:00<00:00, 4936.63it/s]\n",
            "Finding exterior voxels: 100%|██████████| 16/16 [00:00<00:00, 689.00it/s]\n",
            "Meshing: 100%|██████████| 2209/2209 [00:00<00:00, 7441.49it/s]\n",
            "Finding exterior voxels: 100%|██████████| 45/45 [00:00<00:00, 159.39it/s]\n",
            "Meshing: 100%|██████████| 4771/4771 [00:01<00:00, 3485.10it/s]\n",
            "Finding exterior voxels: 100%|██████████| 63/63 [00:00<00:00, 176.70it/s]\n",
            "Meshing: 100%|██████████| 14491/14491 [00:04<00:00, 3163.66it/s]\n",
            "Finding exterior voxels: 100%|██████████| 64/64 [00:00<00:00, 297.37it/s]\n",
            "Meshing: 100%|██████████| 26446/26446 [00:06<00:00, 3845.03it/s]\n",
            "Finding exterior voxels: 100%|██████████| 23/23 [00:00<00:00, 431.19it/s]\n",
            "Meshing: 100%|██████████| 2557/2557 [00:00<00:00, 6059.56it/s]\n",
            "Finding exterior voxels: 100%|██████████| 24/24 [00:00<00:00, 470.55it/s]\n",
            "Meshing: 100%|██████████| 3308/3308 [00:00<00:00, 6347.69it/s]\n",
            "Finding exterior voxels: 100%|██████████| 31/31 [00:00<00:00, 339.72it/s]\n",
            "Meshing: 100%|██████████| 8510/8510 [00:01<00:00, 5007.89it/s]\n",
            "Finding exterior voxels: 100%|██████████| 47/47 [00:00<00:00, 190.11it/s]\n",
            "Meshing: 100%|██████████| 13651/13651 [00:03<00:00, 3478.88it/s]\n",
            "Finding exterior voxels: 100%|██████████| 25/25 [00:00<00:00, 87.93it/s]\n",
            "Meshing: 100%|██████████| 8213/8213 [00:01<00:00, 6302.05it/s]\n",
            "Finding exterior voxels: 100%|██████████| 20/20 [00:00<00:00, 510.40it/s]\n",
            "Meshing: 100%|██████████| 5093/5093 [00:00<00:00, 7048.35it/s]\n",
            "Finding exterior voxels: 100%|██████████| 25/25 [00:00<00:00, 410.81it/s]\n",
            "Meshing: 100%|██████████| 3077/3077 [00:00<00:00, 6605.06it/s]\n",
            "Finding exterior voxels: 100%|██████████| 16/16 [00:00<00:00, 558.78it/s]\n",
            "Meshing: 100%|██████████| 3490/3490 [00:00<00:00, 6561.98it/s]\n",
            "Finding exterior voxels: 100%|██████████| 30/30 [00:00<00:00, 377.40it/s]\n",
            "Meshing: 100%|██████████| 3499/3499 [00:00<00:00, 4632.27it/s]\n",
            "Finding exterior voxels: 100%|██████████| 39/39 [00:00<00:00, 237.61it/s]\n",
            "Meshing: 100%|██████████| 2480/2480 [00:00<00:00, 4046.36it/s]\n",
            "Finding exterior voxels: 100%|██████████| 22/22 [00:00<00:00, 477.60it/s]\n",
            "Meshing: 100%|██████████| 3322/3322 [00:00<00:00, 6481.30it/s]\n",
            "Finding exterior voxels: 100%|██████████| 14/14 [00:00<00:00, 715.07it/s]\n",
            "Meshing: 100%|██████████| 2072/2072 [00:00<00:00, 7417.41it/s]\n",
            "Finding exterior voxels: 100%|██████████| 21/21 [00:00<00:00, 538.94it/s]\n",
            "Meshing: 100%|██████████| 3119/3119 [00:00<00:00, 6389.29it/s]\n",
            "Finding exterior voxels: 100%|██████████| 25/25 [00:00<00:00, 325.80it/s]\n",
            "Meshing: 100%|██████████| 3230/3230 [00:00<00:00, 6838.07it/s]\n",
            "Finding exterior voxels: 100%|██████████| 27/27 [00:00<00:00, 344.55it/s]\n",
            "Meshing: 100%|██████████| 3538/3538 [00:00<00:00, 5978.13it/s]\n",
            "Finding exterior voxels: 100%|██████████| 57/57 [00:00<00:00, 415.07it/s]\n",
            "Meshing: 100%|██████████| 12571/12571 [00:03<00:00, 4101.12it/s]\n",
            "Finding exterior voxels: 100%|██████████| 31/31 [00:00<00:00, 296.93it/s]\n",
            "Meshing: 100%|██████████| 2767/2767 [00:00<00:00, 4890.42it/s]\n",
            "Finding exterior voxels: 100%|██████████| 13/13 [00:00<00:00, 675.96it/s]\n",
            "Meshing: 100%|██████████| 1779/1779 [00:00<00:00, 7232.89it/s]\n",
            "Finding exterior voxels: 100%|██████████| 27/27 [00:00<00:00, 300.02it/s]\n",
            "Meshing: 100%|██████████| 2974/2974 [00:00<00:00, 6655.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXkcAVa4BVIJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}